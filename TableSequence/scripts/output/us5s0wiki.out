Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:25, 16.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:05, 15.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:46<01:46, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:11<01:54, 19.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:26<01:29, 17.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:39<01:04, 16.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:55<00:48, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:09<00:30, 15.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:24<00:15, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:40<00:00, 15.55s/it]Generating: 100%|██████████| 10/10 [02:40<00:00, 16.08s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')", "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_5_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.48s/it]Extractor Estimating: 2it [00:17,  7.78s/it]Extractor Estimating: 3it [00:18,  4.52s/it]Extractor Estimating: 4it [00:20,  3.43s/it]Extractor Estimating: 5it [00:20,  2.41s/it]Extractor Estimating: 6it [00:21,  1.79s/it]Extractor Estimating: 7it [00:22,  1.41s/it]Extractor Estimating: 8it [00:22,  1.16s/it]Extractor Estimating: 9it [00:23,  1.01it/s]Extractor Estimating: 10it [00:23,  1.14it/s]Extractor Estimating: 11it [00:24,  1.27it/s]Extractor Estimating: 12it [00:25,  1.35it/s]Extractor Estimating: 13it [00:25,  1.41it/s]Extractor Estimating: 14it [00:26,  1.44it/s]Extractor Estimating: 15it [00:27,  1.48it/s]Extractor Estimating: 16it [00:27,  1.51it/s]Extractor Estimating: 17it [00:28,  1.58it/s]Extractor Estimating: 18it [00:28,  1.58it/s]Extractor Estimating: 19it [00:29,  1.53it/s]Extractor Estimating: 20it [00:30,  1.59it/s]Extractor Estimating: 21it [00:30,  1.56it/s]Extractor Estimating: 22it [00:31,  1.53it/s]Extractor Estimating: 23it [00:32,  1.46it/s]Extractor Estimating: 24it [00:32,  1.49it/s]Extractor Estimating: 25it [00:33,  1.56it/s]Extractor Estimating: 26it [00:34,  1.54it/s]Extractor Estimating: 27it [00:34,  1.52it/s]Extractor Estimating: 28it [00:35,  1.53it/s]Extractor Estimating: 29it [00:36,  1.60it/s]Extractor Estimating: 30it [00:36,  1.62it/s]Extractor Estimating: 31it [00:37,  1.61it/s]Extractor Estimating: 32it [00:37,  1.62it/s]Extractor Estimating: 33it [00:38,  1.64it/s]Extractor Estimating: 34it [00:39,  1.61it/s]Extractor Estimating: 35it [00:39,  1.62it/s]Extractor Estimating: 36it [00:40,  1.63it/s]Extractor Estimating: 37it [00:40,  1.68it/s]Extractor Estimating: 38it [00:41,  1.72it/s]Extractor Estimating: 39it [00:42,  1.67it/s]Extractor Estimating: 40it [00:42,  1.62it/s]Extractor Estimating: 41it [00:43,  1.57it/s]Extractor Estimating: 42it [00:44,  1.54it/s]Extractor Estimating: 43it [00:44,  1.53it/s]Extractor Estimating: 44it [00:45,  1.53it/s]Extractor Estimating: 45it [00:46,  1.54it/s]Extractor Estimating: 46it [00:46,  1.57it/s]Extractor Estimating: 47it [00:47,  1.57it/s]Extractor Estimating: 48it [00:47,  1.56it/s]Extractor Estimating: 49it [00:48,  1.57it/s]Extractor Estimating: 50it [00:49,  1.58it/s]Extractor Estimating: 51it [00:49,  1.57it/s]Extractor Estimating: 52it [00:50,  1.64it/s]Extractor Estimating: 53it [00:50,  1.68it/s]Extractor Estimating: 54it [00:51,  1.56it/s]Extractor Estimating: 55it [00:52,  1.61it/s]Extractor Estimating: 56it [00:52,  1.65it/s]Extractor Estimating: 57it [00:53,  1.71it/s]Extractor Estimating: 58it [00:53,  1.73it/s]Extractor Estimating: 59it [00:54,  1.77it/s]Extractor Estimating: 60it [00:55,  1.75it/s]Extractor Estimating: 61it [00:55,  1.76it/s]Extractor Estimating: 62it [00:56,  1.75it/s]Extractor Estimating: 63it [00:56,  1.70it/s]Extractor Estimating: 64it [00:57,  1.71it/s]Extractor Estimating: 65it [00:57,  1.73it/s]Extractor Estimating: 66it [00:58,  1.76it/s]Extractor Estimating: 67it [00:59,  1.78it/s]Extractor Estimating: 68it [00:59,  1.74it/s]Extractor Estimating: 69it [01:00,  1.71it/s]Extractor Estimating: 70it [01:00,  1.72it/s]Extractor Estimating: 71it [01:01,  1.74it/s]Extractor Estimating: 72it [01:02,  1.74it/s]Extractor Estimating: 73it [01:02,  1.77it/s]Extractor Estimating: 74it [01:03,  1.67it/s]Extractor Estimating: 75it [01:03,  1.75it/s]Extractor Estimating: 76it [01:07,  1.62s/it]Extractor Estimating: 77it [01:08,  1.32s/it]Extractor Estimating: 78it [01:08,  1.09s/it]Extractor Estimating: 79it [01:09,  1.05it/s]Extractor Estimating: 80it [01:10,  1.18it/s]Extractor Estimating: 81it [01:10,  1.29it/s]Extractor Estimating: 82it [01:11,  1.32it/s]Extractor Estimating: 83it [01:12,  1.37it/s]Extractor Estimating: 84it [01:12,  1.41it/s]Extractor Estimating: 85it [01:13,  1.48it/s]Extractor Estimating: 86it [01:14,  1.45it/s]Extractor Estimating: 87it [01:14,  1.51it/s]Extractor Estimating: 88it [01:15,  1.55it/s]Extractor Estimating: 89it [01:15,  1.59it/s]Extractor Estimating: 90it [01:16,  1.57it/s]Extractor Estimating: 91it [01:17,  1.57it/s]Extractor Estimating: 92it [01:17,  1.57it/s]Extractor Estimating: 93it [01:18,  1.58it/s]Extractor Estimating: 94it [01:19,  1.58it/s]Extractor Estimating: 95it [01:19,  1.56it/s]Extractor Estimating: 96it [01:20,  1.51it/s]Extractor Estimating: 97it [01:21,  1.50it/s]Extractor Estimating: 98it [01:21,  1.52it/s]Extractor Estimating: 99it [01:22,  1.53it/s]Extractor Estimating: 100it [01:23,  1.54it/s]Extractor Estimating: 101it [01:23,  1.56it/s]Extractor Estimating: 102it [01:24,  1.58it/s]Extractor Estimating: 103it [01:24,  1.64it/s]Extractor Estimating: 104it [01:25,  1.66it/s]Extractor Estimating: 105it [01:26,  1.72it/s]Extractor Estimating: 106it [01:26,  1.72it/s]Extractor Estimating: 107it [01:27,  1.62it/s]Extractor Estimating: 108it [01:27,  1.66it/s]Extractor Estimating: 109it [01:28,  1.64it/s]Extractor Estimating: 110it [01:29,  1.63it/s]Extractor Estimating: 111it [01:29,  1.63it/s]Extractor Estimating: 112it [01:30,  1.60it/s]Extractor Estimating: 113it [01:30,  1.65it/s]Extractor Estimating: 114it [01:31,  1.70it/s]Extractor Estimating: 115it [01:32,  1.70it/s]Extractor Estimating: 116it [01:32,  1.54it/s]Extractor Estimating: 117it [01:33,  1.54it/s]Extractor Estimating: 118it [01:34,  1.58it/s]Extractor Estimating: 119it [01:34,  1.62it/s]Extractor Estimating: 120it [01:35,  1.64it/s]Extractor Estimating: 121it [01:35,  1.63it/s]Extractor Estimating: 122it [01:36,  1.66it/s]Extractor Estimating: 123it [01:37,  1.71it/s]Extractor Estimating: 124it [01:37,  1.67it/s]Extractor Estimating: 125it [01:38,  1.62it/s]Extractor Estimating: 126it [01:38,  1.64it/s]Extractor Estimating: 127it [01:39,  1.63it/s]Extractor Estimating: 128it [01:40,  1.60it/s]Extractor Estimating: 129it [01:40,  1.62it/s]Extractor Estimating: 130it [01:41,  1.66it/s]Extractor Estimating: 131it [01:42,  1.62it/s]Extractor Estimating: 132it [01:42,  1.62it/s]Extractor Estimating: 133it [01:43,  1.60it/s]Extractor Estimating: 134it [01:43,  1.61it/s]Extractor Estimating: 135it [01:44,  1.61it/s]Extractor Estimating: 136it [01:45,  1.56it/s]Extractor Estimating: 137it [01:45,  1.54it/s]Extractor Estimating: 138it [01:46,  1.51it/s]Extractor Estimating: 139it [01:47,  1.53it/s]Extractor Estimating: 140it [01:47,  1.55it/s]Extractor Estimating: 141it [01:48,  1.56it/s]Extractor Estimating: 142it [01:49,  1.58it/s]Extractor Estimating: 143it [01:49,  1.57it/s]Extractor Estimating: 144it [01:50,  1.58it/s]Extractor Estimating: 145it [01:50,  1.62it/s]Extractor Estimating: 146it [01:51,  1.60it/s]Extractor Estimating: 147it [01:52,  1.62it/s]Extractor Estimating: 148it [01:52,  1.56it/s]Extractor Estimating: 149it [01:53,  1.53it/s]Extractor Estimating: 150it [01:54,  1.57it/s]Extractor Estimating: 151it [01:54,  1.53it/s]Extractor Estimating: 152it [01:55,  1.56it/s]Extractor Estimating: 153it [01:56,  1.57it/s]Extractor Estimating: 154it [01:56,  1.63it/s]Extractor Estimating: 155it [01:57,  1.63it/s]Extractor Estimating: 156it [01:57,  1.55it/s]Extractor Estimating: 157it [01:58,  1.57it/s]Extractor Estimating: 158it [01:59,  1.61it/s]Extractor Estimating: 159it [01:59,  1.64it/s]Extractor Estimating: 160it [02:00,  1.69it/s]Extractor Estimating: 161it [02:00,  1.68it/s]Extractor Estimating: 162it [02:01,  1.61it/s]Extractor Estimating: 163it [02:02,  1.60it/s]Extractor Estimating: 164it [02:02,  1.56it/s]Extractor Estimating: 165it [02:03,  1.59it/s]Extractor Estimating: 166it [02:04,  1.56it/s]Extractor Estimating: 167it [02:04,  1.55it/s]Extractor Estimating: 168it [02:05,  1.62it/s]Extractor Estimating: 169it [02:05,  1.67it/s]Extractor Estimating: 170it [02:06,  1.70it/s]Extractor Estimating: 171it [02:07,  1.72it/s]Extractor Estimating: 172it [02:07,  1.66it/s]Extractor Estimating: 173it [02:08,  1.63it/s]Extractor Estimating: 174it [02:08,  1.63it/s]Extractor Estimating: 175it [02:09,  1.61it/s]Extractor Estimating: 176it [02:10,  1.63it/s]Extractor Estimating: 177it [02:10,  1.65it/s]Extractor Estimating: 178it [02:11,  1.67it/s]Extractor Estimating: 179it [02:11,  1.72it/s]Extractor Estimating: 180it [02:12,  1.71it/s]Extractor Estimating: 181it [02:13,  1.72it/s]Extractor Estimating: 182it [02:13,  1.71it/s]Extractor Estimating: 183it [02:14,  1.66it/s]Extractor Estimating: 184it [02:14,  1.68it/s]Extractor Estimating: 185it [02:15,  1.63it/s]Extractor Estimating: 186it [02:16,  1.62it/s]Extractor Estimating: 187it [02:16,  1.59it/s]Extractor Estimating: 188it [02:17,  1.63it/s]Extractor Estimating: 189it [02:18,  1.65it/s]Extractor Estimating: 190it [02:18,  1.65it/s]Extractor Estimating: 191it [02:19,  1.65it/s]Extractor Estimating: 192it [02:19,  1.53it/s]Extractor Estimating: 193it [02:20,  1.55it/s]Extractor Estimating: 194it [02:21,  1.53it/s]Extractor Estimating: 195it [02:21,  1.61it/s]Extractor Estimating: 196it [02:22,  1.61it/s]Extractor Estimating: 197it [02:23,  1.65it/s]Extractor Estimating: 198it [02:23,  1.65it/s]Extractor Estimating: 199it [02:24,  1.68it/s]Extractor Estimating: 200it [02:24,  1.66it/s]Extractor Estimating: 201it [02:25,  1.60it/s]Extractor Estimating: 202it [02:26,  1.59it/s]Extractor Estimating: 203it [02:26,  1.58it/s]Extractor Estimating: 204it [02:27,  1.66it/s]Extractor Estimating: 205it [02:27,  1.69it/s]Extractor Estimating: 206it [02:28,  1.68it/s]Extractor Estimating: 207it [02:29,  1.64it/s]Extractor Estimating: 208it [02:29,  1.62it/s]Extractor Estimating: 209it [02:30,  1.63it/s]Extractor Estimating: 210it [02:31,  1.60it/s]Extractor Estimating: 211it [02:31,  1.62it/s]Extractor Estimating: 212it [02:32,  1.54it/s]Extractor Estimating: 213it [02:32,  1.54it/s]Extractor Estimating: 214it [02:33,  1.57it/s]Extractor Estimating: 215it [02:34,  1.59it/s]Extractor Estimating: 216it [02:34,  1.62it/s]Extractor Estimating: 217it [02:35,  1.63it/s]Extractor Estimating: 218it [02:35,  1.66it/s]Extractor Estimating: 219it [02:36,  1.66it/s]Extractor Estimating: 220it [02:37,  1.63it/s]Extractor Estimating: 221it [02:37,  1.61it/s]Extractor Estimating: 222it [02:38,  1.63it/s]Extractor Estimating: 223it [02:39,  1.57it/s]Extractor Estimating: 224it [02:39,  1.54it/s]Extractor Estimating: 225it [02:40,  1.58it/s]Extractor Estimating: 226it [02:41,  1.54it/s]Extractor Estimating: 227it [02:41,  1.58it/s]Extractor Estimating: 228it [02:42,  1.61it/s]Extractor Estimating: 229it [02:42,  1.64it/s]Extractor Estimating: 230it [02:43,  1.60it/s]Extractor Estimating: 231it [02:44,  1.58it/s]Extractor Estimating: 232it [02:44,  1.59it/s]Extractor Estimating: 233it [02:45,  1.62it/s]Extractor Estimating: 234it [02:46,  1.61it/s]Extractor Estimating: 235it [02:46,  1.58it/s]Extractor Estimating: 236it [02:47,  1.63it/s]Extractor Estimating: 237it [02:47,  1.61it/s]Extractor Estimating: 238it [02:48,  1.61it/s]Extractor Estimating: 239it [02:49,  1.63it/s]Extractor Estimating: 240it [02:49,  1.59it/s]Extractor Estimating: 241it [02:50,  1.57it/s]Extractor Estimating: 242it [02:51,  1.61it/s]Extractor Estimating: 243it [02:51,  1.62it/s]Extractor Estimating: 244it [02:52,  1.65it/s]Extractor Estimating: 245it [02:52,  1.62it/s]Extractor Estimating: 246it [02:53,  1.59it/s]Extractor Estimating: 247it [02:54,  1.58it/s]Extractor Estimating: 248it [02:54,  1.63it/s]Extractor Estimating: 249it [02:55,  1.63it/s]Extractor Estimating: 250it [02:56,  1.54it/s]Extractor Estimating: 250it [02:56,  1.42it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4993 mean pseudo reward: 0.9695490483165474
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 26307
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26407, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_5_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26407, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.249, loss:2684.9421
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.964, loss:1896.7154
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.971, loss:1655.9100
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.969, loss:1532.6328
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.976, loss:1461.8705
>> valid entity prec:0.6044, rec:0.5084, f1:0.5523
>> valid relation prec:0.4265, rec:0.0607, f1:0.1063
>> valid relation with NER prec:0.4265, rec:0.0607, f1:0.1063
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.304, loss:1431.9263
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.942, loss:1347.9325
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.974, loss:1383.5033
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.960, loss:1236.9961
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.970, loss:1223.8425
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5098, rec:0.4468, f1:0.4762
>> valid relation prec:0.3296, rec:0.0576, f1:0.0980
>> valid relation with NER prec:0.3296, rec:0.0576, f1:0.0980
g_step 1100, step 55, avg_time 3.293, loss:1192.4804
g_step 1200, step 155, avg_time 0.964, loss:1099.0793
g_step 1300, step 46, avg_time 0.961, loss:1055.7284
g_step 1400, step 146, avg_time 0.972, loss:1036.0595
g_step 1500, step 37, avg_time 0.964, loss:1036.5275
>> valid entity prec:0.6276, rec:0.3278, f1:0.4306
>> valid relation prec:0.2688, rec:0.0231, f1:0.0426
>> valid relation with NER prec:0.2688, rec:0.0231, f1:0.0426
g_step 1600, step 137, avg_time 3.282, loss:1004.4507
g_step 1700, step 28, avg_time 0.962, loss:958.5447
g_step 1800, step 128, avg_time 0.974, loss:906.1539
g_step 1900, step 19, avg_time 0.952, loss:943.1042
g_step 2000, step 119, avg_time 0.964, loss:892.4891
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5717, rec:0.4277, f1:0.4893
>> valid relation prec:0.2889, rec:0.0328, f1:0.0590
>> valid relation with NER prec:0.2889, rec:0.0328, f1:0.0590
g_step 2100, step 10, avg_time 3.289, loss:908.6838
g_step 2200, step 110, avg_time 0.963, loss:851.7654
g_step 2300, step 1, avg_time 0.966, loss:837.4032
g_step 2400, step 101, avg_time 0.971, loss:784.8431
g_step 2500, step 201, avg_time 0.967, loss:852.8076
>> valid entity prec:0.5816, rec:0.4429, f1:0.5029
>> valid relation prec:0.3430, rec:0.0490, f1:0.0858
>> valid relation with NER prec:0.3430, rec:0.0490, f1:0.0858
g_step 2600, step 92, avg_time 3.295, loss:747.5761
g_step 2700, step 192, avg_time 0.977, loss:804.5719
g_step 2800, step 83, avg_time 0.956, loss:750.6564
g_step 2900, step 183, avg_time 0.974, loss:737.2007
g_step 3000, step 74, avg_time 0.962, loss:723.8354
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5782, rec:0.4053, f1:0.4765
>> valid relation prec:0.3481, rec:0.0545, f1:0.0942
>> valid relation with NER prec:0.3481, rec:0.0545, f1:0.0942
g_step 3100, step 174, avg_time 3.292, loss:710.1643
g_step 3200, step 65, avg_time 0.968, loss:678.9322
g_step 3300, step 165, avg_time 0.969, loss:700.3107
g_step 3400, step 56, avg_time 0.958, loss:640.4769
g_step 3500, step 156, avg_time 0.973, loss:686.7008
>> valid entity prec:0.5379, rec:0.4509, f1:0.4905
>> valid relation prec:0.3114, rec:0.0602, f1:0.1009
>> valid relation with NER prec:0.3114, rec:0.0602, f1:0.1009
g_step 3600, step 47, avg_time 3.288, loss:631.6469
g_step 3700, step 147, avg_time 0.972, loss:633.9798
g_step 3800, step 38, avg_time 0.958, loss:618.5117
g_step 3900, step 138, avg_time 0.971, loss:636.8441
g_step 4000, step 29, avg_time 0.960, loss:574.3173
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5280, rec:0.4054, f1:0.4586
>> valid relation prec:0.2394, rec:0.0268, f1:0.0482
>> valid relation with NER prec:0.2394, rec:0.0268, f1:0.0482
g_step 4100, step 129, avg_time 3.295, loss:567.6647
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:48:09 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:48:09 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-48-09_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:48:10 - WARNING - datasets.builder -   Using custom data configuration default-01f31d53dd27ce8c
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-01f31d53dd27ce8c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:48:12,711 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:48:12,750 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:48:12,751 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:48:12,752 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:48:12,903 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:48:12,982 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:48:12,982 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:48:12,982 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:48:12,983 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:48:12,983 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:48:12,983 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:48:13,539 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:48:16,723 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:48:16,723 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-01f31d53dd27ce8c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:48:16 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14625586a050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.45ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.43ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.87ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.11ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.27ba/s]100%|██████████| 6/6 [00:01<00:00,  4.65ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:03,  2.29ba/s] 22%|██▏       | 2/9 [00:00<00:02,  3.08ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.60ba/s] 44%|████▍     | 4/9 [00:01<00:01,  3.91ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.12ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.24ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.34ba/s] 89%|████████▉ | 8/9 [00:02<00:00,  4.40ba/s]100%|██████████| 9/9 [00:02<00:00,  4.93ba/s]100%|██████████| 9/9 [00:02<00:00,  4.17ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.63ba/s] 33%|███▎      | 2/6 [00:00<00:00,  8.14ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.29ba/s]100%|██████████| 6/6 [00:00<00:00, 12.25ba/s]100%|██████████| 6/6 [00:00<00:00, 10.78ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  6.16ba/s] 33%|███▎      | 3/9 [00:00<00:00,  8.68ba/s] 56%|█████▌    | 5/9 [00:00<00:00,  9.41ba/s] 78%|███████▊  | 7/9 [00:00<00:00,  9.74ba/s]100%|██████████| 9/9 [00:00<00:00, 10.49ba/s]100%|██████████| 9/9 [00:00<00:00,  9.81ba/s]
[INFO|trainer.py:414] 2023-08-27 22:48:22,709 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:48:22,717 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:48:22,717 >>   Num examples = 5034
[INFO|trainer.py:1149] 2023-08-27 22:48:22,717 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:48:22,717 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:48:22,717 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:48:22,717 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:48:22,717 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<01:58,  3.32it/s]  1%|          | 2/395 [00:00<01:55,  3.39it/s]  1%|          | 3/395 [00:00<01:54,  3.42it/s]  1%|          | 4/395 [00:01<01:53,  3.44it/s]  1%|▏         | 5/395 [00:01<01:53,  3.45it/s]  2%|▏         | 6/395 [00:01<01:52,  3.45it/s]  2%|▏         | 7/395 [00:02<01:52,  3.45it/s]  2%|▏         | 8/395 [00:02<01:52,  3.45it/s]  2%|▏         | 9/395 [00:02<01:51,  3.45it/s]  3%|▎         | 10/395 [00:02<01:51,  3.45it/s]  3%|▎         | 11/395 [00:03<01:51,  3.46it/s]  3%|▎         | 12/395 [00:03<01:50,  3.46it/s]  3%|▎         | 13/395 [00:03<01:50,  3.46it/s]  4%|▎         | 14/395 [00:04<01:50,  3.46it/s]  4%|▍         | 15/395 [00:04<01:49,  3.46it/s]  4%|▍         | 16/395 [00:04<01:49,  3.46it/s]  4%|▍         | 17/395 [00:04<01:49,  3.46it/s]  5%|▍         | 18/395 [00:05<01:49,  3.46it/s]  5%|▍         | 19/395 [00:05<01:48,  3.45it/s]  5%|▌         | 20/395 [00:05<01:48,  3.45it/s]  5%|▌         | 21/395 [00:06<01:48,  3.45it/s]  6%|▌         | 22/395 [00:06<01:47,  3.45it/s]  6%|▌         | 23/395 [00:06<01:47,  3.45it/s]  6%|▌         | 24/395 [00:06<01:47,  3.45it/s]  6%|▋         | 25/395 [00:07<01:47,  3.45it/s]  7%|▋         | 26/395 [00:07<01:46,  3.45it/s]  7%|▋         | 27/395 [00:07<01:46,  3.45it/s]  7%|▋         | 28/395 [00:08<01:46,  3.46it/s]  7%|▋         | 29/395 [00:08<01:52,  3.26it/s]  8%|▊         | 30/395 [00:08<01:50,  3.31it/s]  8%|▊         | 31/395 [00:09<01:48,  3.36it/s]  8%|▊         | 32/395 [00:09<01:47,  3.38it/s]  8%|▊         | 33/395 [00:09<01:46,  3.40it/s]  9%|▊         | 34/395 [00:09<01:45,  3.42it/s]  9%|▉         | 35/395 [00:10<01:45,  3.42it/s]  9%|▉         | 36/395 [00:10<01:44,  3.43it/s]  9%|▉         | 37/395 [00:10<01:44,  3.43it/s] 10%|▉         | 38/395 [00:11<01:43,  3.44it/s] 10%|▉         | 39/395 [00:11<01:43,  3.44it/s] 10%|█         | 40/395 [00:11<01:43,  3.44it/s] 10%|█         | 41/395 [00:11<01:42,  3.44it/s] 11%|█         | 42/395 [00:12<01:42,  3.44it/s] 11%|█         | 43/395 [00:12<01:42,  3.45it/s] 11%|█         | 44/395 [00:12<01:41,  3.45it/s] 11%|█▏        | 45/395 [00:13<01:41,  3.45it/s] 12%|█▏        | 46/395 [00:13<01:41,  3.45it/s] 12%|█▏        | 47/395 [00:13<01:41,  3.44it/s] 12%|█▏        | 48/395 [00:13<01:40,  3.44it/s] 12%|█▏        | 49/395 [00:14<01:40,  3.45it/s] 13%|█▎        | 50/395 [00:14<01:40,  3.45it/s] 13%|█▎        | 51/395 [00:14<01:39,  3.45it/s] 13%|█▎        | 52/395 [00:15<01:39,  3.45it/s] 13%|█▎        | 53/395 [00:15<01:39,  3.45it/s] 14%|█▎        | 54/395 [00:15<01:38,  3.45it/s] 14%|█▍        | 55/395 [00:16<01:38,  3.45it/s] 14%|█▍        | 56/395 [00:16<01:38,  3.45it/s] 14%|█▍        | 57/395 [00:16<01:38,  3.45it/s] 15%|█▍        | 58/395 [00:16<01:37,  3.45it/s] 15%|█▍        | 59/395 [00:17<01:37,  3.44it/s] 15%|█▌        | 60/395 [00:17<01:37,  3.45it/s] 15%|█▌        | 61/395 [00:17<01:36,  3.44it/s] 16%|█▌        | 62/395 [00:18<01:36,  3.45it/s] 16%|█▌        | 63/395 [00:18<01:36,  3.45it/s] 16%|█▌        | 64/395 [00:18<01:36,  3.45it/s] 16%|█▋        | 65/395 [00:18<01:35,  3.45it/s] 17%|█▋        | 66/395 [00:19<01:35,  3.45it/s] 17%|█▋        | 67/395 [00:19<01:35,  3.45it/s] 17%|█▋        | 68/395 [00:19<01:34,  3.45it/s] 17%|█▋        | 69/395 [00:20<01:34,  3.45it/s] 18%|█▊        | 70/395 [00:20<01:34,  3.45it/s] 18%|█▊        | 71/395 [00:20<01:33,  3.45it/s] 18%|█▊        | 72/395 [00:20<01:33,  3.44it/s] 18%|█▊        | 73/395 [00:21<01:33,  3.45it/s] 19%|█▊        | 74/395 [00:21<01:33,  3.45it/s] 19%|█▉        | 75/395 [00:21<01:32,  3.45it/s] 19%|█▉        | 76/395 [00:22<01:32,  3.45it/s] 19%|█▉        | 77/395 [00:22<01:32,  3.45it/s] 20%|█▉        | 78/395 [00:22<01:31,  3.45it/s] 20%|██        | 79/395 [00:22<01:23,  3.78it/s][INFO|trainer.py:2140] 2023-08-27 22:48:45,602 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:48:45,602 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-27 22:48:45,602 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.47it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.22it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.61it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.41it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.95it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.52it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.32it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.02it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.93it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.18it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.23it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.26it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.13it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.10it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.99it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.98it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.76it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.79it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.91it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.08it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.24it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.12it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.12it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.04it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.86it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.85it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.74it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.96it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.10it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.13it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.13it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.11it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.90it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.87it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.84it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.76it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.97it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.02it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.09it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.16it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.02it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.00it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.85it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.77it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.83it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.88it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.98it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.14it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.04it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.09it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.94it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.80it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.83it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.79it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.94it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.01it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.10it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.11it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.15it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.97it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.89it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.82it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.79it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.97it/s][A
 30%|███       | 327/1083 [00:07<00:16, 45.01it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.12it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.13it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.00it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.88it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.81it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.76it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.82it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.02it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.01it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.06it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.02it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.81it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.86it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.71it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.71it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.86it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.96it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.07it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.20it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.12it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.99it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.83it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.83it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.84it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.78it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.93it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.07it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.22it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.13it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.01it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.88it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.78it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.71it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.80it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.94it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.06it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.20it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.10it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.99it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.97it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.72it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.83it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.89it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.90it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.00it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.08it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.22it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.08it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.90it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.81it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.74it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.83it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.97it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.02it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.11it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.16it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.07it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.87it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.79it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.77it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.82it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.94it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.05it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.15it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.14it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.06it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.97it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.77it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.74it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.78it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.92it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.02it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.14it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.20it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.00it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.97it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 44.80it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.83it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.82it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.96it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.04it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.12it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 45.02it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.85it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 44.67it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.55it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.73it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.83it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.94it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.12it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.10it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.10it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.87it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 44.79it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.75it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.77it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.85it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.02it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.12it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.14it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.15it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.94it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.79it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.67it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.69it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.75it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.88it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.99it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.19it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.10it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.98it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.85it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.78it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.79it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.81it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.86it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.04it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.06it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.10it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.89it/s][A
 87%|████████▋ | 942/1083 [00:20<00:03, 44.72it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.71it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.55it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.68it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.69it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.72it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.89it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.78it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.86it/s][A
 91%|█████████ | 987/1083 [00:21<00:02, 44.81it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.65it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.73it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.68it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.90it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.01it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 45.12it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 43.12it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 43.80it/s][A
 95%|█████████▌| 1032/1083 [00:22<00:01, 44.08it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.26it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.31it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.50it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.72it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.90it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.64it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.73it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.93it/s][A
 99%|█████████▉| 1077/1083 [00:23<00:00, 44.93it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.93it/s][A                                                
                                                   [A 20%|██        | 79/395 [00:46<01:23,  3.78it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.93it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-27 22:49:10,041 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-27 22:49:10,282 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:49:12,596 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:49:12,624 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:49:12,633 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [00:55<53:06, 10.11s/it] 21%|██        | 81/395 [00:56<37:44,  7.21s/it] 21%|██        | 82/395 [00:56<26:47,  5.14s/it] 21%|██        | 83/395 [00:57<19:09,  3.68s/it] 21%|██▏       | 84/395 [00:57<13:49,  2.67s/it] 22%|██▏       | 85/395 [00:57<10:05,  1.95s/it] 22%|██▏       | 86/395 [00:57<07:29,  1.45s/it] 22%|██▏       | 87/395 [00:58<05:40,  1.11s/it] 22%|██▏       | 88/395 [00:58<04:24,  1.16it/s] 23%|██▎       | 89/395 [00:58<03:30,  1.45it/s] 23%|██▎       | 90/395 [00:59<02:53,  1.76it/s] 23%|██▎       | 91/395 [00:59<02:29,  2.03it/s] 23%|██▎       | 92/395 [00:59<02:10,  2.32it/s] 24%|██▎       | 93/395 [00:59<01:57,  2.57it/s] 24%|██▍       | 94/395 [01:00<01:48,  2.78it/s] 24%|██▍       | 95/395 [01:00<01:41,  2.95it/s] 24%|██▍       | 96/395 [01:00<01:36,  3.09it/s] 25%|██▍       | 97/395 [01:01<01:33,  3.19it/s] 25%|██▍       | 98/395 [01:01<01:31,  3.26it/s] 25%|██▌       | 99/395 [01:01<01:29,  3.31it/s] 25%|██▌       | 100/395 [01:01<01:27,  3.35it/s] 26%|██▌       | 101/395 [01:02<01:26,  3.38it/s] 26%|██▌       | 102/395 [01:02<01:28,  3.33it/s] 26%|██▌       | 103/395 [01:02<01:26,  3.36it/s] 26%|██▋       | 104/395 [01:03<01:25,  3.39it/s] 27%|██▋       | 105/395 [01:03<01:25,  3.41it/s] 27%|██▋       | 106/395 [01:03<01:24,  3.42it/s] 27%|██▋       | 107/395 [01:04<01:24,  3.43it/s] 27%|██▋       | 108/395 [01:04<01:23,  3.43it/s] 28%|██▊       | 109/395 [01:04<01:23,  3.44it/s] 28%|██▊       | 110/395 [01:04<01:22,  3.44it/s] 28%|██▊       | 111/395 [01:05<01:22,  3.44it/s] 28%|██▊       | 112/395 [01:05<01:22,  3.44it/s] 29%|██▊       | 113/395 [01:05<01:24,  3.33it/s] 29%|██▉       | 114/395 [01:06<01:23,  3.36it/s] 29%|██▉       | 115/395 [01:06<01:22,  3.39it/s] 29%|██▉       | 116/395 [01:06<01:21,  3.41it/s] 30%|██▉       | 117/395 [01:06<01:21,  3.42it/s] 30%|██▉       | 118/395 [01:07<01:20,  3.43it/s] 30%|███       | 119/395 [01:07<01:20,  3.43it/s] 30%|███       | 120/395 [01:07<01:20,  3.44it/s] 31%|███       | 121/395 [01:08<01:19,  3.44it/s] 31%|███       | 122/395 [01:08<01:19,  3.44it/s] 31%|███       | 123/395 [01:08<01:18,  3.45it/s] 31%|███▏      | 124/395 [01:08<01:20,  3.37it/s] 32%|███▏      | 125/395 [01:09<01:19,  3.39it/s] 32%|███▏      | 126/395 [01:09<01:18,  3.41it/s] 32%|███▏      | 127/395 [01:09<01:18,  3.42it/s] 32%|███▏      | 128/395 [01:10<01:17,  3.43it/s] 33%|███▎      | 129/395 [01:10<01:17,  3.43it/s] 33%|███▎      | 130/395 [01:10<01:17,  3.44it/s] 33%|███▎      | 131/395 [01:11<01:16,  3.44it/s] 33%|███▎      | 132/395 [01:11<01:16,  3.44it/s] 34%|███▎      | 133/395 [01:11<01:16,  3.44it/s] 34%|███▍      | 134/395 [01:11<01:15,  3.45it/s] 34%|███▍      | 135/395 [01:12<01:18,  3.31it/s] 34%|███▍      | 136/395 [01:12<01:17,  3.35it/s] 35%|███▍      | 137/395 [01:12<01:16,  3.38it/s] 35%|███▍      | 138/395 [01:13<01:15,  3.40it/s] 35%|███▌      | 139/395 [01:13<01:15,  3.41it/s] 35%|███▌      | 140/395 [01:13<01:14,  3.42it/s] 36%|███▌      | 141/395 [01:13<01:14,  3.43it/s] 36%|███▌      | 142/395 [01:14<01:13,  3.44it/s] 36%|███▌      | 143/395 [01:14<01:13,  3.44it/s] 36%|███▋      | 144/395 [01:14<01:12,  3.44it/s] 37%|███▋      | 145/395 [01:15<01:12,  3.44it/s] 37%|███▋      | 146/395 [01:15<01:15,  3.31it/s] 37%|███▋      | 147/395 [01:15<01:14,  3.35it/s] 37%|███▋      | 148/395 [01:16<01:13,  3.38it/s] 38%|███▊      | 149/395 [01:16<01:12,  3.40it/s] 38%|███▊      | 150/395 [01:16<01:11,  3.41it/s] 38%|███▊      | 151/395 [01:16<01:11,  3.42it/s] 38%|███▊      | 152/395 [01:17<01:10,  3.43it/s] 39%|███▊      | 153/395 [01:17<01:10,  3.44it/s] 39%|███▉      | 154/395 [01:17<01:10,  3.44it/s] 39%|███▉      | 155/395 [01:18<01:09,  3.44it/s] 39%|███▉      | 156/395 [01:18<01:09,  3.44it/s] 40%|███▉      | 157/395 [01:18<01:10,  3.37it/s] 40%|████      | 158/395 [01:18<01:03,  3.71it/s][INFO|trainer.py:2140] 2023-08-27 22:49:41,597 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:49:41,597 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-27 22:49:41,597 >>   Batch size = 8
{'eval_loss': 0.8854014277458191, 'eval_runtime': 24.1054, 'eval_samples_per_second': 359.172, 'eval_steps_per_second': 44.928, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.32it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.36it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.52it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.53it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.93it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.55it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.27it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.88it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.90it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.05it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.05it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.13it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.17it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.14it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.08it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.86it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.78it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.69it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.80it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.99it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.03it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.20it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.18it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.35it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.54it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.49it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.55it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.72it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.76it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.01it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.11it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.09it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.95it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.93it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.78it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.65it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.81it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.86it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.13it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.13it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.14it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.01it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.99it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.85it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.76it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.76it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.85it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.02it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.11it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.17it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.11it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 45.00it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.89it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.86it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.71it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.88it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.00it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.90it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.20it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.17it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.02it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.88it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.76it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 43.71it/s][A
 30%|███       | 327/1083 [00:07<00:17, 44.23it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.45it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.76it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.93it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.00it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.01it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.95it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.72it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.77it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.92it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.92it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.05it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.03it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.06it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.01it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.78it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.83it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.77it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.93it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.96it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.02it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.09it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.00it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.96it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.75it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.75it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 44.01it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.44it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.68it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.82it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.82it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.86it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.86it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.78it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.55it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.70it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.88it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.99it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.06it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.04it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.99it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.93it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.78it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.68it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.76it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.94it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.08it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.08it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.01it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.97it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.90it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.69it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.64it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.67it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.91it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.93it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.97it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.94it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.81it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.74it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.65it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.67it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.63it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.76it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.97it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.08it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.22it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.61it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.94it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.86it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.77it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.76it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.74it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.99it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.07it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.05it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.93it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.95it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 44.88it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.82it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 44.12it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.50it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.74it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.80it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.82it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.75it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.66it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 44.67it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.53it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.54it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.68it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.83it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.03it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.87it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.89it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.86it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 44.82it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.69it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.75it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.89it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.95it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.09it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.11it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.04it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 45.00it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.81it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.77it/s][A
 80%|███████▉  | 862/1083 [00:19<00:05, 41.16it/s][A
 80%|████████  | 867/1083 [00:19<00:05, 42.41it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 43.30it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 43.90it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.35it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.71it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.77it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.66it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.40it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.32it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.48it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.70it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.82it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.02it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.06it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 45.07it/s][A
 87%|████████▋ | 942/1083 [00:20<00:03, 45.13it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.90it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.82it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.79it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.78it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.87it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.01it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 45.06it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 45.04it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.90it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.86it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.27it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.36it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.58it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.74it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.92it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.98it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 45.05it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.70it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.65it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.70it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.68it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.75it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.95it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.99it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 45.07it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 45.08it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.99it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.99it/s][A                                                 
                                                   [A 40%|████      | 158/395 [01:43<01:03,  3.71it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.99it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-27 22:50:05,803 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-27 22:50:05,845 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:50:08,231 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:50:08,337 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:50:08,394 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:51<38:41,  9.84s/it] 41%|████      | 160/395 [01:51<27:20,  6.98s/it] 41%|████      | 161/395 [01:51<19:23,  4.97s/it] 41%|████      | 162/395 [01:52<14:03,  3.62s/it] 41%|████▏     | 163/395 [01:52<10:08,  2.62s/it] 42%|████▏     | 164/395 [01:52<07:24,  1.92s/it] 42%|████▏     | 165/395 [01:52<05:30,  1.44s/it] 42%|████▏     | 166/395 [01:53<04:10,  1.09s/it] 42%|████▏     | 167/395 [01:53<03:14,  1.17it/s] 43%|████▎     | 168/395 [01:53<02:35,  1.46it/s] 43%|████▎     | 169/395 [01:54<02:08,  1.76it/s] 43%|████▎     | 170/395 [01:54<01:49,  2.06it/s] 43%|████▎     | 171/395 [01:54<01:36,  2.33it/s] 44%|████▎     | 172/395 [01:55<01:26,  2.58it/s] 44%|████▍     | 173/395 [01:55<01:19,  2.78it/s] 44%|████▍     | 174/395 [01:55<01:15,  2.94it/s] 44%|████▍     | 175/395 [01:55<01:11,  3.06it/s] 45%|████▍     | 176/395 [01:56<01:09,  3.16it/s] 45%|████▍     | 177/395 [01:56<01:07,  3.23it/s] 45%|████▌     | 178/395 [01:56<01:06,  3.27it/s] 45%|████▌     | 179/395 [01:57<01:06,  3.25it/s] 46%|████▌     | 180/395 [01:57<01:05,  3.29it/s] 46%|████▌     | 181/395 [01:57<01:04,  3.32it/s] 46%|████▌     | 182/395 [01:58<01:03,  3.34it/s] 46%|████▋     | 183/395 [01:58<01:03,  3.36it/s] 47%|████▋     | 184/395 [01:58<01:02,  3.37it/s] 47%|████▋     | 185/395 [01:58<01:02,  3.38it/s] 47%|████▋     | 186/395 [01:59<01:01,  3.38it/s] 47%|████▋     | 187/395 [01:59<01:01,  3.39it/s] 48%|████▊     | 188/395 [01:59<01:01,  3.39it/s] 48%|████▊     | 189/395 [02:00<01:00,  3.39it/s] 48%|████▊     | 190/395 [02:00<01:01,  3.31it/s] 48%|████▊     | 191/395 [02:00<01:01,  3.33it/s] 49%|████▊     | 192/395 [02:00<01:00,  3.35it/s] 49%|████▉     | 193/395 [02:01<01:00,  3.36it/s] 49%|████▉     | 194/395 [02:01<00:59,  3.37it/s] 49%|████▉     | 195/395 [02:01<00:58,  3.39it/s] 50%|████▉     | 196/395 [02:02<00:58,  3.41it/s] 50%|████▉     | 197/395 [02:02<00:57,  3.42it/s] 50%|█████     | 198/395 [02:02<00:57,  3.43it/s] 50%|█████     | 199/395 [02:03<00:57,  3.43it/s] 51%|█████     | 200/395 [02:03<00:56,  3.44it/s] 51%|█████     | 201/395 [02:03<00:57,  3.38it/s] 51%|█████     | 202/395 [02:03<00:56,  3.40it/s] 51%|█████▏    | 203/395 [02:04<00:56,  3.41it/s] 52%|█████▏    | 204/395 [02:04<00:55,  3.42it/s] 52%|█████▏    | 205/395 [02:04<00:55,  3.43it/s] 52%|█████▏    | 206/395 [02:05<00:54,  3.44it/s] 52%|█████▏    | 207/395 [02:05<00:54,  3.44it/s] 53%|█████▎    | 208/395 [02:05<00:54,  3.44it/s] 53%|█████▎    | 209/395 [02:05<00:53,  3.45it/s] 53%|█████▎    | 210/395 [02:06<00:53,  3.45it/s] 53%|█████▎    | 211/395 [02:06<00:53,  3.45it/s] 54%|█████▎    | 212/395 [02:06<00:56,  3.22it/s] 54%|█████▍    | 213/395 [02:07<00:55,  3.29it/s] 54%|█████▍    | 214/395 [02:07<00:54,  3.33it/s] 54%|█████▍    | 215/395 [02:07<00:53,  3.37it/s] 55%|█████▍    | 216/395 [02:08<00:52,  3.39it/s] 55%|█████▍    | 217/395 [02:08<00:52,  3.41it/s] 55%|█████▌    | 218/395 [02:08<00:51,  3.42it/s] 55%|█████▌    | 219/395 [02:08<00:51,  3.43it/s] 56%|█████▌    | 220/395 [02:09<00:50,  3.43it/s] 56%|█████▌    | 221/395 [02:09<00:50,  3.44it/s] 56%|█████▌    | 222/395 [02:09<00:50,  3.44it/s] 56%|█████▋    | 223/395 [02:10<00:51,  3.34it/s] 57%|█████▋    | 224/395 [02:10<00:50,  3.37it/s] 57%|█████▋    | 225/395 [02:10<00:50,  3.39it/s] 57%|█████▋    | 226/395 [02:10<00:49,  3.41it/s] 57%|█████▋    | 227/395 [02:11<00:49,  3.42it/s] 58%|█████▊    | 228/395 [02:11<00:48,  3.43it/s] 58%|█████▊    | 229/395 [02:11<00:48,  3.43it/s] 58%|█████▊    | 230/395 [02:12<00:48,  3.44it/s] 58%|█████▊    | 231/395 [02:12<00:47,  3.44it/s] 59%|█████▊    | 232/395 [02:12<00:47,  3.44it/s] 59%|█████▉    | 233/395 [02:12<00:47,  3.45it/s] 59%|█████▉    | 234/395 [02:13<00:48,  3.29it/s] 59%|█████▉    | 235/395 [02:13<00:47,  3.34it/s] 60%|█████▉    | 236/395 [02:13<00:47,  3.37it/s] 60%|██████    | 237/395 [02:14<00:42,  3.71it/s][INFO|trainer.py:2140] 2023-08-27 22:50:36,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:50:36,841 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-27 22:50:36,841 >>   Batch size = 8
{'eval_loss': 0.8558412194252014, 'eval_runtime': 24.1592, 'eval_samples_per_second': 358.373, 'eval_steps_per_second': 44.828, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.46it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.37it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.56it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.69it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.06it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.65it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.21it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.86it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.96it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.01it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.16it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.25it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.26it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.16it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.02it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.79it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.74it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 43.82it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.34it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.62it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.74it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.91it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.96it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.83it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.73it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.65it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.68it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.88it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.99it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.07it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.22it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.23it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.07it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.97it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.87it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.81it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.88it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.94it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.11it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.13it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.21it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.08it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.96it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.88it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.66it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.84it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.91it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.08it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.22it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.06it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.09it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.92it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.90it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.81it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.88it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.99it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.11it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.24it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.13it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.99it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.91it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.88it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.80it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.88it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.94it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.15it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.14it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.12it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.00it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.82it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.80it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.71it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.85it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.91it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.15it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.15it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.04it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.99it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.90it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.82it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.74it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.87it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.04it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.12it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.21it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.01it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.06it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.92it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.83it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.66it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 44.63it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.75it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.86it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.97it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.04it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.00it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.95it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.82it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.68it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.71it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.91it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.11it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.07it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.17it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.05it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 45.03it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.80it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.74it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.73it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.02it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.07it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.12it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.05it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.12it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.98it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.79it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.77it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.81it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.91it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.02it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.96it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.06it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.93it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.83it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.63it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.53it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.63it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.69it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.81it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.98it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.03it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.07it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.99it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.75it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.70it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.77it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.88it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.94it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.09it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.15it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.02it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.87it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 44.70it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.73it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.86it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.88it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 43.55it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.10it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.49it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.58it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.63it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 44.62it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.65it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.81it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.77it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.75it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.82it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.03it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.14it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.95it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 44.78it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.86it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.83it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.86it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.84it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.93it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.04it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.05it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 45.03it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.90it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.82it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.82it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.85it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.92it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.93it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.01it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.04it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 45.01it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.93it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.84it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.86it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.80it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.79it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.97it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.97it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.09it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.96it/s][A
 87%|████████▋ | 942/1083 [00:20<00:03, 45.01it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.84it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.81it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.93it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.85it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.99it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.96it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 45.11it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.97it/s][A
 91%|█████████ | 987/1083 [00:21<00:02, 44.95it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.81it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.83it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.85it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.82it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.92it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.99it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 45.02it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.99it/s][A
 95%|█████████▌| 1032/1083 [00:22<00:01, 44.94it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.85it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.80it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.85it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.82it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.84it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.89it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 45.07it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 45.00it/s][A
 99%|█████████▉| 1077/1083 [00:23<00:00, 45.02it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.88it/s][A                                                 
                                                   [A 60%|██████    | 237/395 [02:38<00:42,  3.71it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.88it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-27 22:51:00,987 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-27 22:51:01,012 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:51:03,264 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:51:03,279 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:51:03,295 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:45<24:58,  9.54s/it] 61%|██████    | 239/395 [02:45<17:35,  6.77s/it] 61%|██████    | 240/395 [02:45<12:27,  4.83s/it] 61%|██████    | 241/395 [02:46<08:53,  3.47s/it] 61%|██████▏   | 242/395 [02:46<06:24,  2.51s/it] 62%|██████▏   | 243/395 [02:46<04:40,  1.85s/it] 62%|██████▏   | 244/395 [02:47<03:28,  1.38s/it] 62%|██████▏   | 245/395 [02:47<02:38,  1.06s/it] 62%|██████▏   | 246/395 [02:47<02:03,  1.21it/s] 63%|██████▎   | 247/395 [02:47<01:38,  1.50it/s] 63%|██████▎   | 248/395 [02:48<01:21,  1.80it/s] 63%|██████▎   | 249/395 [02:48<01:09,  2.10it/s] 63%|██████▎   | 250/395 [02:48<01:01,  2.37it/s] 64%|██████▎   | 251/395 [02:49<00:55,  2.60it/s] 64%|██████▍   | 252/395 [02:49<00:51,  2.80it/s] 64%|██████▍   | 253/395 [02:49<00:47,  2.96it/s] 64%|██████▍   | 254/395 [02:50<00:45,  3.08it/s] 65%|██████▍   | 255/395 [02:50<00:44,  3.17it/s] 65%|██████▍   | 256/395 [02:50<00:42,  3.23it/s] 65%|██████▌   | 257/395 [02:50<00:41,  3.29it/s] 65%|██████▌   | 258/395 [02:51<00:41,  3.33it/s] 66%|██████▌   | 259/395 [02:51<00:40,  3.37it/s] 66%|██████▌   | 260/395 [02:51<00:39,  3.39it/s] 66%|██████▌   | 261/395 [02:52<00:40,  3.34it/s] 66%|██████▋   | 262/395 [02:52<00:39,  3.37it/s] 67%|██████▋   | 263/395 [02:52<00:38,  3.40it/s] 67%|██████▋   | 264/395 [02:52<00:38,  3.41it/s] 67%|██████▋   | 265/395 [02:53<00:37,  3.42it/s] 67%|██████▋   | 266/395 [02:53<00:37,  3.43it/s] 68%|██████▊   | 267/395 [02:53<00:37,  3.44it/s] 68%|██████▊   | 268/395 [02:54<00:36,  3.44it/s] 68%|██████▊   | 269/395 [02:54<00:36,  3.44it/s] 68%|██████▊   | 270/395 [02:54<00:36,  3.45it/s] 69%|██████▊   | 271/395 [02:54<00:35,  3.45it/s] 69%|██████▉   | 272/395 [02:55<00:35,  3.44it/s] 69%|██████▉   | 273/395 [02:55<00:35,  3.44it/s] 69%|██████▉   | 274/395 [02:55<00:35,  3.45it/s] 70%|██████▉   | 275/395 [02:56<00:34,  3.45it/s] 70%|██████▉   | 276/395 [02:56<00:34,  3.45it/s] 70%|███████   | 277/395 [02:56<00:34,  3.45it/s] 70%|███████   | 278/395 [02:56<00:33,  3.45it/s] 71%|███████   | 279/395 [02:57<00:33,  3.45it/s] 71%|███████   | 280/395 [02:57<00:33,  3.45it/s] 71%|███████   | 281/395 [02:57<00:33,  3.45it/s] 71%|███████▏  | 282/395 [02:58<00:32,  3.45it/s] 72%|███████▏  | 283/395 [02:58<00:32,  3.44it/s] 72%|███████▏  | 284/395 [02:58<00:32,  3.44it/s] 72%|███████▏  | 285/395 [02:59<00:31,  3.44it/s] 72%|███████▏  | 286/395 [02:59<00:31,  3.44it/s] 73%|███████▎  | 287/395 [02:59<00:31,  3.45it/s] 73%|███████▎  | 288/395 [02:59<00:31,  3.45it/s] 73%|███████▎  | 289/395 [03:00<00:30,  3.45it/s] 73%|███████▎  | 290/395 [03:00<00:30,  3.45it/s] 74%|███████▎  | 291/395 [03:00<00:30,  3.45it/s] 74%|███████▍  | 292/395 [03:01<00:29,  3.45it/s] 74%|███████▍  | 293/395 [03:01<00:29,  3.45it/s] 74%|███████▍  | 294/395 [03:01<00:29,  3.44it/s] 75%|███████▍  | 295/395 [03:01<00:29,  3.45it/s] 75%|███████▍  | 296/395 [03:02<00:28,  3.45it/s] 75%|███████▌  | 297/395 [03:02<00:28,  3.45it/s] 75%|███████▌  | 298/395 [03:02<00:28,  3.45it/s] 76%|███████▌  | 299/395 [03:03<00:27,  3.45it/s] 76%|███████▌  | 300/395 [03:03<00:27,  3.45it/s] 76%|███████▌  | 301/395 [03:03<00:27,  3.45it/s] 76%|███████▋  | 302/395 [03:03<00:26,  3.45it/s] 77%|███████▋  | 303/395 [03:04<00:26,  3.45it/s] 77%|███████▋  | 304/395 [03:04<00:26,  3.45it/s] 77%|███████▋  | 305/395 [03:04<00:26,  3.44it/s] 77%|███████▋  | 306/395 [03:05<00:25,  3.44it/s] 78%|███████▊  | 307/395 [03:05<00:25,  3.45it/s] 78%|███████▊  | 308/395 [03:05<00:25,  3.45it/s] 78%|███████▊  | 309/395 [03:05<00:24,  3.45it/s] 78%|███████▊  | 310/395 [03:06<00:24,  3.45it/s] 79%|███████▊  | 311/395 [03:06<00:24,  3.45it/s] 79%|███████▉  | 312/395 [03:06<00:24,  3.45it/s] 79%|███████▉  | 313/395 [03:07<00:23,  3.45it/s] 79%|███████▉  | 314/395 [03:07<00:23,  3.45it/s] 80%|███████▉  | 315/395 [03:07<00:23,  3.45it/s] 80%|████████  | 316/395 [03:07<00:22,  3.56it/s][INFO|trainer.py:2140] 2023-08-27 22:51:30,706 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:51:30,706 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-27 22:51:30,706 >>   Batch size = 8
{'eval_loss': 0.8647099733352661, 'eval_runtime': 24.1074, 'eval_samples_per_second': 359.142, 'eval_steps_per_second': 44.924, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.19it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.41it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.78it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.94it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.44it/s][A
  3%|▎         | 32/1083 [00:00<00:22, 45.95it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.28it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.67it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.70it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.80it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.99it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.09it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.23it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.19it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.27it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 45.14it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.83it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.84it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.87it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.98it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.03it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.11it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.20it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.21it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.05it/s][A
 12%|█▏        | 132/1083 [00:02<00:22, 41.69it/s][A
 13%|█▎        | 137/1083 [00:03<00:22, 42.75it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 43.50it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.94it/s][A
 14%|█▍        | 152/1083 [00:03<00:21, 44.31it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.61it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.84it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.78it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.56it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.54it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.63it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.90it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.98it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.07it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.15it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.21it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.00it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.76it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.79it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.77it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.93it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.96it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.07it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.17it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.17it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.02it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.76it/s][A
 25%|██▍       | 267/1083 [00:05<00:19, 42.28it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 43.15it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 43.81it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.03it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.61it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.86it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.92it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.96it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.67it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.63it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.64it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.79it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.81it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.03it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.99it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.13it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.94it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.76it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.70it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.78it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.96it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.98it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.11it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.09it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.07it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.98it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.81it/s][A
 37%|███▋      | 402/1083 [00:08<00:16, 41.44it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 42.52it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 43.40it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 43.84it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.27it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.57it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.81it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.87it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.62it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.60it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.84it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.89it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.11it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.08it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.09it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.05it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.95it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.80it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.67it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.79it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.90it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.05it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.07it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.10it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.09it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.84it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.70it/s][A
 50%|████▉     | 537/1083 [00:12<00:13, 41.27it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 42.38it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 43.20it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 43.80it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.24it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.50it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.58it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.70it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.46it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.25it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.55it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.74it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.95it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.05it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.21it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.29it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.15it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.83it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.59it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.74it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.79it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.97it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.11it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.08it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.28it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.17it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.85it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 42.11it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 42.96it/s][A
 63%|██████▎   | 682/1083 [00:15<00:09, 43.74it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.15it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.55it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.67it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.91it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.95it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.72it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.69it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.68it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.94it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 45.09it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.21it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.12it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.10it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 45.00it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.81it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.65it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.72it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.86it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.83it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.00it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.02it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.13it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.00it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.80it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.69it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.77it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.94it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.01it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.10it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.13it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.20it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.04it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.76it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.65it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.66it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.92it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 45.07it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.18it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.12it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.20it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.03it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.64it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.71it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.67it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.75it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.99it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.09it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.18it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.10it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.92it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 42.26it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 43.12it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 43.72it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.20it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.49it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.78it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.87it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.98it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.67it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.62it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.78it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.93it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.02it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 45.00it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 45.12it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.18it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.92it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.37it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.87it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.82it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.93it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.96it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 45.08it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 45.01it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.16it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.91it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.76it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 43.59it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.11it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.34it/s][A                                                 
                                                   [A 80%|████████  | 316/395 [03:32<00:22,  3.56it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.34it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-27 22:51:55,012 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-27 22:51:55,402 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:52:00,104 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:52:00,178 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:52:00,214 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [03:44<14:21, 11.05s/it] 81%|████████  | 318/395 [03:44<10:02,  7.83s/it] 81%|████████  | 319/395 [03:44<07:03,  5.57s/it] 81%|████████  | 320/395 [03:45<04:58,  3.98s/it] 81%|████████▏ | 321/395 [03:45<03:32,  2.87s/it] 82%|████████▏ | 322/395 [03:45<02:33,  2.10s/it] 82%|████████▏ | 323/395 [03:45<01:52,  1.56s/it] 82%|████████▏ | 324/395 [03:46<01:23,  1.18s/it] 82%|████████▏ | 325/395 [03:46<01:03,  1.10it/s] 83%|████████▎ | 326/395 [03:46<00:49,  1.38it/s] 83%|████████▎ | 327/395 [03:47<00:40,  1.69it/s] 83%|████████▎ | 328/395 [03:47<00:33,  1.99it/s] 83%|████████▎ | 329/395 [03:47<00:29,  2.25it/s] 84%|████████▎ | 330/395 [03:47<00:25,  2.51it/s] 84%|████████▍ | 331/395 [03:48<00:23,  2.74it/s] 84%|████████▍ | 332/395 [03:48<00:21,  2.92it/s] 84%|████████▍ | 333/395 [03:48<00:20,  3.06it/s] 85%|████████▍ | 334/395 [03:49<00:19,  3.17it/s] 85%|████████▍ | 335/395 [03:49<00:18,  3.25it/s] 85%|████████▌ | 336/395 [03:49<00:17,  3.31it/s] 85%|████████▌ | 337/395 [03:49<00:17,  3.35it/s] 86%|████████▌ | 338/395 [03:50<00:16,  3.38it/s] 86%|████████▌ | 339/395 [03:50<00:16,  3.40it/s] 86%|████████▌ | 340/395 [03:50<00:16,  3.28it/s] 86%|████████▋ | 341/395 [03:51<00:16,  3.33it/s] 87%|████████▋ | 342/395 [03:51<00:15,  3.36it/s] 87%|████████▋ | 343/395 [03:51<00:15,  3.39it/s] 87%|████████▋ | 344/395 [03:52<00:17,  3.00it/s] 87%|████████▋ | 345/395 [03:52<00:16,  3.12it/s] 88%|████████▊ | 346/395 [03:52<00:15,  3.22it/s] 88%|████████▊ | 347/395 [03:53<00:14,  3.28it/s] 88%|████████▊ | 348/395 [03:53<00:14,  3.33it/s] 88%|████████▊ | 349/395 [03:53<00:13,  3.37it/s] 89%|████████▊ | 350/395 [03:53<00:13,  3.34it/s] 89%|████████▉ | 351/395 [03:54<00:13,  3.37it/s] 89%|████████▉ | 352/395 [03:54<00:12,  3.40it/s] 89%|████████▉ | 353/395 [03:54<00:12,  3.41it/s] 90%|████████▉ | 354/395 [03:55<00:11,  3.43it/s] 90%|████████▉ | 355/395 [03:55<00:11,  3.43it/s] 90%|█████████ | 356/395 [03:55<00:11,  3.44it/s] 90%|█████████ | 357/395 [03:55<00:11,  3.44it/s] 91%|█████████ | 358/395 [03:56<00:10,  3.45it/s] 91%|█████████ | 359/395 [03:56<00:10,  3.45it/s] 91%|█████████ | 360/395 [03:56<00:10,  3.45it/s] 91%|█████████▏| 361/395 [03:57<00:09,  3.45it/s] 92%|█████████▏| 362/395 [03:57<00:09,  3.45it/s] 92%|█████████▏| 363/395 [03:57<00:09,  3.45it/s] 92%|█████████▏| 364/395 [03:58<00:08,  3.45it/s] 92%|█████████▏| 365/395 [03:58<00:08,  3.45it/s] 93%|█████████▎| 366/395 [03:58<00:08,  3.45it/s] 93%|█████████▎| 367/395 [03:58<00:08,  3.45it/s] 93%|█████████▎| 368/395 [03:59<00:07,  3.46it/s] 93%|█████████▎| 369/395 [03:59<00:07,  3.46it/s] 94%|█████████▎| 370/395 [03:59<00:07,  3.45it/s] 94%|█████████▍| 371/395 [04:00<00:07,  3.37it/s] 94%|█████████▍| 372/395 [04:00<00:06,  3.39it/s] 94%|█████████▍| 373/395 [04:00<00:06,  3.41it/s] 95%|█████████▍| 374/395 [04:00<00:06,  3.42it/s] 95%|█████████▍| 375/395 [04:01<00:05,  3.43it/s] 95%|█████████▌| 376/395 [04:01<00:05,  3.44it/s] 95%|█████████▌| 377/395 [04:01<00:05,  3.44it/s] 96%|█████████▌| 378/395 [04:02<00:04,  3.44it/s] 96%|█████████▌| 379/395 [04:02<00:04,  3.45it/s] 96%|█████████▌| 380/395 [04:02<00:04,  3.45it/s] 96%|█████████▋| 381/395 [04:02<00:04,  3.45it/s] 97%|█████████▋| 382/395 [04:03<00:03,  3.35it/s] 97%|█████████▋| 383/395 [04:03<00:03,  3.38it/s] 97%|█████████▋| 384/395 [04:03<00:03,  3.40it/s] 97%|█████████▋| 385/395 [04:04<00:02,  3.41it/s] 98%|█████████▊| 386/395 [04:04<00:02,  3.42it/s] 98%|█████████▊| 387/395 [04:04<00:02,  3.43it/s] 98%|█████████▊| 388/395 [04:05<00:02,  3.44it/s] 98%|█████████▊| 389/395 [04:05<00:01,  3.44it/s] 99%|█████████▊| 390/395 [04:05<00:01,  3.44it/s] 99%|█████████▉| 391/395 [04:05<00:01,  3.45it/s] 99%|█████████▉| 392/395 [04:06<00:00,  3.45it/s] 99%|█████████▉| 393/395 [04:06<00:00,  3.33it/s]100%|█████████▉| 394/395 [04:06<00:00,  3.36it/s]100%|██████████| 395/395 [04:06<00:00,  3.71it/s][INFO|trainer.py:2140] 2023-08-27 22:52:29,709 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:52:29,709 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-27 22:52:29,709 >>   Batch size = 8
{'eval_loss': 0.8679139614105225, 'eval_runtime': 24.2357, 'eval_samples_per_second': 357.242, 'eval_steps_per_second': 44.686, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.02it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.23it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.51it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.35it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.91it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.40it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.14it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.94it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.91it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.06it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.14it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.22it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.26it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.23it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.99it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.98it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.90it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.98it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.06it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.08it/s][A
 10%|▉         | 107/1083 [00:02<00:23, 42.33it/s][A
 10%|█         | 112/1083 [00:02<00:22, 43.08it/s][A
 11%|█         | 117/1083 [00:02<00:22, 43.88it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.15it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.34it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.51it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.70it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.87it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.63it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.73it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.91it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.07it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.06it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.94it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.92it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 45.02it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.92it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.82it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.88it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.93it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.99it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.07it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.89it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.90it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.91it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.95it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.85it/s][A
 22%|██▏       | 242/1083 [00:05<00:20, 41.39it/s][A
 23%|██▎       | 247/1083 [00:05<00:19, 42.58it/s][A
 23%|██▎       | 252/1083 [00:05<00:19, 43.51it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.01it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.52it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.75it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.75it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.67it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.45it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.48it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.72it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.74it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.01it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.15it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 45.32it/s][A
 29%|██▉       | 317/1083 [00:07<00:16, 45.23it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 45.01it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.71it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.66it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.79it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.78it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.99it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.22it/s][A
 33%|███▎      | 357/1083 [00:07<00:15, 45.39it/s][A
 33%|███▎      | 362/1083 [00:08<00:15, 45.15it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 45.03it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.86it/s][A
 35%|███▍      | 377/1083 [00:08<00:16, 42.23it/s][A
 35%|███▌      | 382/1083 [00:08<00:16, 42.96it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 43.76it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.24it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.57it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.77it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.97it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.03it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.62it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.58it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.74it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.92it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.06it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.12it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.02it/s][A
 42%|████▏     | 452/1083 [00:10<00:13, 45.17it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.01it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.77it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.71it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.67it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.04it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.12it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 45.21it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 45.08it/s][A
 46%|████▌     | 497/1083 [00:11<00:12, 45.21it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.00it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.74it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.64it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.68it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.95it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.05it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 45.09it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 45.16it/s][A
 50%|█████     | 542/1083 [00:12<00:11, 45.16it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.98it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.71it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.68it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.80it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.99it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.12it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.03it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 45.08it/s][A
 54%|█████▍    | 587/1083 [00:13<00:10, 45.15it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.95it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.60it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.71it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.74it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.02it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.12it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 45.14it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 45.16it/s][A
 58%|█████▊    | 632/1083 [00:14<00:09, 45.18it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 45.03it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.72it/s][A
 60%|█████▉    | 647/1083 [00:14<00:10, 42.36it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 43.22it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 43.88it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.37it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.54it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.79it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.98it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.95it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.65it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.68it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.75it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.99it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.10it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.99it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 45.11it/s][A
 67%|██████▋   | 722/1083 [00:16<00:07, 45.19it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.90it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.73it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.74it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.73it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.90it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.98it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.15it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 45.22it/s][A
 71%|███████   | 767/1083 [00:17<00:06, 45.16it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 45.01it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.80it/s][A
 72%|███████▏  | 782/1083 [00:17<00:07, 41.63it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 42.75it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 43.56it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.10it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.45it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.78it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.83it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.76it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.38it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.40it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.67it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.81it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.96it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 45.12it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 45.24it/s][A
 79%|███████▉  | 857/1083 [00:19<00:04, 45.26it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.08it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.64it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.56it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.70it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.82it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.90it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 45.05it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 45.14it/s][A
 83%|████████▎ | 902/1083 [00:20<00:03, 45.26it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 45.00it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.77it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.63it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.68it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.68it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.83it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.94it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 45.13it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 45.26it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 45.01it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.89it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.74it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.77it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.78it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.88it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 45.00it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 45.17it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 45.17it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.03it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.93it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.82it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.72it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.73it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.78it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 45.02it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 45.19it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 45.10it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 45.15it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.71it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.70it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.58it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.64it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.73it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.89it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 45.00it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.63it/s][A                                                 
                                                   [A100%|██████████| 395/395 [04:31<00:00,  3.71it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.63it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-27 22:52:54,090 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-27 22:52:54,213 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:52:56,923 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:52:57,055 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:52:57,108 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:53:02,440 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:53:02,465 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158 (score: 0.8558412194252014).
                                                 100%|██████████| 395/395 [04:46<00:00,  3.71it/s]100%|██████████| 395/395 [04:46<00:00,  1.38it/s]
[INFO|trainer.py:1894] 2023-08-27 22:53:08,870 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:53:08,944 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:53:11,296 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:53:11,383 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:53:11,430 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:53:11,830 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:11,830 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:11,830 >>   train_loss               =     0.7285
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:11,830 >>   train_runtime            = 0:04:46.12
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:11,830 >>   train_samples            =       5034
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:11,830 >>   train_samples_per_second =     87.967
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:11,830 >>   train_steps_per_second   =       1.38
{'eval_loss': 0.8702279925346375, 'eval_runtime': 24.1964, 'eval_samples_per_second': 357.822, 'eval_steps_per_second': 44.759, 'epoch': 5.0}
{'train_runtime': 286.129, 'train_samples_per_second': 87.967, 'train_steps_per_second': 1.38, 'train_loss': 0.7285165521162975, 'epoch': 5.0}
08/27/2023 22:53:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:53:12,034 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:53:12,034 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-27 22:53:12,034 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.92it/s]  1%|          | 12/1083 [00:00<00:21, 49.35it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.86it/s]  2%|▏         | 22/1083 [00:00<00:22, 47.06it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.53it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.18it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.91it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.64it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.12it/s]  5%|▍         | 52/1083 [00:01<00:23, 44.82it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.88it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.02it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.24it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.29it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.44it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.33it/s]  8%|▊         | 87/1083 [00:01<00:22, 45.21it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.97it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.73it/s]  9%|▉         | 102/1083 [00:02<00:21, 44.84it/s] 10%|▉         | 107/1083 [00:02<00:21, 44.90it/s] 10%|█         | 112/1083 [00:02<00:21, 45.19it/s] 11%|█         | 117/1083 [00:02<00:21, 45.23it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.40it/s] 12%|█▏        | 127/1083 [00:02<00:28, 33.29it/s] 12%|█▏        | 132/1083 [00:03<00:26, 36.27it/s] 13%|█▎        | 137/1083 [00:03<00:24, 38.21it/s] 13%|█▎        | 142/1083 [00:03<00:23, 40.12it/s] 14%|█▎        | 147/1083 [00:03<00:22, 41.56it/s] 14%|█▍        | 152/1083 [00:03<00:21, 42.58it/s] 14%|█▍        | 157/1083 [00:03<00:21, 43.41it/s] 15%|█▍        | 162/1083 [00:03<00:20, 43.96it/s] 15%|█▌        | 167/1083 [00:03<00:20, 44.07it/s] 16%|█▌        | 172/1083 [00:03<00:20, 44.32it/s] 16%|█▋        | 177/1083 [00:04<00:20, 44.54it/s] 17%|█▋        | 182/1083 [00:04<00:20, 44.77it/s] 17%|█▋        | 187/1083 [00:04<00:19, 44.88it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.97it/s] 18%|█▊        | 197/1083 [00:04<00:19, 45.09it/s] 19%|█▊        | 202/1083 [00:04<00:19, 45.12it/s] 19%|█▉        | 207/1083 [00:04<00:19, 44.92it/s] 20%|█▉        | 212/1083 [00:04<00:19, 44.82it/s] 20%|██        | 217/1083 [00:04<00:19, 44.85it/s] 20%|██        | 222/1083 [00:05<00:19, 44.90it/s] 21%|██        | 227/1083 [00:05<00:19, 44.96it/s] 21%|██▏       | 232/1083 [00:05<00:18, 45.02it/s] 22%|██▏       | 237/1083 [00:05<00:18, 45.20it/s] 22%|██▏       | 242/1083 [00:05<00:18, 45.13it/s] 23%|██▎       | 247/1083 [00:05<00:18, 45.02it/s] 23%|██▎       | 252/1083 [00:05<00:18, 45.02it/s] 24%|██▎       | 257/1083 [00:05<00:18, 44.89it/s] 24%|██▍       | 262/1083 [00:05<00:18, 44.84it/s] 25%|██▍       | 267/1083 [00:06<00:18, 44.94it/s] 25%|██▌       | 272/1083 [00:06<00:18, 43.45it/s] 26%|██▌       | 277/1083 [00:06<00:18, 44.04it/s] 26%|██▌       | 282/1083 [00:06<00:18, 44.48it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.76it/s] 27%|██▋       | 292/1083 [00:06<00:17, 44.80it/s] 27%|██▋       | 297/1083 [00:06<00:17, 44.82it/s] 28%|██▊       | 302/1083 [00:06<00:17, 44.74it/s] 28%|██▊       | 307/1083 [00:06<00:17, 44.80it/s] 29%|██▉       | 312/1083 [00:07<00:17, 44.60it/s] 29%|██▉       | 317/1083 [00:07<00:17, 44.85it/s] 30%|██▉       | 322/1083 [00:07<00:16, 44.94it/s] 30%|███       | 327/1083 [00:07<00:16, 45.07it/s] 31%|███       | 332/1083 [00:07<00:16, 45.16it/s] 31%|███       | 337/1083 [00:07<00:16, 45.21it/s] 32%|███▏      | 342/1083 [00:07<00:16, 45.13it/s] 32%|███▏      | 347/1083 [00:07<00:16, 44.95it/s] 33%|███▎      | 352/1083 [00:07<00:16, 44.87it/s] 33%|███▎      | 357/1083 [00:08<00:16, 44.84it/s] 33%|███▎      | 362/1083 [00:08<00:16, 44.89it/s] 34%|███▍      | 367/1083 [00:08<00:15, 44.88it/s] 34%|███▍      | 372/1083 [00:08<00:15, 45.03it/s] 35%|███▍      | 377/1083 [00:08<00:15, 45.12it/s] 35%|███▌      | 382/1083 [00:08<00:15, 45.17it/s] 36%|███▌      | 387/1083 [00:08<00:16, 42.50it/s] 36%|███▌      | 392/1083 [00:08<00:15, 43.39it/s] 37%|███▋      | 397/1083 [00:08<00:15, 43.86it/s] 37%|███▋      | 402/1083 [00:09<00:15, 44.14it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.38it/s] 38%|███▊      | 412/1083 [00:09<00:15, 44.48it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.79it/s] 39%|███▉      | 422/1083 [00:09<00:14, 44.96it/s] 39%|███▉      | 427/1083 [00:09<00:14, 44.69it/s] 40%|███▉      | 432/1083 [00:09<00:14, 44.79it/s] 40%|████      | 437/1083 [00:09<00:14, 44.91it/s] 41%|████      | 442/1083 [00:09<00:14, 45.02it/s] 41%|████▏     | 447/1083 [00:10<00:14, 44.94it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.90it/s] 42%|████▏     | 457/1083 [00:10<00:13, 44.94it/s] 43%|████▎     | 462/1083 [00:10<00:13, 45.08it/s] 43%|████▎     | 467/1083 [00:10<00:13, 45.03it/s] 44%|████▎     | 472/1083 [00:10<00:13, 44.89it/s] 44%|████▍     | 477/1083 [00:10<00:13, 44.90it/s] 45%|████▍     | 482/1083 [00:10<00:13, 44.93it/s] 45%|████▍     | 487/1083 [00:10<00:13, 44.95it/s] 45%|████▌     | 492/1083 [00:11<00:13, 44.90it/s] 46%|████▌     | 497/1083 [00:11<00:13, 44.91it/s] 46%|████▋     | 502/1083 [00:11<00:12, 45.05it/s] 47%|████▋     | 507/1083 [00:11<00:12, 45.10it/s] 47%|████▋     | 512/1083 [00:11<00:12, 45.00it/s] 48%|████▊     | 517/1083 [00:11<00:12, 44.90it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.05it/s] 49%|████▊     | 527/1083 [00:11<00:12, 44.41it/s] 49%|████▉     | 532/1083 [00:11<00:12, 44.64it/s] 50%|████▉     | 537/1083 [00:12<00:12, 44.61it/s] 50%|█████     | 542/1083 [00:12<00:12, 44.70it/s] 51%|█████     | 547/1083 [00:12<00:11, 44.94it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.99it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 44.90it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 44.75it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 44.85it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 44.92it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 44.98it/s] 54%|█████▎    | 582/1083 [00:13<00:11, 44.82it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 45.01it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 45.14it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 45.11it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.86it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 44.90it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 44.85it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 44.90it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 44.91it/s] 58%|█████▊    | 627/1083 [00:14<00:10, 44.92it/s] 58%|█████▊    | 632/1083 [00:14<00:10, 45.01it/s] 59%|█████▉    | 637/1083 [00:14<00:09, 45.12it/s] 59%|█████▉    | 642/1083 [00:14<00:09, 44.97it/s] 60%|█████▉    | 647/1083 [00:14<00:09, 45.00it/s] 60%|██████    | 652/1083 [00:14<00:09, 44.93it/s] 61%|██████    | 657/1083 [00:14<00:09, 43.32it/s] 61%|██████    | 662/1083 [00:14<00:09, 43.86it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 44.32it/s] 62%|██████▏   | 672/1083 [00:15<00:09, 44.53it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.81it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.97it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.88it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.93it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 44.72it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 44.82it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 44.96it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 44.98it/s] 66%|██████▌   | 717/1083 [00:16<00:08, 45.00it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 45.11it/s] 67%|██████▋   | 727/1083 [00:16<00:07, 45.11it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 45.03it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.85it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.75it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 44.80it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 44.89it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 44.90it/s] 70%|███████   | 762/1083 [00:17<00:07, 45.00it/s] 71%|███████   | 767/1083 [00:17<00:07, 45.13it/s] 71%|███████▏  | 772/1083 [00:17<00:06, 44.98it/s] 72%|███████▏  | 777/1083 [00:17<00:06, 43.88it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 44.21it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 44.45it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.05it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 44.33it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 44.52it/s] 75%|███████▍  | 807/1083 [00:18<00:06, 44.75it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 44.93it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.73it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.80it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 44.69it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 44.70it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 44.80it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 44.85it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 44.99it/s] 79%|███████▊  | 852/1083 [00:19<00:05, 45.02it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 45.11it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.87it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.89it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.82it/s] 81%|████████  | 877/1083 [00:19<00:04, 44.71it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 44.72it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 44.85it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 45.02it/s] 83%|████████▎ | 897/1083 [00:20<00:04, 45.10it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 45.12it/s] 84%|████████▎ | 907/1083 [00:20<00:03, 45.16it/s] 84%|████████▍ | 912/1083 [00:20<00:03, 45.06it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 45.08it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 44.94it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 43.94it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 44.43it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 44.73it/s] 87%|████████▋ | 942/1083 [00:21<00:03, 44.80it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.80it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 45.09it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 45.05it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 45.04it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 44.78it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 44.84it/s] 90%|█████████ | 977/1083 [00:21<00:02, 45.01it/s] 91%|█████████ | 982/1083 [00:21<00:02, 45.22it/s] 91%|█████████ | 987/1083 [00:22<00:02, 45.18it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 45.23it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 45.20it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 45.17it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 45.10it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 44.84it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 44.92it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 44.91it/s] 95%|█████████▍| 1027/1083 [00:22<00:01, 45.02it/s] 95%|█████████▌| 1032/1083 [00:23<00:01, 45.21it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 45.34it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 45.31it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 45.21it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 45.06it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 44.92it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 41.35it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 42.52it/s] 99%|█████████▉| 1072/1083 [00:24<00:00, 43.34it/s] 99%|█████████▉| 1077/1083 [00:24<00:00, 44.07it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 44.57it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.64it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:53:36,315 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   eval_loss               =     0.8558
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   eval_runtime            = 0:00:24.28
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   eval_samples_per_second =    356.574
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   eval_steps_per_second   =     44.603
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:53:36,315 >>   perplexity              =     2.3534
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_0', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:19, 15.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:30<02:03, 15.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:45<01:44, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:10<01:53, 18.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:25<01:28, 17.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:38<01:04, 16.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:54<00:47, 15.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:08<00:30, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:22<00:15, 15.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:38<00:00, 15.31s/it]Generating: 100%|██████████| 10/10 [02:38<00:00, 15.87s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')", "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_5_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.20s/it]Extractor Estimating: 2it [00:18,  8.07s/it]Extractor Estimating: 3it [00:19,  4.68s/it]Extractor Estimating: 4it [00:20,  3.51s/it]Extractor Estimating: 5it [00:21,  2.47s/it]Extractor Estimating: 6it [00:22,  1.82s/it]Extractor Estimating: 7it [00:22,  1.44s/it]Extractor Estimating: 8it [00:23,  1.17s/it]Extractor Estimating: 9it [00:23,  1.00it/s]Extractor Estimating: 10it [00:24,  1.13it/s]Extractor Estimating: 11it [00:25,  1.26it/s]Extractor Estimating: 12it [00:25,  1.35it/s]Extractor Estimating: 13it [00:26,  1.41it/s]Extractor Estimating: 14it [00:27,  1.44it/s]Extractor Estimating: 15it [00:27,  1.48it/s]Extractor Estimating: 16it [00:28,  1.50it/s]Extractor Estimating: 17it [00:28,  1.57it/s]Extractor Estimating: 18it [00:29,  1.58it/s]Extractor Estimating: 19it [00:30,  1.53it/s]Extractor Estimating: 20it [00:30,  1.58it/s]Extractor Estimating: 21it [00:31,  1.55it/s]Extractor Estimating: 22it [00:32,  1.52it/s]Extractor Estimating: 23it [00:32,  1.51it/s]Extractor Estimating: 24it [00:33,  1.53it/s]Extractor Estimating: 25it [00:34,  1.59it/s]Extractor Estimating: 26it [00:34,  1.56it/s]Extractor Estimating: 27it [00:35,  1.53it/s]Extractor Estimating: 28it [00:36,  1.54it/s]Extractor Estimating: 29it [00:36,  1.60it/s]Extractor Estimating: 30it [00:37,  1.61it/s]Extractor Estimating: 31it [00:37,  1.60it/s]Extractor Estimating: 32it [00:38,  1.61it/s]Extractor Estimating: 33it [00:39,  1.63it/s]Extractor Estimating: 34it [00:39,  1.59it/s]Extractor Estimating: 35it [00:40,  1.60it/s]Extractor Estimating: 36it [00:41,  1.60it/s]Extractor Estimating: 37it [00:41,  1.65it/s]Extractor Estimating: 38it [00:42,  1.70it/s]Extractor Estimating: 39it [00:42,  1.65it/s]Extractor Estimating: 40it [00:43,  1.61it/s]Extractor Estimating: 41it [00:44,  1.56it/s]Extractor Estimating: 42it [00:44,  1.53it/s]Extractor Estimating: 43it [00:45,  1.52it/s]Extractor Estimating: 44it [00:46,  1.52it/s]Extractor Estimating: 45it [00:46,  1.53it/s]Extractor Estimating: 46it [00:47,  1.55it/s]Extractor Estimating: 47it [00:48,  1.56it/s]Extractor Estimating: 48it [00:48,  1.55it/s]Extractor Estimating: 49it [00:49,  1.57it/s]Extractor Estimating: 50it [00:49,  1.58it/s]Extractor Estimating: 51it [00:50,  1.56it/s]Extractor Estimating: 52it [00:51,  1.63it/s]Extractor Estimating: 53it [00:51,  1.67it/s]Extractor Estimating: 54it [00:52,  1.56it/s]Extractor Estimating: 55it [00:53,  1.61it/s]Extractor Estimating: 56it [00:53,  1.54it/s]Extractor Estimating: 57it [00:54,  1.62it/s]Extractor Estimating: 58it [00:54,  1.67it/s]Extractor Estimating: 59it [00:55,  1.73it/s]Extractor Estimating: 60it [00:55,  1.71it/s]Extractor Estimating: 61it [00:56,  1.74it/s]Extractor Estimating: 62it [00:57,  1.74it/s]Extractor Estimating: 63it [00:57,  1.69it/s]Extractor Estimating: 64it [00:58,  1.70it/s]Extractor Estimating: 65it [00:58,  1.73it/s]Extractor Estimating: 66it [00:59,  1.75it/s]Extractor Estimating: 67it [00:59,  1.76it/s]Extractor Estimating: 68it [01:00,  1.74it/s]Extractor Estimating: 69it [01:01,  1.71it/s]Extractor Estimating: 70it [01:01,  1.71it/s]Extractor Estimating: 71it [01:02,  1.72it/s]Extractor Estimating: 72it [01:02,  1.73it/s]Extractor Estimating: 73it [01:03,  1.75it/s]Extractor Estimating: 74it [01:04,  1.66it/s]Extractor Estimating: 75it [01:04,  1.74it/s]Extractor Estimating: 76it [01:08,  1.65s/it]Extractor Estimating: 77it [01:09,  1.35s/it]Extractor Estimating: 78it [01:10,  1.12s/it]Extractor Estimating: 79it [01:10,  1.03it/s]Extractor Estimating: 80it [01:11,  1.16it/s]Extractor Estimating: 81it [01:11,  1.24it/s]Extractor Estimating: 82it [01:12,  1.28it/s]Extractor Estimating: 83it [01:13,  1.33it/s]Extractor Estimating: 84it [01:14,  1.37it/s]Extractor Estimating: 85it [01:14,  1.45it/s]Extractor Estimating: 86it [01:15,  1.47it/s]Extractor Estimating: 87it [01:15,  1.52it/s]Extractor Estimating: 88it [01:16,  1.55it/s]Extractor Estimating: 89it [01:17,  1.58it/s]Extractor Estimating: 90it [01:17,  1.55it/s]Extractor Estimating: 91it [01:18,  1.58it/s]Extractor Estimating: 92it [01:19,  1.58it/s]Extractor Estimating: 93it [01:19,  1.59it/s]Extractor Estimating: 94it [01:20,  1.59it/s]Extractor Estimating: 95it [01:20,  1.56it/s]Extractor Estimating: 96it [01:21,  1.53it/s]Extractor Estimating: 97it [01:22,  1.51it/s]Extractor Estimating: 98it [01:22,  1.54it/s]Extractor Estimating: 99it [01:23,  1.57it/s]Extractor Estimating: 100it [01:24,  1.56it/s]Extractor Estimating: 101it [01:24,  1.60it/s]Extractor Estimating: 102it [01:25,  1.61it/s]Extractor Estimating: 103it [01:25,  1.67it/s]Extractor Estimating: 104it [01:26,  1.68it/s]Extractor Estimating: 105it [01:27,  1.74it/s]Extractor Estimating: 106it [01:27,  1.74it/s]Extractor Estimating: 107it [01:28,  1.68it/s]Extractor Estimating: 108it [01:28,  1.71it/s]Extractor Estimating: 109it [01:29,  1.67it/s]Extractor Estimating: 110it [01:30,  1.65it/s]Extractor Estimating: 111it [01:30,  1.64it/s]Extractor Estimating: 112it [01:31,  1.63it/s]Extractor Estimating: 113it [01:31,  1.66it/s]Extractor Estimating: 114it [01:32,  1.72it/s]Extractor Estimating: 115it [01:33,  1.71it/s]Extractor Estimating: 116it [01:33,  1.55it/s]Extractor Estimating: 117it [01:34,  1.55it/s]Extractor Estimating: 118it [01:35,  1.59it/s]Extractor Estimating: 119it [01:35,  1.63it/s]Extractor Estimating: 120it [01:36,  1.65it/s]Extractor Estimating: 121it [01:36,  1.63it/s]Extractor Estimating: 122it [01:37,  1.67it/s]Extractor Estimating: 123it [01:37,  1.72it/s]Extractor Estimating: 124it [01:38,  1.68it/s]Extractor Estimating: 125it [01:39,  1.62it/s]Extractor Estimating: 126it [01:39,  1.64it/s]Extractor Estimating: 127it [01:40,  1.63it/s]Extractor Estimating: 128it [01:41,  1.60it/s]Extractor Estimating: 129it [01:41,  1.61it/s]Extractor Estimating: 130it [01:42,  1.64it/s]Extractor Estimating: 131it [01:42,  1.62it/s]Extractor Estimating: 132it [01:43,  1.62it/s]Extractor Estimating: 133it [01:44,  1.60it/s]Extractor Estimating: 134it [01:44,  1.60it/s]Extractor Estimating: 135it [01:45,  1.58it/s]Extractor Estimating: 136it [01:46,  1.56it/s]Extractor Estimating: 137it [01:46,  1.54it/s]Extractor Estimating: 138it [01:47,  1.50it/s]Extractor Estimating: 139it [01:48,  1.53it/s]Extractor Estimating: 140it [01:48,  1.51it/s]Extractor Estimating: 141it [01:49,  1.55it/s]Extractor Estimating: 142it [01:50,  1.56it/s]Extractor Estimating: 143it [01:50,  1.56it/s]Extractor Estimating: 144it [01:51,  1.57it/s]Extractor Estimating: 145it [01:51,  1.58it/s]Extractor Estimating: 146it [01:52,  1.58it/s]Extractor Estimating: 147it [01:53,  1.61it/s]Extractor Estimating: 148it [01:53,  1.55it/s]Extractor Estimating: 149it [01:54,  1.51it/s]Extractor Estimating: 150it [01:55,  1.53it/s]Extractor Estimating: 151it [01:55,  1.52it/s]Extractor Estimating: 152it [01:56,  1.55it/s]Extractor Estimating: 153it [01:57,  1.56it/s]Extractor Estimating: 154it [01:57,  1.62it/s]Extractor Estimating: 155it [01:58,  1.61it/s]Extractor Estimating: 156it [01:58,  1.56it/s]Extractor Estimating: 157it [01:59,  1.59it/s]Extractor Estimating: 158it [02:00,  1.61it/s]Extractor Estimating: 159it [02:00,  1.64it/s]Extractor Estimating: 160it [02:01,  1.65it/s]Extractor Estimating: 161it [02:01,  1.67it/s]Extractor Estimating: 162it [02:02,  1.60it/s]Extractor Estimating: 163it [02:03,  1.59it/s]Extractor Estimating: 164it [02:03,  1.55it/s]Extractor Estimating: 165it [02:04,  1.53it/s]Extractor Estimating: 166it [02:05,  1.54it/s]Extractor Estimating: 167it [02:05,  1.54it/s]Extractor Estimating: 168it [02:06,  1.60it/s]Extractor Estimating: 169it [02:07,  1.66it/s]Extractor Estimating: 170it [02:07,  1.65it/s]Extractor Estimating: 171it [02:08,  1.69it/s]Extractor Estimating: 172it [02:08,  1.66it/s]Extractor Estimating: 173it [02:09,  1.62it/s]Extractor Estimating: 174it [02:10,  1.63it/s]Extractor Estimating: 175it [02:10,  1.60it/s]Extractor Estimating: 176it [02:11,  1.62it/s]Extractor Estimating: 177it [02:11,  1.64it/s]Extractor Estimating: 178it [02:12,  1.67it/s]Extractor Estimating: 179it [02:13,  1.70it/s]Extractor Estimating: 180it [02:13,  1.72it/s]Extractor Estimating: 181it [02:14,  1.72it/s]Extractor Estimating: 182it [02:14,  1.72it/s]Extractor Estimating: 183it [02:15,  1.66it/s]Extractor Estimating: 184it [02:16,  1.68it/s]Extractor Estimating: 185it [02:16,  1.60it/s]Extractor Estimating: 186it [02:17,  1.64it/s]Extractor Estimating: 187it [02:17,  1.60it/s]Extractor Estimating: 188it [02:18,  1.63it/s]Extractor Estimating: 189it [02:19,  1.65it/s]Extractor Estimating: 190it [02:19,  1.64it/s]Extractor Estimating: 191it [02:20,  1.68it/s]Extractor Estimating: 192it [02:21,  1.54it/s]Extractor Estimating: 193it [02:21,  1.56it/s]Extractor Estimating: 194it [02:22,  1.53it/s]Extractor Estimating: 195it [02:22,  1.59it/s]Extractor Estimating: 196it [02:23,  1.63it/s]Extractor Estimating: 197it [02:24,  1.66it/s]Extractor Estimating: 198it [02:24,  1.65it/s]Extractor Estimating: 199it [02:25,  1.68it/s]Extractor Estimating: 200it [02:25,  1.66it/s]Extractor Estimating: 201it [02:26,  1.57it/s]Extractor Estimating: 202it [02:27,  1.59it/s]Extractor Estimating: 203it [02:27,  1.58it/s]Extractor Estimating: 204it [02:28,  1.65it/s]Extractor Estimating: 205it [02:29,  1.68it/s]Extractor Estimating: 206it [02:29,  1.66it/s]Extractor Estimating: 207it [02:30,  1.62it/s]Extractor Estimating: 208it [02:30,  1.63it/s]Extractor Estimating: 209it [02:31,  1.63it/s]Extractor Estimating: 210it [02:32,  1.60it/s]Extractor Estimating: 211it [02:32,  1.60it/s]Extractor Estimating: 212it [02:33,  1.53it/s]Extractor Estimating: 213it [02:34,  1.55it/s]Extractor Estimating: 214it [02:34,  1.57it/s]Extractor Estimating: 215it [02:35,  1.58it/s]Extractor Estimating: 216it [02:35,  1.60it/s]Extractor Estimating: 217it [02:36,  1.61it/s]Extractor Estimating: 218it [02:37,  1.66it/s]Extractor Estimating: 219it [02:37,  1.65it/s]Extractor Estimating: 220it [02:38,  1.61it/s]Extractor Estimating: 221it [02:39,  1.59it/s]Extractor Estimating: 222it [02:39,  1.61it/s]Extractor Estimating: 223it [02:40,  1.57it/s]Extractor Estimating: 224it [02:41,  1.54it/s]Extractor Estimating: 225it [02:41,  1.57it/s]Extractor Estimating: 226it [02:42,  1.53it/s]Extractor Estimating: 227it [02:42,  1.57it/s]Extractor Estimating: 228it [02:43,  1.60it/s]Extractor Estimating: 229it [02:44,  1.61it/s]Extractor Estimating: 230it [02:44,  1.63it/s]Extractor Estimating: 231it [02:45,  1.60it/s]Extractor Estimating: 232it [02:46,  1.60it/s]Extractor Estimating: 233it [02:46,  1.62it/s]Extractor Estimating: 234it [02:47,  1.58it/s]Extractor Estimating: 235it [02:47,  1.56it/s]Extractor Estimating: 236it [02:48,  1.61it/s]Extractor Estimating: 237it [02:49,  1.60it/s]Extractor Estimating: 238it [02:49,  1.60it/s]Extractor Estimating: 239it [02:50,  1.60it/s]Extractor Estimating: 240it [02:51,  1.60it/s]Extractor Estimating: 241it [02:51,  1.57it/s]Extractor Estimating: 242it [02:52,  1.61it/s]Extractor Estimating: 243it [02:52,  1.62it/s]Extractor Estimating: 244it [02:53,  1.61it/s]Extractor Estimating: 245it [02:54,  1.61it/s]Extractor Estimating: 246it [02:54,  1.58it/s]Extractor Estimating: 247it [02:55,  1.57it/s]Extractor Estimating: 248it [02:56,  1.62it/s]Extractor Estimating: 249it [02:56,  1.58it/s]Extractor Estimating: 250it [02:57,  1.51it/s]Extractor Estimating: 250it [02:57,  1.41it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5118 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 26638
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26738, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_5_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26738, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.269, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.108, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 86, avg_time 0.996, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 186, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 72, avg_time 0.985, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 172, avg_time 3.016, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 0.989, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.017, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 44, avg_time 1.006, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 144, avg_time 1.014, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 30, avg_time 2.966, loss:nan
g_step 1200, step 130, avg_time 1.013, loss:nan
g_step 1300, step 16, avg_time 1.000, loss:nan
g_step 1400, step 116, avg_time 1.009, loss:nan
g_step 1500, step 2, avg_time 0.995, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 102, avg_time 2.967, loss:nan
g_step 1700, step 202, avg_time 1.002, loss:nan
g_step 1800, step 88, avg_time 1.002, loss:nan
g_step 1900, step 188, avg_time 1.012, loss:nan
g_step 2000, step 74, avg_time 0.999, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.997, loss:nan
g_step 2200, step 60, avg_time 0.993, loss:nan
g_step 2300, step 160, avg_time 1.000, loss:nan
g_step 2400, step 46, avg_time 1.012, loss:nan
g_step 2500, step 146, avg_time 0.995, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.974, loss:nan
g_step 2700, step 132, avg_time 0.983, loss:nan
g_step 2800, step 18, avg_time 0.969, loss:nan
g_step 2900, step 118, avg_time 0.985, loss:nan
g_step 3000, step 4, avg_time 1.021, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 104, avg_time 2.974, loss:nan
g_step 3200, step 204, avg_time 0.967, loss:nan
g_step 3300, step 90, avg_time 0.964, loss:nan
g_step 3400, step 190, avg_time 0.999, loss:nan
g_step 3500, step 76, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 176, avg_time 2.968, loss:nan
g_step 3700, step 62, avg_time 0.996, loss:nan
g_step 3800, step 162, avg_time 1.016, loss:nan
g_step 3900, step 48, avg_time 1.007, loss:nan
g_step 4000, step 148, avg_time 1.007, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 34, avg_time 2.969, loss:nan
g_step 4200, step 134, avg_time 1.014, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:41:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:41:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-41-48_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:41:49 - WARNING - datasets.builder -   Using custom data configuration default-f225b7690b75bf91
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f225b7690b75bf91/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:41:50,957 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:41:50,958 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:41:50,959 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:41:50,960 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:41:51,027 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:41:51,084 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:41:51,084 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:41:51,084 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:41:51,085 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:41:51,085 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:41:51,085 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:41:51,366 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:41:54,508 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:41:54,508 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f225b7690b75bf91/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:41:54 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x146a4c144200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:04,  1.04ba/s] 33%|███▎      | 2/6 [00:01<00:02,  1.91ba/s] 50%|█████     | 3/6 [00:01<00:01,  2.60ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.13ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.52ba/s]100%|██████████| 6/6 [00:01<00:00,  3.19ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  2.88ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.64ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.99ba/s] 44%|████▍     | 4/9 [00:01<00:01,  4.19ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.29ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.36ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.41ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.44ba/s]100%|██████████| 9/9 [00:02<00:00,  4.99ba/s]100%|██████████| 9/9 [00:02<00:00,  4.39ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.79ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.87ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.82ba/s]100%|██████████| 6/6 [00:00<00:00, 10.81ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  6.50ba/s] 33%|███▎      | 3/9 [00:00<00:00,  9.23ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.10ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.40ba/s]100%|██████████| 9/9 [00:00<00:00, 11.34ba/s]100%|██████████| 9/9 [00:00<00:00, 10.54ba/s]
[INFO|trainer.py:414] 2023-08-28 00:42:00,893 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:42:00,933 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:42:00,933 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:42:00,933 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:42:00,933 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:42:00,933 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:42:00,933 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:42:00,933 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:55,  3.49it/s]  0%|          | 2/405 [00:00<01:52,  3.58it/s]  1%|          | 3/405 [00:00<01:51,  3.60it/s]  1%|          | 4/405 [00:01<01:50,  3.62it/s]  1%|          | 5/405 [00:01<01:50,  3.62it/s]  1%|▏         | 6/405 [00:01<01:50,  3.61it/s]  2%|▏         | 7/405 [00:01<01:50,  3.60it/s]  2%|▏         | 8/405 [00:02<01:50,  3.59it/s]  2%|▏         | 9/405 [00:02<01:56,  3.39it/s]  2%|▏         | 10/405 [00:02<01:54,  3.45it/s]  3%|▎         | 11/405 [00:03<01:52,  3.49it/s]  3%|▎         | 12/405 [00:03<01:51,  3.52it/s]  3%|▎         | 13/405 [00:03<01:50,  3.54it/s]  3%|▎         | 14/405 [00:03<01:50,  3.55it/s]  4%|▎         | 15/405 [00:04<01:49,  3.56it/s]  4%|▍         | 16/405 [00:04<01:49,  3.57it/s]  4%|▍         | 17/405 [00:04<01:48,  3.57it/s]  4%|▍         | 18/405 [00:05<01:48,  3.57it/s]  5%|▍         | 19/405 [00:05<01:48,  3.57it/s]  5%|▍         | 20/405 [00:05<01:55,  3.34it/s]  5%|▌         | 21/405 [00:05<01:52,  3.41it/s]  5%|▌         | 22/405 [00:06<01:50,  3.46it/s]  6%|▌         | 23/405 [00:06<01:49,  3.50it/s]  6%|▌         | 24/405 [00:06<01:48,  3.52it/s]  6%|▌         | 25/405 [00:07<01:47,  3.54it/s]  6%|▋         | 26/405 [00:07<01:46,  3.55it/s]  7%|▋         | 27/405 [00:07<01:46,  3.56it/s]  7%|▋         | 28/405 [00:07<01:45,  3.56it/s]  7%|▋         | 29/405 [00:08<01:45,  3.57it/s]  7%|▋         | 30/405 [00:08<01:45,  3.57it/s]  8%|▊         | 31/405 [00:08<01:51,  3.37it/s]  8%|▊         | 32/405 [00:09<01:48,  3.43it/s]  8%|▊         | 33/405 [00:09<01:47,  3.47it/s]  8%|▊         | 34/405 [00:09<01:46,  3.50it/s]  9%|▊         | 35/405 [00:09<01:45,  3.52it/s]  9%|▉         | 36/405 [00:10<01:44,  3.53it/s]  9%|▉         | 37/405 [00:10<01:43,  3.54it/s]  9%|▉         | 38/405 [00:10<01:43,  3.55it/s] 10%|▉         | 39/405 [00:11<01:42,  3.56it/s] 10%|▉         | 40/405 [00:11<01:42,  3.56it/s] 10%|█         | 41/405 [00:11<01:41,  3.58it/s] 10%|█         | 42/405 [00:11<01:45,  3.43it/s] 11%|█         | 43/405 [00:12<01:43,  3.48it/s] 11%|█         | 44/405 [00:12<01:42,  3.52it/s] 11%|█         | 45/405 [00:12<01:41,  3.55it/s] 11%|█▏        | 46/405 [00:13<01:40,  3.58it/s] 12%|█▏        | 47/405 [00:13<01:39,  3.59it/s] 12%|█▏        | 48/405 [00:13<01:39,  3.60it/s] 12%|█▏        | 49/405 [00:13<01:38,  3.61it/s] 12%|█▏        | 50/405 [00:14<01:38,  3.61it/s] 13%|█▎        | 51/405 [00:14<01:37,  3.62it/s] 13%|█▎        | 52/405 [00:14<01:37,  3.62it/s] 13%|█▎        | 53/405 [00:15<01:41,  3.48it/s] 13%|█▎        | 54/405 [00:15<01:39,  3.52it/s] 14%|█▎        | 55/405 [00:15<01:38,  3.55it/s] 14%|█▍        | 56/405 [00:15<01:37,  3.57it/s] 14%|█▍        | 57/405 [00:16<01:37,  3.59it/s] 14%|█▍        | 58/405 [00:16<01:36,  3.60it/s] 15%|█▍        | 59/405 [00:16<01:35,  3.61it/s] 15%|█▍        | 60/405 [00:16<01:35,  3.61it/s] 15%|█▌        | 61/405 [00:17<01:35,  3.62it/s] 15%|█▌        | 62/405 [00:17<01:34,  3.62it/s] 16%|█▌        | 63/405 [00:17<01:34,  3.61it/s] 16%|█▌        | 64/405 [00:18<01:37,  3.51it/s] 16%|█▌        | 65/405 [00:18<01:35,  3.55it/s] 16%|█▋        | 66/405 [00:18<01:34,  3.57it/s] 17%|█▋        | 67/405 [00:18<01:34,  3.59it/s] 17%|█▋        | 68/405 [00:19<01:33,  3.59it/s] 17%|█▋        | 69/405 [00:19<01:33,  3.60it/s] 17%|█▋        | 70/405 [00:19<01:32,  3.61it/s] 18%|█▊        | 71/405 [00:20<01:32,  3.61it/s] 18%|█▊        | 72/405 [00:20<01:32,  3.62it/s] 18%|█▊        | 73/405 [00:20<01:31,  3.62it/s] 18%|█▊        | 74/405 [00:20<01:31,  3.62it/s] 19%|█▊        | 75/405 [00:21<01:35,  3.47it/s] 19%|█▉        | 76/405 [00:21<01:33,  3.51it/s] 19%|█▉        | 77/405 [00:21<01:32,  3.54it/s] 19%|█▉        | 78/405 [00:21<01:31,  3.57it/s] 20%|█▉        | 79/405 [00:22<01:30,  3.59it/s] 20%|█▉        | 80/405 [00:22<01:30,  3.60it/s] 20%|██        | 81/405 [00:22<01:21,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 00:42:23,657 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:42:23,657 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 00:42:23,657 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.16it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.17it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.31it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.45it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.96it/s][A
  3%|▎         | 32/1083 [00:00<00:22, 45.75it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.46it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.88it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.79it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.90it/s][A
  5%|▌         | 57/1083 [00:01<00:24, 41.92it/s][A
  6%|▌         | 62/1083 [00:01<00:23, 42.90it/s][A
  6%|▌         | 67/1083 [00:01<00:23, 43.58it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.12it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.47it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.66it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.77it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.65it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.51it/s][A
  9%|▉         | 102/1083 [00:02<00:22, 44.55it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.68it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.86it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.01it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.13it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.05it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 45.01it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.80it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.65it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.75it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.78it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.87it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.97it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.03it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 45.14it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.96it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.87it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.71it/s][A
 18%|█▊        | 192/1083 [00:04<00:23, 37.31it/s][A
 18%|█▊        | 197/1083 [00:04<00:22, 39.47it/s][A
 19%|█▊        | 202/1083 [00:04<00:21, 40.98it/s][A
 19%|█▉        | 207/1083 [00:04<00:20, 42.17it/s][A
 20%|█▉        | 212/1083 [00:04<00:20, 43.11it/s][A
 20%|██        | 217/1083 [00:04<00:19, 43.74it/s][A
 20%|██        | 222/1083 [00:05<00:19, 44.20it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.33it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.10it/s][A
 22%|██▏       | 237/1083 [00:05<00:19, 44.25it/s][A
 22%|██▏       | 242/1083 [00:05<00:31, 26.95it/s][A
 23%|██▎       | 247/1083 [00:05<00:27, 30.67it/s][A
 23%|██▎       | 252/1083 [00:05<00:24, 33.95it/s][A
 24%|██▎       | 257/1083 [00:06<00:22, 36.64it/s][A
 24%|██▍       | 262/1083 [00:06<00:21, 38.74it/s][A
 25%|██▍       | 267/1083 [00:06<00:20, 40.44it/s][A
 25%|██▌       | 272/1083 [00:06<00:19, 41.70it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 42.65it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 43.13it/s][A
 27%|██▋       | 287/1083 [00:06<00:18, 43.67it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.08it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.39it/s][A
 28%|██▊       | 302/1083 [00:07<00:17, 44.51it/s][A
 28%|██▊       | 307/1083 [00:07<00:17, 44.57it/s][A
 29%|██▉       | 312/1083 [00:07<00:17, 44.51it/s][A
 29%|██▉       | 317/1083 [00:07<00:18, 40.84it/s][A
 30%|██▉       | 322/1083 [00:07<00:18, 42.07it/s][A
 30%|███       | 327/1083 [00:07<00:17, 42.94it/s][A
 31%|███       | 332/1083 [00:07<00:17, 43.62it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.13it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.46it/s][A
 32%|███▏      | 347/1083 [00:08<00:16, 44.76it/s][A
 33%|███▎      | 352/1083 [00:08<00:16, 44.85it/s][A
 33%|███▎      | 357/1083 [00:08<00:16, 44.53it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.67it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.71it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.90it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.01it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.12it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.07it/s][A
 36%|███▌      | 392/1083 [00:09<00:17, 39.77it/s][A
 37%|███▋      | 398/1083 [00:09<00:16, 42.67it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 43.55it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 43.91it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 44.37it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.65it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.85it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.86it/s][A
 40%|███▉      | 433/1083 [00:10<00:14, 44.89it/s][A
 40%|████      | 438/1083 [00:10<00:14, 44.56it/s][A
 41%|████      | 443/1083 [00:10<00:14, 44.51it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 44.77it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 42.67it/s][A
 42%|████▏     | 458/1083 [00:10<00:14, 43.53it/s][A
 43%|████▎     | 463/1083 [00:10<00:14, 44.11it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.53it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 44.76it/s][A
 44%|████▍     | 478/1083 [00:11<00:13, 44.82it/s][A
 45%|████▍     | 483/1083 [00:11<00:13, 44.75it/s][A
 45%|████▌     | 488/1083 [00:11<00:13, 44.72it/s][A
 46%|████▌     | 493/1083 [00:11<00:13, 44.47it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.58it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.82it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 45.06it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 45.24it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.30it/s][A
 48%|████▊     | 523/1083 [00:12<00:12, 45.19it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 45.06it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 44.86it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.66it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.64it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 44.79it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.97it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 45.10it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 45.19it/s][A
 52%|█████▏    | 568/1083 [00:13<00:11, 45.23it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 45.18it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 44.99it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.79it/s][A
 54%|█████▍    | 588/1083 [00:13<00:12, 40.74it/s][A
 55%|█████▍    | 593/1083 [00:13<00:11, 42.07it/s][A
 55%|█████▌    | 598/1083 [00:13<00:11, 43.02it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 43.74it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.14it/s][A
 57%|█████▋    | 613/1083 [00:14<00:10, 44.49it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 44.76it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 45.00it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.61it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.43it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.82it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.92it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 45.12it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.14it/s][A
 61%|██████    | 658/1083 [00:15<00:09, 45.21it/s][A
 61%|██████    | 663/1083 [00:15<00:09, 45.27it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 45.20it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.96it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.71it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.81it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.88it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.93it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 45.06it/s][A
 65%|██████▍   | 703/1083 [00:16<00:08, 45.12it/s][A
 65%|██████▌   | 708/1083 [00:16<00:08, 45.15it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 45.03it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.91it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 43.20it/s][A
 67%|██████▋   | 728/1083 [00:16<00:08, 43.83it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.28it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.58it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.69it/s][A
 69%|██████▉   | 748/1083 [00:17<00:07, 44.84it/s][A
 70%|██████▉   | 753/1083 [00:17<00:07, 45.02it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.94it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.57it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.64it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.69it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.96it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.03it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.06it/s][A
 73%|███████▎  | 793/1083 [00:18<00:06, 45.12it/s][A
 74%|███████▎  | 798/1083 [00:18<00:06, 45.14it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.98it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.73it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.66it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.77it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.86it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.96it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.08it/s][A
 77%|███████▋  | 838/1083 [00:19<00:05, 45.19it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 45.05it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.97it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.78it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 43.29it/s][A
 80%|███████▉  | 863/1083 [00:19<00:05, 43.85it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.20it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.44it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.75it/s][A
 82%|████████▏ | 883/1083 [00:20<00:04, 44.96it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 45.01it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.82it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.68it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.57it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.70it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.82it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.96it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.08it/s][A
 86%|████████▌ | 928/1083 [00:21<00:03, 45.17it/s][A
 86%|████████▌ | 933/1083 [00:21<00:03, 45.08it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 45.00it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.77it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.65it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.63it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.78it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.94it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 45.13it/s][A
 90%|████████▉ | 973/1083 [00:22<00:02, 45.17it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 45.12it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 45.02it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.83it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 41.64it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 42.76it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 43.46it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.06it/s][A
 94%|█████████▎| 1013/1083 [00:23<00:01, 44.44it/s][A
 94%|█████████▍| 1018/1083 [00:23<00:01, 44.66it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 44.84it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 44.85it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.67it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.56it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.69it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.83it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.92it/s][A
 98%|█████████▊| 1058/1083 [00:24<00:00, 45.08it/s][A
 98%|█████████▊| 1063/1083 [00:24<00:00, 45.10it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 45.16it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 45.07it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.92it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.76it/s][A                                                
                                                   [A 20%|██        | 81/405 [00:47<01:21,  3.99it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.76it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 00:42:48,340 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:42:48,408 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:42:51,923 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:42:52,145 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:42:52,245 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:53<50:58,  9.47s/it] 20%|██        | 83/405 [00:53<36:01,  6.71s/it] 21%|██        | 84/405 [00:54<25:35,  4.78s/it] 21%|██        | 85/405 [00:54<18:21,  3.44s/it] 21%|██        | 86/405 [00:54<13:15,  2.49s/it] 21%|██▏       | 87/405 [00:55<09:41,  1.83s/it] 22%|██▏       | 88/405 [00:55<07:12,  1.37s/it] 22%|██▏       | 89/405 [00:55<05:28,  1.04s/it] 22%|██▏       | 90/405 [00:55<04:15,  1.23it/s] 22%|██▏       | 91/405 [00:56<03:25,  1.53it/s] 23%|██▎       | 92/405 [00:56<02:49,  1.85it/s] 23%|██▎       | 93/405 [00:56<02:24,  2.16it/s] 23%|██▎       | 94/405 [00:57<02:06,  2.45it/s] 23%|██▎       | 95/405 [00:57<01:54,  2.70it/s] 24%|██▎       | 96/405 [00:57<01:49,  2.81it/s] 24%|██▍       | 97/405 [00:57<01:42,  3.00it/s] 24%|██▍       | 98/405 [00:58<01:37,  3.15it/s] 24%|██▍       | 99/405 [00:58<01:33,  3.27it/s] 25%|██▍       | 100/405 [00:58<01:31,  3.35it/s] 25%|██▍       | 101/405 [00:59<01:29,  3.41it/s] 25%|██▌       | 102/405 [00:59<01:27,  3.46it/s] 25%|██▌       | 103/405 [00:59<01:26,  3.49it/s] 26%|██▌       | 104/405 [00:59<01:25,  3.52it/s] 26%|██▌       | 105/405 [01:00<01:24,  3.53it/s] 26%|██▌       | 106/405 [01:00<01:24,  3.54it/s] 26%|██▋       | 107/405 [01:00<01:23,  3.55it/s] 27%|██▋       | 108/405 [01:01<01:25,  3.49it/s] 27%|██▋       | 109/405 [01:01<01:24,  3.51it/s] 27%|██▋       | 110/405 [01:01<01:23,  3.53it/s] 27%|██▋       | 111/405 [01:01<01:23,  3.54it/s] 28%|██▊       | 112/405 [01:02<01:22,  3.55it/s] 28%|██▊       | 113/405 [01:02<01:22,  3.55it/s] 28%|██▊       | 114/405 [01:02<01:21,  3.56it/s] 28%|██▊       | 115/405 [01:03<01:21,  3.56it/s] 29%|██▊       | 116/405 [01:03<01:21,  3.56it/s] 29%|██▉       | 117/405 [01:03<01:20,  3.57it/s] 29%|██▉       | 118/405 [01:03<01:20,  3.57it/s] 29%|██▉       | 119/405 [01:04<01:25,  3.35it/s] 30%|██▉       | 120/405 [01:04<01:23,  3.41it/s] 30%|██▉       | 121/405 [01:04<01:22,  3.46it/s] 30%|███       | 122/405 [01:05<01:21,  3.49it/s] 30%|███       | 123/405 [01:05<01:20,  3.52it/s] 31%|███       | 124/405 [01:05<01:19,  3.53it/s] 31%|███       | 125/405 [01:05<01:19,  3.54it/s] 31%|███       | 126/405 [01:06<01:18,  3.55it/s] 31%|███▏      | 127/405 [01:06<01:18,  3.56it/s] 32%|███▏      | 128/405 [01:06<01:17,  3.56it/s] 32%|███▏      | 129/405 [01:07<01:17,  3.56it/s] 32%|███▏      | 130/405 [01:07<01:22,  3.33it/s] 32%|███▏      | 131/405 [01:07<01:20,  3.39it/s] 33%|███▎      | 132/405 [01:07<01:19,  3.44it/s] 33%|███▎      | 133/405 [01:08<01:18,  3.48it/s] 33%|███▎      | 134/405 [01:08<01:17,  3.50it/s] 33%|███▎      | 135/405 [01:08<01:16,  3.53it/s] 34%|███▎      | 136/405 [01:09<01:15,  3.56it/s] 34%|███▍      | 137/405 [01:09<01:14,  3.58it/s] 34%|███▍      | 138/405 [01:09<01:14,  3.59it/s] 34%|███▍      | 139/405 [01:09<01:13,  3.60it/s] 35%|███▍      | 140/405 [01:10<01:13,  3.60it/s] 35%|███▍      | 141/405 [01:10<01:17,  3.39it/s] 35%|███▌      | 142/405 [01:10<01:16,  3.46it/s] 35%|███▌      | 143/405 [01:11<01:14,  3.50it/s] 36%|███▌      | 144/405 [01:11<01:13,  3.54it/s] 36%|███▌      | 145/405 [01:11<01:12,  3.56it/s] 36%|███▌      | 146/405 [01:11<01:12,  3.58it/s] 36%|███▋      | 147/405 [01:12<01:11,  3.59it/s] 37%|███▋      | 148/405 [01:12<01:11,  3.60it/s] 37%|███▋      | 149/405 [01:12<01:10,  3.61it/s] 37%|███▋      | 150/405 [01:12<01:10,  3.61it/s] 37%|███▋      | 151/405 [01:13<01:10,  3.61it/s] 38%|███▊      | 152/405 [01:13<01:12,  3.49it/s] 38%|███▊      | 153/405 [01:13<01:11,  3.53it/s] 38%|███▊      | 154/405 [01:14<01:10,  3.55it/s] 38%|███▊      | 155/405 [01:14<01:10,  3.57it/s] 39%|███▊      | 156/405 [01:14<01:09,  3.58it/s] 39%|███▉      | 157/405 [01:14<01:08,  3.60it/s] 39%|███▉      | 158/405 [01:15<01:08,  3.61it/s] 39%|███▉      | 159/405 [01:15<01:08,  3.61it/s] 40%|███▉      | 160/405 [01:15<01:07,  3.61it/s] 40%|███▉      | 161/405 [01:16<01:07,  3.61it/s] 40%|████      | 162/405 [01:16<01:00,  4.01it/s][INFO|trainer.py:2140] 2023-08-28 00:43:17,179 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:43:17,179 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 00:43:17,179 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.5891, 'eval_samples_per_second': 352.107, 'eval_steps_per_second': 44.044, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 55.85it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.07it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.27it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.65it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.06it/s][A
  3%|▎         | 32/1083 [00:00<00:22, 45.86it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.43it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.96it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.83it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.92it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.01it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.04it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.15it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.26it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.22it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 45.05it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.78it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.71it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.80it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.79it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.85it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.99it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.10it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.14it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.93it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.80it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.67it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.80it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.95it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.38it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.66it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.83it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.97it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.92it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.87it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.77it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.63it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.76it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.91it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.01it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.10it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.19it/s][A
 20%|██        | 217/1083 [00:04<00:19, 45.05it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.97it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.91it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.86it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.94it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.85it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.03it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.94it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.06it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.99it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.79it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.88it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.79it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.36it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.69it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.81it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.94it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.00it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.07it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.89it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.86it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.79it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.72it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.97it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.93it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.05it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.01it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.96it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.86it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.89it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.81it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.72it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.81it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.95it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.08it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.11it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.08it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.03it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.91it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.87it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 43.33it/s][A
 39%|███▉      | 422/1083 [00:09<00:15, 43.87it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.27it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.57it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.79it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.84it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.94it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.92it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 44.70it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.70it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.80it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.89it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.98it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.02it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 43.04it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 43.65it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 43.99it/s][A
 46%|████▋     | 502/1083 [00:11<00:13, 44.12it/s][A
 47%|████▋     | 507/1083 [00:11<00:13, 44.29it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.56it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.75it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.87it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.73it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 43.35it/s][A
 50%|████▉     | 537/1083 [00:12<00:17, 30.91it/s][A
 50%|█████     | 543/1083 [00:12<00:15, 35.49it/s][A
 51%|█████     | 548/1083 [00:12<00:14, 37.85it/s][A
 51%|█████     | 553/1083 [00:12<00:13, 39.79it/s][A
 52%|█████▏    | 558/1083 [00:12<00:12, 41.27it/s][A
 52%|█████▏    | 563/1083 [00:12<00:12, 42.39it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 43.22it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 43.85it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 43.82it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 43.81it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 43.93it/s][A
 55%|█████▍    | 593/1083 [00:13<00:11, 44.29it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.63it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.81it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.99it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.01it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.11it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 44.93it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.72it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.64it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.76it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.94it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 45.00it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.06it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.05it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 45.00it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 44.85it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.70it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 42.16it/s][A
 63%|██████▎   | 683/1083 [00:15<00:09, 43.29it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 43.92it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 43.69it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.29it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.54it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.73it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.51it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.34it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.34it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.58it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.78it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.96it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.13it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.14it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.03it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.77it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.60it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.52it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.66it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.81it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.00it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.09it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.15it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 45.10it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.82it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.67it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 42.81it/s][A
 76%|███████▌  | 818/1083 [00:18<00:06, 43.48it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 43.96it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.24it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.56it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.87it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.97it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.78it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.42it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.51it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.56it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.72it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.88it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.01it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 45.18it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 45.09it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.82it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.62it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.60it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.66it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.75it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.78it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.02it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.09it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 45.17it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 44.94it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.76it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 42.42it/s][A
 88%|████████▊ | 953/1083 [00:21<00:03, 43.21it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 43.66it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.07it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.32it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.58it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 44.65it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 44.72it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.51it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.34it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.50it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.62it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.81it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.99it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 45.05it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 45.07it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 44.92it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.80it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.65it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.60it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.59it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.82it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.98it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 45.03it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 45.09it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 45.07it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 45.01it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 42.51it/s][A                                                 
                                                   [A 40%|████      | 162/405 [01:40<01:00,  4.01it/s]
100%|██████████| 1083/1083 [00:24<00:00, 42.51it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 00:43:41,699 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:43:41,832 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:43:45,294 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:43:45,443 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:43:45,513 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:46<36:43,  9.11s/it] 40%|████      | 164/405 [01:46<25:56,  6.46s/it] 41%|████      | 165/405 [01:46<18:25,  4.60s/it] 41%|████      | 166/405 [01:46<13:10,  3.31s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:47<09:40,  2.44s/it] 41%|████▏     | 168/405 [01:47<07:04,  1.79s/it] 42%|████▏     | 169/405 [01:47<05:17,  1.35s/it] 42%|████▏     | 170/405 [01:48<04:01,  1.03s/it] 42%|████▏     | 171/405 [01:48<03:07,  1.25it/s] 42%|████▏     | 172/405 [01:48<02:30,  1.55it/s] 43%|████▎     | 173/405 [01:48<02:04,  1.86it/s] 43%|████▎     | 174/405 [01:49<01:46,  2.18it/s] 43%|████▎     | 175/405 [01:49<01:33,  2.46it/s] 43%|████▎     | 176/405 [01:49<01:24,  2.72it/s] 44%|████▎     | 177/405 [01:50<01:17,  2.93it/s] 44%|████▍     | 178/405 [01:50<01:13,  3.09it/s] 44%|████▍     | 179/405 [01:50<01:10,  3.22it/s] 44%|████▍     | 180/405 [01:50<01:08,  3.26it/s] 45%|████▍     | 181/405 [01:51<01:07,  3.34it/s] 45%|████▍     | 182/405 [01:51<01:05,  3.40it/s] 45%|████▌     | 183/405 [01:51<01:04,  3.45it/s] 45%|████▌     | 184/405 [01:52<01:03,  3.48it/s] 46%|████▌     | 185/405 [01:52<01:02,  3.51it/s] 46%|████▌     | 186/405 [01:52<01:02,  3.53it/s] 46%|████▌     | 187/405 [01:52<01:01,  3.54it/s] 46%|████▋     | 188/405 [01:53<01:01,  3.55it/s] 47%|████▋     | 189/405 [01:53<01:00,  3.56it/s] 47%|████▋     | 190/405 [01:53<01:00,  3.58it/s] 47%|████▋     | 191/405 [01:54<01:02,  3.45it/s] 47%|████▋     | 192/405 [01:54<01:00,  3.50it/s] 48%|████▊     | 193/405 [01:54<01:00,  3.53it/s] 48%|████▊     | 194/405 [01:54<00:59,  3.55it/s] 48%|████▊     | 195/405 [01:55<00:58,  3.57it/s] 48%|████▊     | 196/405 [01:55<00:58,  3.59it/s] 49%|████▊     | 197/405 [01:55<00:57,  3.60it/s] 49%|████▉     | 198/405 [01:56<00:57,  3.60it/s] 49%|████▉     | 199/405 [01:56<00:57,  3.60it/s] 49%|████▉     | 200/405 [01:56<00:56,  3.60it/s] 50%|████▉     | 201/405 [01:56<00:56,  3.61it/s] 50%|████▉     | 202/405 [01:57<00:58,  3.50it/s] 50%|█████     | 203/405 [01:57<00:57,  3.53it/s] 50%|█████     | 204/405 [01:57<00:56,  3.56it/s] 51%|█████     | 205/405 [01:57<00:55,  3.58it/s] 51%|█████     | 206/405 [01:58<00:55,  3.59it/s] 51%|█████     | 207/405 [01:58<00:55,  3.60it/s] 51%|█████▏    | 208/405 [01:58<00:54,  3.60it/s] 52%|█████▏    | 209/405 [01:59<00:54,  3.60it/s] 52%|█████▏    | 210/405 [01:59<00:57,  3.39it/s] 52%|█████▏    | 211/405 [01:59<00:56,  3.46it/s] 52%|█████▏    | 212/405 [01:59<00:55,  3.50it/s] 53%|█████▎    | 213/405 [02:00<00:54,  3.53it/s] 53%|█████▎    | 214/405 [02:00<00:53,  3.56it/s] 53%|█████▎    | 215/405 [02:00<00:53,  3.58it/s] 53%|█████▎    | 216/405 [02:01<00:52,  3.59it/s] 54%|█████▎    | 217/405 [02:01<00:52,  3.60it/s] 54%|█████▍    | 218/405 [02:01<00:51,  3.60it/s] 54%|█████▍    | 219/405 [02:01<00:51,  3.60it/s] 54%|█████▍    | 220/405 [02:02<00:51,  3.61it/s] 55%|█████▍    | 221/405 [02:02<00:52,  3.52it/s] 55%|█████▍    | 222/405 [02:02<00:51,  3.55it/s] 55%|█████▌    | 223/405 [02:03<00:50,  3.57it/s] 55%|█████▌    | 224/405 [02:03<00:50,  3.59it/s] 56%|█████▌    | 225/405 [02:03<00:50,  3.59it/s] 56%|█████▌    | 226/405 [02:03<00:49,  3.60it/s] 56%|█████▌    | 227/405 [02:04<00:49,  3.60it/s] 56%|█████▋    | 228/405 [02:04<00:49,  3.61it/s] 57%|█████▋    | 229/405 [02:04<00:48,  3.61it/s] 57%|█████▋    | 230/405 [02:04<00:48,  3.61it/s] 57%|█████▋    | 231/405 [02:05<00:48,  3.61it/s] 57%|█████▋    | 232/405 [02:05<00:51,  3.37it/s] 58%|█████▊    | 233/405 [02:05<00:49,  3.44it/s] 58%|█████▊    | 234/405 [02:06<00:48,  3.49it/s] 58%|█████▊    | 235/405 [02:06<00:48,  3.52it/s] 58%|█████▊    | 236/405 [02:06<00:47,  3.55it/s] 59%|█████▊    | 237/405 [02:06<00:47,  3.57it/s] 59%|█████▉    | 238/405 [02:07<00:46,  3.59it/s] 59%|█████▉    | 239/405 [02:07<00:46,  3.59it/s] 59%|█████▉    | 240/405 [02:07<00:45,  3.60it/s] 60%|█████▉    | 241/405 [02:08<00:45,  3.60it/s] 60%|█████▉    | 242/405 [02:08<00:45,  3.60it/s] 60%|██████    | 243/405 [02:08<00:44,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 00:44:09,560 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:44:09,560 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 00:44:09,560 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.3777, 'eval_samples_per_second': 355.16, 'eval_steps_per_second': 44.426, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.20it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.12it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.23it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.50it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.91it/s][A
  3%|▎         | 32/1083 [00:00<00:22, 45.73it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.37it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.02it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.97it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.89it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.05it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.00it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.13it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.98it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.96it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.90it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.75it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.73it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.73it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.80it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.94it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.96it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.98it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.93it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.90it/s][A
 12%|█▏        | 132/1083 [00:02<00:23, 40.48it/s][A
 13%|█▎        | 137/1083 [00:03<00:22, 41.88it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 42.88it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.58it/s][A
 14%|█▍        | 152/1083 [00:03<00:21, 44.11it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.43it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.53it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.58it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.29it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.24it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.42it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.76it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.86it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.11it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.12it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.21it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.96it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.62it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.49it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.52it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.68it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.86it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.04it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.17it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.14it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.04it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.76it/s][A
 25%|██▍       | 267/1083 [00:05<00:19, 42.00it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 42.92it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 43.61it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.01it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.41it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.66it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.84it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.81it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.51it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.64it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.67it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.85it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.83it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.97it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.86it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.95it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.73it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.73it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.78it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.91it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.91it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.04it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.02it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.08it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.89it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.74it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 43.75it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.13it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 44.46it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.64it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.90it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.97it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.01it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.92it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.62it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.68it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.76it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.91it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.95it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.05it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.02it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.01it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.81it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.61it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.69it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.71it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.80it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.92it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.95it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.00it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.89it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.82it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.73it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 43.89it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.24it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 44.41it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.66it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.89it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.93it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.95it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.86it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.62it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.58it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.74it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.84it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.98it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.06it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.05it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.00it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.91it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.73it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.62it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.74it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.72it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.93it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.03it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.06it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.94it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.84it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.75it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 43.66it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.23it/s][A
 63%|██████▎   | 682/1083 [00:15<00:09, 44.38it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.75it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.87it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.91it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.94it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.72it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.72it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.64it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.72it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.83it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.85it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.93it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.99it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.89it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.75it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.72it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.76it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.88it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.84it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.89it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.95it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.91it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.81it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 44.69it/s][A
 75%|███████▍  | 807/1083 [00:18<00:08, 31.29it/s][A
 75%|███████▍  | 812/1083 [00:18<00:07, 34.53it/s][A
 75%|███████▌  | 816/1083 [00:18<00:07, 34.17it/s][A
 76%|███████▌  | 821/1083 [00:18<00:06, 37.47it/s][A
 76%|███████▋  | 826/1083 [00:18<00:06, 39.52it/s][A
 77%|███████▋  | 831/1083 [00:18<00:06, 40.96it/s][A
 77%|███████▋  | 836/1083 [00:18<00:05, 42.17it/s][A
 78%|███████▊  | 841/1083 [00:18<00:05, 43.03it/s][A
 78%|███████▊  | 846/1083 [00:19<00:05, 43.44it/s][A
 79%|███████▊  | 851/1083 [00:19<00:05, 43.80it/s][A
 79%|███████▉  | 856/1083 [00:19<00:05, 44.18it/s][A
 80%|███████▉  | 861/1083 [00:19<00:05, 44.16it/s][A
 80%|███████▉  | 866/1083 [00:19<00:09, 22.95it/s][A
 80%|████████  | 871/1083 [00:19<00:07, 26.96it/s][A
 81%|████████  | 876/1083 [00:20<00:06, 30.71it/s][A
 81%|████████▏ | 881/1083 [00:20<00:05, 33.96it/s][A
 82%|████████▏ | 886/1083 [00:20<00:05, 36.69it/s][A
 82%|████████▏ | 891/1083 [00:20<00:04, 38.91it/s][A
 83%|████████▎ | 896/1083 [00:20<00:04, 40.68it/s][A
 83%|████████▎ | 901/1083 [00:20<00:04, 41.91it/s][A
 84%|████████▎ | 906/1083 [00:20<00:04, 42.54it/s][A
 84%|████████▍ | 911/1083 [00:20<00:04, 42.98it/s][A
 85%|████████▍ | 916/1083 [00:21<00:03, 43.49it/s][A
 85%|████████▌ | 921/1083 [00:21<00:04, 40.20it/s][A
 86%|████████▌ | 926/1083 [00:21<00:04, 36.52it/s][A
 86%|████████▌ | 931/1083 [00:21<00:03, 38.75it/s][A
 86%|████████▋ | 936/1083 [00:21<00:03, 40.51it/s][A
 87%|████████▋ | 941/1083 [00:21<00:03, 41.81it/s][A
 87%|████████▋ | 946/1083 [00:21<00:03, 42.80it/s][A
 88%|████████▊ | 951/1083 [00:21<00:03, 43.49it/s][A
 88%|████████▊ | 956/1083 [00:21<00:02, 43.95it/s][A
 89%|████████▊ | 961/1083 [00:22<00:02, 44.26it/s][A
 89%|████████▉ | 966/1083 [00:22<00:02, 44.13it/s][A
 90%|████████▉ | 971/1083 [00:22<00:02, 44.23it/s][A
 90%|█████████ | 976/1083 [00:22<00:02, 44.37it/s][A
 91%|█████████ | 981/1083 [00:22<00:02, 44.62it/s][A
 91%|█████████ | 986/1083 [00:22<00:02, 44.70it/s][A
 92%|█████████▏| 991/1083 [00:22<00:02, 44.98it/s][A
 92%|█████████▏| 996/1083 [00:22<00:01, 45.01it/s][A
 92%|█████████▏| 1001/1083 [00:22<00:01, 45.05it/s][A
 93%|█████████▎| 1006/1083 [00:23<00:01, 44.84it/s][A
 93%|█████████▎| 1011/1083 [00:23<00:01, 41.53it/s][A
 94%|█████████▍| 1017/1083 [00:23<00:01, 43.96it/s][A
 94%|█████████▍| 1022/1083 [00:23<00:01, 44.31it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 44.50it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.72it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.85it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.98it/s][A
 97%|█████████▋| 1047/1083 [00:24<00:00, 44.86it/s][A
 97%|█████████▋| 1052/1083 [00:24<00:00, 44.64it/s][A
 98%|█████████▊| 1057/1083 [00:24<00:00, 44.44it/s][A
 98%|█████████▊| 1062/1083 [00:24<00:00, 43.80it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 44.26it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.53it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.76it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.87it/s][A                                                 
                                                   [A 60%|██████    | 243/405 [02:33<00:44,  3.65it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.87it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 00:44:34,460 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:44:34,606 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:44:38,055 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:44:38,235 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:44:38,298 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [02:38<24:44,  9.22s/it] 60%|██████    | 245/405 [02:38<17:25,  6.54s/it] 61%|██████    | 246/405 [02:39<12:20,  4.66s/it] 61%|██████    | 247/405 [02:39<08:48,  3.35s/it] 61%|██████    | 248/405 [02:39<06:20,  2.43s/it] 61%|██████▏   | 249/405 [02:40<04:38,  1.78s/it] 62%|██████▏   | 250/405 [02:40<03:26,  1.33s/it] 62%|██████▏   | 251/405 [02:40<02:37,  1.02s/it] 62%|██████▏   | 252/405 [02:40<02:02,  1.25it/s] 62%|██████▏   | 253/405 [02:41<01:37,  1.55it/s] 63%|██████▎   | 254/405 [02:41<01:20,  1.87it/s] 63%|██████▎   | 255/405 [02:41<01:08,  2.18it/s] 63%|██████▎   | 256/405 [02:42<01:00,  2.47it/s] 63%|██████▎   | 257/405 [02:42<00:54,  2.72it/s] 64%|██████▎   | 258/405 [02:42<00:50,  2.93it/s] 64%|██████▍   | 259/405 [02:42<00:47,  3.09it/s] 64%|██████▍   | 260/405 [02:43<00:45,  3.22it/s] 64%|██████▍   | 261/405 [02:43<00:43,  3.32it/s] 65%|██████▍   | 262/405 [02:43<00:43,  3.30it/s] 65%|██████▍   | 263/405 [02:44<00:42,  3.38it/s] 65%|██████▌   | 264/405 [02:44<00:41,  3.43it/s] 65%|██████▌   | 265/405 [02:44<00:40,  3.47it/s] 66%|██████▌   | 266/405 [02:44<00:39,  3.50it/s] 66%|██████▌   | 267/405 [02:45<00:39,  3.52it/s] 66%|██████▌   | 268/405 [02:45<00:38,  3.53it/s] 66%|██████▋   | 269/405 [02:45<00:38,  3.54it/s] 67%|██████▋   | 270/405 [02:46<00:38,  3.55it/s] 67%|██████▋   | 271/405 [02:46<00:37,  3.55it/s] 67%|██████▋   | 272/405 [02:46<00:37,  3.55it/s] 67%|██████▋   | 273/405 [02:46<00:37,  3.48it/s] 68%|██████▊   | 274/405 [02:47<00:37,  3.50it/s] 68%|██████▊   | 275/405 [02:47<00:36,  3.52it/s] 68%|██████▊   | 276/405 [02:47<00:36,  3.53it/s] 68%|██████▊   | 277/405 [02:48<00:36,  3.54it/s] 69%|██████▊   | 278/405 [02:48<00:35,  3.55it/s] 69%|██████▉   | 279/405 [02:48<00:35,  3.55it/s] 69%|██████▉   | 280/405 [02:48<00:35,  3.56it/s] 69%|██████▉   | 281/405 [02:49<00:34,  3.56it/s] 70%|██████▉   | 282/405 [02:49<00:34,  3.56it/s] 70%|██████▉   | 283/405 [02:49<00:34,  3.57it/s] 70%|███████   | 284/405 [02:50<00:34,  3.48it/s] 70%|███████   | 285/405 [02:50<00:34,  3.51it/s] 71%|███████   | 286/405 [02:50<00:33,  3.53it/s] 71%|███████   | 287/405 [02:50<00:33,  3.54it/s] 71%|███████   | 288/405 [02:51<00:32,  3.55it/s] 71%|███████▏  | 289/405 [02:51<00:32,  3.56it/s] 72%|███████▏  | 290/405 [02:51<00:32,  3.56it/s] 72%|███████▏  | 291/405 [02:51<00:31,  3.56it/s] 72%|███████▏  | 292/405 [02:52<00:31,  3.56it/s] 72%|███████▏  | 293/405 [02:52<00:31,  3.57it/s] 73%|███████▎  | 294/405 [02:52<00:31,  3.57it/s] 73%|███████▎  | 295/405 [02:53<00:31,  3.49it/s] 73%|███████▎  | 296/405 [02:53<00:31,  3.51it/s] 73%|███████▎  | 297/405 [02:53<00:30,  3.53it/s] 74%|███████▎  | 298/405 [02:53<00:30,  3.54it/s] 74%|███████▍  | 299/405 [02:54<00:29,  3.55it/s] 74%|███████▍  | 300/405 [02:54<00:29,  3.56it/s] 74%|███████▍  | 301/405 [02:54<00:29,  3.56it/s] 75%|███████▍  | 302/405 [02:55<00:28,  3.56it/s] 75%|███████▍  | 303/405 [02:55<00:28,  3.56it/s] 75%|███████▌  | 304/405 [02:55<00:28,  3.56it/s] 75%|███████▌  | 305/405 [02:55<00:28,  3.56it/s] 76%|███████▌  | 306/405 [02:56<00:28,  3.43it/s] 76%|███████▌  | 307/405 [02:56<00:28,  3.47it/s] 76%|███████▌  | 308/405 [02:56<00:27,  3.50it/s] 76%|███████▋  | 309/405 [02:57<00:27,  3.52it/s] 77%|███████▋  | 310/405 [02:57<00:26,  3.53it/s] 77%|███████▋  | 311/405 [02:57<00:26,  3.54it/s] 77%|███████▋  | 312/405 [02:57<00:26,  3.55it/s] 77%|███████▋  | 313/405 [02:58<00:25,  3.55it/s] 78%|███████▊  | 314/405 [02:58<00:25,  3.55it/s] 78%|███████▊  | 315/405 [02:58<00:25,  3.56it/s] 78%|███████▊  | 316/405 [02:59<00:25,  3.56it/s] 78%|███████▊  | 317/405 [02:59<00:25,  3.47it/s] 79%|███████▊  | 318/405 [02:59<00:24,  3.50it/s] 79%|███████▉  | 319/405 [02:59<00:24,  3.52it/s] 79%|███████▉  | 320/405 [03:00<00:24,  3.53it/s] 79%|███████▉  | 321/405 [03:00<00:23,  3.54it/s] 80%|███████▉  | 322/405 [03:00<00:23,  3.55it/s] 80%|███████▉  | 323/405 [03:01<00:23,  3.56it/s] 80%|████████  | 324/405 [03:01<00:20,  3.95it/s][INFO|trainer.py:2140] 2023-08-28 00:45:02,160 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:45:02,160 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 00:45:02,160 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.8413, 'eval_samples_per_second': 348.532, 'eval_steps_per_second': 43.597, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.06it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.08it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.48it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.46it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.76it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.39it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.17it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.74it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.80it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.95it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.04it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.09it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.11it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.05it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.97it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.80it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.69it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.61it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.78it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.88it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.09it/s][A
 10%|█         | 112/1083 [00:02<00:22, 43.86it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.26it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.45it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.42it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.44it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.46it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.59it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.83it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.83it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.91it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.10it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.02it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.87it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.71it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.67it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.72it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.83it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.91it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.06it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.01it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.92it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.85it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.71it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.74it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.74it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.80it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.97it/s][A
 23%|██▎       | 247/1083 [00:05<00:19, 41.92it/s][A
 23%|██▎       | 252/1083 [00:05<00:19, 42.93it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 43.61it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 43.86it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.06it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.33it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.54it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.66it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.40it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.54it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.60it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.96it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.89it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.89it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.85it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.85it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.85it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.75it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.75it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.79it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.88it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.70it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.85it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.93it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.89it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.85it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.69it/s][A
 35%|███▌      | 382/1083 [00:08<00:16, 42.49it/s][A
 36%|███▌      | 387/1083 [00:08<00:16, 43.33it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 43.92it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.27it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.53it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.67it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 44.68it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.64it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.40it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.55it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.70it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.92it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.01it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.06it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 45.03it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.93it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.70it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.61it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.65it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.76it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.91it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.97it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 45.19it/s][A
 46%|████▌     | 497/1083 [00:11<00:12, 45.11it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.01it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.63it/s][A
 48%|████▊     | 517/1083 [00:11<00:14, 40.05it/s][A
 48%|████▊     | 522/1083 [00:11<00:13, 41.49it/s][A
 49%|████▊     | 527/1083 [00:11<00:13, 42.58it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 43.45it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 43.85it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.28it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 44.50it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.70it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.41it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.44it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.51it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.74it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.94it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 45.00it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 45.03it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.03it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.93it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.62it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.59it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.64it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.78it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.87it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 45.09it/s][A
 58%|█████▊    | 632/1083 [00:14<00:09, 45.14it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 45.05it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.86it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.73it/s][A
 60%|██████    | 652/1083 [00:14<00:10, 41.83it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 42.91it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 43.61it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.10it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.46it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.71it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.74it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.63it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.38it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.45it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.62it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.86it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.98it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 45.06it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.88it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 45.06it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.82it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.56it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.57it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.72it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.96it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.04it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 45.10it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 45.01it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 45.00it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.79it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.65it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.38it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.54it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.79it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.94it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 45.07it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.07it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 45.12it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.90it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.77it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.76it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.76it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.77it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.89it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 45.11it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 45.10it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.06it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.88it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.65it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.61it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.72it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.79it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.98it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.99it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 45.11it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 45.00it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.91it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.82it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 43.03it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 43.72it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.13it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.53it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.71it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.84it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.85it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.69it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.44it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.48it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.55it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.71it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.68it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 45.00it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 45.05it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.06it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.88it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.69it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.65it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.70it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.72it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.94it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.92it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.95it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.96it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.78it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.81it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 40.93it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 42.23it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 43.11it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 43.75it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.23it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.40it/s][A                                                 
                                                   [A 80%|████████  | 324/405 [03:25<00:20,  3.95it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.40it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 00:45:26,551 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:45:26,688 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:45:30,001 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:45:30,166 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:45:30,274 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [03:31<12:11,  9.14s/it] 80%|████████  | 326/405 [03:31<08:32,  6.48s/it] 81%|████████  | 327/405 [03:31<06:00,  4.62s/it] 81%|████████  | 328/405 [03:31<04:15,  3.32s/it] 81%|████████  | 329/405 [03:32<03:03,  2.41s/it] 81%|████████▏ | 330/405 [03:32<02:12,  1.77s/it] 82%|████████▏ | 331/405 [03:32<01:38,  1.34s/it] 82%|████████▏ | 332/405 [03:33<01:14,  1.02s/it] 82%|████████▏ | 333/405 [03:33<00:57,  1.25it/s] 82%|████████▏ | 334/405 [03:33<00:45,  1.56it/s] 83%|████████▎ | 335/405 [03:33<00:37,  1.87it/s] 83%|████████▎ | 336/405 [03:34<00:31,  2.18it/s] 83%|████████▎ | 337/405 [03:34<00:27,  2.47it/s] 83%|████████▎ | 338/405 [03:34<00:24,  2.72it/s] 84%|████████▎ | 339/405 [03:35<00:22,  2.93it/s] 84%|████████▍ | 340/405 [03:35<00:21,  3.09it/s] 84%|████████▍ | 341/405 [03:35<00:19,  3.22it/s] 84%|████████▍ | 342/405 [03:35<00:19,  3.19it/s] 85%|████████▍ | 343/405 [03:36<00:18,  3.29it/s] 85%|████████▍ | 344/405 [03:36<00:18,  3.37it/s] 85%|████████▌ | 345/405 [03:36<00:17,  3.43it/s] 85%|████████▌ | 346/405 [03:37<00:17,  3.47it/s] 86%|████████▌ | 347/405 [03:37<00:16,  3.50it/s] 86%|████████▌ | 348/405 [03:37<00:16,  3.52it/s] 86%|████████▌ | 349/405 [03:37<00:15,  3.54it/s] 86%|████████▋ | 350/405 [03:38<00:15,  3.55it/s] 87%|████████▋ | 351/405 [03:38<00:15,  3.55it/s] 87%|████████▋ | 352/405 [03:38<00:14,  3.56it/s] 87%|████████▋ | 353/405 [03:39<00:15,  3.43it/s] 87%|████████▋ | 354/405 [03:39<00:14,  3.47it/s] 88%|████████▊ | 355/405 [03:39<00:14,  3.49it/s] 88%|████████▊ | 356/405 [03:39<00:13,  3.51it/s] 88%|████████▊ | 357/405 [03:40<00:13,  3.53it/s] 88%|████████▊ | 358/405 [03:40<00:13,  3.54it/s] 89%|████████▊ | 359/405 [03:40<00:12,  3.55it/s] 89%|████████▉ | 360/405 [03:41<00:12,  3.56it/s] 89%|████████▉ | 361/405 [03:41<00:12,  3.56it/s] 89%|████████▉ | 362/405 [03:41<00:12,  3.56it/s] 90%|████████▉ | 363/405 [03:41<00:11,  3.56it/s] 90%|████████▉ | 364/405 [03:42<00:11,  3.49it/s] 90%|█████████ | 365/405 [03:42<00:11,  3.51it/s] 90%|█████████ | 366/405 [03:42<00:11,  3.53it/s] 91%|█████████ | 367/405 [03:43<00:10,  3.54it/s] 91%|█████████ | 368/405 [03:43<00:10,  3.54it/s] 91%|█████████ | 369/405 [03:43<00:10,  3.55it/s] 91%|█████████▏| 370/405 [03:43<00:09,  3.55it/s] 92%|█████████▏| 371/405 [03:44<00:09,  3.55it/s] 92%|█████████▏| 372/405 [03:44<00:09,  3.55it/s] 92%|█████████▏| 373/405 [03:44<00:09,  3.55it/s] 92%|█████████▏| 374/405 [03:44<00:08,  3.55it/s] 93%|█████████▎| 375/405 [03:45<00:08,  3.44it/s] 93%|█████████▎| 376/405 [03:45<00:08,  3.47it/s] 93%|█████████▎| 377/405 [03:45<00:08,  3.50it/s] 93%|█████████▎| 378/405 [03:46<00:07,  3.52it/s] 94%|█████████▎| 379/405 [03:46<00:07,  3.53it/s] 94%|█████████▍| 380/405 [03:46<00:07,  3.53it/s] 94%|█████████▍| 381/405 [03:46<00:06,  3.54it/s] 94%|█████████▍| 382/405 [03:47<00:06,  3.55it/s] 95%|█████████▍| 383/405 [03:47<00:06,  3.55it/s] 95%|█████████▍| 384/405 [03:47<00:05,  3.55it/s] 95%|█████████▌| 385/405 [03:48<00:05,  3.56it/s] 95%|█████████▌| 386/405 [03:48<00:05,  3.47it/s] 96%|█████████▌| 387/405 [03:48<00:05,  3.50it/s] 96%|█████████▌| 388/405 [03:48<00:04,  3.52it/s] 96%|█████████▌| 389/405 [03:49<00:04,  3.53it/s] 96%|█████████▋| 390/405 [03:49<00:04,  3.54it/s] 97%|█████████▋| 391/405 [03:49<00:03,  3.54it/s] 97%|█████████▋| 392/405 [03:50<00:03,  3.55it/s] 97%|█████████▋| 393/405 [03:50<00:03,  3.55it/s] 97%|█████████▋| 394/405 [03:50<00:03,  3.55it/s] 98%|█████████▊| 395/405 [03:50<00:02,  3.55it/s] 98%|█████████▊| 396/405 [03:51<00:02,  3.55it/s] 98%|█████████▊| 397/405 [03:51<00:02,  3.41it/s] 98%|█████████▊| 398/405 [03:51<00:02,  3.45it/s] 99%|█████████▊| 399/405 [03:52<00:01,  3.48it/s] 99%|█████████▉| 400/405 [03:52<00:01,  3.51it/s] 99%|█████████▉| 401/405 [03:52<00:01,  3.53it/s] 99%|█████████▉| 402/405 [03:52<00:00,  3.54it/s]100%|█████████▉| 403/405 [03:53<00:00,  3.55it/s]100%|█████████▉| 404/405 [03:53<00:00,  3.55it/s]100%|██████████| 405/405 [03:53<00:00,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 00:45:54,648 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:45:54,648 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 00:45:54,648 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2898, 'eval_samples_per_second': 356.445, 'eval_steps_per_second': 44.587, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.12it/s][A
  1%|          | 12/1083 [00:00<00:21, 48.83it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.09it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.47it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.17it/s][A
  3%|▎         | 32/1083 [00:00<00:24, 43.71it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 43.98it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.08it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.40it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.68it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.69it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 44.74it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 44.88it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.88it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.96it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.88it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.76it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.77it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.93it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.97it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.02it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.01it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.00it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.03it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.84it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.88it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.89it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.98it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.87it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.35it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.49it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.66it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.70it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.69it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.67it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.78it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.73it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.80it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.94it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.95it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.99it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.90it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.87it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.79it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.79it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.82it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.83it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.93it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.99it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.00it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.08it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.91it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.86it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.89it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.83it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.18it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.47it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.66it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.84it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.89it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.91it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.87it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.93it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.87it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.81it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.93it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.84it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.98it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.97it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.93it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 45.10it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.97it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.95it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.87it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.88it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.87it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.88it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.87it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.90it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.97it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.98it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.87it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 42.96it/s][A
 39%|███▉      | 422/1083 [00:09<00:15, 43.63it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.13it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.44it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.58it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.75it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.82it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.93it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.76it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.65it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.79it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.93it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.05it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.00it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 45.01it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 45.03it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 45.00it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.99it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.91it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.87it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.91it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.99it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.90it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.94it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.95it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.87it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.84it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 43.47it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 43.99it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.32it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.51it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.64it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.73it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.79it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.83it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.66it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.62it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.74it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.98it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.07it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.00it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 45.05it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.99it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.88it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.79it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.69it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.74it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.74it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.94it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.93it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 45.00it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.97it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.95it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.87it/s][A
 63%|██████▎   | 687/1083 [00:15<00:10, 38.81it/s][A
 64%|██████▍   | 692/1083 [00:15<00:09, 40.56it/s][A
 64%|██████▍   | 697/1083 [00:15<00:09, 41.94it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 42.86it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 43.57it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.04it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.37it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.64it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 44.39it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.37it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.54it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.71it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.83it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.97it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.05it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 45.13it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.94it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.85it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.52it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.70it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.85it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.92it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.93it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 45.05it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 45.08it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.14it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.88it/s][A
 76%|███████▌  | 822/1083 [00:18<00:06, 40.97it/s][A
 76%|███████▋  | 827/1083 [00:18<00:06, 42.25it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 43.21it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 43.77it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.20it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.49it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.78it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.83it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.32it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.30it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.51it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.69it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.95it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.05it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 45.14it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 45.09it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.95it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.72it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.48it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.68it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.78it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.96it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.06it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 45.11it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 45.09it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.97it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.67it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 42.15it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 43.05it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 43.71it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.21it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.48it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.71it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.87it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.85it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.52it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.44it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.63it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.84it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.96it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.98it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 45.06it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 45.06it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.92it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.70it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.57it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.63it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.80it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.93it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 45.03it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 45.16it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 45.16it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.04it/s][A                                                 
                                                   [A100%|██████████| 405/405 [04:17<00:00,  3.94it/s]
100%|██████████| 1083/1083 [00:24<00:00, 45.04it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 00:46:19,176 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:46:19,404 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:46:21,995 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:46:22,092 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:46:22,174 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:46:23,221 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:46:23,246 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81 (score: 0.9504339694976807).
                                                 100%|██████████| 405/405 [04:30<00:00,  3.94it/s]100%|██████████| 405/405 [04:30<00:00,  1.49it/s]
[INFO|trainer.py:1894] 2023-08-28 00:46:32,587 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:46:32,734 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:46:39,251 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:46:39,435 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:46:39,528 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:46:40,059 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:46:40,060 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:46:40,060 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:46:40,060 >>   train_runtime            = 0:04:30.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:46:40,060 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:46:40,060 >>   train_samples_per_second =     95.244
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:46:40,060 >>   train_steps_per_second   =      1.495
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2616, 'eval_samples_per_second': 356.86, 'eval_steps_per_second': 44.638, 'epoch': 5.0}
{'train_runtime': 270.8835, 'train_samples_per_second': 95.244, 'train_steps_per_second': 1.495, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:46:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:46:40,361 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:46:40,361 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 00:46:40,361 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.69it/s]  1%|          | 12/1083 [00:00<00:21, 49.46it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.86it/s]  2%|▏         | 22/1083 [00:00<00:22, 46.93it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.48it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.12it/s]  3%|▎         | 37/1083 [00:00<00:22, 46.01it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.69it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.10it/s]  5%|▍         | 52/1083 [00:01<00:23, 44.78it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.70it/s]  6%|▌         | 62/1083 [00:01<00:22, 44.89it/s]  6%|▌         | 67/1083 [00:01<00:22, 44.97it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.18it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.30it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.45it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.36it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.61it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.67it/s]  9%|▉         | 102/1083 [00:02<00:22, 44.57it/s] 10%|▉         | 107/1083 [00:02<00:21, 44.73it/s] 10%|█         | 112/1083 [00:02<00:21, 44.84it/s] 11%|█         | 117/1083 [00:02<00:21, 44.95it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.12it/s] 12%|█▏        | 127/1083 [00:02<00:21, 45.22it/s] 12%|█▏        | 132/1083 [00:02<00:21, 45.17it/s] 13%|█▎        | 137/1083 [00:03<00:21, 44.97it/s] 13%|█▎        | 142/1083 [00:03<00:20, 44.86it/s] 14%|█▎        | 147/1083 [00:03<00:20, 44.73it/s] 14%|█▍        | 152/1083 [00:03<00:20, 44.71it/s] 14%|█▍        | 157/1083 [00:03<00:20, 44.95it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.04it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.22it/s] 16%|█▌        | 172/1083 [00:03<00:20, 45.21it/s] 16%|█▋        | 177/1083 [00:03<00:20, 45.11it/s] 17%|█▋        | 182/1083 [00:04<00:19, 45.06it/s] 17%|█▋        | 187/1083 [00:04<00:19, 44.85it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.77it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.72it/s] 19%|█▊        | 202/1083 [00:04<00:19, 44.77it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.01it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.14it/s] 20%|██        | 217/1083 [00:04<00:19, 45.22it/s] 20%|██        | 222/1083 [00:04<00:19, 45.14it/s] 21%|██        | 227/1083 [00:05<00:19, 44.22it/s] 21%|██▏       | 232/1083 [00:05<00:19, 44.27it/s] 22%|██▏       | 237/1083 [00:05<00:19, 44.45it/s] 22%|██▏       | 242/1083 [00:05<00:18, 44.47it/s] 23%|██▎       | 247/1083 [00:05<00:18, 44.68it/s] 23%|██▎       | 252/1083 [00:05<00:18, 44.84it/s] 24%|██▎       | 257/1083 [00:05<00:18, 45.05it/s] 24%|██▍       | 262/1083 [00:05<00:18, 45.09it/s] 25%|██▍       | 267/1083 [00:05<00:18, 45.01it/s] 25%|██▌       | 272/1083 [00:06<00:18, 44.91it/s] 26%|██▌       | 277/1083 [00:06<00:17, 44.85it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.75it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.77it/s] 27%|██▋       | 292/1083 [00:06<00:17, 44.87it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.02it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.10it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.21it/s] 29%|██▉       | 312/1083 [00:06<00:17, 45.02it/s] 29%|██▉       | 317/1083 [00:07<00:17, 44.91it/s] 30%|██▉       | 322/1083 [00:07<00:17, 44.76it/s] 30%|███       | 327/1083 [00:07<00:16, 44.66it/s] 31%|███       | 332/1083 [00:07<00:16, 44.74it/s] 31%|███       | 337/1083 [00:07<00:16, 44.91it/s] 32%|███▏      | 342/1083 [00:07<00:16, 44.96it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.15it/s] 33%|███▎      | 352/1083 [00:07<00:16, 45.08it/s] 33%|███▎      | 357/1083 [00:07<00:16, 45.04it/s] 33%|███▎      | 362/1083 [00:08<00:16, 44.45it/s] 34%|███▍      | 367/1083 [00:08<00:16, 44.44it/s] 34%|███▍      | 372/1083 [00:08<00:15, 44.46it/s] 35%|███▍      | 377/1083 [00:08<00:15, 44.69it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.72it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.93it/s] 36%|███▌      | 392/1083 [00:08<00:15, 45.05it/s] 37%|███▋      | 397/1083 [00:08<00:15, 45.15it/s] 37%|███▋      | 402/1083 [00:08<00:15, 45.04it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.76it/s] 38%|███▊      | 412/1083 [00:09<00:14, 44.74it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.74it/s] 39%|███▉      | 422/1083 [00:09<00:14, 44.89it/s] 39%|███▉      | 427/1083 [00:09<00:14, 44.91it/s] 40%|███▉      | 432/1083 [00:09<00:14, 44.97it/s] 40%|████      | 437/1083 [00:09<00:14, 45.13it/s] 41%|████      | 442/1083 [00:09<00:14, 45.12it/s] 41%|████▏     | 447/1083 [00:09<00:14, 45.05it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.78it/s] 42%|████▏     | 457/1083 [00:10<00:14, 44.68it/s] 43%|████▎     | 462/1083 [00:10<00:13, 44.73it/s] 43%|████▎     | 467/1083 [00:10<00:13, 44.88it/s] 44%|████▎     | 472/1083 [00:10<00:13, 44.95it/s] 44%|████▍     | 477/1083 [00:10<00:13, 45.11it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.08it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.03it/s] 45%|████▌     | 492/1083 [00:10<00:13, 45.00it/s] 46%|████▌     | 497/1083 [00:11<00:13, 43.75it/s] 46%|████▋     | 502/1083 [00:11<00:13, 44.01it/s] 47%|████▋     | 507/1083 [00:11<00:12, 44.31it/s] 47%|████▋     | 512/1083 [00:11<00:12, 44.41it/s] 48%|████▊     | 517/1083 [00:11<00:12, 44.52it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.73it/s] 49%|████▊     | 527/1083 [00:11<00:12, 44.92it/s] 49%|████▉     | 532/1083 [00:11<00:12, 45.05it/s] 50%|████▉     | 537/1083 [00:11<00:12, 44.90it/s] 50%|█████     | 542/1083 [00:12<00:12, 44.83it/s] 51%|█████     | 547/1083 [00:12<00:11, 44.83it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.90it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 44.92it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 44.89it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 44.94it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 44.90it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 44.94it/s] 54%|█████▎    | 582/1083 [00:12<00:11, 44.94it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 44.70it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 44.78it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 44.85it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.89it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 44.91it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 44.89it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 44.95it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 44.53it/s] 58%|█████▊    | 627/1083 [00:13<00:10, 44.73it/s] 58%|█████▊    | 632/1083 [00:14<00:10, 44.58it/s] 59%|█████▉    | 637/1083 [00:14<00:09, 44.70it/s] 59%|█████▉    | 642/1083 [00:14<00:09, 44.90it/s] 60%|█████▉    | 647/1083 [00:14<00:09, 44.89it/s] 60%|██████    | 652/1083 [00:14<00:09, 44.82it/s] 61%|██████    | 657/1083 [00:14<00:09, 44.85it/s] 61%|██████    | 662/1083 [00:14<00:09, 44.90it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 44.93it/s] 62%|██████▏   | 672/1083 [00:14<00:09, 44.84it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.81it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.81it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 45.00it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.94it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 44.94it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 44.92it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 44.81it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 44.90it/s] 66%|██████▌   | 717/1083 [00:15<00:08, 44.80it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 44.81it/s] 67%|██████▋   | 727/1083 [00:16<00:07, 44.86it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.88it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.88it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.91it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 44.89it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 44.85it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 44.81it/s] 70%|███████   | 762/1083 [00:16<00:07, 44.76it/s] 71%|███████   | 767/1083 [00:17<00:07, 44.20it/s] 71%|███████▏  | 772/1083 [00:17<00:07, 44.33it/s] 72%|███████▏  | 777/1083 [00:17<00:06, 44.65it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 44.81it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 44.71it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.73it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 44.70it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 44.81it/s] 75%|███████▍  | 807/1083 [00:17<00:06, 44.76it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 44.80it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.89it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.98it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 45.03it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 44.92it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 44.86it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 44.78it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 44.68it/s] 79%|███████▊  | 852/1083 [00:18<00:05, 44.70it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.78it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.87it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.94it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.88it/s] 81%|████████  | 877/1083 [00:19<00:04, 44.89it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 44.79it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 44.79it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 44.63it/s] 83%|████████▎ | 897/1083 [00:19<00:04, 44.67it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 44.77it/s] 84%|████████▎ | 907/1083 [00:20<00:03, 44.90it/s] 84%|████████▍ | 912/1083 [00:20<00:03, 44.92it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 44.97it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 44.87it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 44.75it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 44.67it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 44.74it/s] 87%|████████▋ | 942/1083 [00:20<00:03, 44.77it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.95it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 44.88it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 44.97it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 44.98it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 44.91it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 44.85it/s] 90%|█████████ | 977/1083 [00:21<00:02, 44.70it/s] 91%|█████████ | 982/1083 [00:21<00:02, 44.63it/s] 91%|█████████ | 987/1083 [00:21<00:02, 43.83it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 44.29it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 44.59it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 44.70it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 44.87it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 44.87it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 44.83it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 44.84it/s] 95%|█████████▍| 1027/1083 [00:22<00:01, 44.70it/s] 95%|█████████▌| 1032/1083 [00:22<00:01, 44.72it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 44.77it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 44.82it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 44.92it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 44.88it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 44.94it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 44.96it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 44.85it/s] 99%|█████████▉| 1072/1083 [00:23<00:00, 44.75it/s] 99%|█████████▉| 1077/1083 [00:23<00:00, 44.72it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 44.81it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.89it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:47:04,502 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   eval_loss               =     0.9504
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   eval_runtime            = 0:00:24.14
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   eval_samples_per_second =    358.642
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   eval_steps_per_second   =     44.861
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:47:04,503 >>   perplexity              =     2.5868
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:14,887 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:14,922 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:14,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:14,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:14,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:47:15,567 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:47:15,568 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:47:16,184 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:47:17,225 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:47:17,225 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:20,286 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:20,310 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:20,310 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:20,310 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:47:20,310 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:47:21,062 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:47:21,063 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:47:21,657 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:47:21,806 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:47:21,806 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-162
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-405
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-243
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-324
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/checkpoint-81
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.77it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.71it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:09,  1.72it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:11,  1.62it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.71it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:15,  1.75it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:17,  1.74it/s]Extractor Predicting: 31it [00:18,  1.76it/s]Extractor Predicting: 32it [00:18,  1.74it/s]Extractor Predicting: 33it [00:19,  1.74it/s]Extractor Predicting: 34it [00:19,  1.75it/s]Extractor Predicting: 35it [00:20,  1.74it/s]Extractor Predicting: 36it [00:21,  1.71it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:22,  1.71it/s]Extractor Predicting: 39it [00:22,  1.73it/s]Extractor Predicting: 40it [00:23,  1.76it/s]Extractor Predicting: 41it [00:24,  1.63it/s]Extractor Predicting: 42it [00:24,  1.69it/s]Extractor Predicting: 43it [00:25,  1.72it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.69it/s]Extractor Predicting: 46it [00:26,  1.71it/s]Extractor Predicting: 47it [00:27,  1.57it/s]Extractor Predicting: 48it [00:28,  1.58it/s]Extractor Predicting: 49it [00:28,  1.57it/s]Extractor Predicting: 50it [00:29,  1.61it/s]Extractor Predicting: 51it [00:30,  1.67it/s]Extractor Predicting: 52it [00:30,  1.67it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:32,  1.70it/s]Extractor Predicting: 56it [00:33,  1.72it/s]Extractor Predicting: 57it [00:33,  1.73it/s]Extractor Predicting: 58it [00:34,  1.72it/s]Extractor Predicting: 59it [00:34,  1.68it/s]Extractor Predicting: 60it [00:35,  1.63it/s]Extractor Predicting: 61it [00:36,  1.63it/s]Extractor Predicting: 62it [00:36,  1.67it/s]Extractor Predicting: 63it [00:37,  1.77it/s]Extractor Predicting: 64it [00:37,  1.80it/s]Extractor Predicting: 65it [00:38,  1.77it/s]Extractor Predicting: 66it [00:38,  1.78it/s]Extractor Predicting: 67it [00:39,  1.70it/s]Extractor Predicting: 68it [00:40,  1.74it/s]Extractor Predicting: 69it [00:40,  1.76it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:41,  1.69it/s]Extractor Predicting: 72it [00:42,  1.71it/s]Extractor Predicting: 73it [00:42,  1.73it/s]Extractor Predicting: 74it [00:43,  1.75it/s]Extractor Predicting: 75it [00:44,  1.72it/s]Extractor Predicting: 76it [00:44,  1.74it/s]Extractor Predicting: 77it [00:45,  1.72it/s]Extractor Predicting: 78it [00:45,  1.73it/s]Extractor Predicting: 79it [00:46,  1.69it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:47,  1.75it/s]Extractor Predicting: 82it [00:48,  1.74it/s]Extractor Predicting: 83it [00:48,  1.73it/s]Extractor Predicting: 84it [00:49,  1.75it/s]Extractor Predicting: 85it [00:49,  1.67it/s]Extractor Predicting: 86it [00:50,  1.66it/s]Extractor Predicting: 87it [00:51,  1.65it/s]Extractor Predicting: 88it [00:51,  1.66it/s]Extractor Predicting: 89it [00:52,  1.70it/s]Extractor Predicting: 90it [00:52,  1.67it/s]Extractor Predicting: 91it [00:53,  1.73it/s]Extractor Predicting: 92it [00:54,  1.73it/s]Extractor Predicting: 93it [00:54,  1.71it/s]Extractor Predicting: 94it [00:55,  1.72it/s]Extractor Predicting: 95it [00:55,  1.75it/s]Extractor Predicting: 96it [00:56,  1.69it/s]Extractor Predicting: 97it [00:56,  1.70it/s]Extractor Predicting: 98it [00:57,  1.69it/s]Extractor Predicting: 99it [00:58,  1.71it/s]Extractor Predicting: 100it [00:58,  1.71it/s]Extractor Predicting: 101it [00:59,  1.73it/s]Extractor Predicting: 102it [00:59,  1.73it/s]Extractor Predicting: 103it [01:00,  1.73it/s]Extractor Predicting: 104it [01:01,  1.73it/s]Extractor Predicting: 105it [01:01,  1.73it/s]Extractor Predicting: 106it [01:02,  1.74it/s]Extractor Predicting: 107it [01:02,  1.72it/s]Extractor Predicting: 108it [01:03,  1.70it/s]Extractor Predicting: 109it [01:03,  1.71it/s]Extractor Predicting: 110it [01:04,  1.72it/s]Extractor Predicting: 111it [01:05,  1.71it/s]Extractor Predicting: 112it [01:05,  1.72it/s]Extractor Predicting: 113it [01:06,  1.73it/s]Extractor Predicting: 114it [01:06,  1.75it/s]Extractor Predicting: 115it [01:07,  1.70it/s]Extractor Predicting: 116it [01:08,  1.68it/s]Extractor Predicting: 117it [01:08,  1.73it/s]Extractor Predicting: 118it [01:09,  1.73it/s]Extractor Predicting: 119it [01:09,  1.75it/s]Extractor Predicting: 120it [01:10,  1.71it/s]Extractor Predicting: 121it [01:10,  1.68it/s]Extractor Predicting: 122it [01:11,  1.81it/s]Extractor Predicting: 123it [01:11,  1.80it/s]Extractor Predicting: 124it [01:12,  1.76it/s]Extractor Predicting: 125it [01:13,  1.70it/s]Extractor Predicting: 126it [01:13,  1.64it/s]Extractor Predicting: 127it [01:14,  1.69it/s]Extractor Predicting: 128it [01:15,  1.70it/s]Extractor Predicting: 129it [01:15,  1.71it/s]Extractor Predicting: 130it [01:16,  1.72it/s]Extractor Predicting: 131it [01:16,  1.77it/s]Extractor Predicting: 132it [01:17,  1.52it/s]Extractor Predicting: 133it [01:18,  1.55it/s]Extractor Predicting: 134it [01:18,  1.61it/s]Extractor Predicting: 135it [01:19,  1.65it/s]Extractor Predicting: 136it [01:19,  1.66it/s]Extractor Predicting: 137it [01:20,  1.65it/s]Extractor Predicting: 138it [01:21,  1.67it/s]Extractor Predicting: 139it [01:21,  1.69it/s]Extractor Predicting: 140it [01:22,  1.70it/s]Extractor Predicting: 141it [01:22,  1.73it/s]Extractor Predicting: 142it [01:23,  1.74it/s]Extractor Predicting: 143it [01:23,  1.71it/s]Extractor Predicting: 144it [01:24,  1.73it/s]Extractor Predicting: 145it [01:25,  1.71it/s]Extractor Predicting: 146it [01:25,  1.69it/s]Extractor Predicting: 147it [01:26,  1.73it/s]Extractor Predicting: 148it [01:26,  1.75it/s]Extractor Predicting: 149it [01:27,  1.77it/s]Extractor Predicting: 150it [01:27,  1.77it/s]Extractor Predicting: 151it [01:28,  1.78it/s]Extractor Predicting: 152it [01:29,  1.71it/s]Extractor Predicting: 153it [01:29,  1.64it/s]Extractor Predicting: 154it [01:30,  1.62it/s]Extractor Predicting: 155it [01:31,  1.57it/s]Extractor Predicting: 156it [01:31,  1.56it/s]Extractor Predicting: 157it [01:32,  1.55it/s]Extractor Predicting: 158it [01:33,  1.55it/s]Extractor Predicting: 159it [01:33,  1.55it/s]Extractor Predicting: 160it [01:34,  1.53it/s]Extractor Predicting: 161it [01:35,  1.53it/s]Extractor Predicting: 162it [01:35,  1.53it/s]Extractor Predicting: 163it [01:36,  1.53it/s]Extractor Predicting: 164it [01:37,  1.53it/s]Extractor Predicting: 165it [01:37,  1.54it/s]Extractor Predicting: 166it [01:38,  1.56it/s]Extractor Predicting: 167it [01:38,  1.55it/s]Extractor Predicting: 168it [01:39,  1.58it/s]Extractor Predicting: 169it [01:40,  1.63it/s]Extractor Predicting: 170it [01:40,  1.66it/s]Extractor Predicting: 171it [01:41,  1.67it/s]Extractor Predicting: 172it [01:41,  1.65it/s]Extractor Predicting: 173it [01:42,  1.65it/s]Extractor Predicting: 174it [01:43,  1.65it/s]Extractor Predicting: 175it [01:43,  1.64it/s]Extractor Predicting: 176it [01:44,  1.60it/s]Extractor Predicting: 177it [01:45,  1.60it/s]Extractor Predicting: 178it [01:45,  1.59it/s]Extractor Predicting: 179it [01:46,  1.62it/s]Extractor Predicting: 180it [01:46,  1.61it/s]Extractor Predicting: 181it [01:47,  1.56it/s]Extractor Predicting: 182it [01:48,  1.59it/s]Extractor Predicting: 183it [01:48,  1.66it/s]Extractor Predicting: 184it [01:49,  1.68it/s]Extractor Predicting: 185it [01:49,  1.73it/s]Extractor Predicting: 186it [01:50,  1.69it/s]Extractor Predicting: 187it [01:51,  1.64it/s]Extractor Predicting: 188it [01:51,  1.66it/s]Extractor Predicting: 189it [01:52,  1.69it/s]Extractor Predicting: 190it [01:52,  1.68it/s]Extractor Predicting: 191it [01:53,  1.67it/s]Extractor Predicting: 192it [01:54,  1.65it/s]Extractor Predicting: 193it [01:54,  1.69it/s]Extractor Predicting: 194it [01:55,  1.71it/s]Extractor Predicting: 195it [01:55,  1.69it/s]Extractor Predicting: 196it [01:56,  1.67it/s]Extractor Predicting: 197it [01:57,  1.70it/s]Extractor Predicting: 198it [01:57,  1.64it/s]Extractor Predicting: 199it [01:58,  1.70it/s]Extractor Predicting: 200it [01:58,  1.70it/s]Extractor Predicting: 201it [01:59,  1.72it/s]Extractor Predicting: 202it [01:59,  1.70it/s]Extractor Predicting: 203it [02:00,  1.69it/s]Extractor Predicting: 204it [02:01,  1.63it/s]Extractor Predicting: 205it [02:01,  1.64it/s]Extractor Predicting: 206it [02:02,  1.67it/s]Extractor Predicting: 207it [02:03,  1.68it/s]Extractor Predicting: 208it [02:03,  1.71it/s]Extractor Predicting: 209it [02:04,  1.73it/s]Extractor Predicting: 210it [02:04,  1.70it/s]Extractor Predicting: 211it [02:05,  1.70it/s]Extractor Predicting: 212it [02:05,  1.71it/s]Extractor Predicting: 213it [02:06,  1.72it/s]Extractor Predicting: 214it [02:07,  1.76it/s]Extractor Predicting: 215it [02:07,  1.79it/s]Extractor Predicting: 216it [02:08,  1.76it/s]Extractor Predicting: 217it [02:08,  1.76it/s]Extractor Predicting: 218it [02:09,  1.76it/s]Extractor Predicting: 219it [02:09,  1.76it/s]Extractor Predicting: 220it [02:10,  1.74it/s]Extractor Predicting: 221it [02:10,  1.78it/s]Extractor Predicting: 222it [02:11,  1.56it/s]Extractor Predicting: 223it [02:12,  1.61it/s]Extractor Predicting: 224it [02:12,  1.62it/s]Extractor Predicting: 225it [02:13,  1.59it/s]Extractor Predicting: 226it [02:14,  1.64it/s]Extractor Predicting: 227it [02:14,  1.63it/s]Extractor Predicting: 228it [02:15,  1.65it/s]Extractor Predicting: 229it [02:15,  1.71it/s]Extractor Predicting: 230it [02:16,  1.69it/s]Extractor Predicting: 231it [02:17,  1.68it/s]Extractor Predicting: 232it [02:17,  1.72it/s]Extractor Predicting: 233it [02:18,  1.72it/s]Extractor Predicting: 234it [02:18,  1.70it/s]Extractor Predicting: 235it [02:19,  1.69it/s]Extractor Predicting: 236it [02:20,  1.67it/s]Extractor Predicting: 237it [02:20,  1.68it/s]Extractor Predicting: 238it [02:21,  1.71it/s]Extractor Predicting: 239it [02:21,  1.72it/s]Extractor Predicting: 240it [02:22,  1.71it/s]Extractor Predicting: 241it [02:22,  1.73it/s]Extractor Predicting: 242it [02:23,  1.75it/s]Extractor Predicting: 243it [02:24,  1.70it/s]Extractor Predicting: 244it [02:24,  1.72it/s]Extractor Predicting: 245it [02:25,  1.70it/s]Extractor Predicting: 246it [02:25,  1.70it/s]Extractor Predicting: 247it [02:26,  1.71it/s]Extractor Predicting: 248it [02:27,  1.74it/s]Extractor Predicting: 249it [02:27,  1.73it/s]Extractor Predicting: 250it [02:28,  1.71it/s]Extractor Predicting: 251it [02:28,  1.66it/s]Extractor Predicting: 252it [02:29,  1.69it/s]Extractor Predicting: 253it [02:30,  1.70it/s]Extractor Predicting: 254it [02:30,  1.72it/s]Extractor Predicting: 255it [02:31,  1.70it/s]Extractor Predicting: 256it [02:31,  1.73it/s]Extractor Predicting: 257it [02:32,  1.71it/s]Extractor Predicting: 258it [02:32,  1.74it/s]Extractor Predicting: 259it [02:33,  1.72it/s]Extractor Predicting: 260it [02:34,  1.69it/s]Extractor Predicting: 261it [02:34,  1.72it/s]Extractor Predicting: 262it [02:35,  1.72it/s]Extractor Predicting: 263it [02:35,  1.65it/s]Extractor Predicting: 264it [02:36,  1.65it/s]Extractor Predicting: 265it [02:37,  1.67it/s]Extractor Predicting: 266it [02:37,  1.64it/s]Extractor Predicting: 267it [02:38,  1.67it/s]Extractor Predicting: 268it [02:38,  1.65it/s]Extractor Predicting: 269it [02:39,  1.76it/s]Extractor Predicting: 269it [02:39,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:14,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:14,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:14,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:14,120 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:14,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:50:15,178 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:50:15,179 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:50:15,870 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:50:17,056 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:50:17,056 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:20,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:20,415 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:20,416 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:20,416 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:50:20,416 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:50:21,138 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:50:21,139 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:50:21,761 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:50:21,937 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:50:21,937 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:11,  1.66it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.62it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.51it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:23,  1.71it/s]Extractor Predicting: 40it [00:24,  1.72it/s]Extractor Predicting: 41it [00:25,  1.73it/s]Extractor Predicting: 42it [00:25,  1.76it/s]Extractor Predicting: 43it [00:26,  1.79it/s]Extractor Predicting: 44it [00:26,  1.76it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:30,  1.75it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:31,  1.74it/s]Extractor Predicting: 53it [00:31,  1.76it/s]Extractor Predicting: 54it [00:32,  1.78it/s]Extractor Predicting: 55it [00:33,  1.77it/s]Extractor Predicting: 56it [00:33,  1.80it/s]Extractor Predicting: 57it [00:34,  1.76it/s]Extractor Predicting: 58it [00:34,  1.77it/s]Extractor Predicting: 59it [00:35,  1.75it/s]Extractor Predicting: 60it [00:35,  1.78it/s]Extractor Predicting: 61it [00:36,  1.76it/s]Extractor Predicting: 62it [00:37,  1.74it/s]Extractor Predicting: 63it [00:37,  1.68it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:38,  1.69it/s]Extractor Predicting: 66it [00:39,  1.69it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:40,  1.64it/s]Extractor Predicting: 69it [00:41,  1.69it/s]Extractor Predicting: 70it [00:41,  1.72it/s]Extractor Predicting: 71it [00:42,  1.75it/s]Extractor Predicting: 72it [00:43,  1.70it/s]Extractor Predicting: 73it [00:43,  1.71it/s]Extractor Predicting: 74it [00:44,  1.69it/s]Extractor Predicting: 75it [00:44,  1.70it/s]Extractor Predicting: 76it [00:45,  1.74it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:46,  1.70it/s]Extractor Predicting: 79it [00:47,  1.74it/s]Extractor Predicting: 80it [00:47,  1.71it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:48,  1.78it/s]Extractor Predicting: 83it [00:49,  1.78it/s]Extractor Predicting: 84it [00:49,  1.83it/s]Extractor Predicting: 85it [00:50,  1.84it/s]Extractor Predicting: 86it [00:50,  1.80it/s]Extractor Predicting: 87it [00:51,  1.73it/s]Extractor Predicting: 88it [00:52,  1.75it/s]Extractor Predicting: 89it [00:52,  1.72it/s]Extractor Predicting: 90it [00:53,  1.78it/s]Extractor Predicting: 91it [00:53,  1.77it/s]Extractor Predicting: 92it [00:54,  1.76it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:55,  1.70it/s]Extractor Predicting: 95it [00:56,  1.81it/s]Extractor Predicting: 96it [00:56,  1.79it/s]Extractor Predicting: 97it [00:57,  1.78it/s]Extractor Predicting: 98it [00:57,  1.75it/s]Extractor Predicting: 99it [00:58,  1.66it/s]Extractor Predicting: 100it [00:59,  1.70it/s]Extractor Predicting: 101it [00:59,  1.75it/s]Extractor Predicting: 102it [01:00,  1.80it/s]Extractor Predicting: 103it [01:00,  1.79it/s]Extractor Predicting: 104it [01:01,  1.80it/s]Extractor Predicting: 105it [01:01,  1.80it/s]Extractor Predicting: 106it [01:02,  1.86it/s]Extractor Predicting: 107it [01:02,  1.85it/s]Extractor Predicting: 108it [01:03,  1.73it/s]Extractor Predicting: 109it [01:04,  1.80it/s]Extractor Predicting: 110it [01:04,  1.80it/s]Extractor Predicting: 111it [01:05,  1.82it/s]Extractor Predicting: 112it [01:05,  1.83it/s]Extractor Predicting: 113it [01:06,  1.83it/s]Extractor Predicting: 114it [01:06,  1.75it/s]Extractor Predicting: 115it [01:07,  1.71it/s]Extractor Predicting: 116it [01:08,  1.70it/s]Extractor Predicting: 117it [01:08,  1.63it/s]Extractor Predicting: 118it [01:09,  1.64it/s]Extractor Predicting: 119it [01:09,  1.68it/s]Extractor Predicting: 120it [01:10,  1.70it/s]Extractor Predicting: 121it [01:11,  1.70it/s]Extractor Predicting: 122it [01:11,  1.71it/s]Extractor Predicting: 123it [01:12,  1.64it/s]Extractor Predicting: 124it [01:12,  1.69it/s]Extractor Predicting: 125it [01:13,  1.70it/s]Extractor Predicting: 126it [01:14,  1.66it/s]Extractor Predicting: 127it [01:14,  1.63it/s]Extractor Predicting: 128it [01:15,  1.63it/s]Extractor Predicting: 129it [01:15,  1.68it/s]Extractor Predicting: 130it [01:16,  1.71it/s]Extractor Predicting: 131it [01:17,  1.69it/s]Extractor Predicting: 132it [01:17,  1.67it/s]Extractor Predicting: 133it [01:18,  1.67it/s]Extractor Predicting: 134it [01:18,  1.65it/s]Extractor Predicting: 135it [01:19,  1.65it/s]Extractor Predicting: 136it [01:20,  1.70it/s]Extractor Predicting: 137it [01:20,  1.68it/s]Extractor Predicting: 138it [01:21,  1.73it/s]Extractor Predicting: 139it [01:21,  1.76it/s]Extractor Predicting: 140it [01:22,  1.70it/s]Extractor Predicting: 141it [01:22,  1.67it/s]Extractor Predicting: 142it [01:23,  1.70it/s]Extractor Predicting: 143it [01:23,  2.10it/s]Extractor Predicting: 143it [01:23,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:56,011 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:56,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:56,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:56,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:56,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:51:56,338 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:51:56,339 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:51:57,080 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:51:58,161 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:51:58,161 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:51:59,991 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:00,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:00,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:00,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:52:00,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:52:00,756 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:52:00,757 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:52:01,078 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:52:01,238 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:52:01,238 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:13,  1.64it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.67it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:16,  1.64it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:19,  1.48it/s]Extractor Predicting: 32it [00:19,  1.53it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.74it/s]Extractor Predicting: 39it [00:23,  1.82it/s]Extractor Predicting: 40it [00:24,  1.93it/s]Extractor Predicting: 41it [00:24,  2.03it/s]Extractor Predicting: 42it [00:25,  2.02it/s]Extractor Predicting: 43it [00:25,  2.05it/s]Extractor Predicting: 44it [00:26,  2.08it/s]Extractor Predicting: 45it [00:26,  2.14it/s]Extractor Predicting: 46it [00:27,  2.10it/s]Extractor Predicting: 47it [00:27,  2.08it/s]Extractor Predicting: 48it [00:28,  2.09it/s]Extractor Predicting: 49it [00:28,  2.04it/s]Extractor Predicting: 50it [00:29,  2.10it/s]Extractor Predicting: 51it [00:29,  2.05it/s]Extractor Predicting: 52it [00:30,  2.07it/s]Extractor Predicting: 53it [00:30,  2.08it/s]Extractor Predicting: 54it [00:30,  2.14it/s]Extractor Predicting: 55it [00:31,  2.16it/s]Extractor Predicting: 56it [00:31,  2.09it/s]Extractor Predicting: 57it [00:32,  2.11it/s]Extractor Predicting: 58it [00:32,  2.11it/s]Extractor Predicting: 59it [00:33,  2.13it/s]Extractor Predicting: 60it [00:33,  2.06it/s]Extractor Predicting: 61it [00:34,  2.02it/s]Extractor Predicting: 62it [00:34,  2.10it/s]Extractor Predicting: 63it [00:35,  2.07it/s]Extractor Predicting: 64it [00:35,  2.09it/s]Extractor Predicting: 65it [00:36,  2.09it/s]Extractor Predicting: 66it [00:36,  2.13it/s]Extractor Predicting: 67it [00:37,  2.01it/s]Extractor Predicting: 68it [00:37,  1.89it/s]Extractor Predicting: 69it [00:38,  1.77it/s]Extractor Predicting: 70it [00:39,  1.71it/s]Extractor Predicting: 71it [00:39,  1.67it/s]Extractor Predicting: 72it [00:40,  1.70it/s]Extractor Predicting: 72it [00:40,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 00:52:44,263 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:52:44,265 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:52:44,321 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:52:44,322 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 00:52:44,363 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:52:56,050 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 00:52:56,068 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 00:52:56,237 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:52:56,237 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:52:56,293 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:52:56,323 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:52:56,324 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:52:56,324 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:52:56,324 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:52:56,324 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:52:56,324 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 00:52:56,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:57,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:57,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:58,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:59,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:52:59,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:00,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:00,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:01,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:02,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:02,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:03,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:03,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:04,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:05,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:05,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:06,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:07,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:07,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:08,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:09,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:09,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:10,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:10, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-28 00:53:11,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:11,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:12,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:13,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:13,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:14,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:14,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:15,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:16,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:16,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:17,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:17,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:18,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:19,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:19,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:20,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:21,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:22,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:23,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:24,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:24,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:25,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:25,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:29<02:00, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-28 00:53:26,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:27,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:27,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:28,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:29,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:30,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:30,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:31,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:32,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:32,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:33,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:33,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:34,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:35,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:35,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:36,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:36,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:37,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:38,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:38,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:39,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:40,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:43, 14.77s/it][WARNING|generation_utils.py:914] 2023-08-28 00:53:41,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:41,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:42,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:42,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:43,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:44,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:45,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:45,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:46,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:46,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:47,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:48,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:48,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:49,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:50,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:50,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:51,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:52,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:52,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:53,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:54,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:54,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:55,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:56,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:56,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:57,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:57,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:58,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:59,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:53:59,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:00,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:00,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:01,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:02,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:02,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:03,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:03,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:04,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:05,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:09<01:52, 18.68s/it][WARNING|generation_utils.py:914] 2023-08-28 00:54:05,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:06,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:06,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:07,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:08,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:08,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:09,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:09,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:10,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:11,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:11,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:12,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:13,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:13,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:14,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:14,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:15,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:16,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:16,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:17,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:18,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:18,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:19,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:19,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:20,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:27, 17.54s/it][WARNING|generation_utils.py:914] 2023-08-28 00:54:21,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:21,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:22,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:22,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:23,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:24,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:24,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:25,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:26,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:26,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:27,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:27,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:28,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:29,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:29,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:30,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:30,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:31,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:31,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:32,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:33,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:37<01:03, 15.94s/it][WARNING|generation_utils.py:914] 2023-08-28 00:54:34,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:34,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:35,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:35,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:36,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:36,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:37,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:38,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:38,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:39,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:39,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:40,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:41,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:41,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:42,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:43,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:43,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:44,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:44,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:45,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:46,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:46,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:47,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:47,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:48,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:49,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:52<00:47, 15.79s/it][WARNING|generation_utils.py:914] 2023-08-28 00:54:49,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:50,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:50,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:51,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:51,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:52,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:52,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:53,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:54,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:55,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:55,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:56,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:57,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:57,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:58,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:58,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:54:59,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:00,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:00,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:01,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:02,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:02,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:06<00:30, 15.11s/it][WARNING|generation_utils.py:914] 2023-08-28 00:55:03,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:03,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:04,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:05,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:05,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:06,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:06,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:07,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:08,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:09,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:09,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:10,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:11,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:11,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:12,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:12,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:13,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:14,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:14,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:15,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:16,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:16,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:17,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:21<00:15, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-28 00:55:18,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:19,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:19,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:20,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:21,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:21,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:22,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:22,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:23,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:24,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:25,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:25,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:26,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:27,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:27,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:28,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:29,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:29,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:30,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:31,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:31,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:32,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:55:33,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:37<00:00, 15.40s/it]Generating: 100%|██████████| 10/10 [02:37<00:00, 15.77s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,177 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:41,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:55:41,669 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:55:41,670 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:55:42,415 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:55:43,508 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:55:43,508 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:46,180 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:46,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:46,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:46,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:55:46,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:55:46,683 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:55:46,684 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:55:47,457 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:55:47,632 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:55:47,632 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')", "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.53it/s]Extractor Estimating: 6it [00:04,  1.54it/s]Extractor Estimating: 7it [00:04,  1.54it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.60it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.57it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.48it/s]Extractor Estimating: 22it [00:14,  1.47it/s]Extractor Estimating: 23it [00:14,  1.49it/s]Extractor Estimating: 24it [00:15,  1.51it/s]Extractor Estimating: 25it [00:16,  1.58it/s]Extractor Estimating: 26it [00:16,  1.52it/s]Extractor Estimating: 27it [00:17,  1.50it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:19,  1.60it/s]Extractor Estimating: 31it [00:19,  1.58it/s]Extractor Estimating: 32it [00:20,  1.59it/s]Extractor Estimating: 33it [00:21,  1.62it/s]Extractor Estimating: 34it [00:21,  1.58it/s]Extractor Estimating: 35it [00:22,  1.59it/s]Extractor Estimating: 36it [00:23,  1.60it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:24,  1.69it/s]Extractor Estimating: 39it [00:24,  1.65it/s]Extractor Estimating: 40it [00:25,  1.60it/s]Extractor Estimating: 41it [00:26,  1.55it/s]Extractor Estimating: 42it [00:26,  1.52it/s]Extractor Estimating: 43it [00:27,  1.51it/s]Extractor Estimating: 44it [00:28,  1.51it/s]Extractor Estimating: 45it [00:28,  1.52it/s]Extractor Estimating: 46it [00:29,  1.56it/s]Extractor Estimating: 47it [00:30,  1.56it/s]Extractor Estimating: 48it [00:30,  1.52it/s]Extractor Estimating: 49it [00:31,  1.54it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:32,  1.57it/s]Extractor Estimating: 52it [00:33,  1.64it/s]Extractor Estimating: 53it [00:33,  1.64it/s]Extractor Estimating: 54it [00:34,  1.62it/s]Extractor Estimating: 55it [00:35,  1.66it/s]Extractor Estimating: 56it [00:35,  1.69it/s]Extractor Estimating: 57it [00:36,  1.74it/s]Extractor Estimating: 58it [00:36,  1.74it/s]Extractor Estimating: 59it [00:37,  1.72it/s]Extractor Estimating: 60it [00:37,  1.70it/s]Extractor Estimating: 61it [00:38,  1.73it/s]Extractor Estimating: 62it [00:39,  1.76it/s]Extractor Estimating: 63it [00:39,  1.69it/s]Extractor Estimating: 64it [00:40,  1.70it/s]Extractor Estimating: 65it [00:41,  1.52it/s]Extractor Estimating: 66it [00:41,  1.59it/s]Extractor Estimating: 67it [00:42,  1.64it/s]Extractor Estimating: 68it [00:42,  1.67it/s]Extractor Estimating: 69it [00:43,  1.65it/s]Extractor Estimating: 70it [00:44,  1.63it/s]Extractor Estimating: 71it [00:44,  1.65it/s]Extractor Estimating: 72it [00:45,  1.67it/s]Extractor Estimating: 73it [00:45,  1.70it/s]Extractor Estimating: 74it [00:46,  1.64it/s]Extractor Estimating: 75it [00:46,  1.69it/s]Extractor Estimating: 76it [00:47,  1.47it/s]Extractor Estimating: 77it [00:48,  1.49it/s]Extractor Estimating: 78it [00:49,  1.55it/s]Extractor Estimating: 79it [00:49,  1.55it/s]Extractor Estimating: 80it [00:50,  1.55it/s]Extractor Estimating: 81it [00:51,  1.56it/s]Extractor Estimating: 82it [00:51,  1.50it/s]Extractor Estimating: 83it [00:52,  1.49it/s]Extractor Estimating: 84it [00:53,  1.49it/s]Extractor Estimating: 85it [00:53,  1.51it/s]Extractor Estimating: 86it [00:54,  1.52it/s]Extractor Estimating: 87it [00:54,  1.57it/s]Extractor Estimating: 88it [00:55,  1.58it/s]Extractor Estimating: 89it [00:56,  1.61it/s]Extractor Estimating: 90it [00:56,  1.55it/s]Extractor Estimating: 91it [00:57,  1.59it/s]Extractor Estimating: 92it [00:58,  1.58it/s]Extractor Estimating: 93it [00:58,  1.59it/s]Extractor Estimating: 94it [00:59,  1.59it/s]Extractor Estimating: 95it [01:00,  1.56it/s]Extractor Estimating: 96it [01:00,  1.56it/s]Extractor Estimating: 97it [01:01,  1.52it/s]Extractor Estimating: 98it [01:02,  1.52it/s]Extractor Estimating: 99it [01:02,  1.56it/s]Extractor Estimating: 100it [01:03,  1.56it/s]Extractor Estimating: 101it [01:03,  1.61it/s]Extractor Estimating: 102it [01:04,  1.61it/s]Extractor Estimating: 103it [01:05,  1.64it/s]Extractor Estimating: 104it [01:05,  1.66it/s]Extractor Estimating: 105it [01:06,  1.73it/s]Extractor Estimating: 106it [01:06,  1.72it/s]Extractor Estimating: 107it [01:07,  1.69it/s]Extractor Estimating: 108it [01:07,  1.71it/s]Extractor Estimating: 109it [01:08,  1.65it/s]Extractor Estimating: 110it [01:09,  1.63it/s]Extractor Estimating: 111it [01:09,  1.63it/s]Extractor Estimating: 112it [01:10,  1.63it/s]Extractor Estimating: 113it [01:11,  1.66it/s]Extractor Estimating: 114it [01:11,  1.69it/s]Extractor Estimating: 115it [01:12,  1.69it/s]Extractor Estimating: 116it [01:12,  1.67it/s]Extractor Estimating: 117it [01:13,  1.64it/s]Extractor Estimating: 118it [01:14,  1.66it/s]Extractor Estimating: 119it [01:14,  1.62it/s]Extractor Estimating: 120it [01:15,  1.64it/s]Extractor Estimating: 121it [01:15,  1.62it/s]Extractor Estimating: 122it [01:16,  1.68it/s]Extractor Estimating: 123it [01:17,  1.71it/s]Extractor Estimating: 124it [01:17,  1.67it/s]Extractor Estimating: 125it [01:18,  1.60it/s]Extractor Estimating: 126it [01:18,  1.62it/s]Extractor Estimating: 127it [01:19,  1.61it/s]Extractor Estimating: 128it [01:20,  1.59it/s]Extractor Estimating: 129it [01:20,  1.61it/s]Extractor Estimating: 130it [01:21,  1.63it/s]Extractor Estimating: 131it [01:22,  1.61it/s]Extractor Estimating: 132it [01:22,  1.61it/s]Extractor Estimating: 133it [01:23,  1.59it/s]Extractor Estimating: 134it [01:23,  1.59it/s]Extractor Estimating: 135it [01:24,  1.58it/s]Extractor Estimating: 136it [01:25,  1.55it/s]Extractor Estimating: 137it [01:25,  1.53it/s]Extractor Estimating: 138it [01:26,  1.50it/s]Extractor Estimating: 139it [01:27,  1.52it/s]Extractor Estimating: 140it [01:27,  1.52it/s]Extractor Estimating: 141it [01:28,  1.55it/s]Extractor Estimating: 142it [01:29,  1.57it/s]Extractor Estimating: 143it [01:29,  1.57it/s]Extractor Estimating: 144it [01:30,  1.57it/s]Extractor Estimating: 145it [01:31,  1.61it/s]Extractor Estimating: 146it [01:31,  1.60it/s]Extractor Estimating: 147it [01:32,  1.53it/s]Extractor Estimating: 148it [01:33,  1.49it/s]Extractor Estimating: 149it [01:33,  1.48it/s]Extractor Estimating: 150it [01:34,  1.40it/s]Extractor Estimating: 151it [01:35,  1.43it/s]Extractor Estimating: 152it [01:35,  1.45it/s]Extractor Estimating: 153it [01:36,  1.49it/s]Extractor Estimating: 154it [01:37,  1.56it/s]Extractor Estimating: 155it [01:37,  1.58it/s]Extractor Estimating: 156it [01:38,  1.54it/s]Extractor Estimating: 157it [01:39,  1.52it/s]Extractor Estimating: 158it [01:39,  1.56it/s]Extractor Estimating: 159it [01:40,  1.60it/s]Extractor Estimating: 160it [01:40,  1.65it/s]Extractor Estimating: 161it [01:41,  1.67it/s]Extractor Estimating: 162it [01:42,  1.56it/s]Extractor Estimating: 163it [01:42,  1.56it/s]Extractor Estimating: 164it [01:43,  1.52it/s]Extractor Estimating: 165it [01:44,  1.56it/s]Extractor Estimating: 166it [01:44,  1.56it/s]Extractor Estimating: 167it [01:45,  1.50it/s]Extractor Estimating: 168it [01:46,  1.57it/s]Extractor Estimating: 169it [01:46,  1.63it/s]Extractor Estimating: 170it [01:47,  1.67it/s]Extractor Estimating: 171it [01:47,  1.69it/s]Extractor Estimating: 172it [01:48,  1.66it/s]Extractor Estimating: 173it [01:49,  1.60it/s]Extractor Estimating: 174it [01:49,  1.61it/s]Extractor Estimating: 175it [01:50,  1.59it/s]Extractor Estimating: 176it [01:50,  1.61it/s]Extractor Estimating: 177it [01:51,  1.63it/s]Extractor Estimating: 178it [01:52,  1.64it/s]Extractor Estimating: 179it [01:52,  1.69it/s]Extractor Estimating: 180it [01:53,  1.72it/s]Extractor Estimating: 181it [01:53,  1.71it/s]Extractor Estimating: 182it [01:54,  1.71it/s]Extractor Estimating: 183it [01:55,  1.65it/s]Extractor Estimating: 184it [01:55,  1.64it/s]Extractor Estimating: 185it [01:56,  1.60it/s]Extractor Estimating: 186it [01:56,  1.63it/s]Extractor Estimating: 187it [01:57,  1.59it/s]Extractor Estimating: 188it [01:58,  1.63it/s]Extractor Estimating: 189it [01:58,  1.59it/s]Extractor Estimating: 190it [01:59,  1.61it/s]Extractor Estimating: 191it [01:59,  1.66it/s]Extractor Estimating: 192it [02:00,  1.66it/s]Extractor Estimating: 193it [02:01,  1.64it/s]Extractor Estimating: 194it [02:01,  1.58it/s]Extractor Estimating: 195it [02:02,  1.65it/s]Extractor Estimating: 196it [02:03,  1.64it/s]Extractor Estimating: 197it [02:03,  1.66it/s]Extractor Estimating: 198it [02:04,  1.65it/s]Extractor Estimating: 199it [02:04,  1.68it/s]Extractor Estimating: 200it [02:05,  1.65it/s]Extractor Estimating: 201it [02:06,  1.55it/s]Extractor Estimating: 202it [02:06,  1.57it/s]Extractor Estimating: 203it [02:07,  1.57it/s]Extractor Estimating: 204it [02:07,  1.64it/s]Extractor Estimating: 205it [02:08,  1.67it/s]Extractor Estimating: 206it [02:09,  1.63it/s]Extractor Estimating: 207it [02:09,  1.59it/s]Extractor Estimating: 208it [02:10,  1.60it/s]Extractor Estimating: 209it [02:11,  1.61it/s]Extractor Estimating: 210it [02:11,  1.57it/s]Extractor Estimating: 211it [02:12,  1.57it/s]Extractor Estimating: 212it [02:13,  1.50it/s]Extractor Estimating: 213it [02:13,  1.52it/s]Extractor Estimating: 214it [02:14,  1.56it/s]Extractor Estimating: 215it [02:14,  1.57it/s]Extractor Estimating: 216it [02:15,  1.58it/s]Extractor Estimating: 217it [02:16,  1.60it/s]Extractor Estimating: 218it [02:16,  1.64it/s]Extractor Estimating: 219it [02:17,  1.64it/s]Extractor Estimating: 220it [02:18,  1.60it/s]Extractor Estimating: 221it [02:18,  1.57it/s]Extractor Estimating: 222it [02:19,  1.59it/s]Extractor Estimating: 223it [02:20,  1.56it/s]Extractor Estimating: 224it [02:20,  1.53it/s]Extractor Estimating: 225it [02:21,  1.56it/s]Extractor Estimating: 226it [02:22,  1.50it/s]Extractor Estimating: 227it [02:22,  1.54it/s]Extractor Estimating: 228it [02:23,  1.58it/s]Extractor Estimating: 229it [02:23,  1.61it/s]Extractor Estimating: 230it [02:24,  1.63it/s]Extractor Estimating: 231it [02:25,  1.58it/s]Extractor Estimating: 232it [02:25,  1.59it/s]Extractor Estimating: 233it [02:26,  1.61it/s]Extractor Estimating: 234it [02:26,  1.61it/s]Extractor Estimating: 235it [02:27,  1.44it/s]Extractor Estimating: 236it [02:28,  1.51it/s]Extractor Estimating: 237it [02:29,  1.52it/s]Extractor Estimating: 238it [02:29,  1.54it/s]Extractor Estimating: 239it [02:30,  1.58it/s]Extractor Estimating: 240it [02:30,  1.58it/s]Extractor Estimating: 241it [02:31,  1.56it/s]Extractor Estimating: 242it [02:32,  1.60it/s]Extractor Estimating: 243it [02:32,  1.61it/s]Extractor Estimating: 244it [02:33,  1.63it/s]Extractor Estimating: 245it [02:34,  1.59it/s]Extractor Estimating: 246it [02:34,  1.56it/s]Extractor Estimating: 247it [02:35,  1.56it/s]Extractor Estimating: 248it [02:35,  1.61it/s]Extractor Estimating: 249it [02:36,  1.61it/s]Extractor Estimating: 250it [02:37,  1.50it/s]Extractor Estimating: 250it [02:37,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:45,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:58:46,720 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:58:46,721 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:58:47,350 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:58:48,541 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:58:48,541 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:51,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:51,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:51,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:51,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:51,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:58:52,370 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:58:52,371 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:58:52,989 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:58:53,217 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:58:53,217 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 02:39:38,983 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 02:39:39,000 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5213 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 25687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25787, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.000, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.026, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.019, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.011, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 3.015, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.040, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.001, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.990, loss:nan
g_step 1200, step 110, avg_time 1.013, loss:nan
g_step 1300, step 210, avg_time 1.033, loss:nan
g_step 1400, step 92, avg_time 1.022, loss:nan
g_step 1500, step 192, avg_time 1.013, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 3.009, loss:nan
g_step 1700, step 174, avg_time 1.012, loss:nan
g_step 1800, step 56, avg_time 1.020, loss:nan
g_step 1900, step 156, avg_time 1.023, loss:nan
g_step 2000, step 38, avg_time 1.027, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 3.004, loss:nan
g_step 2200, step 20, avg_time 1.022, loss:nan
g_step 2300, step 120, avg_time 1.030, loss:nan
g_step 2400, step 2, avg_time 1.014, loss:nan
g_step 2500, step 102, avg_time 1.030, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 3.012, loss:nan
g_step 2700, step 84, avg_time 1.014, loss:nan
g_step 2800, step 184, avg_time 1.027, loss:nan
g_step 2900, step 66, avg_time 1.014, loss:nan
g_step 3000, step 166, avg_time 1.037, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.999, loss:nan
g_step 3200, step 148, avg_time 1.026, loss:nan
g_step 3300, step 30, avg_time 1.011, loss:nan
g_step 3400, step 130, avg_time 1.049, loss:nan
g_step 3500, step 12, avg_time 1.015, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 3.003, loss:nan
g_step 3700, step 212, avg_time 1.021, loss:nan
g_step 3800, step 94, avg_time 1.020, loss:nan
g_step 3900, step 194, avg_time 1.017, loss:nan
g_step 4000, step 76, avg_time 1.012, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 3.012, loss:nan
g_step 4200, step 58, avg_time 1.029, loss:nan
g_step 4300, step 158, avg_time 1.024, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 02:39:39 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 02:39:39 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-39-38_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 02:39:40 - WARNING - datasets.builder -   Using custom data configuration default-3f2510fe90de1445
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3f2510fe90de1445/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 02:39:41,252 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:39:41,254 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:39:41,254 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:39:41,255 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:39:41,332 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:41,356 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:41,356 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:41,356 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:41,356 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:41,356 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:41,356 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 02:39:41,633 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:39:44,769 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 02:39:44,792 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3f2510fe90de1445/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.03ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.06ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.59ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.90ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.10ba/s]100%|██████████| 6/6 [00:01<00:00,  4.20ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  2.99ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.72ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.04ba/s] 44%|████▍     | 4/9 [00:01<00:01,  4.20ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.32ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.39ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.13ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.25ba/s]100%|██████████| 9/9 [00:02<00:00,  4.79ba/s]100%|██████████| 9/9 [00:02<00:00,  4.32ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.92ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.58ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.00ba/s]100%|██████████| 6/6 [00:00<00:00,  9.40ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.11ba/s] 22%|██▏       | 2/9 [00:00<00:01,  4.64ba/s] 44%|████▍     | 4/9 [00:00<00:00,  7.30ba/s] 67%|██████▋   | 6/9 [00:00<00:00,  8.70ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  9.45ba/s]100%|██████████| 9/9 [00:01<00:00,  8.43ba/s]
[INFO|trainer.py:414] 2023-08-28 02:39:52,647 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 02:39:52,800 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 02:39:52,800 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 02:39:52,800 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 02:39:52,800 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 02:39:52,800 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 02:39:52,800 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 02:39:52,800 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:01<07:51,  1.15s/it]  0%|          | 2/410 [00:01<06:08,  1.11it/s]  1%|          | 3/410 [00:02<04:44,  1.43it/s]  1%|          | 4/410 [00:02<03:44,  1.81it/s]  1%|          | 5/410 [00:02<03:09,  2.14it/s]  1%|▏         | 6/410 [00:03<02:42,  2.48it/s]  2%|▏         | 7/410 [00:03<02:26,  2.76it/s]  2%|▏         | 8/410 [00:03<02:19,  2.89it/s]  2%|▏         | 9/410 [00:04<02:10,  3.07it/s]  2%|▏         | 10/410 [00:04<02:17,  2.90it/s]  3%|▎         | 11/410 [00:04<02:15,  2.95it/s]  3%|▎         | 12/410 [00:05<02:35,  2.56it/s]  3%|▎         | 13/410 [00:05<02:26,  2.72it/s]  3%|▎         | 14/410 [00:05<02:15,  2.93it/s]  4%|▎         | 15/410 [00:06<02:07,  3.10it/s]  4%|▍         | 16/410 [00:06<02:02,  3.23it/s]  4%|▍         | 17/410 [00:06<01:58,  3.33it/s]  4%|▍         | 18/410 [00:07<01:55,  3.40it/s]  5%|▍         | 19/410 [00:07<01:53,  3.45it/s]  5%|▍         | 20/410 [00:07<01:51,  3.49it/s]  5%|▌         | 21/410 [00:07<01:50,  3.52it/s]  5%|▌         | 22/410 [00:08<01:49,  3.54it/s]  6%|▌         | 23/410 [00:08<01:49,  3.55it/s]  6%|▌         | 24/410 [00:08<01:49,  3.52it/s]  6%|▌         | 25/410 [00:09<01:48,  3.53it/s]  6%|▋         | 26/410 [00:09<01:48,  3.54it/s]  7%|▋         | 27/410 [00:09<01:47,  3.55it/s]  7%|▋         | 28/410 [00:09<01:47,  3.56it/s]  7%|▋         | 29/410 [00:10<01:47,  3.56it/s]  7%|▋         | 30/410 [00:10<01:46,  3.56it/s]  8%|▊         | 31/410 [00:10<01:46,  3.56it/s]  8%|▊         | 32/410 [00:10<01:46,  3.56it/s]  8%|▊         | 33/410 [00:11<01:45,  3.57it/s]  8%|▊         | 34/410 [00:11<01:45,  3.57it/s]  9%|▊         | 35/410 [00:11<01:46,  3.53it/s]  9%|▉         | 36/410 [00:12<01:45,  3.55it/s]  9%|▉         | 37/410 [00:12<01:44,  3.56it/s]  9%|▉         | 38/410 [00:12<01:44,  3.56it/s] 10%|▉         | 39/410 [00:12<01:44,  3.57it/s] 10%|▉         | 40/410 [00:13<01:43,  3.57it/s] 10%|█         | 41/410 [00:13<01:43,  3.57it/s] 10%|█         | 42/410 [00:13<01:42,  3.57it/s] 10%|█         | 43/410 [00:14<01:42,  3.57it/s] 11%|█         | 44/410 [00:14<01:42,  3.58it/s] 11%|█         | 45/410 [00:14<01:41,  3.58it/s] 11%|█         | 46/410 [00:14<01:43,  3.52it/s] 11%|█▏        | 47/410 [00:15<01:42,  3.55it/s] 12%|█▏        | 48/410 [00:15<01:41,  3.56it/s] 12%|█▏        | 49/410 [00:15<01:40,  3.58it/s] 12%|█▏        | 50/410 [00:16<01:40,  3.60it/s] 12%|█▏        | 51/410 [00:16<01:39,  3.61it/s] 13%|█▎        | 52/410 [00:16<01:39,  3.61it/s] 13%|█▎        | 53/410 [00:16<01:38,  3.61it/s] 13%|█▎        | 54/410 [00:17<01:38,  3.62it/s] 13%|█▎        | 55/410 [00:17<01:38,  3.62it/s] 14%|█▎        | 56/410 [00:17<01:37,  3.62it/s] 14%|█▍        | 57/410 [00:18<01:41,  3.46it/s] 14%|█▍        | 58/410 [00:18<01:40,  3.50it/s] 14%|█▍        | 59/410 [00:18<01:39,  3.54it/s] 15%|█▍        | 60/410 [00:18<01:38,  3.56it/s] 15%|█▍        | 61/410 [00:19<01:37,  3.58it/s] 15%|█▌        | 62/410 [00:19<01:36,  3.59it/s] 15%|█▌        | 63/410 [00:19<01:36,  3.60it/s] 16%|█▌        | 64/410 [00:19<01:35,  3.61it/s] 16%|█▌        | 65/410 [00:20<01:35,  3.61it/s] 16%|█▌        | 66/410 [00:20<01:35,  3.61it/s] 16%|█▋        | 67/410 [00:20<01:34,  3.62it/s] 17%|█▋        | 68/410 [00:21<01:34,  3.62it/s] 17%|█▋        | 69/410 [00:21<01:34,  3.62it/s] 17%|█▋        | 70/410 [00:21<01:33,  3.62it/s] 17%|█▋        | 71/410 [00:21<01:33,  3.62it/s] 18%|█▊        | 72/410 [00:22<01:33,  3.62it/s] 18%|█▊        | 73/410 [00:22<01:33,  3.62it/s] 18%|█▊        | 74/410 [00:22<01:34,  3.55it/s] 18%|█▊        | 75/410 [00:22<01:33,  3.58it/s] 19%|█▊        | 76/410 [00:23<01:32,  3.59it/s] 19%|█▉        | 77/410 [00:23<01:32,  3.60it/s] 19%|█▉        | 78/410 [00:23<01:32,  3.61it/s] 19%|█▉        | 79/410 [00:24<01:31,  3.61it/s] 20%|█▉        | 80/410 [00:24<01:31,  3.61it/s] 20%|█▉        | 81/410 [00:24<01:30,  3.62it/s] 20%|██        | 82/410 [00:24<01:27,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 02:40:17,705 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:40:17,705 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 02:40:17,705 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.54it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.28it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.67it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.71it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.15it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 43.85it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.24it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.17it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.43it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.53it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.90it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.09it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.22it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.94it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.77it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.81it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.87it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.98it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.96it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.10it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.24it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.26it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.09it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.90it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.92it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.91it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 45.04it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.93it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.15it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.24it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.17it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.05it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.48it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.62it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.66it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.65it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.90it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.98it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.24it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.22it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.99it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.86it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.89it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.87it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.84it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 45.04it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.05it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.23it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.15it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.08it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.01it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.81it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.84it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.94it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.99it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.09it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.11it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.08it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.95it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 43.61it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 43.99it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.35it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.52it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.62it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.88it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.99it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.10it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.79it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.86it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.95it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.88it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.98it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.92it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.11it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.04it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.02it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.01it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.90it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.91it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.91it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 45.00it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.05it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.03it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.02it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.99it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.98it/s][A
 40%|████      | 437/1083 [00:09<00:14, 43.55it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.06it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.32it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.58it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.77it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.89it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.01it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.96it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.67it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.73it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.78it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.97it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 45.04it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.12it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.08it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.09it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.04it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.88it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.82it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.79it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 45.01it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 45.04it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 45.16it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.12it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.02it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.02it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.91it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.68it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.74it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.75it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 45.04it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.18it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 45.10it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.13it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.06it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.89it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.80it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.78it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.92it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 45.03it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 45.10it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 45.08it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.10it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.03it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.91it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.79it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.71it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.77it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 45.01it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 45.20it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.20it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.06it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.07it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.91it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 42.49it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 43.23it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 43.77it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.35it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.64it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.88it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.84it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.76it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.60it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.59it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.70it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 44.83it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.94it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 45.13it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 45.24it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.15it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.03it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.71it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.69it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.74it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 44.85it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.00it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 45.14it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.12it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.12it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.10it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.98it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 43.98it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.25it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.45it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.72it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.88it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.86it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.94it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.80it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.71it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.82it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.92it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.99it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.88it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.96it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.97it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.06it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.95it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.77it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.80it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.03it/s][A
 87%|████████▋ | 942/1083 [00:20<00:03, 44.52it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.69it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.80it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.91it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.81it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.77it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.70it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 43.71it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.14it/s][A
 91%|█████████ | 987/1083 [00:21<00:02, 44.47it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.68it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.75it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.87it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.86it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.68it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.65it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.62it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.86it/s][A
 95%|█████████▌| 1032/1083 [00:22<00:01, 45.09it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 45.02it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 45.08it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.98it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.98it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.89it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.76it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.80it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.82it/s][A
 99%|█████████▉| 1077/1083 [00:23<00:00, 45.05it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.16it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:24<00:00, 45.16it/s][A 20%|██        | 82/410 [00:49<01:27,  3.75it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 02:40:42,215 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 02:40:42,399 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:40:44,948 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:40:45,094 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:40:45,186 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:53<48:18,  8.87s/it] 20%|██        | 84/410 [00:54<34:13,  6.30s/it] 21%|██        | 85/410 [00:54<24:20,  4.49s/it] 21%|██        | 86/410 [00:54<17:26,  3.23s/it] 21%|██        | 87/410 [00:54<12:37,  2.34s/it] 21%|██▏       | 88/410 [00:55<09:15,  1.73s/it] 22%|██▏       | 89/410 [00:55<06:54,  1.29s/it] 22%|██▏       | 90/410 [00:55<05:16,  1.01it/s] 22%|██▏       | 91/410 [00:56<04:07,  1.29it/s] 22%|██▏       | 92/410 [00:56<03:19,  1.59it/s] 23%|██▎       | 93/410 [00:56<02:45,  1.91it/s] 23%|██▎       | 94/410 [00:56<02:22,  2.22it/s] 23%|██▎       | 95/410 [00:57<02:08,  2.46it/s] 23%|██▎       | 96/410 [00:57<01:55,  2.71it/s] 24%|██▎       | 97/410 [00:57<01:47,  2.92it/s] 24%|██▍       | 98/410 [00:58<01:40,  3.09it/s] 24%|██▍       | 99/410 [00:58<01:36,  3.22it/s] 24%|██▍       | 100/410 [00:58<01:33,  3.31it/s] 25%|██▍       | 101/410 [00:58<01:31,  3.38it/s] 25%|██▍       | 102/410 [00:59<01:29,  3.44it/s] 25%|██▌       | 103/410 [00:59<01:28,  3.47it/s] 25%|██▌       | 104/410 [00:59<01:27,  3.50it/s] 26%|██▌       | 105/410 [01:00<01:26,  3.52it/s] 26%|██▌       | 106/410 [01:00<01:30,  3.37it/s] 26%|██▌       | 107/410 [01:00<01:28,  3.42it/s] 26%|██▋       | 108/410 [01:00<01:27,  3.47it/s] 27%|██▋       | 109/410 [01:01<01:26,  3.50it/s] 27%|██▋       | 110/410 [01:01<01:25,  3.52it/s] 27%|██▋       | 111/410 [01:01<01:24,  3.55it/s] 27%|██▋       | 112/410 [01:02<01:23,  3.56it/s] 28%|██▊       | 113/410 [01:02<01:23,  3.58it/s] 28%|██▊       | 114/410 [01:02<01:22,  3.59it/s] 28%|██▊       | 115/410 [01:02<01:21,  3.60it/s] 28%|██▊       | 116/410 [01:03<01:24,  3.48it/s] 29%|██▊       | 117/410 [01:03<01:27,  3.37it/s] 29%|██▉       | 118/410 [01:03<01:24,  3.44it/s] 29%|██▉       | 119/410 [01:04<01:23,  3.49it/s] 29%|██▉       | 120/410 [01:04<01:22,  3.53it/s] 30%|██▉       | 121/410 [01:04<01:29,  3.22it/s] 30%|██▉       | 122/410 [01:05<01:29,  3.24it/s] 30%|███       | 123/410 [01:05<01:49,  2.62it/s] 30%|███       | 124/410 [01:05<01:40,  2.84it/s] 30%|███       | 125/410 [01:06<01:33,  3.04it/s] 31%|███       | 126/410 [01:06<01:28,  3.19it/s] 31%|███       | 127/410 [01:06<01:26,  3.26it/s] 31%|███       | 128/410 [01:06<01:23,  3.36it/s] 31%|███▏      | 129/410 [01:07<01:21,  3.43it/s] 32%|███▏      | 130/410 [01:07<01:20,  3.49it/s] 32%|███▏      | 131/410 [01:07<01:19,  3.52it/s] 32%|███▏      | 132/410 [01:08<01:18,  3.55it/s] 32%|███▏      | 133/410 [01:08<01:17,  3.57it/s] 33%|███▎      | 134/410 [01:08<01:16,  3.59it/s] 33%|███▎      | 135/410 [01:08<01:16,  3.60it/s] 33%|███▎      | 136/410 [01:09<01:16,  3.60it/s] 33%|███▎      | 137/410 [01:09<01:15,  3.61it/s] 34%|███▎      | 138/410 [01:09<01:17,  3.51it/s] 34%|███▍      | 139/410 [01:10<01:16,  3.54it/s] 34%|███▍      | 140/410 [01:10<01:15,  3.57it/s] 34%|███▍      | 141/410 [01:10<01:15,  3.58it/s] 35%|███▍      | 142/410 [01:10<01:14,  3.59it/s] 35%|███▍      | 143/410 [01:11<01:14,  3.60it/s] 35%|███▌      | 144/410 [01:11<01:13,  3.61it/s] 35%|███▌      | 145/410 [01:11<01:13,  3.61it/s] 36%|███▌      | 146/410 [01:11<01:13,  3.62it/s] 36%|███▌      | 147/410 [01:12<01:12,  3.62it/s] 36%|███▌      | 148/410 [01:12<01:12,  3.62it/s] 36%|███▋      | 149/410 [01:12<01:14,  3.50it/s] 37%|███▋      | 150/410 [01:13<01:13,  3.54it/s] 37%|███▋      | 151/410 [01:13<01:12,  3.56it/s] 37%|███▋      | 152/410 [01:13<01:12,  3.58it/s] 37%|███▋      | 153/410 [01:13<01:11,  3.59it/s] 38%|███▊      | 154/410 [01:14<01:11,  3.60it/s] 38%|███▊      | 155/410 [01:14<01:10,  3.61it/s] 38%|███▊      | 156/410 [01:14<01:10,  3.61it/s] 38%|███▊      | 157/410 [01:15<01:09,  3.62it/s] 39%|███▊      | 158/410 [01:15<01:09,  3.62it/s] 39%|███▉      | 159/410 [01:15<01:09,  3.62it/s] 39%|███▉      | 160/410 [01:15<01:10,  3.52it/s] 39%|███▉      | 161/410 [01:16<01:10,  3.55it/s] 40%|███▉      | 162/410 [01:16<01:09,  3.57it/s] 40%|███▉      | 163/410 [01:16<01:08,  3.58it/s] 40%|████      | 164/410 [01:16<01:06,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 02:41:09,775 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:41:09,775 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 02:41:09,775 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.144, 'eval_samples_per_second': 358.599, 'eval_steps_per_second': 44.856, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.51it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.31it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.56it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.44it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.52it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.23it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.94it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.77it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.89it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.95it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.09it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.13it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.17it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.14it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.07it/s][A
  8%|▊         | 82/1083 [00:01<00:23, 42.99it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 43.63it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.05it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.36it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.71it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.90it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.96it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.97it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.71it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.65it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.71it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.75it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.91it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.00it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.17it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.12it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.90it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.83it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.77it/s][A
 16%|█▋        | 177/1083 [00:03<00:22, 39.78it/s][A
 17%|█▋        | 182/1083 [00:04<00:21, 41.27it/s][A
 17%|█▋        | 187/1083 [00:04<00:21, 42.37it/s][A
 18%|█▊        | 192/1083 [00:04<00:20, 43.21it/s][A
 18%|█▊        | 197/1083 [00:04<00:20, 43.81it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.24it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.57it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.70it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.46it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.51it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.61it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.87it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.95it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.99it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.07it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.10it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.90it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.73it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.68it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.77it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.89it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.93it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.06it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.12it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.20it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.93it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.37it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 43.83it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.35it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.56it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.80it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.87it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.06it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.06it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.96it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.72it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.64it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.80it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.89it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.04it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.04it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.12it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.06it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.90it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.76it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.74it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.79it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.91it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.96it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.01it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.09it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.02it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.87it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.79it/s][A
 41%|████▏     | 447/1083 [00:10<00:14, 42.94it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 43.69it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 44.17it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.50it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.65it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.77it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.83it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.73it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.46it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.54it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.71it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.81it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.90it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.06it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.14it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.06it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.92it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.69it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 44.58it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.63it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.75it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.98it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.00it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.09it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.08it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.85it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.75it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 43.24it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 43.89it/s][A
 55%|█████▍    | 592/1083 [00:13<00:11, 44.21it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.61it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.71it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.97it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.92it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.77it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.56it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.49it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.75it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.92it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 45.05it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.03it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.08it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.12it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.84it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.60it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.65it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.55it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.97it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.08it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.11it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.21it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.95it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.74it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.70it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 43.57it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.12it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 44.44it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.67it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.90it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.95it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.81it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.68it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.51it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.57it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.85it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.93it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 45.01it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.07it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.24it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.08it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.91it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.77it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.69it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.95it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.98it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.99it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.93it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.04it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.03it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.87it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.70it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 43.76it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.28it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.57it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.72it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.78it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.87it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.90it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.78it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.52it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.58it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.66it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.93it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 45.05it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.13it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.14it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.04it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.86it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.58it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.53it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.68it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.88it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 45.05it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.15it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 45.18it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.05it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.74it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.57it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 43.91it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.34it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.59it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.75it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.97it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.03it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.97it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.60it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.49it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.63it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.77it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.94it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.81it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.93it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.98it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.88it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.74it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.63it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.65it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.70it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.70it/s][A 40%|████      | 164/410 [01:41<01:06,  3.73it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 02:41:34,076 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 02:41:34,239 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:41:37,224 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:41:37,312 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:41:37,360 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:45<36:13,  8.87s/it] 40%|████      | 166/410 [01:46<25:35,  6.29s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:46<18:10,  4.49s/it] 41%|████      | 168/410 [01:46<13:00,  3.22s/it] 41%|████      | 169/410 [01:47<09:23,  2.34s/it] 41%|████▏     | 170/410 [01:47<06:53,  1.72s/it] 42%|████▏     | 171/410 [01:47<05:07,  1.29s/it] 42%|████▏     | 172/410 [01:47<03:55,  1.01it/s] 42%|████▏     | 173/410 [01:48<03:03,  1.29it/s] 42%|████▏     | 174/410 [01:48<02:27,  1.60it/s] 43%|████▎     | 175/410 [01:48<02:02,  1.92it/s] 43%|████▎     | 176/410 [01:48<01:44,  2.23it/s] 43%|████▎     | 177/410 [01:49<01:32,  2.52it/s] 43%|████▎     | 178/410 [01:49<01:23,  2.78it/s] 44%|████▎     | 179/410 [01:49<01:17,  2.99it/s] 44%|████▍     | 180/410 [01:50<01:12,  3.15it/s] 44%|████▍     | 181/410 [01:50<01:09,  3.28it/s] 44%|████▍     | 182/410 [01:50<01:07,  3.37it/s] 45%|████▍     | 183/410 [01:50<01:07,  3.39it/s] 45%|████▍     | 184/410 [01:51<01:05,  3.45it/s] 45%|████▌     | 185/410 [01:51<01:04,  3.50it/s] 45%|████▌     | 186/410 [01:51<01:03,  3.53it/s] 46%|████▌     | 187/410 [01:52<01:02,  3.56it/s] 46%|████▌     | 188/410 [01:52<01:02,  3.58it/s] 46%|████▌     | 189/410 [01:52<01:01,  3.59it/s] 46%|████▋     | 190/410 [01:52<01:01,  3.60it/s] 47%|████▋     | 191/410 [01:53<01:00,  3.61it/s] 47%|████▋     | 192/410 [01:53<01:00,  3.61it/s] 47%|████▋     | 193/410 [01:53<01:00,  3.62it/s] 47%|████▋     | 194/410 [01:53<00:59,  3.62it/s] 48%|████▊     | 195/410 [01:54<00:59,  3.62it/s] 48%|████▊     | 196/410 [01:54<00:59,  3.62it/s] 48%|████▊     | 197/410 [01:54<01:00,  3.51it/s] 48%|████▊     | 198/410 [01:55<00:59,  3.55it/s] 49%|████▊     | 199/410 [01:55<00:59,  3.57it/s] 49%|████▉     | 200/410 [01:55<00:58,  3.58it/s] 49%|████▉     | 201/410 [01:55<00:58,  3.60it/s] 49%|████▉     | 202/410 [01:56<00:57,  3.60it/s] 50%|████▉     | 203/410 [01:56<00:57,  3.61it/s] 50%|████▉     | 204/410 [01:56<00:57,  3.61it/s] 50%|█████     | 205/410 [01:57<00:56,  3.62it/s] 50%|█████     | 206/410 [01:57<00:56,  3.62it/s] 50%|█████     | 207/410 [01:57<00:56,  3.62it/s] 51%|█████     | 208/410 [01:57<00:59,  3.40it/s] 51%|█████     | 209/410 [01:58<00:58,  3.46it/s] 51%|█████     | 210/410 [01:58<00:56,  3.51it/s] 51%|█████▏    | 211/410 [01:58<00:56,  3.54it/s] 52%|█████▏    | 212/410 [01:59<00:55,  3.57it/s] 52%|█████▏    | 213/410 [01:59<00:54,  3.58it/s] 52%|█████▏    | 214/410 [01:59<00:54,  3.59it/s] 52%|█████▏    | 215/410 [01:59<00:54,  3.60it/s] 53%|█████▎    | 216/410 [02:00<00:53,  3.61it/s] 53%|█████▎    | 217/410 [02:00<00:53,  3.61it/s] 53%|█████▎    | 218/410 [02:00<00:53,  3.62it/s] 53%|█████▎    | 219/410 [02:01<00:56,  3.40it/s] 54%|█████▎    | 220/410 [02:01<00:54,  3.47it/s] 54%|█████▍    | 221/410 [02:01<00:53,  3.51it/s] 54%|█████▍    | 222/410 [02:01<00:53,  3.54it/s] 54%|█████▍    | 223/410 [02:02<00:52,  3.56it/s] 55%|█████▍    | 224/410 [02:02<00:51,  3.58it/s] 55%|█████▍    | 225/410 [02:02<00:51,  3.59it/s] 55%|█████▌    | 226/410 [02:02<00:51,  3.60it/s] 55%|█████▌    | 227/410 [02:03<00:52,  3.51it/s] 56%|█████▌    | 228/410 [02:03<00:51,  3.53it/s] 56%|█████▌    | 229/410 [02:03<00:50,  3.56it/s] 56%|█████▌    | 230/410 [02:04<00:53,  3.37it/s] 56%|█████▋    | 231/410 [02:04<00:52,  3.44it/s] 57%|█████▋    | 232/410 [02:04<00:55,  3.18it/s] 57%|█████▋    | 233/410 [02:05<00:56,  3.11it/s] 57%|█████▋    | 234/410 [02:05<01:13,  2.40it/s] 57%|█████▋    | 235/410 [02:06<01:05,  2.66it/s] 58%|█████▊    | 236/410 [02:06<01:00,  2.89it/s] 58%|█████▊    | 237/410 [02:06<00:56,  3.07it/s] 58%|█████▊    | 238/410 [02:06<00:53,  3.22it/s] 58%|█████▊    | 239/410 [02:07<00:55,  3.11it/s] 59%|█████▊    | 240/410 [02:07<00:52,  3.25it/s] 59%|█████▉    | 241/410 [02:07<00:50,  3.35it/s] 59%|█████▉    | 242/410 [02:08<00:48,  3.43it/s] 59%|█████▉    | 243/410 [02:08<00:47,  3.49it/s] 60%|█████▉    | 244/410 [02:08<00:47,  3.53it/s] 60%|█████▉    | 245/410 [02:08<00:46,  3.55it/s] 60%|██████    | 246/410 [02:09<00:44,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 02:42:01,923 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:42:01,923 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 02:42:01,923 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2219, 'eval_samples_per_second': 357.446, 'eval_steps_per_second': 44.712, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.30it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.41it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.64it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.52it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.85it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.25it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.97it/s][A
  4%|▍         | 42/1083 [00:00<00:24, 42.25it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 43.17it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 43.92it/s][A
  5%|▌         | 57/1083 [00:01<00:23, 44.25it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 44.68it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 44.82it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.88it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.75it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.43it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.21it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.60it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.86it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.99it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.21it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.10it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.18it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.96it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.60it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.50it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.61it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.80it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.00it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.25it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.27it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.24it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.95it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.60it/s][A
 16%|█▋        | 177/1083 [00:03<00:21, 43.11it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 43.67it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.22it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.64it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.79it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.04it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.01it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.62it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.47it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.45it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.45it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.63it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.87it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.03it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.25it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.23it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.96it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.73it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.63it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.67it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.70it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.81it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.95it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.17it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.24it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.10it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.88it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 42.89it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 43.41it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 43.78it/s][A
 30%|███       | 327/1083 [00:07<00:17, 44.04it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.40it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.67it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.76it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.89it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.60it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.75it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.77it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.70it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.82it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.93it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.12it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.08it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.02it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.72it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.71it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.71it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.75it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.93it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.97it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.05it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.01it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.96it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.90it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 42.81it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 43.40it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 43.97it/s][A
 43%|████▎     | 462/1083 [00:10<00:14, 44.30it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.56it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.72it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.82it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.80it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.60it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.55it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.69it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.85it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.96it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.00it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.09it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.08it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.99it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.75it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 44.54it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.77it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.84it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.07it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.08it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.21it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.03it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.01it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.84it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 42.39it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 43.28it/s][A
 55%|█████▍    | 592/1083 [00:13<00:11, 42.92it/s][A
 55%|█████▌    | 597/1083 [00:13<00:11, 43.68it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.26it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.47it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.69it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.65it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.41it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.44it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.54it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.79it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.94it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.05it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.12it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.11it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.96it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.74it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.54it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.61it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.81it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.88it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.15it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.14it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.03it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.93it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.72it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.65it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.69it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 44.05it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.47it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.71it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.93it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.01it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.90it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.80it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.73it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.68it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.76it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.85it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.01it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.07it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.12it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.85it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.86it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.79it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.65it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.85it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.85it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.09it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.10it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.12it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.93it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.86it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.73it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.78it/s][A
 80%|███████▉  | 862/1083 [00:19<00:05, 43.93it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.27it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.58it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.85it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.00it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.02it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.84it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.85it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.67it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.81it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.86it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.92it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.06it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.13it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.06it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.82it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.73it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.71it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.56it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.77it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.85it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 45.08it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.09it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.98it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.91it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.76it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.80it/s][A
 92%|█████████▏| 997/1083 [00:22<00:02, 41.56it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 42.69it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 43.57it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.07it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.50it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.56it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.66it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.60it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.35it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.44it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.56it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.91it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.04it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 45.17it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 45.04it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 45.07it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.83it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.56it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.56it/s][A 60%|██████    | 246/410 [02:33<00:44,  3.70it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 02:42:26,311 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 02:42:26,466 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:42:29,882 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:42:30,076 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:42:30,155 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [02:38<24:39,  9.08s/it] 60%|██████    | 248/410 [02:39<17:23,  6.44s/it] 61%|██████    | 249/410 [02:39<12:19,  4.59s/it] 61%|██████    | 250/410 [02:39<08:47,  3.30s/it] 61%|██████    | 251/410 [02:39<06:20,  2.39s/it] 61%|██████▏   | 252/410 [02:40<04:37,  1.76s/it] 62%|██████▏   | 253/410 [02:40<03:28,  1.32s/it] 62%|██████▏   | 254/410 [02:40<02:37,  1.01s/it] 62%|██████▏   | 255/410 [02:41<02:02,  1.26it/s] 62%|██████▏   | 256/410 [02:41<01:38,  1.56it/s] 63%|██████▎   | 257/410 [02:41<01:21,  1.88it/s] 63%|██████▎   | 258/410 [02:41<01:09,  2.19it/s] 63%|██████▎   | 259/410 [02:42<01:00,  2.48it/s] 63%|██████▎   | 260/410 [02:42<00:54,  2.73it/s] 64%|██████▎   | 261/410 [02:42<00:50,  2.94it/s] 64%|██████▍   | 262/410 [02:42<00:47,  3.10it/s] 64%|██████▍   | 263/410 [02:43<00:45,  3.23it/s] 64%|██████▍   | 264/410 [02:43<00:45,  3.24it/s] 65%|██████▍   | 265/410 [02:43<00:43,  3.33it/s] 65%|██████▍   | 266/410 [02:44<00:42,  3.40it/s] 65%|██████▌   | 267/410 [02:44<00:41,  3.45it/s] 65%|██████▌   | 268/410 [02:44<00:40,  3.49it/s] 66%|██████▌   | 269/410 [02:44<00:40,  3.51it/s] 66%|██████▌   | 270/410 [02:45<00:39,  3.53it/s] 66%|██████▌   | 271/410 [02:45<00:39,  3.55it/s] 66%|██████▋   | 272/410 [02:45<00:38,  3.56it/s] 67%|██████▋   | 273/410 [02:46<00:38,  3.56it/s] 67%|██████▋   | 274/410 [02:46<00:38,  3.57it/s] 67%|██████▋   | 275/410 [02:46<00:38,  3.50it/s] 67%|██████▋   | 276/410 [02:46<00:38,  3.52it/s] 68%|██████▊   | 277/410 [02:47<00:37,  3.54it/s] 68%|██████▊   | 278/410 [02:47<00:37,  3.55it/s] 68%|██████▊   | 279/410 [02:47<00:36,  3.55it/s] 68%|██████▊   | 280/410 [02:48<00:36,  3.56it/s] 69%|██████▊   | 281/410 [02:48<00:36,  3.56it/s] 69%|██████▉   | 282/410 [02:48<00:35,  3.56it/s] 69%|██████▉   | 283/410 [02:48<00:35,  3.57it/s] 69%|██████▉   | 284/410 [02:49<00:35,  3.57it/s] 70%|██████▉   | 285/410 [02:49<00:35,  3.57it/s] 70%|██████▉   | 286/410 [02:49<00:35,  3.45it/s] 70%|███████   | 287/410 [02:50<00:35,  3.48it/s] 70%|███████   | 288/410 [02:50<00:34,  3.51it/s] 70%|███████   | 289/410 [02:50<00:34,  3.52it/s] 71%|███████   | 290/410 [02:50<00:33,  3.54it/s] 71%|███████   | 291/410 [02:51<00:33,  3.55it/s] 71%|███████   | 292/410 [02:51<00:33,  3.55it/s] 71%|███████▏  | 293/410 [02:51<00:32,  3.55it/s] 72%|███████▏  | 294/410 [02:52<00:32,  3.56it/s] 72%|███████▏  | 295/410 [02:52<00:32,  3.56it/s] 72%|███████▏  | 296/410 [02:52<00:31,  3.56it/s] 72%|███████▏  | 297/410 [02:52<00:32,  3.47it/s] 73%|███████▎  | 298/410 [02:53<00:31,  3.50it/s] 73%|███████▎  | 299/410 [02:53<00:31,  3.52it/s] 73%|███████▎  | 300/410 [02:53<00:31,  3.54it/s] 73%|███████▎  | 301/410 [02:54<00:30,  3.55it/s] 74%|███████▎  | 302/410 [02:54<00:30,  3.55it/s] 74%|███████▍  | 303/410 [02:54<00:30,  3.56it/s] 74%|███████▍  | 304/410 [02:54<00:29,  3.57it/s] 74%|███████▍  | 305/410 [02:55<00:29,  3.57it/s] 75%|███████▍  | 306/410 [02:55<00:29,  3.57it/s] 75%|███████▍  | 307/410 [02:55<00:28,  3.57it/s] 75%|███████▌  | 308/410 [02:55<00:28,  3.57it/s] 75%|███████▌  | 309/410 [02:56<00:28,  3.51it/s] 76%|███████▌  | 310/410 [02:56<00:28,  3.53it/s] 76%|███████▌  | 311/410 [02:56<00:27,  3.54it/s] 76%|███████▌  | 312/410 [02:57<00:27,  3.55it/s] 76%|███████▋  | 313/410 [02:57<00:27,  3.56it/s] 77%|███████▋  | 314/410 [02:57<00:26,  3.56it/s] 77%|███████▋  | 315/410 [02:57<00:26,  3.56it/s] 77%|███████▋  | 316/410 [02:58<00:26,  3.57it/s] 77%|███████▋  | 317/410 [02:58<00:26,  3.57it/s] 78%|███████▊  | 318/410 [02:58<00:25,  3.57it/s] 78%|███████▊  | 319/410 [02:59<00:25,  3.57it/s] 78%|███████▊  | 320/410 [02:59<00:26,  3.40it/s] 78%|███████▊  | 321/410 [02:59<00:25,  3.45it/s] 79%|███████▊  | 322/410 [02:59<00:25,  3.48it/s] 79%|███████▉  | 323/410 [03:00<00:24,  3.50it/s] 79%|███████▉  | 324/410 [03:00<00:24,  3.52it/s] 79%|███████▉  | 325/410 [03:00<00:24,  3.53it/s] 80%|███████▉  | 326/410 [03:01<00:23,  3.54it/s] 80%|███████▉  | 327/410 [03:01<00:23,  3.55it/s] 80%|████████  | 328/410 [03:01<00:22,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:42:54,411 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:42:54,411 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 02:42:54,411 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2389, 'eval_samples_per_second': 357.195, 'eval_steps_per_second': 44.68, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.42it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.41it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.95it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.81it/s][A
  2%|▏         | 27/1083 [00:00<00:26, 39.77it/s][A
  3%|▎         | 32/1083 [00:00<00:25, 41.56it/s][A
  3%|▎         | 37/1083 [00:00<00:24, 42.63it/s][A
  4%|▍         | 42/1083 [00:00<00:24, 43.33it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 43.74it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.22it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.64it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 44.90it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 44.60it/s][A
  7%|▋         | 72/1083 [00:01<00:25, 39.10it/s][A
  7%|▋         | 77/1083 [00:01<00:24, 41.31it/s][A
  8%|▊         | 82/1083 [00:01<00:23, 42.46it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 43.33it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 43.80it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.16it/s][A
  9%|▉         | 102/1083 [00:02<00:22, 44.58it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.77it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.82it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.49it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.56it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.71it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.87it/s][A
 13%|█▎        | 137/1083 [00:03<00:25, 37.08it/s][A
 13%|█▎        | 142/1083 [00:03<00:23, 39.92it/s][A
 14%|█▎        | 147/1083 [00:03<00:22, 41.42it/s][A
 14%|█▍        | 152/1083 [00:03<00:23, 40.02it/s][A
 14%|█▍        | 157/1083 [00:03<00:23, 39.34it/s][A
 15%|█▍        | 162/1083 [00:03<00:30, 30.48it/s][A
 15%|█▌        | 166/1083 [00:04<00:34, 26.39it/s][A
 16%|█▌        | 171/1083 [00:04<00:29, 30.95it/s][A
 16%|█▋        | 176/1083 [00:04<00:26, 34.35it/s][A
 17%|█▋        | 181/1083 [00:04<00:24, 37.20it/s][A
 17%|█▋        | 186/1083 [00:04<00:22, 39.34it/s][A
 18%|█▊        | 191/1083 [00:04<00:21, 41.03it/s][A
 18%|█▊        | 196/1083 [00:04<00:20, 42.29it/s][A
 19%|█▊        | 201/1083 [00:04<00:20, 43.29it/s][A
 19%|█▉        | 206/1083 [00:04<00:20, 43.63it/s][A
 19%|█▉        | 211/1083 [00:05<00:20, 43.52it/s][A
 20%|█▉        | 216/1083 [00:05<00:19, 43.61it/s][A
 20%|██        | 221/1083 [00:05<00:19, 43.98it/s][A
 21%|██        | 226/1083 [00:05<00:19, 44.34it/s][A
 21%|██▏       | 231/1083 [00:05<00:19, 44.63it/s][A
 22%|██▏       | 236/1083 [00:05<00:18, 44.74it/s][A
 22%|██▏       | 241/1083 [00:05<00:18, 44.89it/s][A
 23%|██▎       | 246/1083 [00:05<00:18, 45.06it/s][A
 23%|██▎       | 251/1083 [00:06<00:18, 45.08it/s][A
 24%|██▎       | 256/1083 [00:06<00:18, 44.84it/s][A
 24%|██▍       | 261/1083 [00:06<00:18, 44.72it/s][A
 25%|██▍       | 266/1083 [00:06<00:18, 44.77it/s][A
 25%|██▌       | 271/1083 [00:06<00:18, 44.87it/s][A
 25%|██▌       | 276/1083 [00:06<00:17, 44.93it/s][A
 26%|██▌       | 281/1083 [00:06<00:18, 43.54it/s][A
 26%|██▋       | 286/1083 [00:06<00:18, 44.12it/s][A
 27%|██▋       | 291/1083 [00:06<00:17, 44.51it/s][A
 27%|██▋       | 296/1083 [00:07<00:17, 44.61it/s][A
 28%|██▊       | 301/1083 [00:07<00:17, 44.58it/s][A
 28%|██▊       | 306/1083 [00:07<00:17, 44.56it/s][A
 29%|██▊       | 311/1083 [00:07<00:17, 44.59it/s][A
 29%|██▉       | 316/1083 [00:07<00:17, 44.65it/s][A
 30%|██▉       | 321/1083 [00:07<00:17, 44.66it/s][A
 30%|███       | 326/1083 [00:07<00:16, 44.86it/s][A
 31%|███       | 331/1083 [00:07<00:16, 45.01it/s][A
 31%|███       | 336/1083 [00:07<00:16, 45.16it/s][A
 31%|███▏      | 341/1083 [00:08<00:16, 44.99it/s][A
 32%|███▏      | 346/1083 [00:08<00:16, 44.95it/s][A
 32%|███▏      | 351/1083 [00:08<00:16, 44.90it/s][A
 33%|███▎      | 356/1083 [00:08<00:16, 44.73it/s][A
 33%|███▎      | 361/1083 [00:08<00:16, 44.78it/s][A
 34%|███▍      | 366/1083 [00:08<00:16, 44.73it/s][A
 34%|███▍      | 371/1083 [00:08<00:15, 44.96it/s][A
 35%|███▍      | 376/1083 [00:08<00:15, 45.01it/s][A
 35%|███▌      | 381/1083 [00:08<00:15, 45.06it/s][A
 36%|███▌      | 386/1083 [00:09<00:15, 45.05it/s][A
 36%|███▌      | 391/1083 [00:09<00:15, 44.95it/s][A
 37%|███▋      | 396/1083 [00:09<00:15, 44.89it/s][A
 37%|███▋      | 401/1083 [00:09<00:15, 44.75it/s][A
 37%|███▋      | 406/1083 [00:09<00:15, 44.75it/s][A
 38%|███▊      | 411/1083 [00:09<00:15, 44.78it/s][A
 38%|███▊      | 416/1083 [00:09<00:17, 39.19it/s][A
 39%|███▉      | 421/1083 [00:09<00:16, 40.79it/s][A
 39%|███▉      | 426/1083 [00:09<00:15, 42.06it/s][A
 40%|███▉      | 431/1083 [00:10<00:15, 42.98it/s][A
 40%|████      | 436/1083 [00:10<00:14, 43.78it/s][A
 41%|████      | 441/1083 [00:10<00:14, 44.27it/s][A
 41%|████      | 446/1083 [00:10<00:14, 44.59it/s][A
 42%|████▏     | 451/1083 [00:10<00:14, 44.63it/s][A
 42%|████▏     | 456/1083 [00:10<00:14, 44.44it/s][A
 43%|████▎     | 461/1083 [00:10<00:14, 44.33it/s][A
 43%|████▎     | 466/1083 [00:10<00:13, 44.34it/s][A
 43%|████▎     | 471/1083 [00:10<00:13, 44.59it/s][A
 44%|████▍     | 476/1083 [00:11<00:13, 44.70it/s][A
 44%|████▍     | 481/1083 [00:11<00:13, 44.93it/s][A
 45%|████▍     | 486/1083 [00:11<00:13, 45.15it/s][A
 45%|████▌     | 491/1083 [00:11<00:13, 45.18it/s][A
 46%|████▌     | 496/1083 [00:11<00:13, 45.13it/s][A
 46%|████▋     | 501/1083 [00:11<00:12, 44.84it/s][A
 47%|████▋     | 506/1083 [00:11<00:12, 44.61it/s][A
 47%|████▋     | 511/1083 [00:11<00:12, 44.59it/s][A
 48%|████▊     | 516/1083 [00:11<00:12, 44.75it/s][A
 48%|████▊     | 521/1083 [00:12<00:12, 44.80it/s][A
 49%|████▊     | 526/1083 [00:12<00:12, 44.99it/s][A
 49%|████▉     | 531/1083 [00:12<00:12, 45.08it/s][A
 49%|████▉     | 536/1083 [00:12<00:12, 45.22it/s][A
 50%|████▉     | 541/1083 [00:12<00:11, 45.17it/s][A
 50%|█████     | 546/1083 [00:12<00:11, 44.95it/s][A
 51%|█████     | 551/1083 [00:12<00:12, 43.78it/s][A
 51%|█████▏    | 556/1083 [00:12<00:11, 44.13it/s][A
 52%|█████▏    | 561/1083 [00:12<00:11, 44.32it/s][A
 52%|█████▏    | 566/1083 [00:13<00:11, 44.56it/s][A
 53%|█████▎    | 571/1083 [00:13<00:11, 44.78it/s][A
 53%|█████▎    | 576/1083 [00:13<00:11, 44.89it/s][A
 54%|█████▎    | 581/1083 [00:13<00:11, 45.06it/s][A
 54%|█████▍    | 586/1083 [00:13<00:11, 45.00it/s][A
 55%|█████▍    | 591/1083 [00:13<00:10, 44.88it/s][A
 55%|█████▌    | 596/1083 [00:13<00:10, 44.68it/s][A
 55%|█████▌    | 601/1083 [00:13<00:10, 44.70it/s][A
 56%|█████▌    | 606/1083 [00:13<00:10, 44.74it/s][A
 56%|█████▋    | 611/1083 [00:14<00:10, 44.87it/s][A
 57%|█████▋    | 616/1083 [00:14<00:10, 45.01it/s][A
 57%|█████▋    | 621/1083 [00:14<00:10, 45.09it/s][A
 58%|█████▊    | 626/1083 [00:14<00:10, 45.21it/s][A
 58%|█████▊    | 631/1083 [00:14<00:10, 44.99it/s][A
 59%|█████▊    | 636/1083 [00:14<00:09, 44.87it/s][A
 59%|█████▉    | 641/1083 [00:14<00:09, 44.69it/s][A
 60%|█████▉    | 646/1083 [00:14<00:09, 44.72it/s][A
 60%|██████    | 651/1083 [00:14<00:09, 44.76it/s][A
 61%|██████    | 656/1083 [00:15<00:09, 44.87it/s][A
 61%|██████    | 661/1083 [00:15<00:09, 45.06it/s][A
 61%|██████▏   | 666/1083 [00:15<00:09, 45.18it/s][A
 62%|██████▏   | 671/1083 [00:15<00:09, 45.24it/s][A
 62%|██████▏   | 676/1083 [00:15<00:09, 45.04it/s][A
 63%|██████▎   | 681/1083 [00:15<00:08, 44.79it/s][A
 63%|██████▎   | 686/1083 [00:15<00:09, 43.84it/s][A
 64%|██████▍   | 691/1083 [00:15<00:08, 44.20it/s][A
 64%|██████▍   | 696/1083 [00:15<00:08, 44.51it/s][A
 65%|██████▍   | 701/1083 [00:16<00:08, 44.65it/s][A
 65%|██████▌   | 706/1083 [00:16<00:08, 44.75it/s][A
 66%|██████▌   | 711/1083 [00:16<00:08, 44.85it/s][A
 66%|██████▌   | 716/1083 [00:16<00:08, 44.68it/s][A
 67%|██████▋   | 721/1083 [00:16<00:08, 44.93it/s][A
 67%|██████▋   | 726/1083 [00:16<00:07, 44.71it/s][A
 67%|██████▋   | 731/1083 [00:16<00:07, 44.61it/s][A
 68%|██████▊   | 736/1083 [00:16<00:07, 44.78it/s][A
 68%|██████▊   | 741/1083 [00:16<00:07, 44.93it/s][A
 69%|██████▉   | 746/1083 [00:17<00:07, 45.02it/s][A
 69%|██████▉   | 751/1083 [00:17<00:07, 45.05it/s][A
 70%|██████▉   | 756/1083 [00:17<00:07, 45.13it/s][A
 70%|███████   | 761/1083 [00:17<00:07, 45.02it/s][A
 71%|███████   | 766/1083 [00:17<00:07, 44.92it/s][A
 71%|███████   | 771/1083 [00:17<00:06, 44.75it/s][A
 72%|███████▏  | 776/1083 [00:17<00:06, 44.67it/s][A
 72%|███████▏  | 781/1083 [00:17<00:06, 44.79it/s][A
 73%|███████▎  | 786/1083 [00:17<00:06, 44.80it/s][A
 73%|███████▎  | 791/1083 [00:18<00:06, 45.03it/s][A
 73%|███████▎  | 796/1083 [00:18<00:06, 45.01it/s][A
 74%|███████▍  | 801/1083 [00:18<00:06, 45.11it/s][A
 74%|███████▍  | 806/1083 [00:18<00:06, 45.04it/s][A
 75%|███████▍  | 811/1083 [00:18<00:06, 44.92it/s][A
 75%|███████▌  | 816/1083 [00:18<00:05, 44.75it/s][A
 76%|███████▌  | 821/1083 [00:18<00:05, 43.95it/s][A
 76%|███████▋  | 826/1083 [00:18<00:05, 44.19it/s][A
 77%|███████▋  | 831/1083 [00:19<00:05, 44.56it/s][A
 77%|███████▋  | 836/1083 [00:19<00:05, 44.72it/s][A
 78%|███████▊  | 841/1083 [00:19<00:05, 44.87it/s][A
 78%|███████▊  | 846/1083 [00:19<00:05, 44.95it/s][A
 79%|███████▊  | 851/1083 [00:19<00:05, 44.98it/s][A
 79%|███████▉  | 856/1083 [00:19<00:05, 44.86it/s][A
 80%|███████▉  | 861/1083 [00:19<00:04, 44.60it/s][A
 80%|███████▉  | 866/1083 [00:19<00:04, 44.59it/s][A
 80%|████████  | 871/1083 [00:19<00:04, 44.71it/s][A
 81%|████████  | 876/1083 [00:20<00:04, 44.87it/s][A
 81%|████████▏ | 881/1083 [00:20<00:04, 44.88it/s][A
 82%|████████▏ | 886/1083 [00:20<00:04, 45.11it/s][A
 82%|████████▏ | 891/1083 [00:20<00:04, 45.16it/s][A
 83%|████████▎ | 896/1083 [00:20<00:04, 45.12it/s][A
 83%|████████▎ | 901/1083 [00:20<00:04, 45.00it/s][A
 84%|████████▎ | 906/1083 [00:20<00:03, 44.68it/s][A
 84%|████████▍ | 911/1083 [00:20<00:03, 44.70it/s][A
 85%|████████▍ | 916/1083 [00:20<00:03, 44.71it/s][A
 85%|████████▌ | 921/1083 [00:21<00:03, 44.66it/s][A
 86%|████████▌ | 926/1083 [00:21<00:03, 44.87it/s][A
 86%|████████▌ | 931/1083 [00:21<00:03, 44.95it/s][A
 86%|████████▋ | 936/1083 [00:21<00:03, 45.11it/s][A
 87%|████████▋ | 941/1083 [00:21<00:03, 45.17it/s][A
 87%|████████▋ | 946/1083 [00:21<00:03, 45.04it/s][A
 88%|████████▊ | 951/1083 [00:21<00:02, 44.81it/s][A
 88%|████████▊ | 956/1083 [00:21<00:02, 43.96it/s][A
 89%|████████▊ | 961/1083 [00:21<00:02, 44.24it/s][A
 89%|████████▉ | 966/1083 [00:22<00:02, 44.43it/s][A
 90%|████████▉ | 971/1083 [00:22<00:02, 44.56it/s][A
 90%|█████████ | 976/1083 [00:22<00:02, 44.76it/s][A
 91%|█████████ | 981/1083 [00:22<00:02, 44.94it/s][A
 91%|█████████ | 986/1083 [00:22<00:02, 45.07it/s][A
 92%|█████████▏| 991/1083 [00:22<00:02, 44.97it/s][A
 92%|█████████▏| 996/1083 [00:22<00:01, 44.73it/s][A
 92%|█████████▏| 1001/1083 [00:22<00:01, 44.61it/s][A
 93%|█████████▎| 1006/1083 [00:22<00:01, 44.74it/s][A
 93%|█████████▎| 1011/1083 [00:23<00:01, 44.78it/s][A
 94%|█████████▍| 1016/1083 [00:23<00:01, 44.94it/s][A
 94%|█████████▍| 1021/1083 [00:23<00:01, 44.95it/s][A
 95%|█████████▍| 1026/1083 [00:23<00:01, 45.00it/s][A
 95%|█████████▌| 1031/1083 [00:23<00:01, 45.16it/s][A
 96%|█████████▌| 1036/1083 [00:23<00:01, 45.01it/s][A
 96%|█████████▌| 1041/1083 [00:23<00:00, 44.88it/s][A
 97%|█████████▋| 1046/1083 [00:23<00:00, 44.73it/s][A
 97%|█████████▋| 1051/1083 [00:23<00:00, 44.74it/s][A
 98%|█████████▊| 1056/1083 [00:24<00:00, 44.84it/s][A
 98%|█████████▊| 1061/1083 [00:24<00:00, 44.83it/s][A
 98%|█████████▊| 1066/1083 [00:24<00:00, 44.97it/s][A
 99%|█████████▉| 1071/1083 [00:24<00:00, 44.98it/s][A
 99%|█████████▉| 1076/1083 [00:24<00:00, 45.03it/s][A
100%|█████████▉| 1081/1083 [00:24<00:00, 44.98it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.98it/s][A 80%|████████  | 328/410 [03:26<00:22,  3.69it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 02:43:19,134 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 02:43:19,230 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:43:23,322 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:43:23,457 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:43:23,515 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [03:31<12:32,  9.29s/it] 80%|████████  | 330/410 [03:32<08:47,  6.59s/it] 81%|████████  | 331/410 [03:32<06:10,  4.70s/it] 81%|████████  | 332/410 [03:32<04:22,  3.37s/it] 81%|████████  | 333/410 [03:33<03:08,  2.44s/it] 81%|████████▏ | 334/410 [03:33<02:16,  1.80s/it] 82%|████████▏ | 335/410 [03:33<01:40,  1.34s/it] 82%|████████▏ | 336/410 [03:33<01:16,  1.03s/it] 82%|████████▏ | 337/410 [03:34<00:58,  1.24it/s] 82%|████████▏ | 338/410 [03:34<00:46,  1.54it/s] 83%|████████▎ | 339/410 [03:34<00:38,  1.86it/s] 83%|████████▎ | 340/410 [03:35<00:32,  2.17it/s] 83%|████████▎ | 341/410 [03:35<00:28,  2.46it/s] 83%|████████▎ | 342/410 [03:35<00:25,  2.71it/s] 84%|████████▎ | 343/410 [03:35<00:22,  2.92it/s] 84%|████████▍ | 344/410 [03:36<00:21,  3.09it/s] 84%|████████▍ | 345/410 [03:36<00:20,  3.21it/s] 84%|████████▍ | 346/410 [03:36<00:19,  3.31it/s] 85%|████████▍ | 347/410 [03:37<00:19,  3.30it/s] 85%|████████▍ | 348/410 [03:37<00:18,  3.37it/s] 85%|████████▌ | 349/410 [03:37<00:17,  3.43it/s] 85%|████████▌ | 350/410 [03:37<00:17,  3.48it/s] 86%|████████▌ | 351/410 [03:38<00:16,  3.52it/s] 86%|████████▌ | 352/410 [03:38<00:16,  3.55it/s] 86%|████████▌ | 353/410 [03:38<00:15,  3.57it/s] 86%|████████▋ | 354/410 [03:38<00:15,  3.58it/s] 87%|████████▋ | 355/410 [03:39<00:15,  3.60it/s] 87%|████████▋ | 356/410 [03:39<00:14,  3.61it/s] 87%|████████▋ | 357/410 [03:39<00:14,  3.61it/s] 87%|████████▋ | 358/410 [03:40<00:14,  3.56it/s] 88%|████████▊ | 359/410 [03:40<00:14,  3.58it/s] 88%|████████▊ | 360/410 [03:40<00:13,  3.59it/s] 88%|████████▊ | 361/410 [03:40<00:13,  3.60it/s] 88%|████████▊ | 362/410 [03:41<00:13,  3.61it/s] 89%|████████▊ | 363/410 [03:41<00:13,  3.61it/s] 89%|████████▉ | 364/410 [03:41<00:12,  3.62it/s] 89%|████████▉ | 365/410 [03:42<00:12,  3.62it/s] 89%|████████▉ | 366/410 [03:42<00:12,  3.62it/s] 90%|████████▉ | 367/410 [03:42<00:11,  3.62it/s] 90%|████████▉ | 368/410 [03:42<00:11,  3.62it/s] 90%|█████████ | 369/410 [03:43<00:11,  3.49it/s] 90%|█████████ | 370/410 [03:43<00:11,  3.53it/s] 90%|█████████ | 371/410 [03:43<00:10,  3.55it/s] 91%|█████████ | 372/410 [03:44<00:10,  3.58it/s] 91%|█████████ | 373/410 [03:44<00:10,  3.59it/s] 91%|█████████ | 374/410 [03:44<00:09,  3.60it/s] 91%|█████████▏| 375/410 [03:44<00:09,  3.61it/s] 92%|█████████▏| 376/410 [03:45<00:09,  3.61it/s] 92%|█████████▏| 377/410 [03:45<00:09,  3.61it/s] 92%|█████████▏| 378/410 [03:45<00:08,  3.62it/s] 92%|█████████▏| 379/410 [03:45<00:08,  3.62it/s] 93%|█████████▎| 380/410 [03:46<00:08,  3.55it/s] 93%|█████████▎| 381/410 [03:46<00:08,  3.57it/s] 93%|█████████▎| 382/410 [03:46<00:07,  3.59it/s] 93%|█████████▎| 383/410 [03:47<00:07,  3.60it/s] 94%|█████████▎| 384/410 [03:47<00:07,  3.60it/s] 94%|█████████▍| 385/410 [03:47<00:06,  3.60it/s] 94%|█████████▍| 386/410 [03:47<00:06,  3.61it/s] 94%|█████████▍| 387/410 [03:48<00:06,  3.61it/s] 95%|█████████▍| 388/410 [03:48<00:06,  3.62it/s] 95%|█████████▍| 389/410 [03:48<00:05,  3.62it/s] 95%|█████████▌| 390/410 [03:49<00:05,  3.62it/s] 95%|█████████▌| 391/410 [03:49<00:05,  3.52it/s] 96%|█████████▌| 392/410 [03:49<00:05,  3.55it/s] 96%|█████████▌| 393/410 [03:49<00:04,  3.57it/s] 96%|█████████▌| 394/410 [03:50<00:04,  3.58it/s] 96%|█████████▋| 395/410 [03:50<00:04,  3.60it/s] 97%|█████████▋| 396/410 [03:50<00:03,  3.61it/s] 97%|█████████▋| 397/410 [03:50<00:03,  3.61it/s] 97%|█████████▋| 398/410 [03:51<00:03,  3.61it/s] 97%|█████████▋| 399/410 [03:51<00:03,  3.62it/s] 98%|█████████▊| 400/410 [03:51<00:02,  3.62it/s] 98%|█████████▊| 401/410 [03:52<00:02,  3.62it/s] 98%|█████████▊| 402/410 [03:52<00:02,  3.57it/s] 98%|█████████▊| 403/410 [03:52<00:01,  3.59it/s] 99%|█████████▊| 404/410 [03:52<00:01,  3.60it/s] 99%|█████████▉| 405/410 [03:53<00:01,  3.60it/s] 99%|█████████▉| 406/410 [03:53<00:01,  3.61it/s] 99%|█████████▉| 407/410 [03:53<00:00,  3.61it/s]100%|█████████▉| 408/410 [03:54<00:00,  3.61it/s]100%|█████████▉| 409/410 [03:54<00:00,  3.62it/s]100%|██████████| 410/410 [03:54<00:00,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 02:43:47,338 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:43:47,338 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 02:43:47,338 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.6371, 'eval_samples_per_second': 351.421, 'eval_steps_per_second': 43.958, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.67it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.29it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.26it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.23it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.65it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.27it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.02it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.82it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.99it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.10it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.29it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.25it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.02it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.89it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.91it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.79it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.69it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.72it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.75it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.96it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.09it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.18it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.03it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.05it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.94it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.93it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.61it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.70it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.88it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.96it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.00it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.99it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.88it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.95it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.78it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.69it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.82it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.88it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.04it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.04it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.04it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.99it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.97it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.78it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.76it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.75it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.90it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.85it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.98it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.05it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.98it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 45.02it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.80it/s][A
 25%|██▌       | 272/1083 [00:06<00:19, 40.98it/s][A
 26%|██▌       | 277/1083 [00:06<00:19, 42.20it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 43.23it/s][A
 27%|██▋       | 287/1083 [00:06<00:18, 43.85it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.35it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.47it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.67it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.61it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.38it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.29it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.58it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.66it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.91it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.05it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.23it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.20it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.03it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.87it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.78it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.83it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.89it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.87it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.04it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.14it/s][A
 36%|███▌      | 392/1083 [00:08<00:16, 42.65it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 43.48it/s][A
 37%|███▋      | 402/1083 [00:09<00:15, 43.91it/s][A
 38%|███▊      | 407/1083 [00:09<00:17, 39.55it/s][A
 38%|███▊      | 412/1083 [00:09<00:16, 41.20it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 42.43it/s][A
 39%|███▉      | 422/1083 [00:09<00:15, 43.21it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 43.94it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.26it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.39it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.51it/s][A
 41%|████▏     | 447/1083 [00:10<00:14, 44.29it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.26it/s][A
 42%|████▏     | 457/1083 [00:10<00:16, 38.09it/s][A
 43%|████▎     | 462/1083 [00:10<00:15, 40.04it/s][A
 43%|████▎     | 467/1083 [00:10<00:16, 37.86it/s][A
 44%|████▎     | 473/1083 [00:10<00:14, 41.20it/s][A
 44%|████▍     | 478/1083 [00:10<00:14, 42.34it/s][A
 45%|████▍     | 483/1083 [00:10<00:17, 35.18it/s][A
 45%|████▍     | 487/1083 [00:11<00:20, 29.54it/s][A
 46%|████▌     | 493/1083 [00:11<00:17, 34.41it/s][A
 46%|████▌     | 498/1083 [00:11<00:15, 37.05it/s][A
 46%|████▋     | 503/1083 [00:11<00:14, 39.14it/s][A
 47%|████▋     | 508/1083 [00:11<00:14, 40.92it/s][A
 47%|████▋     | 513/1083 [00:11<00:13, 42.15it/s][A
 48%|████▊     | 518/1083 [00:11<00:13, 43.15it/s][A
 48%|████▊     | 523/1083 [00:12<00:12, 43.72it/s][A
 49%|████▉     | 528/1083 [00:12<00:13, 40.88it/s][A
 49%|████▉     | 533/1083 [00:12<00:13, 41.87it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 42.71it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 43.46it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 43.94it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.41it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.79it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.99it/s][A
 52%|█████▏    | 568/1083 [00:13<00:11, 44.61it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 44.40it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 44.36it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.65it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.75it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.92it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.94it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.13it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.21it/s][A
 57%|█████▋    | 613/1083 [00:14<00:10, 45.01it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 44.76it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 44.53it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.69it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.75it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.95it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 45.14it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 45.11it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.12it/s][A
 61%|██████    | 658/1083 [00:15<00:09, 44.94it/s][A
 61%|██████    | 663/1083 [00:15<00:10, 41.67it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 42.74it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 43.39it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 43.88it/s][A
 63%|██████▎   | 683/1083 [00:15<00:09, 44.23it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.60it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.84it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.83it/s][A
 65%|██████▍   | 703/1083 [00:16<00:08, 44.56it/s][A
 65%|██████▌   | 708/1083 [00:16<00:08, 44.49it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.72it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.79it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.85it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.92it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.08it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.13it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.75it/s][A
 69%|██████▉   | 748/1083 [00:17<00:07, 44.81it/s][A
 70%|██████▉   | 753/1083 [00:17<00:07, 44.66it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.80it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.85it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.91it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 45.00it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.95it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.99it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.91it/s][A
 73%|███████▎  | 793/1083 [00:18<00:06, 44.81it/s][A
 74%|███████▎  | 798/1083 [00:18<00:06, 44.25it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.39it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.64it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.72it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.84it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.88it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.97it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.93it/s][A
 77%|███████▋  | 838/1083 [00:19<00:05, 44.64it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 44.69it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.79it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.83it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.82it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.05it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 45.03it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 45.00it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.90it/s][A
 82%|████████▏ | 883/1083 [00:20<00:04, 44.76it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 44.81it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.82it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.92it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.89it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 45.01it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.99it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.96it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.90it/s][A
 86%|████████▌ | 928/1083 [00:21<00:03, 44.80it/s][A
 86%|████████▌ | 933/1083 [00:21<00:03, 44.47it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 44.69it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.80it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.89it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.94it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.94it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.85it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.60it/s][A
 90%|████████▉ | 973/1083 [00:22<00:02, 44.81it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 44.88it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 44.85it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.95it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.93it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 45.03it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.93it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.94it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.75it/s][A
 94%|█████████▍| 1018/1083 [00:23<00:01, 44.75it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 44.87it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 44.92it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 45.00it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.95it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.96it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.89it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.93it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.88it/s][A
 98%|█████████▊| 1063/1083 [00:24<00:00, 44.70it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 44.24it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 44.57it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.84it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.82it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.82it/s][A100%|██████████| 410/410 [04:19<00:00,  3.76it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 02:44:12,007 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 02:44:12,144 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:44:15,182 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:44:15,388 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:44:15,474 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:44:16,717 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:44:16,717 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82 (score: 0.9504339694976807).
                                                 100%|██████████| 410/410 [04:31<00:00,  3.76it/s]100%|██████████| 410/410 [04:31<00:00,  1.51it/s]
[INFO|trainer.py:1894] 2023-08-28 02:44:24,851 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 02:44:24,963 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:44:28,255 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:44:28,391 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:44:28,461 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:44:28,932 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:28,933 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:28,933 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:28,933 >>   train_runtime            = 0:04:31.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:28,933 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:28,933 >>   train_samples_per_second =     96.341
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:28,933 >>   train_steps_per_second   =      1.508
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.5291, 'eval_samples_per_second': 352.968, 'eval_steps_per_second': 44.152, 'epoch': 5.0}
{'train_runtime': 271.9507, 'train_samples_per_second': 96.341, 'train_steps_per_second': 1.508, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:44:29 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:44:29,186 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:44:29,186 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 02:44:29,186 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 56.18it/s]  1%|          | 12/1083 [00:00<00:21, 49.24it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.69it/s]  2%|▏         | 22/1083 [00:00<00:22, 46.86it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.47it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.14it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.87it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.59it/s]  4%|▍         | 47/1083 [00:01<00:23, 44.93it/s]  5%|▍         | 52/1083 [00:01<00:23, 44.73it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.76it/s]  6%|▌         | 62/1083 [00:01<00:22, 44.86it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.08it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.18it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.32it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.33it/s]  8%|▊         | 87/1083 [00:01<00:22, 45.19it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.96it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.70it/s]  9%|▉         | 102/1083 [00:02<00:22, 44.33it/s] 10%|▉         | 107/1083 [00:02<00:21, 44.66it/s] 10%|█         | 112/1083 [00:02<00:21, 44.87it/s] 11%|█         | 117/1083 [00:02<00:21, 45.03it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.14it/s] 12%|█▏        | 127/1083 [00:02<00:21, 45.19it/s] 12%|█▏        | 132/1083 [00:02<00:21, 45.18it/s] 13%|█▎        | 137/1083 [00:03<00:21, 44.88it/s] 13%|█▎        | 142/1083 [00:03<00:21, 44.69it/s] 14%|█▎        | 147/1083 [00:03<00:20, 44.68it/s] 14%|█▍        | 152/1083 [00:03<00:20, 44.83it/s] 14%|█▍        | 157/1083 [00:03<00:20, 45.08it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.15it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.23it/s] 16%|█▌        | 172/1083 [00:03<00:20, 45.20it/s] 16%|█▋        | 177/1083 [00:03<00:20, 45.17it/s] 17%|█▋        | 182/1083 [00:04<00:20, 44.90it/s] 17%|█▋        | 187/1083 [00:04<00:20, 44.76it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.79it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.83it/s] 19%|█▊        | 202/1083 [00:04<00:19, 45.01it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.08it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.14it/s] 20%|██        | 217/1083 [00:04<00:19, 45.19it/s] 20%|██        | 222/1083 [00:04<00:19, 45.04it/s] 21%|██        | 227/1083 [00:05<00:19, 44.94it/s] 21%|██▏       | 232/1083 [00:05<00:19, 44.76it/s] 22%|██▏       | 237/1083 [00:05<00:19, 43.51it/s] 22%|██▏       | 242/1083 [00:05<00:19, 44.11it/s] 23%|██▎       | 247/1083 [00:05<00:18, 44.52it/s] 23%|██▎       | 252/1083 [00:05<00:18, 44.82it/s] 24%|██▎       | 257/1083 [00:05<00:18, 44.91it/s] 24%|██▍       | 262/1083 [00:05<00:18, 44.99it/s] 25%|██▍       | 267/1083 [00:05<00:18, 44.93it/s] 25%|██▌       | 272/1083 [00:06<00:18, 44.81it/s] 26%|██▌       | 277/1083 [00:06<00:18, 44.52it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.57it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.81it/s] 27%|██▋       | 292/1083 [00:06<00:17, 44.97it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.22it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.04it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.10it/s] 29%|██▉       | 312/1083 [00:06<00:17, 44.96it/s] 29%|██▉       | 317/1083 [00:07<00:17, 44.75it/s] 30%|██▉       | 322/1083 [00:07<00:17, 44.63it/s] 30%|███       | 327/1083 [00:07<00:16, 44.59it/s] 31%|███       | 332/1083 [00:07<00:16, 44.69it/s] 31%|███       | 337/1083 [00:07<00:16, 44.89it/s] 32%|███▏      | 342/1083 [00:07<00:16, 44.91it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.16it/s] 33%|███▎      | 352/1083 [00:07<00:16, 45.07it/s] 33%|███▎      | 357/1083 [00:07<00:16, 45.09it/s] 33%|███▎      | 362/1083 [00:08<00:16, 44.90it/s] 34%|███▍      | 367/1083 [00:08<00:16, 44.64it/s] 34%|███▍      | 372/1083 [00:08<00:16, 43.24it/s] 35%|███▍      | 377/1083 [00:08<00:16, 43.91it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.32it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.65it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.89it/s] 37%|███▋      | 397/1083 [00:08<00:15, 44.92it/s] 37%|███▋      | 402/1083 [00:08<00:15, 44.99it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.81it/s] 38%|███▊      | 412/1083 [00:09<00:15, 44.44it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.53it/s] 39%|███▉      | 422/1083 [00:09<00:15, 42.25it/s] 40%|███▉      | 428/1083 [00:09<00:14, 44.53it/s] 40%|███▉      | 433/1083 [00:09<00:14, 44.73it/s] 40%|████      | 438/1083 [00:09<00:14, 44.90it/s] 41%|████      | 443/1083 [00:09<00:14, 45.10it/s] 41%|████▏     | 448/1083 [00:09<00:14, 44.94it/s] 42%|████▏     | 453/1083 [00:10<00:14, 44.84it/s] 42%|████▏     | 458/1083 [00:10<00:14, 44.64it/s] 43%|████▎     | 463/1083 [00:10<00:13, 44.45it/s] 43%|████▎     | 468/1083 [00:10<00:13, 44.72it/s] 44%|████▎     | 473/1083 [00:10<00:13, 44.81it/s] 44%|████▍     | 478/1083 [00:10<00:13, 45.03it/s] 45%|████▍     | 483/1083 [00:10<00:13, 45.04it/s] 45%|████▌     | 488/1083 [00:10<00:13, 45.27it/s] 46%|████▌     | 493/1083 [00:10<00:13, 45.15it/s] 46%|████▌     | 498/1083 [00:11<00:13, 44.97it/s] 46%|████▋     | 503/1083 [00:11<00:12, 44.75it/s] 47%|████▋     | 508/1083 [00:11<00:13, 43.86it/s] 47%|████▋     | 513/1083 [00:11<00:12, 44.17it/s] 48%|████▊     | 518/1083 [00:11<00:12, 44.37it/s] 48%|████▊     | 523/1083 [00:11<00:12, 44.65it/s] 49%|████▉     | 528/1083 [00:11<00:12, 44.76it/s] 49%|████▉     | 533/1083 [00:11<00:12, 44.95it/s] 50%|████▉     | 538/1083 [00:11<00:12, 45.02it/s] 50%|█████     | 543/1083 [00:12<00:12, 44.87it/s] 51%|█████     | 548/1083 [00:12<00:11, 44.58it/s] 51%|█████     | 553/1083 [00:12<00:11, 44.69it/s] 52%|█████▏    | 558/1083 [00:12<00:11, 44.69it/s] 52%|█████▏    | 563/1083 [00:12<00:11, 44.86it/s] 52%|█████▏    | 568/1083 [00:12<00:11, 44.93it/s] 53%|█████▎    | 573/1083 [00:12<00:11, 45.03it/s] 53%|█████▎    | 578/1083 [00:12<00:11, 45.03it/s] 54%|█████▍    | 583/1083 [00:12<00:11, 45.00it/s] 54%|█████▍    | 588/1083 [00:13<00:11, 44.96it/s] 55%|█████▍    | 593/1083 [00:13<00:10, 44.65it/s] 55%|█████▌    | 598/1083 [00:13<00:10, 44.70it/s] 56%|█████▌    | 603/1083 [00:13<00:10, 44.71it/s] 56%|█████▌    | 608/1083 [00:13<00:10, 44.80it/s] 57%|█████▋    | 613/1083 [00:13<00:10, 44.86it/s] 57%|█████▋    | 618/1083 [00:13<00:10, 44.99it/s] 58%|█████▊    | 623/1083 [00:13<00:10, 45.06it/s] 58%|█████▊    | 628/1083 [00:13<00:10, 45.09it/s] 58%|█████▊    | 633/1083 [00:14<00:09, 45.01it/s] 59%|█████▉    | 638/1083 [00:14<00:09, 44.71it/s] 59%|█████▉    | 643/1083 [00:14<00:10, 43.43it/s] 60%|█████▉    | 648/1083 [00:14<00:09, 43.94it/s] 60%|██████    | 653/1083 [00:14<00:09, 44.30it/s] 61%|██████    | 658/1083 [00:14<00:09, 44.55it/s] 61%|██████    | 663/1083 [00:14<00:09, 44.66it/s] 62%|██████▏   | 668/1083 [00:14<00:09, 44.88it/s] 62%|██████▏   | 673/1083 [00:14<00:09, 44.99it/s] 63%|██████▎   | 678/1083 [00:15<00:09, 44.88it/s] 63%|██████▎   | 683/1083 [00:15<00:08, 44.75it/s] 64%|██████▎   | 688/1083 [00:15<00:08, 44.69it/s] 64%|██████▍   | 693/1083 [00:15<00:08, 44.79it/s] 64%|██████▍   | 698/1083 [00:15<00:08, 45.01it/s] 65%|██████▍   | 703/1083 [00:15<00:08, 45.21it/s] 65%|██████▌   | 708/1083 [00:15<00:08, 45.31it/s] 66%|██████▌   | 713/1083 [00:15<00:08, 45.27it/s] 66%|██████▋   | 718/1083 [00:15<00:08, 45.18it/s] 67%|██████▋   | 723/1083 [00:16<00:08, 44.97it/s] 67%|██████▋   | 728/1083 [00:16<00:07, 44.93it/s] 68%|██████▊   | 733/1083 [00:16<00:07, 44.85it/s] 68%|██████▊   | 738/1083 [00:16<00:07, 44.89it/s] 69%|██████▊   | 743/1083 [00:16<00:07, 44.97it/s] 69%|██████▉   | 748/1083 [00:16<00:07, 45.17it/s] 70%|██████▉   | 753/1083 [00:16<00:07, 45.30it/s] 70%|██████▉   | 758/1083 [00:16<00:07, 45.35it/s] 70%|███████   | 763/1083 [00:16<00:07, 45.27it/s] 71%|███████   | 768/1083 [00:17<00:06, 45.04it/s] 71%|███████▏  | 773/1083 [00:17<00:06, 44.89it/s] 72%|███████▏  | 778/1083 [00:17<00:06, 44.50it/s] 72%|███████▏  | 783/1083 [00:17<00:06, 44.59it/s] 73%|███████▎  | 788/1083 [00:17<00:06, 44.63it/s] 73%|███████▎  | 793/1083 [00:17<00:06, 44.82it/s] 74%|███████▎  | 798/1083 [00:17<00:06, 45.16it/s] 74%|███████▍  | 803/1083 [00:17<00:06, 45.17it/s] 75%|███████▍  | 808/1083 [00:17<00:06, 45.15it/s] 75%|███████▌  | 813/1083 [00:18<00:05, 45.00it/s] 76%|███████▌  | 818/1083 [00:18<00:05, 44.83it/s] 76%|███████▌  | 823/1083 [00:18<00:05, 44.75it/s] 76%|███████▋  | 828/1083 [00:18<00:05, 44.70it/s] 77%|███████▋  | 833/1083 [00:18<00:05, 44.91it/s] 77%|███████▋  | 838/1083 [00:18<00:05, 45.09it/s] 78%|███████▊  | 843/1083 [00:18<00:05, 45.23it/s] 78%|███████▊  | 848/1083 [00:18<00:05, 45.21it/s] 79%|███████▉  | 853/1083 [00:18<00:05, 45.21it/s] 79%|███████▉  | 858/1083 [00:19<00:04, 45.08it/s] 80%|███████▉  | 863/1083 [00:19<00:04, 44.97it/s] 80%|████████  | 868/1083 [00:19<00:04, 44.81it/s] 81%|████████  | 873/1083 [00:19<00:04, 44.78it/s] 81%|████████  | 878/1083 [00:19<00:04, 41.68it/s] 82%|████████▏ | 883/1083 [00:19<00:04, 42.80it/s] 82%|████████▏ | 888/1083 [00:19<00:04, 43.61it/s] 82%|████████▏ | 893/1083 [00:19<00:04, 44.19it/s] 83%|████████▎ | 898/1083 [00:20<00:04, 44.57it/s] 83%|████████▎ | 903/1083 [00:20<00:04, 44.79it/s] 84%|████████▍ | 908/1083 [00:20<00:03, 44.82it/s] 84%|████████▍ | 913/1083 [00:20<00:03, 44.78it/s] 85%|████████▍ | 918/1083 [00:20<00:03, 44.48it/s] 85%|████████▌ | 923/1083 [00:20<00:03, 44.44it/s] 86%|████████▌ | 928/1083 [00:20<00:03, 44.64it/s] 86%|████████▌ | 933/1083 [00:20<00:03, 44.79it/s] 87%|████████▋ | 938/1083 [00:20<00:03, 45.08it/s] 87%|████████▋ | 943/1083 [00:21<00:03, 45.19it/s] 88%|████████▊ | 948/1083 [00:21<00:02, 45.25it/s] 88%|████████▊ | 953/1083 [00:21<00:02, 45.33it/s] 88%|████████▊ | 958/1083 [00:21<00:02, 45.21it/s] 89%|████████▉ | 963/1083 [00:21<00:02, 44.93it/s] 89%|████████▉ | 968/1083 [00:21<00:02, 44.76it/s] 90%|████████▉ | 973/1083 [00:21<00:02, 44.88it/s] 90%|█████████ | 978/1083 [00:21<00:02, 44.92it/s] 91%|█████████ | 983/1083 [00:21<00:02, 44.94it/s] 91%|█████████ | 988/1083 [00:22<00:02, 45.20it/s] 92%|█████████▏| 993/1083 [00:22<00:01, 45.25it/s] 92%|█████████▏| 998/1083 [00:22<00:01, 45.33it/s] 93%|█████████▎| 1003/1083 [00:22<00:01, 45.18it/s] 93%|█████████▎| 1008/1083 [00:22<00:01, 45.09it/s] 94%|█████████▎| 1013/1083 [00:22<00:01, 43.56it/s] 94%|█████████▍| 1018/1083 [00:22<00:01, 43.99it/s] 94%|█████████▍| 1023/1083 [00:22<00:01, 44.26it/s] 95%|█████████▍| 1028/1083 [00:22<00:01, 44.55it/s] 95%|█████████▌| 1033/1083 [00:23<00:01, 44.74it/s] 96%|█████████▌| 1038/1083 [00:23<00:01, 44.98it/s] 96%|█████████▋| 1043/1083 [00:23<00:00, 44.99it/s] 97%|█████████▋| 1048/1083 [00:23<00:00, 44.97it/s] 97%|█████████▋| 1053/1083 [00:23<00:00, 44.81it/s] 98%|█████████▊| 1058/1083 [00:23<00:00, 44.76it/s] 98%|█████████▊| 1063/1083 [00:23<00:00, 44.83it/s] 99%|█████████▊| 1068/1083 [00:23<00:00, 44.91it/s] 99%|█████████▉| 1073/1083 [00:23<00:00, 45.00it/s]100%|█████████▉| 1078/1083 [00:24<00:00, 45.12it/s]100%|██████████| 1083/1083 [00:24<00:00, 45.22it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.86it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:44:53,348 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   eval_loss               =     0.9504
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   eval_runtime            = 0:00:24.16
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   eval_samples_per_second =    358.339
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   eval_steps_per_second   =     44.823
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:44:53,348 >>   perplexity              =     2.5868
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:06,708 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:06,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:06,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:06,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:06,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:45:07,420 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:45:07,421 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:45:07,841 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:45:08,907 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:45:08,908 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:12,190 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:12,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:12,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:12,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:12,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:45:12,960 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:45:12,962 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:45:13,650 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:45:13,824 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:45:13,824 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/checkpoint-246
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.68it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.72it/s]Extractor Predicting: 13it [00:07,  1.76it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.44it/s]Extractor Predicting: 19it [00:11,  1.50it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.73it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:16,  1.74it/s]Extractor Predicting: 28it [00:16,  1.74it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:18,  1.73it/s]Extractor Predicting: 31it [00:18,  1.75it/s]Extractor Predicting: 32it [00:19,  1.75it/s]Extractor Predicting: 33it [00:19,  1.74it/s]Extractor Predicting: 34it [00:20,  1.73it/s]Extractor Predicting: 35it [00:20,  1.72it/s]Extractor Predicting: 36it [00:21,  1.70it/s]Extractor Predicting: 37it [00:22,  1.70it/s]Extractor Predicting: 38it [00:22,  1.72it/s]Extractor Predicting: 39it [00:23,  1.72it/s]Extractor Predicting: 40it [00:23,  1.73it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.66it/s]Extractor Predicting: 43it [00:25,  1.69it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:26,  1.68it/s]Extractor Predicting: 46it [00:27,  1.67it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:28,  1.66it/s]Extractor Predicting: 49it [00:29,  1.64it/s]Extractor Predicting: 50it [00:29,  1.67it/s]Extractor Predicting: 51it [00:30,  1.70it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:32,  1.73it/s]Extractor Predicting: 56it [00:33,  1.73it/s]Extractor Predicting: 57it [00:33,  1.71it/s]Extractor Predicting: 58it [00:34,  1.70it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:35,  1.62it/s]Extractor Predicting: 61it [00:36,  1.66it/s]Extractor Predicting: 62it [00:36,  1.70it/s]Extractor Predicting: 63it [00:37,  1.74it/s]Extractor Predicting: 64it [00:38,  1.77it/s]Extractor Predicting: 65it [00:38,  1.74it/s]Extractor Predicting: 66it [00:39,  1.76it/s]Extractor Predicting: 67it [00:39,  1.72it/s]Extractor Predicting: 68it [00:40,  1.76it/s]Extractor Predicting: 69it [00:40,  1.72it/s]Extractor Predicting: 70it [00:41,  1.66it/s]Extractor Predicting: 71it [00:42,  1.65it/s]Extractor Predicting: 72it [00:42,  1.69it/s]Extractor Predicting: 73it [00:43,  1.75it/s]Extractor Predicting: 74it [00:43,  1.76it/s]Extractor Predicting: 75it [00:44,  1.68it/s]Extractor Predicting: 76it [00:45,  1.72it/s]Extractor Predicting: 77it [00:45,  1.70it/s]Extractor Predicting: 78it [00:46,  1.72it/s]Extractor Predicting: 79it [00:46,  1.73it/s]Extractor Predicting: 80it [00:47,  1.73it/s]Extractor Predicting: 81it [00:47,  1.73it/s]Extractor Predicting: 82it [00:48,  1.73it/s]Extractor Predicting: 83it [00:49,  1.72it/s]Extractor Predicting: 84it [00:49,  1.75it/s]Extractor Predicting: 85it [00:50,  1.71it/s]Extractor Predicting: 86it [00:50,  1.69it/s]Extractor Predicting: 87it [00:51,  1.63it/s]Extractor Predicting: 88it [00:52,  1.64it/s]Extractor Predicting: 89it [00:52,  1.68it/s]Extractor Predicting: 90it [00:53,  1.70it/s]Extractor Predicting: 91it [00:53,  1.75it/s]Extractor Predicting: 92it [00:54,  1.75it/s]Extractor Predicting: 93it [00:55,  1.70it/s]Extractor Predicting: 94it [00:55,  1.71it/s]Extractor Predicting: 95it [00:56,  1.74it/s]Extractor Predicting: 96it [00:56,  1.73it/s]Extractor Predicting: 97it [00:57,  1.53it/s]Extractor Predicting: 98it [00:58,  1.55it/s]Extractor Predicting: 99it [00:58,  1.60it/s]Extractor Predicting: 100it [00:59,  1.62it/s]Extractor Predicting: 101it [00:59,  1.66it/s]Extractor Predicting: 102it [01:00,  1.71it/s]Extractor Predicting: 103it [01:01,  1.70it/s]Extractor Predicting: 104it [01:01,  1.69it/s]Extractor Predicting: 105it [01:02,  1.70it/s]Extractor Predicting: 106it [01:02,  1.71it/s]Extractor Predicting: 107it [01:03,  1.69it/s]Extractor Predicting: 108it [01:04,  1.71it/s]Extractor Predicting: 109it [01:04,  1.71it/s]Extractor Predicting: 110it [01:05,  1.71it/s]Extractor Predicting: 111it [01:05,  1.71it/s]Extractor Predicting: 112it [01:06,  1.71it/s]Extractor Predicting: 113it [01:06,  1.72it/s]Extractor Predicting: 114it [01:07,  1.73it/s]Extractor Predicting: 115it [01:08,  1.65it/s]Extractor Predicting: 116it [01:08,  1.64it/s]Extractor Predicting: 117it [01:09,  1.69it/s]Extractor Predicting: 118it [01:09,  1.69it/s]Extractor Predicting: 119it [01:10,  1.71it/s]Extractor Predicting: 120it [01:11,  1.72it/s]Extractor Predicting: 121it [01:11,  1.66it/s]Extractor Predicting: 122it [01:12,  1.79it/s]Extractor Predicting: 123it [01:12,  1.78it/s]Extractor Predicting: 124it [01:13,  1.75it/s]Extractor Predicting: 125it [01:13,  1.69it/s]Extractor Predicting: 126it [01:14,  1.66it/s]Extractor Predicting: 127it [01:15,  1.67it/s]Extractor Predicting: 128it [01:15,  1.69it/s]Extractor Predicting: 129it [01:16,  1.70it/s]Extractor Predicting: 130it [01:16,  1.71it/s]Extractor Predicting: 131it [01:17,  1.76it/s]Extractor Predicting: 132it [01:18,  1.74it/s]Extractor Predicting: 133it [01:18,  1.68it/s]Extractor Predicting: 134it [01:19,  1.71it/s]Extractor Predicting: 135it [01:19,  1.72it/s]Extractor Predicting: 136it [01:20,  1.71it/s]Extractor Predicting: 137it [01:21,  1.71it/s]Extractor Predicting: 138it [01:21,  1.72it/s]Extractor Predicting: 139it [01:22,  1.69it/s]Extractor Predicting: 140it [01:22,  1.70it/s]Extractor Predicting: 141it [01:23,  1.73it/s]Extractor Predicting: 142it [01:23,  1.74it/s]Extractor Predicting: 143it [01:24,  1.73it/s]Extractor Predicting: 144it [01:25,  1.75it/s]Extractor Predicting: 145it [01:25,  1.70it/s]Extractor Predicting: 146it [01:26,  1.69it/s]Extractor Predicting: 147it [01:26,  1.72it/s]Extractor Predicting: 148it [01:27,  1.74it/s]Extractor Predicting: 149it [01:27,  1.79it/s]Extractor Predicting: 150it [01:28,  1.77it/s]Extractor Predicting: 151it [01:29,  1.77it/s]Extractor Predicting: 152it [01:29,  1.70it/s]Extractor Predicting: 153it [01:30,  1.64it/s]Extractor Predicting: 154it [01:30,  1.62it/s]Extractor Predicting: 155it [01:31,  1.58it/s]Extractor Predicting: 156it [01:32,  1.56it/s]Extractor Predicting: 157it [01:32,  1.55it/s]Extractor Predicting: 158it [01:33,  1.54it/s]Extractor Predicting: 159it [01:34,  1.55it/s]Extractor Predicting: 160it [01:34,  1.55it/s]Extractor Predicting: 161it [01:35,  1.53it/s]Extractor Predicting: 162it [01:36,  1.53it/s]Extractor Predicting: 163it [01:36,  1.52it/s]Extractor Predicting: 164it [01:37,  1.53it/s]Extractor Predicting: 165it [01:38,  1.53it/s]Extractor Predicting: 166it [01:38,  1.53it/s]Extractor Predicting: 167it [01:39,  1.52it/s]Extractor Predicting: 168it [01:40,  1.56it/s]Extractor Predicting: 169it [01:40,  1.61it/s]Extractor Predicting: 170it [01:41,  1.48it/s]Extractor Predicting: 171it [01:42,  1.51it/s]Extractor Predicting: 172it [01:42,  1.53it/s]Extractor Predicting: 173it [01:43,  1.56it/s]Extractor Predicting: 174it [01:44,  1.57it/s]Extractor Predicting: 175it [01:44,  1.57it/s]Extractor Predicting: 176it [01:45,  1.53it/s]Extractor Predicting: 177it [01:45,  1.55it/s]Extractor Predicting: 178it [01:46,  1.55it/s]Extractor Predicting: 179it [01:47,  1.58it/s]Extractor Predicting: 180it [01:47,  1.57it/s]Extractor Predicting: 181it [01:48,  1.50it/s]Extractor Predicting: 182it [01:49,  1.53it/s]Extractor Predicting: 183it [01:49,  1.61it/s]Extractor Predicting: 184it [01:50,  1.64it/s]Extractor Predicting: 185it [01:50,  1.69it/s]Extractor Predicting: 186it [01:51,  1.64it/s]Extractor Predicting: 187it [01:52,  1.66it/s]Extractor Predicting: 188it [01:52,  1.66it/s]Extractor Predicting: 189it [01:53,  1.68it/s]Extractor Predicting: 190it [01:53,  1.67it/s]Extractor Predicting: 191it [01:54,  1.63it/s]Extractor Predicting: 192it [01:55,  1.66it/s]Extractor Predicting: 193it [01:55,  1.68it/s]Extractor Predicting: 194it [01:56,  1.69it/s]Extractor Predicting: 195it [01:56,  1.66it/s]Extractor Predicting: 196it [01:57,  1.62it/s]Extractor Predicting: 197it [01:58,  1.65it/s]Extractor Predicting: 198it [01:58,  1.65it/s]Extractor Predicting: 199it [01:59,  1.70it/s]Extractor Predicting: 200it [01:59,  1.69it/s]Extractor Predicting: 201it [02:00,  1.71it/s]Extractor Predicting: 202it [02:01,  1.66it/s]Extractor Predicting: 203it [02:01,  1.64it/s]Extractor Predicting: 204it [02:02,  1.67it/s]Extractor Predicting: 205it [02:02,  1.66it/s]Extractor Predicting: 206it [02:03,  1.68it/s]Extractor Predicting: 207it [02:04,  1.63it/s]Extractor Predicting: 208it [02:04,  1.67it/s]Extractor Predicting: 209it [02:05,  1.70it/s]Extractor Predicting: 210it [02:05,  1.71it/s]Extractor Predicting: 211it [02:06,  1.67it/s]Extractor Predicting: 212it [02:07,  1.68it/s]Extractor Predicting: 213it [02:07,  1.70it/s]Extractor Predicting: 214it [02:08,  1.73it/s]Extractor Predicting: 215it [02:08,  1.77it/s]Extractor Predicting: 216it [02:09,  1.77it/s]Extractor Predicting: 217it [02:09,  1.74it/s]Extractor Predicting: 218it [02:10,  1.75it/s]Extractor Predicting: 219it [02:11,  1.75it/s]Extractor Predicting: 220it [02:11,  1.72it/s]Extractor Predicting: 221it [02:12,  1.76it/s]Extractor Predicting: 222it [02:12,  1.76it/s]Extractor Predicting: 223it [02:13,  1.72it/s]Extractor Predicting: 224it [02:13,  1.70it/s]Extractor Predicting: 225it [02:14,  1.64it/s]Extractor Predicting: 226it [02:15,  1.67it/s]Extractor Predicting: 227it [02:15,  1.68it/s]Extractor Predicting: 228it [02:16,  1.67it/s]Extractor Predicting: 229it [02:16,  1.72it/s]Extractor Predicting: 230it [02:17,  1.70it/s]Extractor Predicting: 231it [02:18,  1.68it/s]Extractor Predicting: 232it [02:18,  1.72it/s]Extractor Predicting: 233it [02:19,  1.75it/s]Extractor Predicting: 234it [02:19,  1.68it/s]Extractor Predicting: 235it [02:20,  1.68it/s]Extractor Predicting: 236it [02:21,  1.66it/s]Extractor Predicting: 237it [02:21,  1.67it/s]Extractor Predicting: 238it [02:22,  1.70it/s]Extractor Predicting: 239it [02:22,  1.70it/s]Extractor Predicting: 240it [02:23,  1.69it/s]Extractor Predicting: 241it [02:24,  1.71it/s]Extractor Predicting: 242it [02:24,  1.74it/s]Extractor Predicting: 243it [02:25,  1.70it/s]Extractor Predicting: 244it [02:25,  1.71it/s]Extractor Predicting: 245it [02:26,  1.70it/s]Extractor Predicting: 246it [02:26,  1.70it/s]Extractor Predicting: 247it [02:27,  1.71it/s]Extractor Predicting: 248it [02:28,  1.73it/s]Extractor Predicting: 249it [02:28,  1.72it/s]Extractor Predicting: 250it [02:29,  1.70it/s]Extractor Predicting: 251it [02:29,  1.68it/s]Extractor Predicting: 252it [02:30,  1.69it/s]Extractor Predicting: 253it [02:31,  1.70it/s]Extractor Predicting: 254it [02:31,  1.72it/s]Extractor Predicting: 255it [02:32,  1.50it/s]Extractor Predicting: 256it [02:33,  1.56it/s]Extractor Predicting: 257it [02:33,  1.61it/s]Extractor Predicting: 258it [02:34,  1.66it/s]Extractor Predicting: 259it [02:34,  1.65it/s]Extractor Predicting: 260it [02:35,  1.64it/s]Extractor Predicting: 261it [02:36,  1.68it/s]Extractor Predicting: 262it [02:36,  1.67it/s]Extractor Predicting: 263it [02:37,  1.64it/s]Extractor Predicting: 264it [02:37,  1.62it/s]Extractor Predicting: 265it [02:38,  1.63it/s]Extractor Predicting: 266it [02:39,  1.62it/s]Extractor Predicting: 267it [02:39,  1.64it/s]Extractor Predicting: 268it [02:40,  1.63it/s]Extractor Predicting: 269it [02:40,  1.72it/s]Extractor Predicting: 269it [02:40,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:06,856 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:06,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:06,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:06,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:06,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:48:07,605 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:48:07,606 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:48:08,264 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:48:09,334 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:48:09,335 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:12,373 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:12,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:12,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:12,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:48:12,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:48:13,041 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:48:13,042 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:48:13,629 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:48:13,798 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:48:13,798 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:19,  1.67it/s]Extractor Predicting: 33it [00:20,  1.68it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:21,  1.60it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:23,  1.73it/s]Extractor Predicting: 40it [00:24,  1.73it/s]Extractor Predicting: 41it [00:25,  1.72it/s]Extractor Predicting: 42it [00:25,  1.75it/s]Extractor Predicting: 43it [00:26,  1.78it/s]Extractor Predicting: 44it [00:26,  1.75it/s]Extractor Predicting: 45it [00:27,  1.74it/s]Extractor Predicting: 46it [00:27,  1.72it/s]Extractor Predicting: 47it [00:28,  1.67it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:30,  1.74it/s]Extractor Predicting: 51it [00:30,  1.74it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.74it/s]Extractor Predicting: 54it [00:32,  1.77it/s]Extractor Predicting: 55it [00:33,  1.59it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:35,  1.67it/s]Extractor Predicting: 59it [00:35,  1.68it/s]Extractor Predicting: 60it [00:36,  1.72it/s]Extractor Predicting: 61it [00:36,  1.71it/s]Extractor Predicting: 62it [00:37,  1.70it/s]Extractor Predicting: 63it [00:38,  1.67it/s]Extractor Predicting: 64it [00:38,  1.61it/s]Extractor Predicting: 65it [00:39,  1.64it/s]Extractor Predicting: 66it [00:39,  1.65it/s]Extractor Predicting: 67it [00:40,  1.66it/s]Extractor Predicting: 68it [00:41,  1.67it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:42,  1.72it/s]Extractor Predicting: 72it [00:43,  1.67it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:44,  1.69it/s]Extractor Predicting: 75it [00:45,  1.69it/s]Extractor Predicting: 76it [00:45,  1.70it/s]Extractor Predicting: 77it [00:46,  1.59it/s]Extractor Predicting: 78it [00:47,  1.66it/s]Extractor Predicting: 79it [00:47,  1.70it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.72it/s]Extractor Predicting: 82it [00:49,  1.74it/s]Extractor Predicting: 83it [00:49,  1.74it/s]Extractor Predicting: 84it [00:50,  1.79it/s]Extractor Predicting: 85it [00:50,  1.81it/s]Extractor Predicting: 86it [00:51,  1.77it/s]Extractor Predicting: 87it [00:52,  1.72it/s]Extractor Predicting: 88it [00:52,  1.73it/s]Extractor Predicting: 89it [00:53,  1.69it/s]Extractor Predicting: 90it [00:53,  1.74it/s]Extractor Predicting: 91it [00:54,  1.73it/s]Extractor Predicting: 92it [00:55,  1.72it/s]Extractor Predicting: 93it [00:55,  1.64it/s]Extractor Predicting: 94it [00:56,  1.66it/s]Extractor Predicting: 95it [00:56,  1.75it/s]Extractor Predicting: 96it [00:57,  1.75it/s]Extractor Predicting: 97it [00:57,  1.74it/s]Extractor Predicting: 98it [00:58,  1.72it/s]Extractor Predicting: 99it [00:59,  1.69it/s]Extractor Predicting: 100it [00:59,  1.71it/s]Extractor Predicting: 101it [01:00,  1.75it/s]Extractor Predicting: 102it [01:00,  1.79it/s]Extractor Predicting: 103it [01:01,  1.78it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.76it/s]Extractor Predicting: 106it [01:03,  1.82it/s]Extractor Predicting: 107it [01:03,  1.83it/s]Extractor Predicting: 108it [01:04,  1.90it/s]Extractor Predicting: 109it [01:04,  1.93it/s]Extractor Predicting: 110it [01:05,  1.88it/s]Extractor Predicting: 111it [01:05,  1.89it/s]Extractor Predicting: 112it [01:06,  1.88it/s]Extractor Predicting: 113it [01:06,  1.85it/s]Extractor Predicting: 114it [01:07,  1.76it/s]Extractor Predicting: 115it [01:07,  1.72it/s]Extractor Predicting: 116it [01:08,  1.70it/s]Extractor Predicting: 117it [01:09,  1.62it/s]Extractor Predicting: 118it [01:09,  1.63it/s]Extractor Predicting: 119it [01:10,  1.67it/s]Extractor Predicting: 120it [01:11,  1.68it/s]Extractor Predicting: 121it [01:11,  1.69it/s]Extractor Predicting: 122it [01:12,  1.70it/s]Extractor Predicting: 123it [01:12,  1.64it/s]Extractor Predicting: 124it [01:13,  1.67it/s]Extractor Predicting: 125it [01:13,  1.69it/s]Extractor Predicting: 126it [01:14,  1.65it/s]Extractor Predicting: 127it [01:15,  1.62it/s]Extractor Predicting: 128it [01:15,  1.61it/s]Extractor Predicting: 129it [01:16,  1.67it/s]Extractor Predicting: 130it [01:16,  1.70it/s]Extractor Predicting: 131it [01:17,  1.69it/s]Extractor Predicting: 132it [01:18,  1.66it/s]Extractor Predicting: 133it [01:18,  1.66it/s]Extractor Predicting: 134it [01:19,  1.66it/s]Extractor Predicting: 135it [01:20,  1.65it/s]Extractor Predicting: 136it [01:20,  1.69it/s]Extractor Predicting: 137it [01:21,  1.68it/s]Extractor Predicting: 138it [01:21,  1.73it/s]Extractor Predicting: 139it [01:22,  1.76it/s]Extractor Predicting: 140it [01:22,  1.72it/s]Extractor Predicting: 141it [01:23,  1.69it/s]Extractor Predicting: 142it [01:24,  1.71it/s]Extractor Predicting: 143it [01:24,  2.09it/s]Extractor Predicting: 143it [01:24,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:47,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:47,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:47,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:47,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:47,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:49:48,435 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:49:48,436 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:49:49,017 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:49:50,063 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:49:50,063 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:53,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:53,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:53,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:53,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:49:53,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:49:53,658 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:49:53,659 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:49:54,242 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:49:54,403 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:49:54,403 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:16,  1.66it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:18,  1.65it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.67it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.73it/s]Extractor Predicting: 39it [00:23,  1.80it/s]Extractor Predicting: 40it [00:24,  1.90it/s]Extractor Predicting: 41it [00:24,  2.00it/s]Extractor Predicting: 42it [00:25,  2.03it/s]Extractor Predicting: 43it [00:25,  2.04it/s]Extractor Predicting: 44it [00:26,  2.03it/s]Extractor Predicting: 45it [00:26,  2.09it/s]Extractor Predicting: 46it [00:27,  2.05it/s]Extractor Predicting: 47it [00:27,  2.04it/s]Extractor Predicting: 48it [00:28,  2.04it/s]Extractor Predicting: 49it [00:28,  2.04it/s]Extractor Predicting: 50it [00:29,  2.09it/s]Extractor Predicting: 51it [00:29,  1.98it/s]Extractor Predicting: 52it [00:30,  2.01it/s]Extractor Predicting: 53it [00:30,  2.05it/s]Extractor Predicting: 54it [00:31,  2.10it/s]Extractor Predicting: 55it [00:31,  2.13it/s]Extractor Predicting: 56it [00:32,  2.13it/s]Extractor Predicting: 57it [00:32,  2.13it/s]Extractor Predicting: 58it [00:32,  2.13it/s]Extractor Predicting: 59it [00:33,  2.14it/s]Extractor Predicting: 60it [00:33,  2.06it/s]Extractor Predicting: 61it [00:34,  2.02it/s]Extractor Predicting: 62it [00:34,  2.06it/s]Extractor Predicting: 63it [00:35,  2.07it/s]Extractor Predicting: 64it [00:35,  2.09it/s]Extractor Predicting: 65it [00:36,  2.09it/s]Extractor Predicting: 66it [00:36,  2.13it/s]Extractor Predicting: 67it [00:37,  2.01it/s]Extractor Predicting: 68it [00:37,  1.88it/s]Extractor Predicting: 69it [00:38,  1.79it/s]Extractor Predicting: 70it [00:39,  1.71it/s]Extractor Predicting: 71it [00:39,  1.68it/s]Extractor Predicting: 72it [00:40,  1.70it/s]Extractor Predicting: 72it [00:40,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:50:36,703 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:50:36,729 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:50:36,759 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:50:36,760 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:50:36,776 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:50:45,390 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 02:50:45,409 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 02:50:45,509 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:50:45,510 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:50:45,562 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:50:45,590 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:50:45,590 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:50:45,590 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:50:45,590 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:50:45,590 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:50:45,591 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 02:50:45,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:46,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:47,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:47,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:48,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:48,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:49,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:50,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:50,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:51,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:52,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:52,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:53,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:53,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:55,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:55,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:56,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:57,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:57,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:58,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:59,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:50:59,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:10, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-28 02:51:00,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:01,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:01,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:02,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:03,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:03,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:04,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:04,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:05,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:06,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:06,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:07,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:07,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:08,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:09,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:09,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:10,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:11,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:11,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:12,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:13,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:13,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:14,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:15,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:29<01:59, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-28 02:51:15,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:16,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:17,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:17,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:18,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:19,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:19,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:20,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:21,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:21,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:22,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:23,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:23,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:24,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:24,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:25,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:26,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:26,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:27,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:28,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:28,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:29,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:43, 14.78s/it][WARNING|generation_utils.py:914] 2023-08-28 02:51:30,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:31,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:31,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:32,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:32,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:33,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:34,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:34,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:35,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:36,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:36,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:37,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:38,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:38,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:39,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:40,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:40,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:41,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:42,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:42,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:43,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:44,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:44,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:45,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:45,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:46,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:47,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:47,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:48,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:48,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:49,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:50,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:50,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:51,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:52,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:52,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:53,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:53,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:54,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:09<01:52, 18.68s/it][WARNING|generation_utils.py:914] 2023-08-28 02:51:54,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:55,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:56,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:56,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:57,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:57,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:58,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:59,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:51:59,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:00,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:00,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:01,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:02,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:02,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:03,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:04,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:04,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:05,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:06,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:06,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:07,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:07,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:08,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:09,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:09,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:27, 17.53s/it][WARNING|generation_utils.py:914] 2023-08-28 02:52:10,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:11,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:11,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:12,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:12,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:13,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:14,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:14,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:15,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:15,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:16,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:17,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:17,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:18,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:18,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:19,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:20,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:20,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:21,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:21,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:22,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:37<01:03, 15.94s/it][WARNING|generation_utils.py:914] 2023-08-28 02:52:23,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:23,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:24,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:25,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:25,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:26,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:26,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:27,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:27,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:28,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:28,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:29,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:30,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:30,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:31,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:31,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:32,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:33,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:33,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:34,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:34,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:35,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:35,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:36,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:37,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:37,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:38,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:52<00:47, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 02:52:38,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:39,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:39,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:40,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:41,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:41,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:42,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:42,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:43,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:44,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:44,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:45,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:45,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:46,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:46,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:47,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:48,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:48,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:49,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:50,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:50,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:51,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:51,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:06<00:30, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 02:52:52,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:53,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:53,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:54,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:54,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:55,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:56,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:56,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:57,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:58,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:58,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:52:59,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:00,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:00,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:01,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:02,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:02,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:03,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:04,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:04,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:05,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:06,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:06,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:21<00:15, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-28 02:53:07,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:08,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:08,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:09,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:10,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:10,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:11,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:12,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:12,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:13,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:14,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:14,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:15,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:16,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:16,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:17,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:18,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:18,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:19,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:20,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:21,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:21,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:53:22,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:37<00:00, 15.34s/it]Generating: 100%|██████████| 10/10 [02:37<00:00, 15.75s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:30,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:30,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:30,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:30,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:30,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:53:31,189 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:53:31,190 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:53:31,817 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:53:32,969 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:53:32,969 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:35,757 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:35,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:35,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:35,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:35,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:53:36,382 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:53:36,383 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:53:36,804 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:53:37,035 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:53:37,035 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')", "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.58it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.53it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.59it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.56it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:10,  1.59it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:12,  1.50it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:14,  1.50it/s]Extractor Estimating: 24it [00:15,  1.50it/s]Extractor Estimating: 25it [00:16,  1.56it/s]Extractor Estimating: 26it [00:16,  1.53it/s]Extractor Estimating: 27it [00:17,  1.51it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:19,  1.60it/s]Extractor Estimating: 31it [00:19,  1.59it/s]Extractor Estimating: 32it [00:20,  1.57it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:21,  1.57it/s]Extractor Estimating: 35it [00:22,  1.58it/s]Extractor Estimating: 36it [00:23,  1.61it/s]Extractor Estimating: 37it [00:23,  1.66it/s]Extractor Estimating: 38it [00:24,  1.70it/s]Extractor Estimating: 39it [00:24,  1.65it/s]Extractor Estimating: 40it [00:25,  1.58it/s]Extractor Estimating: 41it [00:26,  1.45it/s]Extractor Estimating: 42it [00:27,  1.45it/s]Extractor Estimating: 43it [00:27,  1.46it/s]Extractor Estimating: 44it [00:28,  1.47it/s]Extractor Estimating: 45it [00:29,  1.49it/s]Extractor Estimating: 46it [00:29,  1.52it/s]Extractor Estimating: 47it [00:30,  1.54it/s]Extractor Estimating: 48it [00:31,  1.49it/s]Extractor Estimating: 49it [00:31,  1.51it/s]Extractor Estimating: 50it [00:32,  1.54it/s]Extractor Estimating: 51it [00:32,  1.55it/s]Extractor Estimating: 52it [00:33,  1.62it/s]Extractor Estimating: 53it [00:34,  1.66it/s]Extractor Estimating: 54it [00:34,  1.64it/s]Extractor Estimating: 55it [00:35,  1.66it/s]Extractor Estimating: 56it [00:36,  1.43it/s]Extractor Estimating: 57it [00:36,  1.54it/s]Extractor Estimating: 58it [00:37,  1.59it/s]Extractor Estimating: 59it [00:37,  1.66it/s]Extractor Estimating: 60it [00:38,  1.65it/s]Extractor Estimating: 61it [00:39,  1.69it/s]Extractor Estimating: 62it [00:39,  1.72it/s]Extractor Estimating: 63it [00:40,  1.67it/s]Extractor Estimating: 64it [00:40,  1.66it/s]Extractor Estimating: 65it [00:41,  1.68it/s]Extractor Estimating: 66it [00:41,  1.71it/s]Extractor Estimating: 67it [00:42,  1.72it/s]Extractor Estimating: 68it [00:43,  1.72it/s]Extractor Estimating: 69it [00:43,  1.69it/s]Extractor Estimating: 70it [00:44,  1.69it/s]Extractor Estimating: 71it [00:44,  1.70it/s]Extractor Estimating: 72it [00:45,  1.71it/s]Extractor Estimating: 73it [00:46,  1.71it/s]Extractor Estimating: 74it [00:46,  1.63it/s]Extractor Estimating: 75it [00:47,  1.71it/s]Extractor Estimating: 76it [00:48,  1.47it/s]Extractor Estimating: 77it [00:48,  1.49it/s]Extractor Estimating: 78it [00:49,  1.56it/s]Extractor Estimating: 79it [00:50,  1.56it/s]Extractor Estimating: 80it [00:50,  1.59it/s]Extractor Estimating: 81it [00:51,  1.56it/s]Extractor Estimating: 82it [00:52,  1.50it/s]Extractor Estimating: 83it [00:52,  1.48it/s]Extractor Estimating: 84it [00:53,  1.49it/s]Extractor Estimating: 85it [00:53,  1.53it/s]Extractor Estimating: 86it [00:54,  1.54it/s]Extractor Estimating: 87it [00:55,  1.57it/s]Extractor Estimating: 88it [00:55,  1.58it/s]Extractor Estimating: 89it [00:56,  1.58it/s]Extractor Estimating: 90it [00:57,  1.55it/s]Extractor Estimating: 91it [00:57,  1.58it/s]Extractor Estimating: 92it [00:58,  1.58it/s]Extractor Estimating: 93it [00:59,  1.59it/s]Extractor Estimating: 94it [00:59,  1.58it/s]Extractor Estimating: 95it [01:00,  1.55it/s]Extractor Estimating: 96it [01:00,  1.55it/s]Extractor Estimating: 97it [01:01,  1.50it/s]Extractor Estimating: 98it [01:02,  1.53it/s]Extractor Estimating: 99it [01:02,  1.56it/s]Extractor Estimating: 100it [01:03,  1.56it/s]Extractor Estimating: 101it [01:04,  1.47it/s]Extractor Estimating: 102it [01:04,  1.51it/s]Extractor Estimating: 103it [01:05,  1.59it/s]Extractor Estimating: 104it [01:06,  1.62it/s]Extractor Estimating: 105it [01:06,  1.65it/s]Extractor Estimating: 106it [01:07,  1.66it/s]Extractor Estimating: 107it [01:07,  1.64it/s]Extractor Estimating: 108it [01:08,  1.67it/s]Extractor Estimating: 109it [01:09,  1.64it/s]Extractor Estimating: 110it [01:09,  1.63it/s]Extractor Estimating: 111it [01:10,  1.62it/s]Extractor Estimating: 112it [01:10,  1.62it/s]Extractor Estimating: 113it [01:11,  1.63it/s]Extractor Estimating: 114it [01:12,  1.69it/s]Extractor Estimating: 115it [01:12,  1.68it/s]Extractor Estimating: 116it [01:13,  1.66it/s]Extractor Estimating: 117it [01:14,  1.62it/s]Extractor Estimating: 118it [01:14,  1.63it/s]Extractor Estimating: 119it [01:15,  1.65it/s]Extractor Estimating: 120it [01:15,  1.66it/s]Extractor Estimating: 121it [01:16,  1.62it/s]Extractor Estimating: 122it [01:17,  1.66it/s]Extractor Estimating: 123it [01:17,  1.69it/s]Extractor Estimating: 124it [01:18,  1.66it/s]Extractor Estimating: 125it [01:18,  1.61it/s]Extractor Estimating: 126it [01:19,  1.62it/s]Extractor Estimating: 127it [01:20,  1.62it/s]Extractor Estimating: 128it [01:20,  1.59it/s]Extractor Estimating: 129it [01:21,  1.60it/s]Extractor Estimating: 130it [01:21,  1.62it/s]Extractor Estimating: 131it [01:22,  1.60it/s]Extractor Estimating: 132it [01:23,  1.60it/s]Extractor Estimating: 133it [01:23,  1.59it/s]Extractor Estimating: 134it [01:24,  1.58it/s]Extractor Estimating: 135it [01:25,  1.59it/s]Extractor Estimating: 136it [01:25,  1.56it/s]Extractor Estimating: 137it [01:26,  1.54it/s]Extractor Estimating: 138it [01:27,  1.49it/s]Extractor Estimating: 139it [01:27,  1.51it/s]Extractor Estimating: 140it [01:28,  1.53it/s]Extractor Estimating: 141it [01:29,  1.55it/s]Extractor Estimating: 142it [01:29,  1.57it/s]Extractor Estimating: 143it [01:30,  1.56it/s]Extractor Estimating: 144it [01:31,  1.56it/s]Extractor Estimating: 145it [01:31,  1.60it/s]Extractor Estimating: 146it [01:32,  1.55it/s]Extractor Estimating: 147it [01:32,  1.58it/s]Extractor Estimating: 148it [01:33,  1.52it/s]Extractor Estimating: 149it [01:34,  1.50it/s]Extractor Estimating: 150it [01:34,  1.54it/s]Extractor Estimating: 151it [01:35,  1.53it/s]Extractor Estimating: 152it [01:36,  1.55it/s]Extractor Estimating: 153it [01:36,  1.56it/s]Extractor Estimating: 154it [01:37,  1.58it/s]Extractor Estimating: 155it [01:38,  1.59it/s]Extractor Estimating: 156it [01:38,  1.55it/s]Extractor Estimating: 157it [01:39,  1.57it/s]Extractor Estimating: 158it [01:39,  1.60it/s]Extractor Estimating: 159it [01:40,  1.63it/s]Extractor Estimating: 160it [01:41,  1.67it/s]Extractor Estimating: 161it [01:41,  1.68it/s]Extractor Estimating: 162it [01:42,  1.57it/s]Extractor Estimating: 163it [01:43,  1.56it/s]Extractor Estimating: 164it [01:43,  1.53it/s]Extractor Estimating: 165it [01:44,  1.56it/s]Extractor Estimating: 166it [01:45,  1.42it/s]Extractor Estimating: 167it [01:45,  1.45it/s]Extractor Estimating: 168it [01:46,  1.53it/s]Extractor Estimating: 169it [01:47,  1.60it/s]Extractor Estimating: 170it [01:47,  1.62it/s]Extractor Estimating: 171it [01:48,  1.65it/s]Extractor Estimating: 172it [01:48,  1.63it/s]Extractor Estimating: 173it [01:49,  1.60it/s]Extractor Estimating: 174it [01:50,  1.60it/s]Extractor Estimating: 175it [01:50,  1.58it/s]Extractor Estimating: 176it [01:51,  1.59it/s]Extractor Estimating: 177it [01:51,  1.62it/s]Extractor Estimating: 178it [01:52,  1.60it/s]Extractor Estimating: 179it [01:53,  1.66it/s]Extractor Estimating: 180it [01:53,  1.68it/s]Extractor Estimating: 181it [01:54,  1.69it/s]Extractor Estimating: 182it [01:54,  1.68it/s]Extractor Estimating: 183it [01:55,  1.63it/s]Extractor Estimating: 184it [01:56,  1.65it/s]Extractor Estimating: 185it [01:56,  1.60it/s]Extractor Estimating: 186it [01:57,  1.63it/s]Extractor Estimating: 187it [01:58,  1.55it/s]Extractor Estimating: 188it [01:58,  1.59it/s]Extractor Estimating: 189it [01:59,  1.60it/s]Extractor Estimating: 190it [01:59,  1.61it/s]Extractor Estimating: 191it [02:00,  1.65it/s]Extractor Estimating: 192it [02:01,  1.65it/s]Extractor Estimating: 193it [02:01,  1.63it/s]Extractor Estimating: 194it [02:02,  1.57it/s]Extractor Estimating: 195it [02:03,  1.60it/s]Extractor Estimating: 196it [02:03,  1.63it/s]Extractor Estimating: 197it [02:04,  1.65it/s]Extractor Estimating: 198it [02:04,  1.64it/s]Extractor Estimating: 199it [02:05,  1.66it/s]Extractor Estimating: 200it [02:06,  1.64it/s]Extractor Estimating: 201it [02:06,  1.57it/s]Extractor Estimating: 202it [02:07,  1.58it/s]Extractor Estimating: 203it [02:08,  1.54it/s]Extractor Estimating: 204it [02:08,  1.61it/s]Extractor Estimating: 205it [02:09,  1.64it/s]Extractor Estimating: 206it [02:09,  1.61it/s]Extractor Estimating: 207it [02:10,  1.58it/s]Extractor Estimating: 208it [02:11,  1.59it/s]Extractor Estimating: 209it [02:11,  1.59it/s]Extractor Estimating: 210it [02:12,  1.56it/s]Extractor Estimating: 211it [02:13,  1.57it/s]Extractor Estimating: 212it [02:13,  1.50it/s]Extractor Estimating: 213it [02:14,  1.51it/s]Extractor Estimating: 214it [02:15,  1.54it/s]Extractor Estimating: 215it [02:15,  1.56it/s]Extractor Estimating: 216it [02:16,  1.59it/s]Extractor Estimating: 217it [02:16,  1.60it/s]Extractor Estimating: 218it [02:17,  1.64it/s]Extractor Estimating: 219it [02:18,  1.61it/s]Extractor Estimating: 220it [02:18,  1.58it/s]Extractor Estimating: 221it [02:19,  1.58it/s]Extractor Estimating: 222it [02:20,  1.60it/s]Extractor Estimating: 223it [02:20,  1.55it/s]Extractor Estimating: 224it [02:21,  1.53it/s]Extractor Estimating: 225it [02:22,  1.56it/s]Extractor Estimating: 226it [02:22,  1.52it/s]Extractor Estimating: 227it [02:23,  1.53it/s]Extractor Estimating: 228it [02:23,  1.57it/s]Extractor Estimating: 229it [02:24,  1.61it/s]Extractor Estimating: 230it [02:25,  1.62it/s]Extractor Estimating: 231it [02:25,  1.58it/s]Extractor Estimating: 232it [02:26,  1.58it/s]Extractor Estimating: 233it [02:27,  1.61it/s]Extractor Estimating: 234it [02:27,  1.60it/s]Extractor Estimating: 235it [02:28,  1.54it/s]Extractor Estimating: 236it [02:28,  1.59it/s]Extractor Estimating: 237it [02:29,  1.58it/s]Extractor Estimating: 238it [02:30,  1.58it/s]Extractor Estimating: 239it [02:30,  1.61it/s]Extractor Estimating: 240it [02:31,  1.60it/s]Extractor Estimating: 241it [02:32,  1.57it/s]Extractor Estimating: 242it [02:32,  1.61it/s]Extractor Estimating: 243it [02:33,  1.56it/s]Extractor Estimating: 244it [02:33,  1.59it/s]Extractor Estimating: 245it [02:34,  1.60it/s]Extractor Estimating: 246it [02:35,  1.57it/s]Extractor Estimating: 247it [02:35,  1.56it/s]Extractor Estimating: 248it [02:36,  1.61it/s]Extractor Estimating: 249it [02:37,  1.61it/s]Extractor Estimating: 250it [02:37,  1.51it/s]Extractor Estimating: 250it [02:37,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:35,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:35,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:35,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:35,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:35,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:56:36,508 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:56:36,509 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:56:37,106 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:56:38,216 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:56:38,216 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:41,309 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:41,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:41,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:41,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:56:41,340 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:56:42,155 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:56:42,156 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:56:42,759 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:56:42,978 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:56:42,978 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 04:39:38,398 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 04:39:38,995 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5292 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 24608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24708, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.031, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.032, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.057, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.028, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.035, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 3.047, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.042, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.030, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.047, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 3.018, loss:nan
g_step 1200, step 95, avg_time 1.037, loss:nan
g_step 1300, step 195, avg_time 1.035, loss:nan
g_step 1400, step 74, avg_time 1.026, loss:nan
g_step 1500, step 174, avg_time 1.034, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 3.026, loss:nan
g_step 1700, step 153, avg_time 1.032, loss:nan
g_step 1800, step 32, avg_time 1.027, loss:nan
g_step 1900, step 132, avg_time 1.038, loss:nan
g_step 2000, step 11, avg_time 1.032, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 3.026, loss:nan
g_step 2200, step 211, avg_time 1.034, loss:nan
g_step 2300, step 90, avg_time 1.034, loss:nan
g_step 2400, step 190, avg_time 1.033, loss:nan
g_step 2500, step 69, avg_time 1.027, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 3.037, loss:nan
g_step 2700, step 48, avg_time 1.039, loss:nan
g_step 2800, step 148, avg_time 1.026, loss:nan
g_step 2900, step 27, avg_time 1.051, loss:nan
g_step 3000, step 127, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 3.018, loss:nan
g_step 3200, step 106, avg_time 1.034, loss:nan
g_step 3300, step 206, avg_time 1.038, loss:nan
g_step 3400, step 85, avg_time 1.040, loss:nan
g_step 3500, step 185, avg_time 1.032, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 3.017, loss:nan
g_step 3700, step 164, avg_time 1.044, loss:nan
g_step 3800, step 43, avg_time 1.021, loss:nan
g_step 3900, step 143, avg_time 1.037, loss:nan
g_step 4000, step 22, avg_time 1.048, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 3.007, loss:nan
g_step 4200, step 1, avg_time 1.042, loss:nan
g_step 4300, step 101, avg_time 1.023, loss:nan
g_step 4400, step 201, avg_time 1.046, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 04:39:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 04:39:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_04-39-38_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 04:39:40 - WARNING - datasets.builder -   Using custom data configuration default-7a5aa4ef3efa56f1
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7a5aa4ef3efa56f1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 04:39:44,008 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:39:44,010 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:39:44,010 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:39:44,011 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:39:44,195 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:39:44,276 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:39:44,276 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:39:44,276 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:39:44,276 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:39:44,276 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:39:44,276 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 04:39:44,975 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:39:48,151 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 04:39:48,186 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7a5aa4ef3efa56f1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.52ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.47ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.91ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.14ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.29ba/s]100%|██████████| 6/6 [00:01<00:00,  4.50ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  2.69ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.53ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.92ba/s] 44%|████▍     | 4/9 [00:01<00:01,  4.14ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.26ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.35ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.22ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.31ba/s]100%|██████████| 9/9 [00:02<00:00,  4.87ba/s]100%|██████████| 9/9 [00:02<00:00,  4.29ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.41ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.74ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.83ba/s]100%|██████████| 6/6 [00:00<00:00, 10.47ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  6.72ba/s] 33%|███▎      | 3/9 [00:00<00:00,  9.49ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.24ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.54ba/s]100%|██████████| 9/9 [00:00<00:00, 11.48ba/s]100%|██████████| 9/9 [00:00<00:00, 10.70ba/s]
[INFO|trainer.py:414] 2023-08-28 04:39:54,597 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 04:39:54,683 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 04:39:54,683 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 04:39:54,683 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 04:39:54,683 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 04:39:54,683 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 04:39:54,683 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 04:39:54,683 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<05:43,  1.21it/s]  0%|          | 2/415 [00:01<04:00,  1.71it/s]  1%|          | 3/415 [00:01<03:06,  2.21it/s]  1%|          | 4/415 [00:01<02:40,  2.56it/s]  1%|          | 5/415 [00:02<02:26,  2.80it/s]  1%|▏         | 6/415 [00:02<02:15,  3.02it/s]  2%|▏         | 7/415 [00:02<02:08,  3.18it/s]  2%|▏         | 8/415 [00:02<02:03,  3.30it/s]  2%|▏         | 9/415 [00:03<02:01,  3.34it/s]  2%|▏         | 10/415 [00:03<01:58,  3.41it/s]  3%|▎         | 11/415 [00:03<01:56,  3.46it/s]  3%|▎         | 12/415 [00:04<01:55,  3.49it/s]  3%|▎         | 13/415 [00:04<01:54,  3.52it/s]  3%|▎         | 14/415 [00:04<01:53,  3.54it/s]  4%|▎         | 15/415 [00:04<01:52,  3.55it/s]  4%|▍         | 16/415 [00:05<01:57,  3.40it/s]  4%|▍         | 17/415 [00:05<01:55,  3.45it/s]  4%|▍         | 18/415 [00:05<01:53,  3.48it/s]  5%|▍         | 19/415 [00:06<01:52,  3.51it/s]  5%|▍         | 20/415 [00:06<01:51,  3.53it/s]  5%|▌         | 21/415 [00:06<01:51,  3.54it/s]  5%|▌         | 22/415 [00:06<01:50,  3.55it/s]  6%|▌         | 23/415 [00:07<01:50,  3.56it/s]  6%|▌         | 24/415 [00:07<01:49,  3.56it/s]  6%|▌         | 25/415 [00:07<01:49,  3.56it/s]  6%|▋         | 26/415 [00:08<01:49,  3.56it/s]  7%|▋         | 27/415 [00:08<01:52,  3.45it/s]  7%|▋         | 28/415 [00:08<01:51,  3.49it/s]  7%|▋         | 29/415 [00:08<01:49,  3.51it/s]  7%|▋         | 30/415 [00:09<01:48,  3.53it/s]  7%|▋         | 31/415 [00:09<01:48,  3.55it/s]  8%|▊         | 32/415 [00:09<01:47,  3.56it/s]  8%|▊         | 33/415 [00:10<01:47,  3.56it/s]  8%|▊         | 34/415 [00:10<01:46,  3.56it/s]  8%|▊         | 35/415 [00:10<01:46,  3.57it/s]  9%|▊         | 36/415 [00:10<01:46,  3.57it/s]  9%|▉         | 37/415 [00:11<01:45,  3.57it/s]  9%|▉         | 38/415 [00:11<01:49,  3.45it/s]  9%|▉         | 39/415 [00:11<01:47,  3.48it/s] 10%|▉         | 40/415 [00:12<01:46,  3.51it/s] 10%|▉         | 41/415 [00:12<01:46,  3.52it/s] 10%|█         | 42/415 [00:12<01:45,  3.54it/s] 10%|█         | 43/415 [00:12<01:44,  3.55it/s] 11%|█         | 44/415 [00:13<01:44,  3.55it/s] 11%|█         | 45/415 [00:13<01:44,  3.56it/s] 11%|█         | 46/415 [00:13<01:43,  3.57it/s] 11%|█▏        | 47/415 [00:14<01:42,  3.58it/s] 12%|█▏        | 48/415 [00:14<01:42,  3.59it/s] 12%|█▏        | 49/415 [00:14<01:41,  3.60it/s] 12%|█▏        | 50/415 [00:14<01:41,  3.61it/s] 12%|█▏        | 51/415 [00:15<01:40,  3.61it/s] 13%|█▎        | 52/415 [00:15<01:40,  3.61it/s] 13%|█▎        | 53/415 [00:15<01:40,  3.61it/s] 13%|█▎        | 54/415 [00:15<01:39,  3.62it/s] 13%|█▎        | 55/415 [00:16<01:39,  3.60it/s] 13%|█▎        | 56/415 [00:16<01:40,  3.59it/s] 14%|█▎        | 57/415 [00:16<01:39,  3.58it/s] 14%|█▍        | 58/415 [00:17<01:39,  3.58it/s] 14%|█▍        | 59/415 [00:17<01:39,  3.57it/s] 14%|█▍        | 60/415 [00:17<01:41,  3.49it/s] 15%|█▍        | 61/415 [00:17<01:40,  3.51it/s] 15%|█▍        | 62/415 [00:18<01:40,  3.53it/s] 15%|█▌        | 63/415 [00:18<01:39,  3.53it/s] 15%|█▌        | 64/415 [00:18<01:39,  3.54it/s] 16%|█▌        | 65/415 [00:19<01:38,  3.55it/s] 16%|█▌        | 66/415 [00:19<01:38,  3.55it/s] 16%|█▌        | 67/415 [00:19<01:37,  3.55it/s] 16%|█▋        | 68/415 [00:19<01:37,  3.56it/s] 17%|█▋        | 69/415 [00:20<01:37,  3.56it/s] 17%|█▋        | 70/415 [00:20<01:36,  3.56it/s] 17%|█▋        | 71/415 [00:20<01:36,  3.57it/s] 17%|█▋        | 72/415 [00:21<01:36,  3.57it/s] 18%|█▊        | 73/415 [00:21<01:35,  3.57it/s] 18%|█▊        | 74/415 [00:21<01:35,  3.57it/s] 18%|█▊        | 75/415 [00:21<01:35,  3.57it/s] 18%|█▊        | 76/415 [00:22<01:34,  3.57it/s] 19%|█▊        | 77/415 [00:22<01:34,  3.57it/s] 19%|█▉        | 78/415 [00:22<01:37,  3.46it/s] 19%|█▉        | 79/415 [00:23<01:36,  3.49it/s] 19%|█▉        | 80/415 [00:23<01:35,  3.51it/s] 20%|█▉        | 81/415 [00:23<01:34,  3.52it/s] 20%|█▉        | 82/415 [00:23<01:34,  3.54it/s] 20%|██        | 83/415 [00:24<01:28,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 04:40:18,768 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:40:18,768 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 04:40:18,768 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.23it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.09it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.56it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.77it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.16it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.68it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.33it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.00it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.97it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.94it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.08it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.29it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.43it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.34it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.14it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.92it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.79it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.82it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.89it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.00it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.11it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.33it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.30it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.15it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.81it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.79it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.81it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.97it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.01it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.06it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.76it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.98it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.97it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.64it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.76it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.78it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.77it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.94it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.98it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.15it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.22it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.14it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.99it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.87it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.81it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.83it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.96it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.02it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.11it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.15it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.07it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 45.03it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.88it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.82it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.82it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.98it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.14it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.19it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.07it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.05it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.79it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.69it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.73it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.79it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.97it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.17it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.13it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.06it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.96it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.91it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.84it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.76it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.92it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.00it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.10it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.03it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.99it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.05it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.87it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.85it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.82it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.87it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.04it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.13it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.21it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.08it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.07it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.94it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.83it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.81it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.86it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.00it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.11it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.22it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.09it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 45.08it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.92it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.81it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.90it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.87it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.09it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.08it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.18it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.14it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 45.03it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.99it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.79it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.89it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.86it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.98it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.11it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.18it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.09it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.03it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.96it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.81it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.84it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.93it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.99it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.17it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.12it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 43.75it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.09it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.37it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.45it/s][A
 59%|█████▉    | 637/1083 [00:14<00:10, 44.56it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.74it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.73it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.98it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.92it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.99it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.91it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.94it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.91it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.85it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.94it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.00it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.01it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.04it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.07it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 45.07it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 44.94it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.88it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.81it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.99it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.02it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.06it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.92it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 43.55it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.21it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 45.11it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 45.08it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.88it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.99it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.02it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.02it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.02it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.95it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.88it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 45.00it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.94it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.97it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.95it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.94it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.87it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.93it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.88it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 42.35it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 43.21it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 43.89it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.22it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.57it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.75it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.82it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.98it/s][A
 82%|████████▏ | 887/1083 [00:19<00:05, 37.99it/s][A
 82%|████████▏ | 891/1083 [00:19<00:06, 31.08it/s][A
 83%|████████▎ | 895/1083 [00:20<00:06, 27.07it/s][A
 83%|████████▎ | 901/1083 [00:20<00:05, 32.38it/s][A
 84%|████████▎ | 906/1083 [00:20<00:05, 35.31it/s][A
 84%|████████▍ | 911/1083 [00:20<00:04, 37.99it/s][A
 85%|████████▍ | 916/1083 [00:20<00:04, 39.93it/s][A
 85%|████████▌ | 921/1083 [00:20<00:03, 41.53it/s][A
 86%|████████▌ | 926/1083 [00:20<00:03, 42.70it/s][A
 86%|████████▌ | 931/1083 [00:20<00:03, 42.70it/s][A
 86%|████████▋ | 936/1083 [00:21<00:03, 43.29it/s][A
 87%|████████▋ | 941/1083 [00:21<00:03, 43.43it/s][A
 87%|████████▋ | 946/1083 [00:21<00:03, 43.68it/s][A
 88%|████████▊ | 951/1083 [00:21<00:02, 44.09it/s][A
 88%|████████▊ | 956/1083 [00:21<00:02, 44.38it/s][A
 89%|████████▊ | 961/1083 [00:21<00:02, 44.64it/s][A
 89%|████████▉ | 966/1083 [00:21<00:02, 44.83it/s][A
 90%|████████▉ | 971/1083 [00:21<00:02, 45.00it/s][A
 90%|█████████ | 976/1083 [00:21<00:02, 45.03it/s][A
 91%|█████████ | 981/1083 [00:22<00:02, 45.07it/s][A
 91%|█████████ | 986/1083 [00:22<00:02, 44.85it/s][A
 92%|█████████▏| 991/1083 [00:22<00:02, 40.57it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 43.35it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 43.96it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.47it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.68it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.96it/s][A
 94%|█████████▍| 1022/1083 [00:23<00:01, 45.05it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 44.86it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.70it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.44it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.47it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.68it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.88it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.12it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 41.33it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 42.57it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 43.44it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 43.91it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.11it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:24<00:00, 44.11it/s][A 20%|██        | 83/415 [00:48<01:28,  3.73it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 04:40:43,532 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 04:40:43,752 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:40:47,570 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:40:47,894 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:40:48,022 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:55<52:26,  9.51s/it] 20%|██        | 85/415 [00:55<37:03,  6.74s/it] 21%|██        | 86/415 [00:55<26:19,  4.80s/it] 21%|██        | 87/415 [00:55<18:49,  3.44s/it] 21%|██        | 88/415 [00:56<13:38,  2.50s/it] 21%|██▏       | 89/415 [00:56<09:58,  1.83s/it] 22%|██▏       | 90/415 [00:56<07:24,  1.37s/it] 22%|██▏       | 91/415 [00:57<05:37,  1.04s/it] 22%|██▏       | 92/415 [00:57<04:22,  1.23it/s] 22%|██▏       | 93/415 [00:57<03:30,  1.53it/s] 23%|██▎       | 94/415 [00:57<02:53,  1.85it/s] 23%|██▎       | 95/415 [00:58<02:28,  2.16it/s] 23%|██▎       | 96/415 [00:58<02:10,  2.45it/s] 23%|██▎       | 97/415 [00:58<01:57,  2.71it/s] 24%|██▎       | 98/415 [00:59<01:48,  2.92it/s] 24%|██▍       | 99/415 [00:59<01:45,  2.98it/s] 24%|██▍       | 100/415 [00:59<01:40,  3.14it/s] 24%|██▍       | 101/415 [00:59<01:36,  3.26it/s] 25%|██▍       | 102/415 [01:00<01:33,  3.35it/s] 25%|██▍       | 103/415 [01:00<01:31,  3.41it/s] 25%|██▌       | 104/415 [01:00<01:29,  3.46it/s] 25%|██▌       | 105/415 [01:01<01:28,  3.49it/s] 26%|██▌       | 106/415 [01:01<01:27,  3.52it/s] 26%|██▌       | 107/415 [01:01<01:27,  3.53it/s] 26%|██▌       | 108/415 [01:01<01:26,  3.55it/s] 26%|██▋       | 109/415 [01:02<01:26,  3.55it/s] 27%|██▋       | 110/415 [01:02<01:27,  3.49it/s] 27%|██▋       | 111/415 [01:02<01:26,  3.52it/s] 27%|██▋       | 112/415 [01:03<01:25,  3.53it/s] 27%|██▋       | 113/415 [01:03<01:25,  3.54it/s] 27%|██▋       | 114/415 [01:03<01:24,  3.55it/s] 28%|██▊       | 115/415 [01:03<01:24,  3.56it/s] 28%|██▊       | 116/415 [01:04<01:23,  3.56it/s] 28%|██▊       | 117/415 [01:04<01:23,  3.56it/s] 28%|██▊       | 118/415 [01:04<01:23,  3.56it/s] 29%|██▊       | 119/415 [01:05<01:22,  3.57it/s] 29%|██▉       | 120/415 [01:05<01:22,  3.57it/s] 29%|██▉       | 121/415 [01:05<01:25,  3.45it/s] 29%|██▉       | 122/415 [01:05<01:24,  3.48it/s] 30%|██▉       | 123/415 [01:06<01:23,  3.51it/s] 30%|██▉       | 124/415 [01:06<01:22,  3.52it/s] 30%|███       | 125/415 [01:06<01:22,  3.53it/s] 30%|███       | 126/415 [01:07<01:21,  3.54it/s] 31%|███       | 127/415 [01:07<01:21,  3.55it/s] 31%|███       | 128/415 [01:07<01:20,  3.57it/s] 31%|███       | 129/415 [01:07<01:19,  3.59it/s] 31%|███▏      | 130/415 [01:08<01:19,  3.59it/s] 32%|███▏      | 131/415 [01:08<01:18,  3.61it/s] 32%|███▏      | 132/415 [01:08<01:20,  3.53it/s] 32%|███▏      | 133/415 [01:08<01:19,  3.55it/s] 32%|███▏      | 134/415 [01:09<01:18,  3.57it/s] 33%|███▎      | 135/415 [01:09<01:18,  3.59it/s] 33%|███▎      | 136/415 [01:09<01:17,  3.60it/s] 33%|███▎      | 137/415 [01:10<01:17,  3.61it/s] 33%|███▎      | 138/415 [01:10<01:16,  3.61it/s] 33%|███▎      | 139/415 [01:10<01:16,  3.61it/s] 34%|███▎      | 140/415 [01:10<01:16,  3.61it/s] 34%|███▍      | 141/415 [01:11<01:15,  3.62it/s] 34%|███▍      | 142/415 [01:11<01:15,  3.62it/s] 34%|███▍      | 143/415 [01:11<01:17,  3.50it/s] 35%|███▍      | 144/415 [01:12<01:16,  3.54it/s] 35%|███▍      | 145/415 [01:12<01:15,  3.56it/s] 35%|███▌      | 146/415 [01:12<01:15,  3.58it/s] 35%|███▌      | 147/415 [01:12<01:14,  3.59it/s] 36%|███▌      | 148/415 [01:13<01:14,  3.60it/s] 36%|███▌      | 149/415 [01:13<01:14,  3.59it/s] 36%|███▌      | 150/415 [01:13<01:13,  3.60it/s] 36%|███▋      | 151/415 [01:13<01:13,  3.60it/s] 37%|███▋      | 152/415 [01:14<01:12,  3.61it/s] 37%|███▋      | 153/415 [01:14<01:12,  3.61it/s] 37%|███▋      | 154/415 [01:14<01:14,  3.50it/s] 37%|███▋      | 155/415 [01:15<01:13,  3.54it/s] 38%|███▊      | 156/415 [01:15<01:12,  3.56it/s] 38%|███▊      | 157/415 [01:15<01:12,  3.58it/s] 38%|███▊      | 158/415 [01:15<01:11,  3.59it/s] 38%|███▊      | 159/415 [01:16<01:11,  3.60it/s] 39%|███▊      | 160/415 [01:16<01:10,  3.61it/s] 39%|███▉      | 161/415 [01:16<01:10,  3.61it/s] 39%|███▉      | 162/415 [01:17<01:10,  3.61it/s] 39%|███▉      | 163/415 [01:17<01:09,  3.61it/s] 40%|███▉      | 164/415 [01:17<01:09,  3.61it/s] 40%|███▉      | 165/415 [01:17<01:09,  3.62it/s] 40%|████      | 166/415 [01:18<01:05,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 04:41:12,793 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:41:12,793 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 04:41:12,793 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.4318, 'eval_samples_per_second': 354.374, 'eval_steps_per_second': 44.327, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.30it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.37it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.52it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.34it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.69it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 44.28it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.51it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.41it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.68it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.82it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.98it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.02it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.09it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.07it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.98it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 45.00it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.93it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.69it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.90it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.92it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.05it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.11it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.16it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.03it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.14it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 45.01it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.86it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.86it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.81it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.06it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.03it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.10it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.15it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 45.02it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.93it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.81it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.83it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.88it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.88it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.94it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.13it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.17it/s][A
 20%|██        | 217/1083 [00:04<00:19, 45.03it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.87it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.85it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.81it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.82it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.98it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.93it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.10it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 43.63it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.08it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.34it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.44it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.52it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.62it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.78it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.90it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.79it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.80it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.97it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.98it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.88it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.88it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.79it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.94it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.06it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.95it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.90it/s][A
 33%|███▎      | 352/1083 [00:07<00:17, 42.77it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 43.59it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.04it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.17it/s][A
 34%|███▍      | 372/1083 [00:08<00:16, 44.40it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.61it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.72it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.84it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.59it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.71it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.84it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.90it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.96it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.85it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.98it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.99it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.89it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.81it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.71it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.85it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.90it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.03it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.02it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.04it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.06it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.97it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.84it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.63it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.83it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.92it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.03it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.89it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.80it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.97it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.89it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.91it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.74it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.82it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.89it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 45.08it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.11it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.03it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.94it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.90it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.85it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.73it/s][A
 54%|█████▎    | 582/1083 [00:13<00:12, 39.37it/s][A
 54%|█████▍    | 587/1083 [00:13<00:12, 41.00it/s][A
 55%|█████▍    | 592/1083 [00:13<00:11, 42.32it/s][A
 55%|█████▌    | 597/1083 [00:13<00:11, 43.23it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 43.88it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.31it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.69it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.69it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.39it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.24it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.27it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.65it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.86it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.10it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.11it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.13it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.15it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.95it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.79it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.62it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.77it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.89it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.08it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.14it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.10it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.11it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.93it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.68it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.61it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.73it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.00it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.06it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.13it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 45.18it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.10it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.83it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.79it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.91it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.95it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.02it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.00it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.09it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.02it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.91it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.90it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.75it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.84it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.89it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.95it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.01it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.06it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.99it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.91it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.78it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.87it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.85it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.86it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.89it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.07it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.01it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.96it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.95it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.80it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.90it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.81it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.82it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.98it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.98it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.97it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.96it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.91it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.88it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.21it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.50it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.76it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.87it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.86it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.86it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.90it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.88it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.67it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.61it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.81it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.81it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 45.05it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.89it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 41.30it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 43.80it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 44.12it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.43it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 40.77it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 42.08it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 43.09it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 43.70it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.08it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.16it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.42it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 44.70it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.42it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.36it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.36it/s][A 40%|████      | 166/415 [01:42<01:05,  3.81it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 04:41:37,143 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 04:41:37,283 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:41:40,771 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:41:41,107 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:41:41,286 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:49<39:16,  9.50s/it] 40%|████      | 168/415 [01:49<27:46,  6.75s/it] 41%|████      | 169/415 [01:49<19:42,  4.81s/it] 41%|████      | 170/415 [01:50<14:04,  3.45s/it] 41%|████      | 171/415 [01:50<10:09,  2.50s/it] 41%|████▏     | 172/415 [01:50<07:25,  1.83s/it] 42%|████▏     | 173/415 [01:50<05:30,  1.37s/it] 42%|████▏     | 174/415 [01:51<04:10,  1.04s/it] 42%|████▏     | 175/415 [01:51<03:15,  1.23it/s] 42%|████▏     | 176/415 [01:51<02:36,  1.53it/s] 43%|████▎     | 177/415 [01:52<02:08,  1.85it/s] 43%|████▎     | 178/415 [01:52<01:49,  2.16it/s] 43%|████▎     | 179/415 [01:52<01:39,  2.37it/s] 43%|████▎     | 180/415 [01:52<01:28,  2.64it/s] 44%|████▎     | 181/415 [01:53<01:21,  2.88it/s] 44%|████▍     | 182/415 [01:53<01:15,  3.07it/s] 44%|████▍     | 183/415 [01:53<01:12,  3.21it/s] 44%|████▍     | 184/415 [01:53<01:09,  3.32it/s] 45%|████▍     | 185/415 [01:54<01:07,  3.41it/s] 45%|████▍     | 186/415 [01:54<01:05,  3.47it/s] 45%|████▌     | 187/415 [01:54<01:04,  3.52it/s] 45%|████▌     | 188/415 [01:55<01:04,  3.55it/s] 46%|████▌     | 189/415 [01:55<01:03,  3.57it/s] 46%|████▌     | 190/415 [01:55<01:04,  3.52it/s] 46%|████▌     | 191/415 [01:55<01:03,  3.55it/s] 46%|████▋     | 192/415 [01:56<01:02,  3.57it/s] 47%|████▋     | 193/415 [01:56<01:01,  3.59it/s] 47%|████▋     | 194/415 [01:56<01:01,  3.59it/s] 47%|████▋     | 195/415 [01:57<01:01,  3.60it/s] 47%|████▋     | 196/415 [01:57<01:00,  3.61it/s] 47%|████▋     | 197/415 [01:57<01:00,  3.61it/s] 48%|████▊     | 198/415 [01:57<01:00,  3.62it/s] 48%|████▊     | 199/415 [01:58<00:59,  3.62it/s] 48%|████▊     | 200/415 [01:58<00:59,  3.62it/s] 48%|████▊     | 201/415 [01:58<01:00,  3.54it/s] 49%|████▊     | 202/415 [01:58<00:59,  3.56it/s] 49%|████▉     | 203/415 [01:59<00:59,  3.58it/s] 49%|████▉     | 204/415 [01:59<00:58,  3.59it/s] 49%|████▉     | 205/415 [01:59<00:58,  3.60it/s] 50%|████▉     | 206/415 [02:00<00:57,  3.61it/s] 50%|████▉     | 207/415 [02:00<00:57,  3.61it/s] 50%|█████     | 208/415 [02:00<00:57,  3.62it/s] 50%|█████     | 209/415 [02:00<00:56,  3.62it/s] 51%|█████     | 210/415 [02:01<00:56,  3.62it/s] 51%|█████     | 211/415 [02:01<00:56,  3.62it/s] 51%|█████     | 212/415 [02:01<00:57,  3.52it/s] 51%|█████▏    | 213/415 [02:02<00:56,  3.55it/s] 52%|█████▏    | 214/415 [02:02<00:56,  3.57it/s] 52%|█████▏    | 215/415 [02:02<00:55,  3.59it/s] 52%|█████▏    | 216/415 [02:02<00:55,  3.60it/s] 52%|█████▏    | 217/415 [02:03<00:54,  3.60it/s] 53%|█████▎    | 218/415 [02:03<00:54,  3.61it/s] 53%|█████▎    | 219/415 [02:03<00:54,  3.62it/s] 53%|█████▎    | 220/415 [02:03<00:53,  3.62it/s] 53%|█████▎    | 221/415 [02:04<00:53,  3.62it/s] 53%|█████▎    | 222/415 [02:04<00:53,  3.62it/s] 54%|█████▎    | 223/415 [02:04<00:54,  3.53it/s] 54%|█████▍    | 224/415 [02:05<00:53,  3.56it/s] 54%|█████▍    | 225/415 [02:05<00:53,  3.58it/s] 54%|█████▍    | 226/415 [02:05<00:52,  3.59it/s] 55%|█████▍    | 227/415 [02:05<00:52,  3.60it/s] 55%|█████▍    | 228/415 [02:06<00:51,  3.60it/s] 55%|█████▌    | 229/415 [02:06<00:51,  3.61it/s] 55%|█████▌    | 230/415 [02:06<00:51,  3.62it/s] 56%|█████▌    | 231/415 [02:07<00:50,  3.62it/s] 56%|█████▌    | 232/415 [02:07<00:50,  3.62it/s] 56%|█████▌    | 233/415 [02:07<00:50,  3.62it/s] 56%|█████▋    | 234/415 [02:07<00:51,  3.50it/s] 57%|█████▋    | 235/415 [02:08<00:50,  3.53it/s] 57%|█████▋    | 236/415 [02:08<00:50,  3.56it/s] 57%|█████▋    | 237/415 [02:08<00:49,  3.58it/s] 57%|█████▋    | 238/415 [02:09<00:49,  3.59it/s] 58%|█████▊    | 239/415 [02:09<00:48,  3.60it/s] 58%|█████▊    | 240/415 [02:09<00:48,  3.61it/s] 58%|█████▊    | 241/415 [02:09<00:48,  3.61it/s] 58%|█████▊    | 242/415 [02:10<00:47,  3.61it/s] 59%|█████▊    | 243/415 [02:10<00:47,  3.61it/s] 59%|█████▉    | 244/415 [02:10<00:47,  3.62it/s] 59%|█████▉    | 245/415 [02:10<00:48,  3.51it/s] 59%|█████▉    | 246/415 [02:11<00:47,  3.55it/s] 60%|█████▉    | 247/415 [02:11<00:47,  3.57it/s] 60%|█████▉    | 248/415 [02:11<00:46,  3.58it/s] 60%|██████    | 249/415 [02:12<00:43,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 04:42:06,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:42:06,727 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 04:42:06,727 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2349, 'eval_samples_per_second': 357.253, 'eval_steps_per_second': 44.688, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.35it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.61it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.58it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.40it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.87it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.30it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 44.99it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.84it/s][A
  4%|▍         | 48/1083 [00:01<00:23, 44.88it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.02it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.05it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.12it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.21it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.16it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.13it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.98it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.88it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.80it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 44.99it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.07it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.07it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.06it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.99it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.00it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.89it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.82it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 43.69it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 44.12it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.57it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.68it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 44.87it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 44.87it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.84it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.80it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.62it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.71it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.72it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.93it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.03it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.16it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.25it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.03it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.03it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.89it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.77it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.83it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.83it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 45.04it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.15it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.13it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.95it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 44.90it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 44.87it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.80it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 44.83it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.88it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.02it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.18it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.08it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.91it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 44.82it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.80it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.82it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.90it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.10it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.08it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.13it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.06it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.90it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 44.82it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.74it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 42.90it/s][A
 34%|███▍      | 373/1083 [00:08<00:16, 43.56it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.16it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.49it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.68it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.91it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 44.87it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 44.89it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.67it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 44.71it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.81it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.90it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.99it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.09it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.19it/s][A
 41%|████      | 443/1083 [00:09<00:14, 45.14it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.98it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.95it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 44.88it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.83it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.94it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 44.97it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.02it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 44.98it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.98it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 44.85it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.82it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.94it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.85it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.92it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.01it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.07it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 45.11it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 45.00it/s][A
 50%|████▉     | 538/1083 [00:11<00:12, 44.93it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.83it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 44.87it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.93it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.84it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.98it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 45.08it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 45.13it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 45.07it/s][A
 54%|█████▍    | 583/1083 [00:12<00:11, 44.87it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.88it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.85it/s][A
 55%|█████▌    | 598/1083 [00:13<00:11, 43.31it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 43.98it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.45it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 44.63it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 44.76it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 44.74it/s][A
 58%|█████▊    | 628/1083 [00:13<00:10, 44.73it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.85it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.66it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.68it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.80it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.97it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.19it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 45.15it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 45.11it/s][A
 62%|██████▏   | 673/1083 [00:14<00:09, 44.92it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.82it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.70it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.70it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.81it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.91it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 45.02it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 45.14it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.17it/s][A
 66%|██████▋   | 718/1083 [00:15<00:08, 45.16it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.88it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.80it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.71it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.76it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.83it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.94it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.09it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 45.20it/s][A
 70%|███████   | 763/1083 [00:16<00:07, 45.03it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.97it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.82it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.81it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.64it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.82it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.03it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.92it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 45.13it/s][A
 75%|███████▍  | 808/1083 [00:17<00:06, 45.13it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.97it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.87it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.75it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 43.66it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.01it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.40it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.63it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 44.89it/s][A
 79%|███████▉  | 853/1083 [00:18<00:05, 44.90it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.95it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.93it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.69it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.70it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.77it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.95it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 45.06it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 45.10it/s][A
 83%|████████▎ | 898/1083 [00:19<00:04, 45.08it/s][A
 83%|████████▎ | 903/1083 [00:20<00:03, 45.04it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.88it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.70it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 41.97it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 42.94it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 43.74it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 44.20it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 44.57it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.74it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.83it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.72it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.38it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.37it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.44it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.76it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 44.95it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 45.15it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 45.19it/s][A
 92%|█████████▏| 993/1083 [00:22<00:01, 45.09it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.90it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.60it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.53it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.73it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.83it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 44.94it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 45.08it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 45.16it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:00, 45.17it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 45.02it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.76it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.53it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.71it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.68it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.77it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 44.96it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 45.14it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 45.12it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 45.12it/s][A 60%|██████    | 249/415 [02:36<00:43,  3.78it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 04:42:30,987 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 04:42:31,173 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:42:33,585 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:42:33,681 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:42:33,719 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [02:40<23:50,  8.67s/it] 60%|██████    | 251/415 [02:40<16:48,  6.15s/it] 61%|██████    | 252/415 [02:40<11:56,  4.40s/it] 61%|██████    | 253/415 [02:41<08:32,  3.16s/it] 61%|██████    | 254/415 [02:41<06:10,  2.30s/it] 61%|██████▏   | 255/415 [02:41<04:30,  1.69s/it] 62%|██████▏   | 256/415 [02:42<03:21,  1.27s/it] 62%|██████▏   | 257/415 [02:42<02:35,  1.02it/s] 62%|██████▏   | 258/415 [02:42<02:01,  1.30it/s] 62%|██████▏   | 259/415 [02:42<01:37,  1.60it/s] 63%|██████▎   | 260/415 [02:43<01:20,  1.92it/s] 63%|██████▎   | 261/415 [02:43<01:09,  2.23it/s] 63%|██████▎   | 262/415 [02:43<01:01,  2.51it/s] 63%|██████▎   | 263/415 [02:44<00:55,  2.75it/s] 64%|██████▎   | 264/415 [02:44<00:51,  2.96it/s] 64%|██████▍   | 265/415 [02:44<00:48,  3.11it/s] 64%|██████▍   | 266/415 [02:44<00:46,  3.24it/s] 64%|██████▍   | 267/415 [02:45<00:44,  3.33it/s] 65%|██████▍   | 268/415 [02:45<00:44,  3.28it/s] 65%|██████▍   | 269/415 [02:45<00:43,  3.36it/s] 65%|██████▌   | 270/415 [02:46<00:42,  3.42it/s] 65%|██████▌   | 271/415 [02:46<00:41,  3.46it/s] 66%|██████▌   | 272/415 [02:46<00:40,  3.49it/s] 66%|██████▌   | 273/415 [02:46<00:40,  3.51it/s] 66%|██████▌   | 274/415 [02:47<00:39,  3.53it/s] 66%|██████▋   | 275/415 [02:47<00:59,  2.35it/s] 67%|██████▋   | 276/415 [02:48<00:53,  2.62it/s] 67%|██████▋   | 277/415 [02:48<00:51,  2.69it/s] 67%|██████▋   | 278/415 [02:48<00:47,  2.91it/s] 67%|██████▋   | 279/415 [02:49<00:43,  3.10it/s] 67%|██████▋   | 280/415 [02:49<00:41,  3.24it/s] 68%|██████▊   | 281/415 [02:49<00:40,  3.34it/s] 68%|██████▊   | 282/415 [02:49<00:38,  3.42it/s] 68%|██████▊   | 283/415 [02:50<00:37,  3.48it/s] 68%|██████▊   | 284/415 [02:50<00:37,  3.52it/s] 69%|██████▊   | 285/415 [02:50<00:36,  3.55it/s] 69%|██████▉   | 286/415 [02:51<00:36,  3.57it/s] 69%|██████▉   | 287/415 [02:51<00:35,  3.59it/s] 69%|██████▉   | 288/415 [02:51<00:37,  3.41it/s] 70%|██████▉   | 289/415 [02:51<00:36,  3.47it/s] 70%|██████▉   | 290/415 [02:52<00:35,  3.51it/s] 70%|███████   | 291/415 [02:52<00:34,  3.54it/s] 70%|███████   | 292/415 [02:52<00:34,  3.57it/s] 71%|███████   | 293/415 [02:52<00:34,  3.58it/s] 71%|███████   | 294/415 [02:53<00:33,  3.59it/s] 71%|███████   | 295/415 [02:53<00:33,  3.60it/s] 71%|███████▏  | 296/415 [02:53<00:32,  3.61it/s] 72%|███████▏  | 297/415 [02:54<00:32,  3.61it/s] 72%|███████▏  | 298/415 [02:54<00:32,  3.62it/s] 72%|███████▏  | 299/415 [02:54<00:34,  3.39it/s] 72%|███████▏  | 300/415 [02:54<00:33,  3.45it/s] 73%|███████▎  | 301/415 [02:55<00:32,  3.50it/s] 73%|███████▎  | 302/415 [02:55<00:31,  3.53it/s] 73%|███████▎  | 303/415 [02:55<00:31,  3.56it/s] 73%|███████▎  | 304/415 [02:56<00:30,  3.58it/s] 73%|███████▎  | 305/415 [02:56<00:30,  3.59it/s] 74%|███████▎  | 306/415 [02:56<00:30,  3.60it/s] 74%|███████▍  | 307/415 [02:56<00:29,  3.60it/s] 74%|███████▍  | 308/415 [02:57<00:29,  3.61it/s] 74%|███████▍  | 309/415 [02:57<00:29,  3.61it/s] 75%|███████▍  | 310/415 [02:57<00:30,  3.43it/s] 75%|███████▍  | 311/415 [02:58<00:29,  3.48it/s] 75%|███████▌  | 312/415 [02:58<00:29,  3.52it/s] 75%|███████▌  | 313/415 [02:58<00:28,  3.55it/s] 76%|███████▌  | 314/415 [02:58<00:28,  3.57it/s] 76%|███████▌  | 315/415 [02:59<00:27,  3.58it/s] 76%|███████▌  | 316/415 [02:59<00:27,  3.59it/s] 76%|███████▋  | 317/415 [02:59<00:27,  3.60it/s] 77%|███████▋  | 318/415 [03:00<00:26,  3.61it/s] 77%|███████▋  | 319/415 [03:00<00:26,  3.61it/s] 77%|███████▋  | 320/415 [03:00<00:26,  3.61it/s] 77%|███████▋  | 321/415 [03:00<00:26,  3.51it/s] 78%|███████▊  | 322/415 [03:01<00:26,  3.54it/s] 78%|███████▊  | 323/415 [03:01<00:25,  3.56it/s] 78%|███████▊  | 324/415 [03:01<00:25,  3.58it/s] 78%|███████▊  | 325/415 [03:01<00:25,  3.59it/s] 79%|███████▊  | 326/415 [03:02<00:24,  3.60it/s] 79%|███████▉  | 327/415 [03:02<00:24,  3.60it/s] 79%|███████▉  | 328/415 [03:02<00:24,  3.61it/s] 79%|███████▉  | 329/415 [03:03<00:23,  3.61it/s] 80%|███████▉  | 330/415 [03:03<00:23,  3.61it/s] 80%|███████▉  | 331/415 [03:03<00:23,  3.62it/s] 80%|████████  | 332/415 [03:03<00:21,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 04:42:58,558 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:42:58,559 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 04:42:58,559 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.1511, 'eval_samples_per_second': 358.492, 'eval_steps_per_second': 44.843, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.79it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.43it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.33it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.38it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.69it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.31it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 44.98it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.69it/s][A
  4%|▍         | 48/1083 [00:01<00:23, 44.82it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 44.93it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 44.95it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.16it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.20it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.15it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 44.93it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.75it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.69it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.76it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 44.88it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 44.85it/s][A
 10%|▉         | 108/1083 [00:02<00:22, 43.95it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.41it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.72it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.71it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.63it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.69it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.72it/s][A
 13%|█▎        | 143/1083 [00:03<00:20, 44.84it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.74it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.78it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.04it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.14it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.18it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.87it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.88it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.91it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 44.85it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.84it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 44.77it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 44.96it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.09it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.02it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.99it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.89it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.94it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.85it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.86it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.80it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.94it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.04it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.04it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.15it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 45.06it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.98it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 44.95it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.95it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.87it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.04it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.05it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.01it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.09it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 45.06it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.08it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.98it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.99it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.97it/s][A
 31%|███       | 338/1083 [00:07<00:17, 43.58it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 44.22it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 44.44it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.56it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 44.64it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.71it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 44.75it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.76it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.68it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.81it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.79it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.93it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 45.11it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 45.05it/s][A
 38%|███▊      | 408/1083 [00:09<00:14, 45.12it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 45.06it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 45.00it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.96it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.95it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.90it/s][A
 40%|████      | 438/1083 [00:09<00:14, 44.91it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.99it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.97it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.97it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.04it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.90it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.94it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 44.89it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 44.81it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 44.79it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.86it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 44.91it/s][A
 46%|████▌     | 498/1083 [00:11<00:12, 45.00it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 45.02it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.98it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.95it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 44.97it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 44.93it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 44.91it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 44.99it/s][A
 50%|████▉     | 538/1083 [00:11<00:12, 44.95it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.95it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 44.96it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.86it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.94it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.91it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 43.97it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 44.34it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 44.51it/s][A
 54%|█████▍    | 583/1083 [00:12<00:11, 44.59it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.80it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.79it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.86it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.79it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.71it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 44.84it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 44.92it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.11it/s][A
 58%|█████▊    | 628/1083 [00:13<00:10, 44.92it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.99it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.93it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.90it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.91it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.78it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 44.94it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 44.97it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 45.05it/s][A
 62%|██████▏   | 673/1083 [00:14<00:09, 44.96it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.99it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 45.02it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.88it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.88it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.73it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.74it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 45.01it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.04it/s][A
 66%|██████▋   | 718/1083 [00:15<00:08, 45.02it/s][A
 67%|██████▋   | 723/1083 [00:16<00:07, 45.02it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.05it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.95it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.86it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.80it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.80it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.85it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 44.94it/s][A
 70%|███████   | 763/1083 [00:16<00:07, 45.06it/s][A
 71%|███████   | 768/1083 [00:17<00:06, 45.10it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.97it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.93it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.86it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.88it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 44.84it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 43.12it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 43.87it/s][A
 75%|███████▍  | 808/1083 [00:17<00:06, 44.29it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.53it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.67it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.62it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.66it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.62it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.60it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.66it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 44.93it/s][A
 79%|███████▉  | 853/1083 [00:18<00:05, 45.05it/s][A
 79%|███████▉  | 858/1083 [00:19<00:04, 45.12it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.06it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.93it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.70it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.60it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.62it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.71it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 44.87it/s][A
 83%|████████▎ | 898/1083 [00:19<00:04, 44.99it/s][A
 83%|████████▎ | 903/1083 [00:20<00:03, 45.05it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 45.08it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.92it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.77it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.69it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 44.68it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 44.75it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 44.90it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.92it/s][A
 88%|████████▊ | 948/1083 [00:21<00:02, 45.15it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 45.17it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 45.16it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.95it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.85it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.79it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 44.85it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 44.78it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.87it/s][A
 92%|█████████▏| 993/1083 [00:22<00:01, 45.01it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 45.08it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 45.03it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.95it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.85it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.78it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 44.78it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 42.74it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 43.57it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.11it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.53it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.69it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.67it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.73it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.60it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.41it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 44.47it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.59it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.90it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.90it/s][A 80%|████████  | 332/415 [03:28<00:21,  3.81it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 04:43:22,832 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 04:43:22,921 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:43:25,546 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:43:25,625 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:43:25,661 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [03:31<11:44,  8.59s/it] 80%|████████  | 334/415 [03:32<08:14,  6.10s/it] 81%|████████  | 335/415 [03:32<05:48,  4.36s/it] 81%|████████  | 336/415 [03:32<04:07,  3.13s/it] 81%|████████  | 337/415 [03:33<02:57,  2.28s/it] 81%|████████▏ | 338/415 [03:33<02:09,  1.68s/it] 82%|████████▏ | 339/415 [03:33<01:35,  1.26s/it] 82%|████████▏ | 340/415 [03:33<01:12,  1.04it/s] 82%|████████▏ | 341/415 [03:34<00:56,  1.32it/s] 82%|████████▏ | 342/415 [03:34<00:44,  1.62it/s] 83%|████████▎ | 343/415 [03:34<00:37,  1.94it/s] 83%|████████▎ | 344/415 [03:34<00:31,  2.25it/s] 83%|████████▎ | 345/415 [03:35<00:28,  2.49it/s] 83%|████████▎ | 346/415 [03:35<00:25,  2.74it/s] 84%|████████▎ | 347/415 [03:35<00:23,  2.95it/s] 84%|████████▍ | 348/415 [03:36<00:21,  3.11it/s] 84%|████████▍ | 349/415 [03:36<00:20,  3.24it/s] 84%|████████▍ | 350/415 [03:36<00:19,  3.33it/s] 85%|████████▍ | 351/415 [03:36<00:18,  3.40it/s] 85%|████████▍ | 352/415 [03:37<00:18,  3.45it/s] 85%|████████▌ | 353/415 [03:37<00:17,  3.48it/s] 85%|████████▌ | 354/415 [03:37<00:17,  3.51it/s] 86%|████████▌ | 355/415 [03:38<00:17,  3.53it/s] 86%|████████▌ | 356/415 [03:38<00:17,  3.46it/s] 86%|████████▌ | 357/415 [03:38<00:16,  3.49it/s] 86%|████████▋ | 358/415 [03:38<00:16,  3.52it/s] 87%|████████▋ | 359/415 [03:39<00:15,  3.54it/s] 87%|████████▋ | 360/415 [03:39<00:15,  3.55it/s] 87%|████████▋ | 361/415 [03:39<00:15,  3.56it/s] 87%|████████▋ | 362/415 [03:40<00:14,  3.56it/s] 87%|████████▋ | 363/415 [03:40<00:14,  3.56it/s] 88%|████████▊ | 364/415 [03:40<00:14,  3.56it/s] 88%|████████▊ | 365/415 [03:40<00:14,  3.51it/s] 88%|████████▊ | 366/415 [03:41<00:13,  3.52it/s] 88%|████████▊ | 367/415 [03:41<00:13,  3.48it/s] 89%|████████▊ | 368/415 [03:41<00:13,  3.50it/s] 89%|████████▉ | 369/415 [03:42<00:13,  3.52it/s] 89%|████████▉ | 370/415 [03:42<00:12,  3.54it/s] 89%|████████▉ | 371/415 [03:42<00:12,  3.54it/s] 90%|████████▉ | 372/415 [03:42<00:12,  3.55it/s] 90%|████████▉ | 373/415 [03:43<00:11,  3.56it/s] 90%|█████████ | 374/415 [03:43<00:11,  3.56it/s] 90%|█████████ | 375/415 [03:43<00:11,  3.55it/s] 91%|█████████ | 376/415 [03:44<00:10,  3.56it/s] 91%|█████████ | 377/415 [03:44<00:10,  3.56it/s] 91%|█████████ | 378/415 [03:44<00:10,  3.46it/s] 91%|█████████▏| 379/415 [03:44<00:10,  3.49it/s] 92%|█████████▏| 380/415 [03:45<00:09,  3.51it/s] 92%|█████████▏| 381/415 [03:45<00:09,  3.52it/s] 92%|█████████▏| 382/415 [03:45<00:09,  3.53it/s] 92%|█████████▏| 383/415 [03:46<00:09,  3.54it/s] 93%|█████████▎| 384/415 [03:46<00:08,  3.55it/s] 93%|█████████▎| 385/415 [03:46<00:08,  3.55it/s] 93%|█████████▎| 386/415 [03:46<00:08,  3.56it/s] 93%|█████████▎| 387/415 [03:47<00:07,  3.56it/s] 93%|█████████▎| 388/415 [03:47<00:07,  3.40it/s] 94%|█████████▎| 389/415 [03:48<00:12,  2.05it/s] 94%|█████████▍| 390/415 [03:48<00:10,  2.34it/s] 94%|█████████▍| 391/415 [03:48<00:09,  2.61it/s] 94%|█████████▍| 392/415 [03:49<00:08,  2.84it/s] 95%|█████████▍| 393/415 [03:49<00:07,  3.02it/s] 95%|█████████▍| 394/415 [03:49<00:06,  3.17it/s] 95%|█████████▌| 395/415 [03:50<00:06,  3.27it/s] 95%|█████████▌| 396/415 [03:50<00:05,  3.36it/s] 96%|█████████▌| 397/415 [03:50<00:05,  3.42it/s] 96%|█████████▌| 398/415 [03:50<00:05,  3.30it/s] 96%|█████████▌| 399/415 [03:51<00:04,  3.38it/s] 96%|█████████▋| 400/415 [03:51<00:04,  3.43it/s] 97%|█████████▋| 401/415 [03:51<00:04,  3.47it/s] 97%|█████████▋| 402/415 [03:52<00:03,  3.50it/s] 97%|█████████▋| 403/415 [03:52<00:03,  3.52it/s] 97%|█████████▋| 404/415 [03:52<00:03,  3.53it/s] 98%|█████████▊| 405/415 [03:52<00:02,  3.54it/s] 98%|█████████▊| 406/415 [03:53<00:02,  3.55it/s] 98%|█████████▊| 407/415 [03:53<00:02,  3.55it/s] 98%|█████████▊| 408/415 [03:53<00:01,  3.56it/s] 99%|█████████▊| 409/415 [03:54<00:01,  3.39it/s] 99%|█████████▉| 410/415 [03:54<00:01,  3.44it/s] 99%|█████████▉| 411/415 [03:54<00:01,  3.48it/s] 99%|█████████▉| 412/415 [03:54<00:00,  3.50it/s]100%|█████████▉| 413/415 [03:55<00:00,  3.52it/s]100%|█████████▉| 414/415 [03:55<00:00,  3.54it/s]100%|██████████| 415/415 [03:55<00:00,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 04:43:50,446 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:43:50,446 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 04:43:50,446 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.15, 'eval_samples_per_second': 358.51, 'eval_steps_per_second': 44.845, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.26it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.28it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.63it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.64it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.85it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.35it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.99it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.75it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.84it/s][A
  5%|▍         | 52/1083 [00:01<00:27, 37.81it/s][A
  5%|▌         | 57/1083 [00:01<00:25, 39.82it/s][A
  6%|▌         | 62/1083 [00:01<00:24, 41.47it/s][A
  6%|▌         | 67/1083 [00:01<00:23, 42.62it/s][A
  7%|▋         | 72/1083 [00:01<00:23, 43.54it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.09it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.54it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.70it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.40it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.23it/s][A
  9%|▉         | 102/1083 [00:02<00:22, 44.21it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.47it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.73it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.97it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.16it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.17it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 45.24it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.93it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.72it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 44.55it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.64it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.81it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.04it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.11it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 45.20it/s][A
 16%|█▋        | 177/1083 [00:03<00:19, 45.31it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 45.03it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.75it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.60it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.65it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.78it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.89it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.14it/s][A
 20%|██        | 217/1083 [00:04<00:19, 45.22it/s][A
 20%|██        | 222/1083 [00:04<00:19, 45.27it/s][A
 21%|██        | 227/1083 [00:05<00:18, 45.08it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.79it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.69it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.59it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.84it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.88it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.94it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 45.02it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 45.18it/s][A
 25%|██▌       | 272/1083 [00:06<00:17, 45.11it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.85it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.73it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.67it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.69it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.82it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 43.55it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.11it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.52it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.67it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.62it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.53it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.51it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.64it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.65it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.69it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.83it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.99it/s][A
 33%|███▎      | 362/1083 [00:08<00:15, 45.16it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 45.18it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.94it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.89it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.79it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.83it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.88it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.85it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.08it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 45.05it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.12it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.02it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.85it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.85it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.83it/s][A
 40%|████      | 437/1083 [00:09<00:14, 43.09it/s][A
 41%|████      | 442/1083 [00:09<00:14, 43.84it/s][A
 41%|████▏     | 447/1083 [00:10<00:14, 44.26it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.56it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.76it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.75it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.64it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.62it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.51it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.61it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.85it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.94it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 45.04it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.16it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.06it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.96it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.77it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.68it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.79it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.95it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 45.05it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 45.02it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 45.15it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.05it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.94it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.76it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.69it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.43it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.64it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.85it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 45.01it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.08it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 45.01it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.97it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.89it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.70it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.69it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.92it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.77it/s][A
 58%|█████▊    | 632/1083 [00:14<00:09, 45.21it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 45.16it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 45.12it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.97it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.87it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.80it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.71it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.81it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.92it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 45.09it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 45.20it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.08it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.03it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.93it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.76it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.37it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.59it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.72it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.96it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 45.02it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 45.13it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.11it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.95it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.94it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.83it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.85it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.87it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.96it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 45.04it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 45.07it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.00it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.91it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.88it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.80it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.84it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.96it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.97it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 45.08it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.98it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.98it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.90it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.87it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 42.87it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 43.53it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 43.82it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.42it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.57it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.73it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.77it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.78it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.59it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.59it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.73it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 45.00it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 45.10it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 45.06it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 45.11it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.04it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.91it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.77it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.65it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.79it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.92it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 45.08it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 45.10it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 45.08it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.09it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.93it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.70it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 42.14it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 43.10it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 43.77it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.19it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.49it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.68it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.86it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.90it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.52it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.65it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.74it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.81it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 45.01it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 45.09it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 45.10it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.99it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.85it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.69it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.76it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.84it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.88it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.00it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 45.00it/s][A100%|██████████| 415/415 [04:19<00:00,  3.74it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 04:44:14,735 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 04:44:14,865 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:44:17,725 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:44:17,876 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:44:17,950 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 04:44:18,872 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 04:44:18,872 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83 (score: 0.9504339694976807).
                                                 100%|██████████| 415/415 [04:32<00:00,  3.74it/s]100%|██████████| 415/415 [04:32<00:00,  1.53it/s]
[INFO|trainer.py:1894] 2023-08-28 04:44:26,843 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 04:44:26,938 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:44:30,472 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:44:30,611 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:44:30,677 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:44:31,175 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:31,175 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:31,175 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:31,175 >>   train_runtime            = 0:04:32.06
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:31,175 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:31,176 >>   train_samples_per_second =     97.402
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:31,176 >>   train_steps_per_second   =      1.525
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2251, 'eval_samples_per_second': 357.398, 'eval_steps_per_second': 44.706, 'epoch': 5.0}
{'train_runtime': 272.0684, 'train_samples_per_second': 97.402, 'train_steps_per_second': 1.525, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 04:44:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 04:44:31,422 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:44:31,422 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 04:44:31,423 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.50it/s]  1%|          | 12/1083 [00:00<00:21, 49.42it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.95it/s]  2%|▏         | 22/1083 [00:00<00:22, 47.08it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.45it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.23it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.99it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.65it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.08it/s]  5%|▍         | 52/1083 [00:01<00:23, 44.71it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.80it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.04it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.21it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.22it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.35it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.44it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.33it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.92it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.71it/s]  9%|▉         | 102/1083 [00:02<00:22, 44.51it/s] 10%|▉         | 107/1083 [00:02<00:21, 44.85it/s] 10%|█         | 112/1083 [00:02<00:21, 45.01it/s] 11%|█         | 117/1083 [00:02<00:21, 45.09it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.14it/s] 12%|█▏        | 127/1083 [00:02<00:21, 45.18it/s] 12%|█▏        | 132/1083 [00:02<00:21, 45.24it/s] 13%|█▎        | 137/1083 [00:03<00:20, 45.13it/s] 13%|█▎        | 142/1083 [00:03<00:20, 44.89it/s] 14%|█▎        | 147/1083 [00:03<00:20, 44.86it/s] 14%|█▍        | 152/1083 [00:03<00:20, 44.99it/s] 14%|█▍        | 157/1083 [00:03<00:20, 45.06it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.23it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.14it/s] 16%|█▌        | 172/1083 [00:03<00:20, 44.93it/s] 16%|█▋        | 177/1083 [00:03<00:20, 45.12it/s] 17%|█▋        | 182/1083 [00:04<00:20, 44.94it/s] 17%|█▋        | 187/1083 [00:04<00:19, 44.90it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.57it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.76it/s] 19%|█▊        | 202/1083 [00:04<00:19, 44.94it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.07it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.03it/s] 20%|██        | 217/1083 [00:04<00:19, 45.07it/s] 20%|██        | 222/1083 [00:04<00:19, 45.01it/s] 21%|██        | 227/1083 [00:05<00:19, 44.93it/s] 21%|██▏       | 232/1083 [00:05<00:18, 44.83it/s] 22%|██▏       | 237/1083 [00:05<00:19, 43.84it/s] 22%|██▏       | 242/1083 [00:05<00:18, 44.27it/s] 23%|██▎       | 247/1083 [00:05<00:18, 44.63it/s] 23%|██▎       | 252/1083 [00:05<00:18, 44.75it/s] 24%|██▎       | 257/1083 [00:05<00:18, 44.80it/s] 24%|██▍       | 262/1083 [00:05<00:18, 44.92it/s] 25%|██▍       | 267/1083 [00:05<00:18, 45.00it/s] 25%|██▌       | 272/1083 [00:06<00:18, 44.83it/s] 26%|██▌       | 277/1083 [00:06<00:18, 44.65it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.80it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.90it/s] 27%|██▋       | 292/1083 [00:06<00:17, 45.12it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.09it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.14it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.03it/s] 29%|██▉       | 312/1083 [00:06<00:17, 45.01it/s] 29%|██▉       | 317/1083 [00:07<00:17, 44.88it/s] 30%|██▉       | 322/1083 [00:07<00:17, 44.67it/s] 30%|███       | 327/1083 [00:07<00:16, 44.82it/s] 31%|███       | 332/1083 [00:07<00:16, 44.91it/s] 31%|███       | 337/1083 [00:07<00:16, 45.03it/s] 32%|███▏      | 342/1083 [00:07<00:16, 45.08it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.09it/s] 33%|███▎      | 352/1083 [00:07<00:16, 44.93it/s] 33%|███▎      | 357/1083 [00:07<00:16, 44.94it/s] 33%|███▎      | 362/1083 [00:08<00:16, 44.77it/s] 34%|███▍      | 367/1083 [00:08<00:15, 44.84it/s] 34%|███▍      | 372/1083 [00:08<00:16, 43.09it/s] 35%|███▍      | 377/1083 [00:08<00:16, 43.79it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.31it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.59it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.86it/s] 37%|███▋      | 397/1083 [00:08<00:15, 44.97it/s] 37%|███▋      | 402/1083 [00:08<00:15, 44.96it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.84it/s] 38%|███▊      | 412/1083 [00:09<00:14, 44.74it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.84it/s] 39%|███▉      | 422/1083 [00:09<00:14, 44.88it/s] 39%|███▉      | 427/1083 [00:09<00:14, 45.07it/s] 40%|███▉      | 432/1083 [00:09<00:14, 45.23it/s] 40%|████      | 437/1083 [00:09<00:14, 45.33it/s] 41%|████      | 442/1083 [00:09<00:14, 42.95it/s] 41%|████▏     | 447/1083 [00:09<00:14, 44.81it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.91it/s] 42%|████▏     | 457/1083 [00:10<00:13, 44.92it/s] 43%|████▎     | 462/1083 [00:10<00:13, 45.01it/s] 43%|████▎     | 467/1083 [00:10<00:13, 45.00it/s] 44%|████▎     | 472/1083 [00:10<00:13, 45.19it/s] 44%|████▍     | 477/1083 [00:10<00:13, 45.22it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.10it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.29it/s] 45%|████▌     | 492/1083 [00:10<00:13, 45.10it/s] 46%|████▌     | 497/1083 [00:11<00:12, 45.11it/s] 46%|████▋     | 502/1083 [00:11<00:13, 41.87it/s] 47%|████▋     | 507/1083 [00:11<00:20, 28.33it/s] 47%|████▋     | 512/1083 [00:11<00:17, 31.98it/s] 48%|████▊     | 517/1083 [00:11<00:16, 35.16it/s] 48%|████▊     | 522/1083 [00:11<00:14, 37.78it/s] 49%|████▊     | 527/1083 [00:11<00:13, 39.77it/s] 49%|████▉     | 532/1083 [00:12<00:13, 41.34it/s] 50%|████▉     | 537/1083 [00:12<00:12, 42.59it/s] 50%|█████     | 542/1083 [00:12<00:12, 43.46it/s] 51%|█████     | 547/1083 [00:12<00:12, 43.61it/s] 51%|█████     | 552/1083 [00:12<00:12, 43.70it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 44.06it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 44.45it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 44.76it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 44.94it/s] 53%|█████▎    | 577/1083 [00:13<00:11, 45.16it/s] 54%|█████▎    | 582/1083 [00:13<00:11, 45.35it/s] 54%|█████▍    | 587/1083 [00:13<00:10, 45.23it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 45.05it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 44.74it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.75it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 44.95it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 45.10it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 45.21it/s] 57%|█████▋    | 622/1083 [00:14<00:10, 45.36it/s] 58%|█████▊    | 627/1083 [00:14<00:10, 45.39it/s] 58%|█████▊    | 632/1083 [00:14<00:09, 45.28it/s] 59%|█████▉    | 637/1083 [00:14<00:10, 42.61it/s] 59%|█████▉    | 642/1083 [00:14<00:10, 43.29it/s] 60%|█████▉    | 647/1083 [00:14<00:09, 43.72it/s] 60%|██████    | 652/1083 [00:14<00:09, 44.16it/s] 61%|██████    | 657/1083 [00:14<00:09, 44.47it/s] 61%|██████    | 662/1083 [00:14<00:09, 44.84it/s] 62%|██████▏   | 667/1083 [00:15<00:09, 45.10it/s] 62%|██████▏   | 672/1083 [00:15<00:09, 45.20it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.86it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.87it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.85it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.84it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 44.92it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 45.03it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 45.18it/s] 66%|██████▌   | 712/1083 [00:16<00:08, 45.33it/s] 66%|██████▌   | 717/1083 [00:16<00:08, 45.31it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 45.06it/s] 67%|██████▋   | 727/1083 [00:16<00:07, 45.04it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.99it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.83it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.93it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 45.03it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 45.17it/s] 70%|██████▉   | 757/1083 [00:17<00:07, 45.34it/s] 70%|███████   | 762/1083 [00:17<00:07, 45.25it/s] 71%|███████   | 767/1083 [00:17<00:06, 45.15it/s] 71%|███████▏  | 772/1083 [00:17<00:07, 40.78it/s] 72%|███████▏  | 777/1083 [00:17<00:07, 42.17it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 43.08it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 43.80it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.28it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 44.66it/s] 74%|███████▍  | 802/1083 [00:18<00:06, 44.87it/s] 75%|███████▍  | 807/1083 [00:18<00:06, 44.88it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 44.60it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.35it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.65it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 44.88it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 45.03it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 45.25it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 45.32it/s] 78%|███████▊  | 847/1083 [00:19<00:05, 45.32it/s] 79%|███████▊  | 852/1083 [00:19<00:05, 45.15it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.81it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.66it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.85it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.97it/s] 81%|████████  | 877/1083 [00:19<00:04, 45.05it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 45.14it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 45.35it/s] 82%|████████▏ | 892/1083 [00:20<00:04, 45.38it/s] 83%|████████▎ | 897/1083 [00:20<00:04, 45.21it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 44.82it/s] 84%|████████▎ | 907/1083 [00:20<00:04, 42.22it/s] 84%|████████▍ | 912/1083 [00:20<00:03, 43.22it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 43.81it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 44.32it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 44.62it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 44.96it/s] 87%|████████▋ | 937/1083 [00:21<00:03, 45.01it/s] 87%|████████▋ | 942/1083 [00:21<00:03, 44.87it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.49it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 44.45it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 44.65it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 44.86it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 45.06it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 45.20it/s] 90%|█████████ | 977/1083 [00:21<00:02, 45.32it/s] 91%|█████████ | 982/1083 [00:22<00:02, 45.42it/s] 91%|█████████ | 987/1083 [00:22<00:02, 45.10it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 44.77it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 44.66it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 44.70it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 44.79it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 44.83it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 45.04it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 45.22it/s] 95%|█████████▍| 1027/1083 [00:23<00:01, 45.37it/s] 95%|█████████▌| 1032/1083 [00:23<00:01, 45.19it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 44.96it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 43.77it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 44.15it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 44.51it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 44.66it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 44.97it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 45.10it/s] 99%|█████████▉| 1072/1083 [00:24<00:00, 45.18it/s] 99%|█████████▉| 1077/1083 [00:24<00:00, 45.02it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 44.76it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.48it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:44:55,790 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   eval_loss               =     0.9504
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   eval_runtime            = 0:00:24.36
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   eval_samples_per_second =    355.318
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   eval_steps_per_second   =     44.446
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:44:55,790 >>   perplexity              =     2.5868
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:05,813 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:05,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:05,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:05,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:05,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:45:06,756 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:45:06,757 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:45:07,380 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:45:08,515 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:45:08,515 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:11,525 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:11,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:11,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:11,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:11,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:45:12,394 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:45:12,396 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:45:13,023 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:45:13,268 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:45:13,268 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-332
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-166
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-415
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-249
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/checkpoint-83
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.68it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.74it/s]Extractor Predicting: 14it [00:08,  1.72it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.72it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:11,  1.62it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:13,  1.67it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:17,  1.71it/s]Extractor Predicting: 31it [00:18,  1.74it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:21,  1.69it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:22,  1.70it/s]Extractor Predicting: 39it [00:23,  1.71it/s]Extractor Predicting: 40it [00:23,  1.74it/s]Extractor Predicting: 41it [00:24,  1.61it/s]Extractor Predicting: 42it [00:25,  1.66it/s]Extractor Predicting: 43it [00:25,  1.66it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:26,  1.66it/s]Extractor Predicting: 46it [00:27,  1.67it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:29,  1.52it/s]Extractor Predicting: 50it [00:30,  1.57it/s]Extractor Predicting: 51it [00:30,  1.64it/s]Extractor Predicting: 52it [00:31,  1.64it/s]Extractor Predicting: 53it [00:31,  1.64it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:33,  1.70it/s]Extractor Predicting: 57it [00:34,  1.71it/s]Extractor Predicting: 58it [00:34,  1.70it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:36,  1.57it/s]Extractor Predicting: 61it [00:36,  1.63it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:37,  1.76it/s]Extractor Predicting: 64it [00:38,  1.79it/s]Extractor Predicting: 65it [00:38,  1.75it/s]Extractor Predicting: 66it [00:39,  1.74it/s]Extractor Predicting: 67it [00:39,  1.72it/s]Extractor Predicting: 68it [00:40,  1.75it/s]Extractor Predicting: 69it [00:41,  1.76it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:42,  1.68it/s]Extractor Predicting: 72it [00:42,  1.71it/s]Extractor Predicting: 73it [00:43,  1.74it/s]Extractor Predicting: 74it [00:44,  1.75it/s]Extractor Predicting: 75it [00:44,  1.73it/s]Extractor Predicting: 76it [00:45,  1.75it/s]Extractor Predicting: 77it [00:45,  1.72it/s]Extractor Predicting: 78it [00:46,  1.73it/s]Extractor Predicting: 79it [00:46,  1.71it/s]Extractor Predicting: 80it [00:47,  1.72it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:48,  1.76it/s]Extractor Predicting: 83it [00:49,  1.75it/s]Extractor Predicting: 84it [00:49,  1.76it/s]Extractor Predicting: 85it [00:50,  1.70it/s]Extractor Predicting: 86it [00:51,  1.68it/s]Extractor Predicting: 87it [00:51,  1.66it/s]Extractor Predicting: 88it [00:52,  1.66it/s]Extractor Predicting: 89it [00:52,  1.71it/s]Extractor Predicting: 90it [00:53,  1.70it/s]Extractor Predicting: 91it [00:53,  1.75it/s]Extractor Predicting: 92it [00:54,  1.74it/s]Extractor Predicting: 93it [00:55,  1.73it/s]Extractor Predicting: 94it [00:55,  1.73it/s]Extractor Predicting: 95it [00:56,  1.76it/s]Extractor Predicting: 96it [00:56,  1.70it/s]Extractor Predicting: 97it [00:57,  1.71it/s]Extractor Predicting: 98it [00:58,  1.70it/s]Extractor Predicting: 99it [00:58,  1.71it/s]Extractor Predicting: 100it [00:59,  1.71it/s]Extractor Predicting: 101it [00:59,  1.73it/s]Extractor Predicting: 102it [01:00,  1.72it/s]Extractor Predicting: 103it [01:00,  1.72it/s]Extractor Predicting: 104it [01:01,  1.73it/s]Extractor Predicting: 105it [01:02,  1.73it/s]Extractor Predicting: 106it [01:02,  1.74it/s]Extractor Predicting: 107it [01:03,  1.73it/s]Extractor Predicting: 108it [01:03,  1.71it/s]Extractor Predicting: 109it [01:04,  1.72it/s]Extractor Predicting: 110it [01:04,  1.72it/s]Extractor Predicting: 111it [01:05,  1.72it/s]Extractor Predicting: 112it [01:06,  1.73it/s]Extractor Predicting: 113it [01:06,  1.73it/s]Extractor Predicting: 114it [01:07,  1.72it/s]Extractor Predicting: 115it [01:07,  1.68it/s]Extractor Predicting: 116it [01:08,  1.67it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:09,  1.72it/s]Extractor Predicting: 119it [01:10,  1.74it/s]Extractor Predicting: 120it [01:10,  1.74it/s]Extractor Predicting: 121it [01:11,  1.71it/s]Extractor Predicting: 122it [01:11,  1.83it/s]Extractor Predicting: 123it [01:12,  1.81it/s]Extractor Predicting: 124it [01:13,  1.56it/s]Extractor Predicting: 125it [01:13,  1.55it/s]Extractor Predicting: 126it [01:14,  1.56it/s]Extractor Predicting: 127it [01:15,  1.62it/s]Extractor Predicting: 128it [01:15,  1.64it/s]Extractor Predicting: 129it [01:16,  1.66it/s]Extractor Predicting: 130it [01:16,  1.66it/s]Extractor Predicting: 131it [01:17,  1.72it/s]Extractor Predicting: 132it [01:18,  1.70it/s]Extractor Predicting: 133it [01:18,  1.67it/s]Extractor Predicting: 134it [01:19,  1.69it/s]Extractor Predicting: 135it [01:19,  1.70it/s]Extractor Predicting: 136it [01:20,  1.56it/s]Extractor Predicting: 137it [01:21,  1.60it/s]Extractor Predicting: 138it [01:21,  1.63it/s]Extractor Predicting: 139it [01:22,  1.65it/s]Extractor Predicting: 140it [01:22,  1.67it/s]Extractor Predicting: 141it [01:27,  1.89s/it]Extractor Predicting: 142it [01:28,  1.50s/it]Extractor Predicting: 143it [01:28,  1.23s/it]Extractor Predicting: 144it [01:29,  1.03s/it]Extractor Predicting: 145it [01:30,  1.10it/s]Extractor Predicting: 146it [01:30,  1.22it/s]Extractor Predicting: 147it [01:31,  1.35it/s]Extractor Predicting: 148it [01:31,  1.42it/s]Extractor Predicting: 149it [01:32,  1.53it/s]Extractor Predicting: 150it [01:33,  1.59it/s]Extractor Predicting: 151it [01:33,  1.64it/s]Extractor Predicting: 152it [01:34,  1.61it/s]Extractor Predicting: 153it [01:34,  1.58it/s]Extractor Predicting: 154it [01:35,  1.52it/s]Extractor Predicting: 155it [01:36,  1.51it/s]Extractor Predicting: 156it [01:36,  1.52it/s]Extractor Predicting: 157it [01:37,  1.52it/s]Extractor Predicting: 158it [01:38,  1.52it/s]Extractor Predicting: 159it [01:38,  1.51it/s]Extractor Predicting: 160it [01:39,  1.52it/s]Extractor Predicting: 161it [01:40,  1.52it/s]Extractor Predicting: 162it [01:40,  1.52it/s]Extractor Predicting: 163it [01:41,  1.53it/s]Extractor Predicting: 164it [01:42,  1.53it/s]Extractor Predicting: 165it [01:42,  1.54it/s]Extractor Predicting: 166it [01:43,  1.55it/s]Extractor Predicting: 167it [01:44,  1.54it/s]Extractor Predicting: 168it [01:44,  1.55it/s]Extractor Predicting: 169it [01:45,  1.60it/s]Extractor Predicting: 170it [01:45,  1.66it/s]Extractor Predicting: 171it [01:46,  1.67it/s]Extractor Predicting: 172it [01:47,  1.65it/s]Extractor Predicting: 173it [01:47,  1.65it/s]Extractor Predicting: 174it [01:48,  1.60it/s]Extractor Predicting: 175it [01:49,  1.60it/s]Extractor Predicting: 176it [01:49,  1.60it/s]Extractor Predicting: 177it [01:50,  1.60it/s]Extractor Predicting: 178it [01:50,  1.59it/s]Extractor Predicting: 179it [01:51,  1.59it/s]Extractor Predicting: 180it [01:52,  1.58it/s]Extractor Predicting: 181it [01:52,  1.61it/s]Extractor Predicting: 182it [01:53,  1.62it/s]Extractor Predicting: 183it [01:53,  1.68it/s]Extractor Predicting: 184it [01:54,  1.67it/s]Extractor Predicting: 185it [01:55,  1.72it/s]Extractor Predicting: 186it [01:55,  1.68it/s]Extractor Predicting: 187it [01:56,  1.70it/s]Extractor Predicting: 188it [01:56,  1.70it/s]Extractor Predicting: 189it [01:57,  1.72it/s]Extractor Predicting: 190it [01:58,  1.66it/s]Extractor Predicting: 191it [01:58,  1.66it/s]Extractor Predicting: 192it [01:59,  1.68it/s]Extractor Predicting: 193it [01:59,  1.71it/s]Extractor Predicting: 194it [02:00,  1.72it/s]Extractor Predicting: 195it [02:01,  1.70it/s]Extractor Predicting: 196it [02:01,  1.67it/s]Extractor Predicting: 197it [02:02,  1.69it/s]Extractor Predicting: 198it [02:02,  1.69it/s]Extractor Predicting: 199it [02:03,  1.74it/s]Extractor Predicting: 200it [02:03,  1.73it/s]Extractor Predicting: 201it [02:04,  1.75it/s]Extractor Predicting: 202it [02:05,  1.58it/s]Extractor Predicting: 203it [02:05,  1.61it/s]Extractor Predicting: 204it [02:06,  1.65it/s]Extractor Predicting: 205it [02:07,  1.64it/s]Extractor Predicting: 206it [02:07,  1.67it/s]Extractor Predicting: 207it [02:08,  1.66it/s]Extractor Predicting: 208it [02:08,  1.68it/s]Extractor Predicting: 209it [02:09,  1.72it/s]Extractor Predicting: 210it [02:09,  1.72it/s]Extractor Predicting: 211it [02:10,  1.72it/s]Extractor Predicting: 212it [02:11,  1.71it/s]Extractor Predicting: 213it [02:11,  1.72it/s]Extractor Predicting: 214it [02:12,  1.75it/s]Extractor Predicting: 215it [02:12,  1.78it/s]Extractor Predicting: 216it [02:13,  1.77it/s]Extractor Predicting: 217it [02:13,  1.77it/s]Extractor Predicting: 218it [02:14,  1.76it/s]Extractor Predicting: 219it [02:15,  1.76it/s]Extractor Predicting: 220it [02:15,  1.71it/s]Extractor Predicting: 221it [02:16,  1.75it/s]Extractor Predicting: 222it [02:16,  1.76it/s]Extractor Predicting: 223it [02:17,  1.74it/s]Extractor Predicting: 224it [02:17,  1.71it/s]Extractor Predicting: 225it [02:18,  1.65it/s]Extractor Predicting: 226it [02:19,  1.47it/s]Extractor Predicting: 227it [02:20,  1.53it/s]Extractor Predicting: 228it [02:20,  1.57it/s]Extractor Predicting: 229it [02:21,  1.64it/s]Extractor Predicting: 230it [02:21,  1.64it/s]Extractor Predicting: 231it [02:22,  1.61it/s]Extractor Predicting: 232it [02:23,  1.66it/s]Extractor Predicting: 233it [02:23,  1.70it/s]Extractor Predicting: 234it [02:24,  1.67it/s]Extractor Predicting: 235it [02:24,  1.67it/s]Extractor Predicting: 236it [02:25,  1.64it/s]Extractor Predicting: 237it [02:26,  1.64it/s]Extractor Predicting: 238it [02:26,  1.67it/s]Extractor Predicting: 239it [02:27,  1.70it/s]Extractor Predicting: 240it [02:27,  1.69it/s]Extractor Predicting: 241it [02:28,  1.70it/s]Extractor Predicting: 242it [02:28,  1.72it/s]Extractor Predicting: 243it [02:29,  1.65it/s]Extractor Predicting: 244it [02:30,  1.68it/s]Extractor Predicting: 245it [02:30,  1.68it/s]Extractor Predicting: 246it [02:31,  1.68it/s]Extractor Predicting: 247it [02:31,  1.69it/s]Extractor Predicting: 248it [02:32,  1.69it/s]Extractor Predicting: 249it [02:33,  1.68it/s]Extractor Predicting: 250it [02:33,  1.66it/s]Extractor Predicting: 251it [02:34,  1.67it/s]Extractor Predicting: 252it [02:34,  1.68it/s]Extractor Predicting: 253it [02:35,  1.68it/s]Extractor Predicting: 254it [02:36,  1.65it/s]Extractor Predicting: 255it [02:36,  1.64it/s]Extractor Predicting: 256it [02:37,  1.68it/s]Extractor Predicting: 257it [02:37,  1.70it/s]Extractor Predicting: 258it [02:38,  1.73it/s]Extractor Predicting: 259it [02:39,  1.70it/s]Extractor Predicting: 260it [02:39,  1.65it/s]Extractor Predicting: 261it [02:40,  1.68it/s]Extractor Predicting: 262it [02:40,  1.70it/s]Extractor Predicting: 263it [02:41,  1.66it/s]Extractor Predicting: 264it [02:42,  1.66it/s]Extractor Predicting: 265it [02:42,  1.65it/s]Extractor Predicting: 266it [02:43,  1.63it/s]Extractor Predicting: 267it [02:43,  1.65it/s]Extractor Predicting: 268it [02:44,  1.66it/s]Extractor Predicting: 269it [02:45,  1.74it/s]Extractor Predicting: 269it [02:45,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:10,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:10,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:10,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:10,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:10,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:48:12,352 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:48:12,353 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:48:13,108 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:48:14,197 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:48:14,224 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:16,875 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:16,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:16,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:16,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:48:16,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:48:17,769 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:48:17,795 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:48:18,515 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:48:18,722 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:48:18,722 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.56it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:21,  1.65it/s]Extractor Predicting: 35it [00:21,  1.60it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.71it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:25,  1.70it/s]Extractor Predicting: 42it [00:25,  1.73it/s]Extractor Predicting: 43it [00:26,  1.73it/s]Extractor Predicting: 44it [00:27,  1.71it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:28,  1.68it/s]Extractor Predicting: 47it [00:28,  1.67it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:31,  1.71it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:32,  1.75it/s]Extractor Predicting: 54it [00:32,  1.77it/s]Extractor Predicting: 55it [00:33,  1.72it/s]Extractor Predicting: 56it [00:34,  1.77it/s]Extractor Predicting: 57it [00:34,  1.77it/s]Extractor Predicting: 58it [00:35,  1.77it/s]Extractor Predicting: 59it [00:35,  1.76it/s]Extractor Predicting: 60it [00:36,  1.78it/s]Extractor Predicting: 61it [00:36,  1.72it/s]Extractor Predicting: 62it [00:37,  1.72it/s]Extractor Predicting: 63it [00:38,  1.70it/s]Extractor Predicting: 64it [00:38,  1.67it/s]Extractor Predicting: 65it [00:39,  1.70it/s]Extractor Predicting: 66it [00:39,  1.68it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:41,  1.69it/s]Extractor Predicting: 69it [00:41,  1.73it/s]Extractor Predicting: 70it [00:42,  1.75it/s]Extractor Predicting: 71it [00:42,  1.78it/s]Extractor Predicting: 72it [00:43,  1.72it/s]Extractor Predicting: 73it [00:43,  1.73it/s]Extractor Predicting: 74it [00:44,  1.72it/s]Extractor Predicting: 75it [00:45,  1.72it/s]Extractor Predicting: 76it [00:45,  1.75it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:46,  1.70it/s]Extractor Predicting: 79it [00:47,  1.74it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:49,  1.78it/s]Extractor Predicting: 83it [00:49,  1.77it/s]Extractor Predicting: 84it [00:50,  1.82it/s]Extractor Predicting: 85it [00:50,  1.84it/s]Extractor Predicting: 86it [00:51,  1.75it/s]Extractor Predicting: 87it [00:51,  1.76it/s]Extractor Predicting: 88it [00:52,  1.77it/s]Extractor Predicting: 89it [00:53,  1.73it/s]Extractor Predicting: 90it [00:53,  1.78it/s]Extractor Predicting: 91it [00:54,  1.77it/s]Extractor Predicting: 92it [00:55,  1.57it/s]Extractor Predicting: 93it [00:55,  1.59it/s]Extractor Predicting: 94it [00:56,  1.63it/s]Extractor Predicting: 95it [00:56,  1.73it/s]Extractor Predicting: 96it [00:57,  1.73it/s]Extractor Predicting: 97it [00:57,  1.71it/s]Extractor Predicting: 98it [00:58,  1.69it/s]Extractor Predicting: 99it [00:59,  1.70it/s]Extractor Predicting: 100it [00:59,  1.71it/s]Extractor Predicting: 101it [01:00,  1.75it/s]Extractor Predicting: 102it [01:00,  1.79it/s]Extractor Predicting: 103it [01:01,  1.76it/s]Extractor Predicting: 104it [01:01,  1.78it/s]Extractor Predicting: 105it [01:02,  1.76it/s]Extractor Predicting: 106it [01:02,  1.82it/s]Extractor Predicting: 107it [01:03,  1.82it/s]Extractor Predicting: 108it [01:03,  1.88it/s]Extractor Predicting: 109it [01:04,  1.88it/s]Extractor Predicting: 110it [01:05,  1.83it/s]Extractor Predicting: 111it [01:05,  1.86it/s]Extractor Predicting: 112it [01:06,  1.84it/s]Extractor Predicting: 113it [01:06,  1.82it/s]Extractor Predicting: 114it [01:07,  1.73it/s]Extractor Predicting: 115it [01:08,  1.67it/s]Extractor Predicting: 116it [01:08,  1.66it/s]Extractor Predicting: 117it [01:09,  1.62it/s]Extractor Predicting: 118it [01:09,  1.62it/s]Extractor Predicting: 119it [01:10,  1.66it/s]Extractor Predicting: 120it [01:11,  1.66it/s]Extractor Predicting: 121it [01:11,  1.67it/s]Extractor Predicting: 122it [01:12,  1.68it/s]Extractor Predicting: 123it [01:12,  1.64it/s]Extractor Predicting: 124it [01:13,  1.67it/s]Extractor Predicting: 125it [01:14,  1.69it/s]Extractor Predicting: 126it [01:14,  1.64it/s]Extractor Predicting: 127it [01:15,  1.59it/s]Extractor Predicting: 128it [01:15,  1.61it/s]Extractor Predicting: 129it [01:16,  1.66it/s]Extractor Predicting: 130it [01:17,  1.69it/s]Extractor Predicting: 131it [01:17,  1.66it/s]Extractor Predicting: 132it [01:18,  1.62it/s]Extractor Predicting: 133it [01:19,  1.62it/s]Extractor Predicting: 134it [01:19,  1.64it/s]Extractor Predicting: 135it [01:20,  1.63it/s]Extractor Predicting: 136it [01:20,  1.68it/s]Extractor Predicting: 137it [01:21,  1.65it/s]Extractor Predicting: 138it [01:21,  1.71it/s]Extractor Predicting: 139it [01:22,  1.73it/s]Extractor Predicting: 140it [01:23,  1.71it/s]Extractor Predicting: 141it [01:23,  1.69it/s]Extractor Predicting: 142it [01:24,  1.71it/s]Extractor Predicting: 143it [01:24,  2.05it/s]Extractor Predicting: 143it [01:24,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:53,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:53,562 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:53,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:53,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:53,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:49:54,463 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:49:54,464 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:49:55,109 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:49:56,218 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:49:56,218 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:59,259 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:59,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:59,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:59,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:49:59,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:50:00,154 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:50:00,155 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:50:00,760 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:50:00,938 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:50:00,938 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.65it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.65it/s]Extractor Predicting: 29it [00:18,  1.65it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.73it/s]Extractor Predicting: 39it [00:24,  1.80it/s]Extractor Predicting: 40it [00:24,  1.90it/s]Extractor Predicting: 41it [00:24,  2.01it/s]Extractor Predicting: 42it [00:25,  2.04it/s]Extractor Predicting: 43it [00:25,  2.06it/s]Extractor Predicting: 44it [00:26,  2.05it/s]Extractor Predicting: 45it [00:26,  2.10it/s]Extractor Predicting: 46it [00:27,  2.06it/s]Extractor Predicting: 47it [00:27,  2.04it/s]Extractor Predicting: 48it [00:28,  2.05it/s]Extractor Predicting: 49it [00:28,  2.05it/s]Extractor Predicting: 50it [00:29,  2.10it/s]Extractor Predicting: 51it [00:29,  2.00it/s]Extractor Predicting: 52it [00:30,  2.03it/s]Extractor Predicting: 53it [00:30,  2.06it/s]Extractor Predicting: 54it [00:31,  2.11it/s]Extractor Predicting: 55it [00:31,  2.12it/s]Extractor Predicting: 56it [00:32,  2.12it/s]Extractor Predicting: 57it [00:32,  2.12it/s]Extractor Predicting: 58it [00:33,  2.12it/s]Extractor Predicting: 59it [00:33,  2.12it/s]Extractor Predicting: 60it [00:34,  2.05it/s]Extractor Predicting: 61it [00:34,  2.01it/s]Extractor Predicting: 62it [00:35,  2.07it/s]Extractor Predicting: 63it [00:35,  2.07it/s]Extractor Predicting: 64it [00:35,  2.07it/s]Extractor Predicting: 65it [00:36,  2.07it/s]Extractor Predicting: 66it [00:36,  2.11it/s]Extractor Predicting: 67it [00:37,  1.99it/s]Extractor Predicting: 68it [00:38,  1.86it/s]Extractor Predicting: 69it [00:38,  1.77it/s]Extractor Predicting: 70it [00:39,  1.71it/s]Extractor Predicting: 71it [00:39,  1.68it/s]Extractor Predicting: 72it [00:40,  1.68it/s]Extractor Predicting: 72it [00:40,  1.77it/s]
[INFO|configuration_utils.py:515] 2023-08-28 04:50:43,680 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:50:43,698 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:50:43,743 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:50:43,745 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 04:50:43,763 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:50:51,242 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 04:50:51,269 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 04:50:51,630 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:50:51,631 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:50:51,744 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:50:51,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:50:51,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:50:51,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:50:51,797 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:50:51,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:50:51,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 04:50:52,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:52,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:53,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:53,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:54,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:55,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:55,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:56,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:57,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:57,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:58,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:59,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:50:59,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:00,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:00,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:01,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:02,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:02,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:03,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:04,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:04,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:05,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:06,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:11, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-28 04:51:06,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:07,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:08,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:08,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:09,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:09,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:10,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:11,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:11,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:12,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:12,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:13,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:14,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:14,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:15,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:16,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:16,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:17,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:18,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:18,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:19,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:20,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:20,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:21,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:29<01:59, 14.97s/it][WARNING|generation_utils.py:914] 2023-08-28 04:51:21,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:22,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:23,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:23,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:24,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:25,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:26,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:27,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:27,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:28,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:28,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:29,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:30,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:30,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:31,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:32,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:32,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:33,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:33,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:34,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:35,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:35,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:43, 14.81s/it][WARNING|generation_utils.py:914] 2023-08-28 04:51:36,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:37,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:37,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:38,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:39,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:40,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:40,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:41,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:41,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:42,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:43,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:43,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:44,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:45,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:45,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:46,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:47,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:47,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:48,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:49,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:49,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:50,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:50,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:51,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:52,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:52,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:53,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:54,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:54,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:55,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:55,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:56,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:57,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:57,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:58,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:59,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:51:59,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:00,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:00,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:09<01:52, 18.73s/it][WARNING|generation_utils.py:914] 2023-08-28 04:52:01,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:01,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:02,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:03,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:03,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:04,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:04,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:05,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:06,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:06,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:07,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:08,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:08,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:09,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:09,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:10,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:11,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:11,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:12,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:13,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:13,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:14,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:14,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:15,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:16,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:27, 17.54s/it][WARNING|generation_utils.py:914] 2023-08-28 04:52:16,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:17,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:17,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:18,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:19,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:19,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:20,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:20,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:21,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:22,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:22,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:23,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:24,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:24,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:25,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:25,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:26,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:27,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:27,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:28,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:29,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:37<01:03, 15.96s/it][WARNING|generation_utils.py:914] 2023-08-28 04:52:29,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:30,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:30,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:31,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:31,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:32,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:33,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:33,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:34,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:34,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:35,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:35,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:36,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:37,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:37,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:38,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:38,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:39,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:40,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:40,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:41,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:41,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:42,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:42,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:43,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:43,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:44,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:52<00:47, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-28 04:52:45,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:45,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:46,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:46,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:47,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:47,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:48,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:49,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:49,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:50,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:50,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:51,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:52,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:52,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:53,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:53,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:54,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:55,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:55,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:56,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:56,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:57,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:58,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:06<00:30, 15.11s/it][WARNING|generation_utils.py:914] 2023-08-28 04:52:58,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:59,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:00,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:00,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:01,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:01,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:02,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:03,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:03,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:04,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:05,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:05,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:06,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:07,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:07,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:08,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:09,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:09,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:10,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:11,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:11,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:12,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:12,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:21<00:15, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-28 04:53:13,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:14,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:15,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:15,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:16,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:17,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:17,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:18,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:19,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:19,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:20,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:21,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:21,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:22,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:23,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:23,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:24,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:25,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:25,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:26,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:27,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:28,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:28,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:37<00:00, 15.29s/it]Generating: 100%|██████████| 10/10 [02:37<00:00, 15.74s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:36,705 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:36,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:36,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:36,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:36,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:53:37,530 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:53:37,531 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:53:38,152 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:53:39,287 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:53:39,287 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:42,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:42,555 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:42,555 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:42,555 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:53:42,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:53:43,319 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:53:43,320 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:53:43,924 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:53:44,149 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:53:44,149 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')", "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.36it/s]Extractor Estimating: 3it [00:02,  1.44it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.62it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.55it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.51it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:13,  1.52it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:14,  1.50it/s]Extractor Estimating: 24it [00:15,  1.51it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:16,  1.51it/s]Extractor Estimating: 27it [00:17,  1.48it/s]Extractor Estimating: 28it [00:18,  1.51it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.59it/s]Extractor Estimating: 31it [00:20,  1.59it/s]Extractor Estimating: 32it [00:20,  1.59it/s]Extractor Estimating: 33it [00:21,  1.57it/s]Extractor Estimating: 34it [00:22,  1.55it/s]Extractor Estimating: 35it [00:22,  1.57it/s]Extractor Estimating: 36it [00:23,  1.60it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:24,  1.69it/s]Extractor Estimating: 39it [00:25,  1.65it/s]Extractor Estimating: 40it [00:25,  1.60it/s]Extractor Estimating: 41it [00:26,  1.56it/s]Extractor Estimating: 42it [00:27,  1.52it/s]Extractor Estimating: 43it [00:27,  1.52it/s]Extractor Estimating: 44it [00:28,  1.51it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:29,  1.56it/s]Extractor Estimating: 47it [00:30,  1.57it/s]Extractor Estimating: 48it [00:30,  1.56it/s]Extractor Estimating: 49it [00:31,  1.55it/s]Extractor Estimating: 50it [00:32,  1.57it/s]Extractor Estimating: 51it [00:32,  1.58it/s]Extractor Estimating: 52it [00:33,  1.65it/s]Extractor Estimating: 53it [00:33,  1.68it/s]Extractor Estimating: 54it [00:34,  1.66it/s]Extractor Estimating: 55it [00:35,  1.68it/s]Extractor Estimating: 56it [00:35,  1.70it/s]Extractor Estimating: 57it [00:36,  1.72it/s]Extractor Estimating: 58it [00:36,  1.73it/s]Extractor Estimating: 59it [00:37,  1.77it/s]Extractor Estimating: 60it [00:37,  1.73it/s]Extractor Estimating: 61it [00:38,  1.75it/s]Extractor Estimating: 62it [00:39,  1.77it/s]Extractor Estimating: 63it [00:39,  1.70it/s]Extractor Estimating: 64it [00:40,  1.70it/s]Extractor Estimating: 65it [00:41,  1.57it/s]Extractor Estimating: 66it [00:41,  1.60it/s]Extractor Estimating: 67it [00:42,  1.64it/s]Extractor Estimating: 68it [00:42,  1.66it/s]Extractor Estimating: 69it [00:43,  1.64it/s]Extractor Estimating: 70it [00:44,  1.66it/s]Extractor Estimating: 71it [00:44,  1.67it/s]Extractor Estimating: 72it [00:45,  1.68it/s]Extractor Estimating: 73it [00:45,  1.70it/s]Extractor Estimating: 74it [00:46,  1.61it/s]Extractor Estimating: 75it [00:47,  1.69it/s]Extractor Estimating: 76it [00:47,  1.45it/s]Extractor Estimating: 77it [00:48,  1.47it/s]Extractor Estimating: 78it [00:49,  1.53it/s]Extractor Estimating: 79it [00:49,  1.54it/s]Extractor Estimating: 80it [00:50,  1.57it/s]Extractor Estimating: 81it [00:51,  1.56it/s]Extractor Estimating: 82it [00:51,  1.48it/s]Extractor Estimating: 83it [00:52,  1.47it/s]Extractor Estimating: 84it [00:53,  1.46it/s]Extractor Estimating: 85it [00:53,  1.51it/s]Extractor Estimating: 86it [00:54,  1.51it/s]Extractor Estimating: 87it [00:55,  1.55it/s]Extractor Estimating: 88it [00:55,  1.56it/s]Extractor Estimating: 89it [00:56,  1.59it/s]Extractor Estimating: 90it [00:57,  1.53it/s]Extractor Estimating: 91it [00:57,  1.57it/s]Extractor Estimating: 92it [00:58,  1.56it/s]Extractor Estimating: 93it [00:58,  1.57it/s]Extractor Estimating: 94it [00:59,  1.57it/s]Extractor Estimating: 95it [01:00,  1.53it/s]Extractor Estimating: 96it [01:00,  1.52it/s]Extractor Estimating: 97it [01:01,  1.50it/s]Extractor Estimating: 98it [01:02,  1.50it/s]Extractor Estimating: 99it [01:02,  1.53it/s]Extractor Estimating: 100it [01:03,  1.53it/s]Extractor Estimating: 101it [01:04,  1.58it/s]Extractor Estimating: 102it [01:04,  1.59it/s]Extractor Estimating: 103it [01:05,  1.65it/s]Extractor Estimating: 104it [01:05,  1.66it/s]Extractor Estimating: 105it [01:06,  1.72it/s]Extractor Estimating: 106it [01:07,  1.70it/s]Extractor Estimating: 107it [01:07,  1.67it/s]Extractor Estimating: 108it [01:08,  1.68it/s]Extractor Estimating: 109it [01:08,  1.64it/s]Extractor Estimating: 110it [01:09,  1.63it/s]Extractor Estimating: 111it [01:10,  1.63it/s]Extractor Estimating: 112it [01:10,  1.63it/s]Extractor Estimating: 113it [01:11,  1.67it/s]Extractor Estimating: 114it [01:11,  1.72it/s]Extractor Estimating: 115it [01:12,  1.67it/s]Extractor Estimating: 116it [01:13,  1.66it/s]Extractor Estimating: 117it [01:13,  1.63it/s]Extractor Estimating: 118it [01:14,  1.65it/s]Extractor Estimating: 119it [01:14,  1.67it/s]Extractor Estimating: 120it [01:15,  1.67it/s]Extractor Estimating: 121it [01:16,  1.65it/s]Extractor Estimating: 122it [01:16,  1.69it/s]Extractor Estimating: 123it [01:17,  1.69it/s]Extractor Estimating: 124it [01:17,  1.66it/s]Extractor Estimating: 125it [01:18,  1.61it/s]Extractor Estimating: 126it [01:19,  1.62it/s]Extractor Estimating: 127it [01:19,  1.62it/s]Extractor Estimating: 128it [01:20,  1.59it/s]Extractor Estimating: 129it [01:21,  1.60it/s]Extractor Estimating: 130it [01:21,  1.64it/s]Extractor Estimating: 131it [01:22,  1.59it/s]Extractor Estimating: 132it [01:22,  1.59it/s]Extractor Estimating: 133it [01:23,  1.58it/s]Extractor Estimating: 134it [01:24,  1.58it/s]Extractor Estimating: 135it [01:24,  1.59it/s]Extractor Estimating: 136it [01:25,  1.56it/s]Extractor Estimating: 137it [01:26,  1.54it/s]Extractor Estimating: 138it [01:26,  1.50it/s]Extractor Estimating: 139it [01:27,  1.51it/s]Extractor Estimating: 140it [01:28,  1.53it/s]Extractor Estimating: 141it [01:28,  1.56it/s]Extractor Estimating: 142it [01:29,  1.57it/s]Extractor Estimating: 143it [01:30,  1.57it/s]Extractor Estimating: 144it [01:30,  1.57it/s]Extractor Estimating: 145it [01:31,  1.61it/s]Extractor Estimating: 146it [01:31,  1.60it/s]Extractor Estimating: 147it [01:32,  1.61it/s]Extractor Estimating: 148it [01:33,  1.54it/s]Extractor Estimating: 149it [01:33,  1.51it/s]Extractor Estimating: 150it [01:34,  1.42it/s]Extractor Estimating: 151it [01:35,  1.44it/s]Extractor Estimating: 152it [01:36,  1.48it/s]Extractor Estimating: 153it [01:36,  1.50it/s]Extractor Estimating: 154it [01:37,  1.57it/s]Extractor Estimating: 155it [01:37,  1.55it/s]Extractor Estimating: 156it [01:38,  1.51it/s]Extractor Estimating: 157it [01:39,  1.54it/s]Extractor Estimating: 158it [01:39,  1.57it/s]Extractor Estimating: 159it [01:40,  1.61it/s]Extractor Estimating: 160it [01:40,  1.64it/s]Extractor Estimating: 161it [01:41,  1.66it/s]Extractor Estimating: 162it [01:42,  1.58it/s]Extractor Estimating: 163it [01:42,  1.55it/s]Extractor Estimating: 164it [01:43,  1.51it/s]Extractor Estimating: 165it [01:44,  1.55it/s]Extractor Estimating: 166it [01:44,  1.55it/s]Extractor Estimating: 167it [01:45,  1.53it/s]Extractor Estimating: 168it [01:46,  1.60it/s]Extractor Estimating: 169it [01:46,  1.64it/s]Extractor Estimating: 170it [01:47,  1.67it/s]Extractor Estimating: 171it [01:47,  1.66it/s]Extractor Estimating: 172it [01:48,  1.63it/s]Extractor Estimating: 173it [01:49,  1.59it/s]Extractor Estimating: 174it [01:49,  1.59it/s]Extractor Estimating: 175it [01:50,  1.57it/s]Extractor Estimating: 176it [01:51,  1.59it/s]Extractor Estimating: 177it [01:51,  1.61it/s]Extractor Estimating: 178it [01:52,  1.63it/s]Extractor Estimating: 179it [01:52,  1.68it/s]Extractor Estimating: 180it [01:53,  1.68it/s]Extractor Estimating: 181it [01:54,  1.68it/s]Extractor Estimating: 182it [01:54,  1.68it/s]Extractor Estimating: 183it [01:55,  1.61it/s]Extractor Estimating: 184it [01:55,  1.64it/s]Extractor Estimating: 185it [01:56,  1.59it/s]Extractor Estimating: 186it [01:57,  1.63it/s]Extractor Estimating: 187it [01:57,  1.58it/s]Extractor Estimating: 188it [01:58,  1.60it/s]Extractor Estimating: 189it [01:59,  1.62it/s]Extractor Estimating: 190it [01:59,  1.63it/s]Extractor Estimating: 191it [02:00,  1.67it/s]Extractor Estimating: 192it [02:00,  1.66it/s]Extractor Estimating: 193it [02:01,  1.64it/s]Extractor Estimating: 194it [02:02,  1.58it/s]Extractor Estimating: 195it [02:02,  1.64it/s]Extractor Estimating: 196it [02:03,  1.64it/s]Extractor Estimating: 197it [02:03,  1.66it/s]Extractor Estimating: 198it [02:04,  1.65it/s]Extractor Estimating: 199it [02:05,  1.68it/s]Extractor Estimating: 200it [02:05,  1.65it/s]Extractor Estimating: 201it [02:06,  1.58it/s]Extractor Estimating: 202it [02:07,  1.59it/s]Extractor Estimating: 203it [02:07,  1.58it/s]Extractor Estimating: 204it [02:08,  1.64it/s]Extractor Estimating: 205it [02:08,  1.64it/s]Extractor Estimating: 206it [02:09,  1.64it/s]Extractor Estimating: 207it [02:10,  1.61it/s]Extractor Estimating: 208it [02:10,  1.61it/s]Extractor Estimating: 209it [02:11,  1.62it/s]Extractor Estimating: 210it [02:11,  1.58it/s]Extractor Estimating: 211it [02:12,  1.60it/s]Extractor Estimating: 212it [02:13,  1.52it/s]Extractor Estimating: 213it [02:13,  1.51it/s]Extractor Estimating: 214it [02:14,  1.55it/s]Extractor Estimating: 215it [02:15,  1.57it/s]Extractor Estimating: 216it [02:15,  1.60it/s]Extractor Estimating: 217it [02:16,  1.61it/s]Extractor Estimating: 218it [02:16,  1.65it/s]Extractor Estimating: 219it [02:17,  1.64it/s]Extractor Estimating: 220it [02:18,  1.60it/s]Extractor Estimating: 221it [02:18,  1.56it/s]Extractor Estimating: 222it [02:19,  1.59it/s]Extractor Estimating: 223it [02:20,  1.55it/s]Extractor Estimating: 224it [02:20,  1.53it/s]Extractor Estimating: 225it [02:21,  1.56it/s]Extractor Estimating: 226it [02:22,  1.52it/s]Extractor Estimating: 227it [02:22,  1.56it/s]Extractor Estimating: 228it [02:23,  1.59it/s]Extractor Estimating: 229it [02:24,  1.60it/s]Extractor Estimating: 230it [02:24,  1.62it/s]Extractor Estimating: 231it [02:25,  1.59it/s]Extractor Estimating: 232it [02:25,  1.59it/s]Extractor Estimating: 233it [02:26,  1.61it/s]Extractor Estimating: 234it [02:27,  1.61it/s]Extractor Estimating: 235it [02:28,  1.44it/s]Extractor Estimating: 236it [02:28,  1.51it/s]Extractor Estimating: 237it [02:29,  1.51it/s]Extractor Estimating: 238it [02:29,  1.52it/s]Extractor Estimating: 239it [02:30,  1.56it/s]Extractor Estimating: 240it [02:31,  1.56it/s]Extractor Estimating: 241it [02:31,  1.54it/s]Extractor Estimating: 242it [02:32,  1.58it/s]Extractor Estimating: 243it [02:33,  1.59it/s]Extractor Estimating: 244it [02:33,  1.61it/s]Extractor Estimating: 245it [02:34,  1.59it/s]Extractor Estimating: 246it [02:34,  1.57it/s]Extractor Estimating: 247it [02:35,  1.55it/s]Extractor Estimating: 248it [02:36,  1.60it/s]Extractor Estimating: 249it [02:36,  1.60it/s]Extractor Estimating: 250it [02:37,  1.49it/s]Extractor Estimating: 250it [02:37,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:41,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:41,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:41,248 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:41,248 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:41,248 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:56:42,054 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:56:42,055 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:56:42,669 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:56:43,806 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:56:43,807 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:46,742 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:46,767 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:46,767 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:46,767 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:46,767 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:56:47,542 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:56:47,543 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:56:48,174 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:56:48,402 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:56:48,402 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:39:45,449 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:39:45,474 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5237 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 23532
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23632, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23632, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.042, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.057, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.034, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.060, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.034, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 3.061, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.042, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.046, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.039, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.061, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 3.028, loss:nan
g_step 1200, step 105, avg_time 1.052, loss:nan
g_step 1300, step 205, avg_time 1.039, loss:nan
g_step 1400, step 86, avg_time 1.063, loss:nan
g_step 1500, step 186, avg_time 1.045, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 3.032, loss:nan
g_step 1700, step 167, avg_time 1.046, loss:nan
g_step 1800, step 48, avg_time 1.049, loss:nan
g_step 1900, step 148, avg_time 1.040, loss:nan
g_step 2000, step 29, avg_time 1.031, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 3.036, loss:nan
g_step 2200, step 10, avg_time 1.041, loss:nan
g_step 2300, step 110, avg_time 1.040, loss:nan
g_step 2400, step 210, avg_time 1.049, loss:nan
g_step 2500, step 91, avg_time 1.044, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 3.026, loss:nan
g_step 2700, step 72, avg_time 1.040, loss:nan
g_step 2800, step 172, avg_time 1.052, loss:nan
g_step 2900, step 53, avg_time 1.039, loss:nan
g_step 3000, step 153, avg_time 1.036, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 3.047, loss:nan
g_step 3200, step 134, avg_time 1.041, loss:nan
g_step 3300, step 15, avg_time 1.041, loss:nan
g_step 3400, step 115, avg_time 1.046, loss:nan
g_step 3500, step 215, avg_time 1.055, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 3.021, loss:nan
g_step 3700, step 196, avg_time 1.060, loss:nan
g_step 3800, step 77, avg_time 1.039, loss:nan
g_step 3900, step 177, avg_time 1.054, loss:nan
g_step 4000, step 58, avg_time 1.027, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 3.041, loss:nan
g_step 4200, step 39, avg_time 1.044, loss:nan
g_step 4300, step 139, avg_time 1.044, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:39:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:39:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-39-45_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:39:46 - WARNING - datasets.builder -   Using custom data configuration default-29dc5a9d9249c59b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-29dc5a9d9249c59b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:39:47,929 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:39:47,930 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:39:47,931 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:39:47,932 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:39:47,985 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:48,012 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:48,012 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:48,012 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:48,012 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:48,012 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:39:48,012 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:39:48,325 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:39:51,445 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:39:51,445 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-29dc5a9d9249c59b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.62ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.74ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.38ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.78ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.02ba/s]100%|██████████| 6/6 [00:01<00:00,  4.16ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.34ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.96ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.06ba/s] 44%|████▍     | 4/9 [00:00<00:01,  4.24ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.35ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.41ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.44ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.47ba/s]100%|██████████| 9/9 [00:02<00:00,  5.01ba/s]100%|██████████| 9/9 [00:02<00:00,  4.48ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.97ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.05ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.04ba/s]100%|██████████| 6/6 [00:00<00:00, 10.91ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  6.40ba/s] 33%|███▎      | 3/9 [00:00<00:00,  9.32ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.18ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.58ba/s]100%|██████████| 9/9 [00:00<00:00, 11.41ba/s]100%|██████████| 9/9 [00:00<00:00, 10.62ba/s]
[INFO|trainer.py:414] 2023-08-28 06:39:57,389 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:39:57,449 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:39:57,449 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 06:39:57,449 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:39:57,449 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:39:57,449 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:39:57,449 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:39:57,449 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.47it/s]  0%|          | 2/410 [00:00<01:54,  3.56it/s]  1%|          | 3/410 [00:00<01:53,  3.59it/s]  1%|          | 4/410 [00:01<01:52,  3.61it/s]  1%|          | 5/410 [00:01<01:52,  3.60it/s]  1%|▏         | 6/410 [00:01<01:54,  3.53it/s]  2%|▏         | 7/410 [00:01<01:53,  3.55it/s]  2%|▏         | 8/410 [00:02<01:53,  3.56it/s]  2%|▏         | 9/410 [00:02<01:52,  3.56it/s]  2%|▏         | 10/410 [00:02<01:52,  3.57it/s]  3%|▎         | 11/410 [00:03<01:51,  3.57it/s]  3%|▎         | 12/410 [00:03<01:51,  3.57it/s]  3%|▎         | 13/410 [00:03<01:51,  3.57it/s]  3%|▎         | 14/410 [00:03<01:50,  3.57it/s]  4%|▎         | 15/410 [00:04<01:50,  3.57it/s]  4%|▍         | 16/410 [00:04<01:50,  3.57it/s]  4%|▍         | 17/410 [00:04<01:50,  3.57it/s]  4%|▍         | 18/410 [00:05<01:49,  3.57it/s]  5%|▍         | 19/410 [00:05<01:49,  3.57it/s]  5%|▍         | 20/410 [00:05<01:49,  3.57it/s]  5%|▌         | 21/410 [00:05<01:48,  3.57it/s]  5%|▌         | 22/410 [00:06<01:48,  3.57it/s]  6%|▌         | 23/410 [00:06<01:48,  3.58it/s]  6%|▌         | 24/410 [00:06<01:47,  3.60it/s]  6%|▌         | 25/410 [00:07<01:49,  3.52it/s]  6%|▋         | 26/410 [00:07<01:48,  3.55it/s]  7%|▋         | 27/410 [00:07<01:47,  3.57it/s]  7%|▋         | 28/410 [00:07<01:46,  3.59it/s]  7%|▋         | 29/410 [00:08<01:45,  3.60it/s]  7%|▋         | 30/410 [00:08<01:45,  3.61it/s]  8%|▊         | 31/410 [00:08<01:44,  3.62it/s]  8%|▊         | 32/410 [00:08<01:44,  3.62it/s]  8%|▊         | 33/410 [00:09<01:44,  3.62it/s]  8%|▊         | 34/410 [00:09<01:43,  3.62it/s]  9%|▊         | 35/410 [00:09<01:43,  3.62it/s]  9%|▉         | 36/410 [00:10<01:43,  3.63it/s]  9%|▉         | 37/410 [00:10<01:42,  3.63it/s]  9%|▉         | 38/410 [00:10<01:42,  3.63it/s] 10%|▉         | 39/410 [00:10<01:42,  3.62it/s] 10%|▉         | 40/410 [00:11<01:42,  3.63it/s] 10%|█         | 41/410 [00:11<01:41,  3.63it/s] 10%|█         | 42/410 [00:11<01:41,  3.62it/s] 10%|█         | 43/410 [00:12<01:44,  3.52it/s] 11%|█         | 44/410 [00:12<01:44,  3.49it/s] 11%|█         | 45/410 [00:12<01:43,  3.52it/s] 11%|█         | 46/410 [00:12<01:42,  3.55it/s] 11%|█▏        | 47/410 [00:13<01:41,  3.57it/s] 12%|█▏        | 48/410 [00:13<01:40,  3.59it/s] 12%|█▏        | 49/410 [00:13<01:40,  3.60it/s] 12%|█▏        | 50/410 [00:13<01:39,  3.61it/s] 12%|█▏        | 51/410 [00:14<01:39,  3.61it/s] 13%|█▎        | 52/410 [00:14<01:38,  3.62it/s] 13%|█▎        | 53/410 [00:14<01:38,  3.62it/s] 13%|█▎        | 54/410 [00:15<01:38,  3.62it/s] 13%|█▎        | 55/410 [00:15<01:38,  3.61it/s] 14%|█▎        | 56/410 [00:15<01:38,  3.61it/s] 14%|█▍        | 57/410 [00:15<01:37,  3.61it/s] 14%|█▍        | 58/410 [00:16<01:37,  3.62it/s] 14%|█▍        | 59/410 [00:16<01:36,  3.62it/s] 15%|█▍        | 60/410 [00:16<01:45,  3.32it/s] 15%|█▍        | 61/410 [00:17<01:44,  3.34it/s] 15%|█▌        | 62/410 [00:17<01:41,  3.42it/s] 15%|█▌        | 63/410 [00:17<01:39,  3.48it/s] 16%|█▌        | 64/410 [00:17<01:38,  3.52it/s] 16%|█▌        | 65/410 [00:18<01:37,  3.53it/s] 16%|█▌        | 66/410 [00:18<01:37,  3.55it/s] 16%|█▋        | 67/410 [00:18<01:36,  3.56it/s] 17%|█▋        | 68/410 [00:19<01:36,  3.56it/s] 17%|█▋        | 69/410 [00:19<01:35,  3.56it/s] 17%|█▋        | 70/410 [00:19<01:35,  3.57it/s] 17%|█▋        | 71/410 [00:19<01:35,  3.57it/s] 18%|█▊        | 72/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 73/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 74/410 [00:20<01:34,  3.57it/s] 18%|█▊        | 75/410 [00:21<01:33,  3.57it/s] 19%|█▊        | 76/410 [00:21<01:33,  3.57it/s] 19%|█▉        | 77/410 [00:21<01:33,  3.57it/s] 19%|█▉        | 78/410 [00:21<01:32,  3.57it/s] 19%|█▉        | 79/410 [00:22<01:34,  3.51it/s] 20%|█▉        | 80/410 [00:22<01:33,  3.53it/s] 20%|█▉        | 81/410 [00:22<01:33,  3.54it/s] 20%|██        | 82/410 [00:22<01:28,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 06:40:20,398 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:40:20,398 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 06:40:20,398 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.31it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.18it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.70it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.77it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.22it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.62it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.17it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.98it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.92it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.13it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.14it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.35it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.38it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.19it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.03it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.83it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.74it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.68it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.95it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.05it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.23it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.28it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.23it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.00it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.84it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.77it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.64it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.77it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.98it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.19it/s][A
 14%|█▍        | 157/1083 [00:03<00:22, 40.85it/s][A
 15%|█▌        | 163/1083 [00:03<00:21, 43.45it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.04it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.26it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.42it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.53it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 42.75it/s][A
 18%|█▊        | 193/1083 [00:04<00:20, 43.57it/s][A
 18%|█▊        | 198/1083 [00:04<00:20, 43.95it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 44.35it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 44.64it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 44.75it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.92it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.96it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.78it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.71it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.88it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.99it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.16it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.14it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.13it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.10it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 45.01it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.79it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 44.76it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.83it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.97it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.10it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.14it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.23it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.09it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 44.95it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.78it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.80it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.76it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.91it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.02it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.14it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.24it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 45.13it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 45.02it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.87it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 44.83it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.79it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.89it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.01it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.09it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.24it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 45.13it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 45.07it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.71it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 42.81it/s][A
 39%|███▊      | 418/1083 [00:09<00:15, 43.61it/s][A
 39%|███▉      | 423/1083 [00:09<00:15, 43.99it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.45it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.62it/s][A
 40%|████      | 438/1083 [00:09<00:14, 44.91it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.94it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.95it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.68it/s][A
 42%|████▏     | 458/1083 [00:10<00:14, 44.62it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.87it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.92it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.09it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.18it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.15it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 45.09it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 44.93it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.77it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.72it/s][A
 47%|████▋     | 508/1083 [00:11<00:14, 39.57it/s][A
 47%|████▋     | 513/1083 [00:11<00:13, 41.17it/s][A
 48%|████▊     | 518/1083 [00:11<00:13, 42.34it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 43.21it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 43.84it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 44.14it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.50it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.74it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 44.52it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.62it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.73it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.88it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.99it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 45.10it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 45.02it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 45.01it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.97it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.74it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.77it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.80it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.88it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.07it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.12it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.03it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 45.07it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.98it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.75it/s][A
 59%|█████▉    | 643/1083 [00:14<00:10, 42.78it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 43.51it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.05it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 44.41it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 44.67it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 44.80it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 45.00it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.95it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.67it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.72it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.78it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.99it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 45.06it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 45.17it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.14it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 45.09it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.94it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.77it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.73it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.79it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.89it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.02it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.18it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 45.13it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 45.08it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.95it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.81it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.76it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.82it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.01it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.05it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 45.12it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 45.08it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.98it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.92it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.82it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.89it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.83it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.96it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 45.07it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 45.07it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 45.01it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.89it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.91it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.80it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.78it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.13it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.51it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.73it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.97it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 44.98it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.91it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.76it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.73it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.66it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.83it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.93it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.10it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 45.18it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 45.08it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 45.06it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.94it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.84it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.68it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.77it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.85it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 45.03it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 45.18it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 45.14it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 45.05it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.89it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.83it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.80it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.78it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.89it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.98it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 45.13it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 45.07it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 45.03it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.86it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.83it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.78it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.76it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.74it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.90it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 45.15it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 45.19it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 45.09it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.91it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:24<00:00, 44.91it/s][A 20%|██        | 82/410 [00:47<01:28,  3.69it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 06:40:44,811 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 06:40:44,971 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:40:47,650 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:40:47,758 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:40:47,832 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:51<47:20,  8.69s/it] 20%|██        | 84/410 [00:51<33:30,  6.17s/it] 21%|██        | 85/410 [00:51<23:50,  4.40s/it] 21%|██        | 86/410 [00:52<17:05,  3.16s/it] 21%|██        | 87/410 [00:52<12:22,  2.30s/it] 21%|██▏       | 88/410 [00:52<09:05,  1.69s/it] 22%|██▏       | 89/410 [00:52<06:47,  1.27s/it] 22%|██▏       | 90/410 [00:53<05:11,  1.03it/s] 22%|██▏       | 91/410 [00:53<04:03,  1.31it/s] 22%|██▏       | 92/410 [00:53<03:16,  1.62it/s] 23%|██▎       | 93/410 [00:54<02:44,  1.93it/s] 23%|██▎       | 94/410 [00:54<02:20,  2.24it/s] 23%|██▎       | 95/410 [00:54<02:06,  2.49it/s] 23%|██▎       | 96/410 [00:54<01:54,  2.74it/s] 24%|██▎       | 97/410 [00:55<01:46,  2.94it/s] 24%|██▍       | 98/410 [00:55<01:40,  3.11it/s] 24%|██▍       | 99/410 [00:55<01:36,  3.23it/s] 24%|██▍       | 100/410 [00:56<01:33,  3.33it/s] 25%|██▍       | 101/410 [00:56<01:30,  3.40it/s] 25%|██▍       | 102/410 [00:56<01:29,  3.45it/s] 25%|██▌       | 103/410 [00:56<01:28,  3.48it/s] 25%|██▌       | 104/410 [00:57<01:27,  3.51it/s] 26%|██▌       | 105/410 [00:57<01:26,  3.53it/s] 26%|██▌       | 106/410 [00:57<01:27,  3.46it/s] 26%|██▌       | 107/410 [00:58<01:26,  3.49it/s] 26%|██▋       | 108/410 [00:58<01:25,  3.52it/s] 27%|██▋       | 109/410 [00:58<01:25,  3.54it/s] 27%|██▋       | 110/410 [00:58<01:24,  3.55it/s] 27%|██▋       | 111/410 [00:59<01:24,  3.56it/s] 27%|██▋       | 112/410 [00:59<01:23,  3.56it/s] 28%|██▊       | 113/410 [00:59<01:23,  3.57it/s] 28%|██▊       | 114/410 [00:59<01:22,  3.57it/s] 28%|██▊       | 115/410 [01:00<01:22,  3.57it/s] 28%|██▊       | 116/410 [01:00<01:22,  3.57it/s] 29%|██▊       | 117/410 [01:00<01:24,  3.46it/s] 29%|██▉       | 118/410 [01:01<01:23,  3.49it/s] 29%|██▉       | 119/410 [01:01<01:22,  3.51it/s] 29%|██▉       | 120/410 [01:01<01:22,  3.53it/s] 30%|██▉       | 121/410 [01:01<01:21,  3.54it/s] 30%|██▉       | 122/410 [01:02<01:21,  3.55it/s] 30%|███       | 123/410 [01:02<01:20,  3.55it/s] 30%|███       | 124/410 [01:02<01:20,  3.56it/s] 30%|███       | 125/410 [01:03<01:20,  3.56it/s] 31%|███       | 126/410 [01:03<01:19,  3.56it/s] 31%|███       | 127/410 [01:03<01:19,  3.56it/s] 31%|███       | 128/410 [01:04<01:23,  3.36it/s] 31%|███▏      | 129/410 [01:04<01:22,  3.42it/s] 32%|███▏      | 130/410 [01:04<01:20,  3.46it/s] 32%|███▏      | 131/410 [01:04<01:19,  3.49it/s] 32%|███▏      | 132/410 [01:05<01:19,  3.51it/s] 32%|███▏      | 133/410 [01:05<01:18,  3.53it/s] 33%|███▎      | 134/410 [01:05<01:17,  3.54it/s] 33%|███▎      | 135/410 [01:05<01:17,  3.55it/s] 33%|███▎      | 136/410 [01:06<01:17,  3.56it/s] 33%|███▎      | 137/410 [01:06<01:16,  3.56it/s] 34%|███▎      | 138/410 [01:06<01:16,  3.56it/s] 34%|███▍      | 139/410 [01:07<01:18,  3.43it/s] 34%|███▍      | 140/410 [01:07<01:17,  3.47it/s] 34%|███▍      | 141/410 [01:07<01:16,  3.50it/s] 35%|███▍      | 142/410 [01:07<01:16,  3.52it/s] 35%|███▍      | 143/410 [01:08<01:15,  3.53it/s] 35%|███▌      | 144/410 [01:08<01:15,  3.54it/s] 35%|███▌      | 145/410 [01:08<01:14,  3.55it/s] 36%|███▌      | 146/410 [01:09<01:14,  3.56it/s] 36%|███▌      | 147/410 [01:09<01:13,  3.56it/s] 36%|███▌      | 148/410 [01:09<01:13,  3.56it/s] 36%|███▋      | 149/410 [01:09<01:13,  3.56it/s] 37%|███▋      | 150/410 [01:10<01:14,  3.47it/s] 37%|███▋      | 151/410 [01:10<01:13,  3.50it/s] 37%|███▋      | 152/410 [01:10<01:13,  3.52it/s] 37%|███▋      | 153/410 [01:11<01:12,  3.54it/s] 38%|███▊      | 154/410 [01:11<01:12,  3.55it/s] 38%|███▊      | 155/410 [01:11<01:11,  3.55it/s] 38%|███▊      | 156/410 [01:11<01:11,  3.56it/s] 38%|███▊      | 157/410 [01:12<01:11,  3.56it/s] 39%|███▊      | 158/410 [01:12<01:12,  3.48it/s] 39%|███▉      | 159/410 [01:12<01:11,  3.50it/s] 39%|███▉      | 160/410 [01:13<01:11,  3.52it/s] 39%|███▉      | 161/410 [01:13<01:11,  3.47it/s] 40%|███▉      | 162/410 [01:13<01:10,  3.50it/s] 40%|███▉      | 163/410 [01:13<01:10,  3.52it/s] 40%|████      | 164/410 [01:14<01:06,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 06:41:11,614 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:41:11,614 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 06:41:11,614 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.194, 'eval_samples_per_second': 357.857, 'eval_steps_per_second': 44.763, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 55.72it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.25it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.76it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.61it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.84it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.23it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.95it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.82it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.94it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.00it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.13it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.20it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.30it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.12it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.83it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.76it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.68it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.81it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.89it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.07it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.11it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.07it/s][A
 11%|█         | 117/1083 [00:02<00:30, 31.31it/s][A
 11%|█▏        | 122/1083 [00:02<00:27, 34.95it/s][A
 12%|█▏        | 127/1083 [00:02<00:25, 37.64it/s][A
 12%|█▏        | 132/1083 [00:03<00:23, 39.71it/s][A
 13%|█▎        | 137/1083 [00:03<00:22, 41.30it/s][A
 13%|█▎        | 142/1083 [00:03<00:22, 42.46it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.32it/s][A
 14%|█▍        | 152/1083 [00:03<00:21, 43.97it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.21it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.05it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 43.93it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.16it/s][A
 16%|█▋        | 177/1083 [00:04<00:21, 42.99it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 43.65it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.30it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.56it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.93it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.91it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.73it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.53it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.37it/s][A
 20%|██        | 222/1083 [00:05<00:19, 44.53it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.67it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.91it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.03it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.13it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.12it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.90it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.74it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.58it/s][A
 25%|██▍       | 267/1083 [00:06<00:18, 44.58it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.78it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.96it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.05it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.17it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.11it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.93it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.79it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.68it/s][A
 29%|██▉       | 312/1083 [00:07<00:17, 43.47it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.09it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.44it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.72it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.93it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.90it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.90it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.65it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.51it/s][A
 33%|███▎      | 357/1083 [00:08<00:16, 44.60it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.81it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.98it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.05it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.16it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.12it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.07it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.72it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.65it/s][A
 37%|███▋      | 402/1083 [00:09<00:15, 44.59it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.78it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.87it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.00it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.00it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.13it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.09it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.94it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.74it/s][A
 41%|████▏     | 447/1083 [00:10<00:14, 44.68it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.72it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.90it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.05it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.13it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.16it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.02it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.89it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.77it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.67it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.72it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.83it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.99it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.07it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.21it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.13it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.03it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.90it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 44.92it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.83it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 44.20it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.62it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.64it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.98it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.82it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.76it/s][A
 53%|█████▎    | 577/1083 [00:13<00:11, 44.86it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.76it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.75it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.85it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.82it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.01it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.97it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.98it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.95it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.83it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.74it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.78it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.86it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.95it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.08it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.98it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.93it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.93it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.90it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.69it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.69it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.86it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.92it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.00it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.03it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.01it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.94it/s][A
 66%|██████▌   | 712/1083 [00:16<00:08, 44.76it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 41.74it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 42.81it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 43.57it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.12it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.41it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.67it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.65it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.67it/s][A
 70%|██████▉   | 757/1083 [00:17<00:07, 44.40it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.51it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.68it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.93it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 45.04it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.15it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.06it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.97it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.71it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 44.60it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.63it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.78it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.94it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.04it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.12it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.18it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.10it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.92it/s][A
 78%|███████▊  | 847/1083 [00:19<00:05, 44.78it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 39.29it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 40.92it/s][A
 80%|███████▉  | 862/1083 [00:19<00:05, 42.07it/s][A
 80%|████████  | 867/1083 [00:19<00:05, 43.04it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 43.63it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.12it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.49it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.64it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 44.39it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.41it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.62it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.76it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.89it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.96it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.98it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.01it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.97it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 44.75it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.63it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.78it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.78it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.97it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.06it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 45.15it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.10it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 45.06it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 44.93it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.76it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.85it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.76it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.94it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 45.00it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.15it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 45.08it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.97it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 44.91it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.80it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.75it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.72it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.78it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.87it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.96it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.96it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.83it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.77it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.69it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.71it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.71it/s][A 40%|████      | 164/410 [01:38<01:06,  3.68it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 06:41:36,187 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:41:36,934 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:41:39,414 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:41:39,465 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:41:39,513 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:42<35:45,  8.76s/it] 40%|████      | 166/410 [01:43<25:16,  6.22s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:43<17:57,  4.43s/it] 41%|████      | 168/410 [01:43<12:51,  3.19s/it] 41%|████      | 169/410 [01:43<09:18,  2.32s/it] 41%|████▏     | 170/410 [01:44<06:49,  1.70s/it] 42%|████▏     | 171/410 [01:44<05:05,  1.28s/it] 42%|████▏     | 172/410 [01:44<03:52,  1.02it/s] 42%|████▏     | 173/410 [01:44<03:02,  1.30it/s] 42%|████▏     | 174/410 [01:45<02:26,  1.61it/s] 43%|████▎     | 175/410 [01:45<02:02,  1.92it/s] 43%|████▎     | 176/410 [01:45<01:44,  2.23it/s] 43%|████▎     | 177/410 [01:46<01:32,  2.51it/s] 43%|████▎     | 178/410 [01:46<01:24,  2.76it/s] 44%|████▎     | 179/410 [01:46<01:18,  2.96it/s] 44%|████▍     | 180/410 [01:46<01:13,  3.12it/s] 44%|████▍     | 181/410 [01:47<01:10,  3.24it/s] 44%|████▍     | 182/410 [01:47<01:08,  3.34it/s] 45%|████▍     | 183/410 [01:47<01:06,  3.40it/s] 45%|████▍     | 184/410 [01:48<01:05,  3.45it/s] 45%|████▌     | 185/410 [01:48<01:04,  3.49it/s] 45%|████▌     | 186/410 [01:48<01:04,  3.49it/s] 46%|████▌     | 187/410 [01:48<01:03,  3.51it/s] 46%|████▌     | 188/410 [01:49<01:02,  3.52it/s] 46%|████▌     | 189/410 [01:49<01:02,  3.54it/s] 46%|████▋     | 190/410 [01:49<01:02,  3.55it/s] 47%|████▋     | 191/410 [01:50<01:01,  3.55it/s] 47%|████▋     | 192/410 [01:50<01:01,  3.55it/s] 47%|████▋     | 193/410 [01:50<01:01,  3.56it/s] 47%|████▋     | 194/410 [01:50<01:01,  3.52it/s] 48%|████▊     | 195/410 [01:51<01:00,  3.53it/s] 48%|████▊     | 196/410 [01:51<01:00,  3.54it/s] 48%|████▊     | 197/410 [01:51<01:00,  3.52it/s] 48%|████▊     | 198/410 [01:51<00:59,  3.54it/s] 49%|████▊     | 199/410 [01:52<00:59,  3.55it/s] 49%|████▉     | 200/410 [01:52<00:59,  3.55it/s] 49%|████▉     | 201/410 [01:52<00:58,  3.56it/s] 49%|████▉     | 202/410 [01:53<00:58,  3.56it/s] 50%|████▉     | 203/410 [01:53<00:58,  3.56it/s] 50%|████▉     | 204/410 [01:53<00:57,  3.57it/s] 50%|█████     | 205/410 [01:53<00:57,  3.57it/s] 50%|█████     | 206/410 [01:54<00:57,  3.57it/s] 50%|█████     | 207/410 [01:54<00:56,  3.57it/s] 51%|█████     | 208/410 [01:54<00:57,  3.51it/s] 51%|█████     | 209/410 [01:55<00:56,  3.53it/s] 51%|█████     | 210/410 [01:55<00:56,  3.54it/s] 51%|█████▏    | 211/410 [01:55<00:56,  3.55it/s] 52%|█████▏    | 212/410 [01:55<00:55,  3.55it/s] 52%|█████▏    | 213/410 [01:56<00:55,  3.56it/s] 52%|█████▏    | 214/410 [01:56<00:55,  3.56it/s] 52%|█████▏    | 215/410 [01:56<00:54,  3.56it/s] 53%|█████▎    | 216/410 [01:57<00:54,  3.57it/s] 53%|█████▎    | 217/410 [01:57<00:54,  3.57it/s] 53%|█████▎    | 218/410 [01:57<00:53,  3.57it/s] 53%|█████▎    | 219/410 [01:57<00:54,  3.53it/s] 54%|█████▎    | 220/410 [01:58<00:53,  3.54it/s] 54%|█████▍    | 221/410 [01:58<00:53,  3.55it/s] 54%|█████▍    | 222/410 [01:58<00:52,  3.56it/s] 54%|█████▍    | 223/410 [01:59<00:52,  3.56it/s] 55%|█████▍    | 224/410 [01:59<00:52,  3.56it/s] 55%|█████▍    | 225/410 [01:59<00:51,  3.56it/s] 55%|█████▌    | 226/410 [01:59<00:51,  3.57it/s] 55%|█████▌    | 227/410 [02:00<00:51,  3.57it/s] 56%|█████▌    | 228/410 [02:00<00:51,  3.57it/s] 56%|█████▌    | 229/410 [02:00<00:50,  3.57it/s] 56%|█████▌    | 230/410 [02:00<00:50,  3.56it/s] 56%|█████▋    | 231/410 [02:01<00:50,  3.56it/s] 57%|█████▋    | 232/410 [02:01<00:50,  3.56it/s] 57%|█████▋    | 233/410 [02:01<00:49,  3.56it/s] 57%|█████▋    | 234/410 [02:02<00:49,  3.57it/s] 57%|█████▋    | 235/410 [02:02<00:49,  3.57it/s] 58%|█████▊    | 236/410 [02:02<00:48,  3.57it/s] 58%|█████▊    | 237/410 [02:02<00:48,  3.57it/s] 58%|█████▊    | 238/410 [02:03<00:48,  3.57it/s] 58%|█████▊    | 239/410 [02:03<00:47,  3.57it/s] 59%|█████▊    | 240/410 [02:03<00:47,  3.57it/s] 59%|█████▉    | 241/410 [02:04<00:47,  3.56it/s] 59%|█████▉    | 242/410 [02:04<00:47,  3.57it/s] 59%|█████▉    | 243/410 [02:04<00:46,  3.57it/s] 60%|█████▉    | 244/410 [02:04<00:46,  3.57it/s] 60%|█████▉    | 245/410 [02:05<00:46,  3.57it/s] 60%|██████    | 246/410 [02:05<00:44,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 06:42:02,888 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:42:02,888 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 06:42:02,888 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.3553, 'eval_samples_per_second': 355.487, 'eval_steps_per_second': 44.467, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.30it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.21it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.60it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.52it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.84it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.22it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 44.94it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.79it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.93it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.02it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.09it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.13it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.22it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.24it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.96it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.86it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.82it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.85it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.92it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.02it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.08it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.11it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.05it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.94it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.74it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.72it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.86it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.95it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.07it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.09it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.11it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.03it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.97it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.74it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.61it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.79it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.81it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.12it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.16it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.19it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.07it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.03it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.84it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.66it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.70it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.79it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.97it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.09it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.19it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.09it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.98it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.90it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.67it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.79it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.82it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.03it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.03it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.15it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.15it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.95it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.81it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.80it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.61it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.82it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.93it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.99it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.16it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.06it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.03it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.76it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.77it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.82it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.81it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.89it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.04it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.15it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.13it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.97it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.88it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.75it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.84it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.79it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.93it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.10it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.08it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.16it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.99it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.88it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.82it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.83it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.86it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.95it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.09it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.05it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.13it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.04it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.78it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.72it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.73it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.73it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.96it/s][A
 47%|████▋     | 512/1083 [00:11<00:14, 39.95it/s][A
 48%|████▊     | 517/1083 [00:11<00:13, 42.45it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 43.35it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.06it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.33it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.54it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.58it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.68it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.75it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.53it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.51it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.70it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.93it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.11it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 45.11it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 45.02it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.94it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.82it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.72it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.62it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.68it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.88it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 45.05it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 45.21it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 45.06it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 45.01it/s][A
 59%|█████▉    | 642/1083 [00:14<00:10, 43.73it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.08it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.21it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.42it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.64it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.88it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 45.04it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.94it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.72it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.73it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.83it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.80it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.85it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.95it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.99it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 45.02it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.99it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.91it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.83it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.90it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.90it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.97it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.00it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 44.96it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.95it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.91it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.78it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.81it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.84it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.97it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.99it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.98it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 44.99it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.94it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.88it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.77it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.80it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.84it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.90it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.97it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.94it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.91it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.89it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.83it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.83it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.12it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.37it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.54it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.75it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.81it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.88it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.71it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.69it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.71it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.83it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.79it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.95it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.95it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 45.03it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 40.36it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 43.04it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 43.59it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.12it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.45it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.53it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.81it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 45.03it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 44.89it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.60it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.51it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.69it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.82it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.67it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.84it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.96it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 45.03it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 44.87it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.65it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.59it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.50it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.86it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 45.02it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 45.10it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 45.11it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 45.03it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 44.87it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.76it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.66it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.66it/s][A 60%|██████    | 246/410 [02:29<00:44,  3.72it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 06:42:27,157 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 06:42:27,280 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:42:30,922 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:42:31,129 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:42:31,265 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [02:35<25:06,  9.24s/it] 60%|██████    | 248/410 [02:35<17:41,  6.55s/it] 61%|██████    | 249/410 [02:36<12:31,  4.67s/it] 61%|██████    | 250/410 [02:36<08:56,  3.35s/it] 61%|██████    | 251/410 [02:36<06:26,  2.43s/it] 61%|██████▏   | 252/410 [02:37<04:44,  1.80s/it] 62%|██████▏   | 253/410 [02:37<03:30,  1.34s/it] 62%|██████▏   | 254/410 [02:37<02:39,  1.02s/it] 62%|██████▏   | 255/410 [02:37<02:03,  1.25it/s] 62%|██████▏   | 256/410 [02:38<01:38,  1.56it/s] 63%|██████▎   | 257/410 [02:38<01:21,  1.88it/s] 63%|██████▎   | 258/410 [02:38<01:09,  2.20it/s] 63%|██████▎   | 259/410 [02:38<01:00,  2.49it/s] 63%|██████▎   | 260/410 [02:39<00:54,  2.75it/s] 64%|██████▎   | 261/410 [02:39<00:50,  2.97it/s] 64%|██████▍   | 262/410 [02:39<00:47,  3.14it/s] 64%|██████▍   | 263/410 [02:40<00:46,  3.14it/s] 64%|██████▍   | 264/410 [02:40<00:44,  3.27it/s] 65%|██████▍   | 265/410 [02:40<00:43,  3.36it/s] 65%|██████▍   | 266/410 [02:40<00:41,  3.44it/s] 65%|██████▌   | 267/410 [02:41<00:40,  3.49it/s] 65%|██████▌   | 268/410 [02:41<00:40,  3.53it/s] 66%|██████▌   | 269/410 [02:41<00:39,  3.56it/s] 66%|██████▌   | 270/410 [02:42<00:39,  3.58it/s] 66%|██████▌   | 271/410 [02:42<00:38,  3.59it/s] 66%|██████▋   | 272/410 [02:42<00:38,  3.60it/s] 67%|██████▋   | 273/410 [02:42<00:37,  3.61it/s] 67%|██████▋   | 274/410 [02:43<00:42,  3.17it/s] 67%|██████▋   | 275/410 [02:43<00:40,  3.29it/s] 67%|██████▋   | 276/410 [02:43<00:39,  3.38it/s] 68%|██████▊   | 277/410 [02:44<00:38,  3.45it/s] 68%|██████▊   | 278/410 [02:44<00:37,  3.50it/s] 68%|██████▊   | 279/410 [02:44<00:37,  3.54it/s] 68%|██████▊   | 280/410 [02:44<00:36,  3.56it/s] 69%|██████▊   | 281/410 [02:45<00:36,  3.58it/s] 69%|██████▉   | 282/410 [02:45<00:35,  3.59it/s] 69%|██████▉   | 283/410 [02:45<00:35,  3.60it/s] 69%|██████▉   | 284/410 [02:46<00:34,  3.61it/s] 70%|██████▉   | 285/410 [02:46<00:36,  3.46it/s] 70%|██████▉   | 286/410 [02:46<00:35,  3.51it/s] 70%|███████   | 287/410 [02:46<00:34,  3.54it/s] 70%|███████   | 288/410 [02:47<00:34,  3.56it/s] 70%|███████   | 289/410 [02:47<00:33,  3.58it/s] 71%|███████   | 290/410 [02:47<00:33,  3.60it/s] 71%|███████   | 291/410 [02:48<00:33,  3.60it/s] 71%|███████   | 292/410 [02:48<00:32,  3.60it/s] 71%|███████▏  | 293/410 [02:48<00:32,  3.61it/s] 72%|███████▏  | 294/410 [02:48<00:32,  3.61it/s] 72%|███████▏  | 295/410 [02:49<00:31,  3.62it/s] 72%|███████▏  | 296/410 [02:49<00:32,  3.53it/s] 72%|███████▏  | 297/410 [02:49<00:32,  3.44it/s] 73%|███████▎  | 298/410 [02:50<00:32,  3.48it/s] 73%|███████▎  | 299/410 [02:50<00:31,  3.52it/s] 73%|███████▎  | 300/410 [02:50<00:30,  3.55it/s] 73%|███████▎  | 301/410 [02:50<00:30,  3.58it/s] 74%|███████▎  | 302/410 [02:51<00:30,  3.49it/s] 74%|███████▍  | 303/410 [02:51<00:30,  3.52it/s] 74%|███████▍  | 304/410 [02:51<00:29,  3.55it/s] 74%|███████▍  | 305/410 [02:51<00:29,  3.57it/s] 75%|███████▍  | 306/410 [02:52<00:29,  3.58it/s] 75%|███████▍  | 307/410 [02:52<00:29,  3.54it/s] 75%|███████▌  | 308/410 [02:52<00:28,  3.56it/s] 75%|███████▌  | 309/410 [02:53<00:28,  3.58it/s] 76%|███████▌  | 310/410 [02:53<00:27,  3.59it/s] 76%|███████▌  | 311/410 [02:53<00:27,  3.60it/s] 76%|███████▌  | 312/410 [02:53<00:27,  3.61it/s] 76%|███████▋  | 313/410 [02:54<00:26,  3.61it/s] 77%|███████▋  | 314/410 [02:54<00:26,  3.62it/s] 77%|███████▋  | 315/410 [02:54<00:26,  3.62it/s] 77%|███████▋  | 316/410 [02:55<00:25,  3.62it/s] 77%|███████▋  | 317/410 [02:55<00:25,  3.62it/s] 78%|███████▊  | 318/410 [02:55<00:25,  3.62it/s] 78%|███████▊  | 319/410 [02:55<00:25,  3.58it/s] 78%|███████▊  | 320/410 [02:56<00:25,  3.59it/s] 78%|███████▊  | 321/410 [02:56<00:24,  3.60it/s] 79%|███████▊  | 322/410 [02:56<00:24,  3.61it/s] 79%|███████▉  | 323/410 [02:56<00:24,  3.61it/s] 79%|███████▉  | 324/410 [02:57<00:23,  3.61it/s] 79%|███████▉  | 325/410 [02:57<00:23,  3.62it/s] 80%|███████▉  | 326/410 [02:57<00:23,  3.62it/s] 80%|███████▉  | 327/410 [02:58<00:22,  3.62it/s] 80%|████████  | 328/410 [02:58<00:21,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 06:42:55,763 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:42:55,763 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 06:42:55,763 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.1791, 'eval_samples_per_second': 358.077, 'eval_steps_per_second': 44.791, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.66it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.53it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.35it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.49it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.83it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.35it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 44.99it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.82it/s][A
  4%|▍         | 48/1083 [00:01<00:23, 44.76it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 44.88it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.01it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.22it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.24it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.12it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 44.92it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.59it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.30it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.72it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 44.92it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.11it/s][A
 10%|▉         | 108/1083 [00:02<00:22, 43.82it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.29it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.50it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.60it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.54it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.41it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.51it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 44.60it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.77it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.93it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.11it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.16it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.04it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.87it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.66it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.68it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.72it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.76it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 44.86it/s][A
 19%|█▊        | 203/1083 [00:04<00:34, 25.33it/s][A
 19%|█▉        | 208/1083 [00:04<00:29, 29.21it/s][A
 20%|█▉        | 213/1083 [00:05<00:26, 32.71it/s][A
 20%|██        | 218/1083 [00:05<00:24, 35.80it/s][A
 21%|██        | 223/1083 [00:05<00:22, 38.20it/s][A
 21%|██        | 228/1083 [00:05<00:21, 40.16it/s][A
 22%|██▏       | 233/1083 [00:05<00:20, 41.64it/s][A
 22%|██▏       | 238/1083 [00:05<00:19, 42.62it/s][A
 22%|██▏       | 243/1083 [00:05<00:19, 43.01it/s][A
 23%|██▎       | 248/1083 [00:05<00:19, 43.24it/s][A
 23%|██▎       | 253/1083 [00:05<00:19, 43.39it/s][A
 24%|██▍       | 258/1083 [00:06<00:18, 43.91it/s][A
 24%|██▍       | 263/1083 [00:06<00:18, 44.36it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 44.76it/s][A
 25%|██▌       | 273/1083 [00:06<00:17, 45.05it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 45.08it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 45.06it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.90it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.60it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.40it/s][A
 28%|██▊       | 303/1083 [00:07<00:17, 44.60it/s][A
 28%|██▊       | 308/1083 [00:07<00:17, 44.75it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 44.99it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.16it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.01it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.98it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.85it/s][A
 31%|███       | 338/1083 [00:07<00:16, 44.70it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 44.50it/s][A
 32%|███▏      | 348/1083 [00:08<00:16, 44.61it/s][A
 33%|███▎      | 353/1083 [00:08<00:16, 44.70it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 44.80it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 45.00it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 45.03it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 45.12it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 45.04it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.90it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.76it/s][A
 36%|███▋      | 393/1083 [00:09<00:15, 44.76it/s][A
 37%|███▋      | 398/1083 [00:09<00:15, 44.77it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.76it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.88it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 45.03it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 45.13it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 45.12it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.99it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.79it/s][A
 40%|████      | 438/1083 [00:10<00:14, 44.70it/s][A
 41%|████      | 443/1083 [00:10<00:14, 44.72it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 44.83it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.85it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.04it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.05it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.06it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.00it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 44.78it/s][A
 45%|████▍     | 483/1083 [00:11<00:13, 44.76it/s][A
 45%|████▌     | 488/1083 [00:11<00:13, 44.79it/s][A
 46%|████▌     | 493/1083 [00:11<00:13, 44.80it/s][A
 46%|████▌     | 498/1083 [00:11<00:12, 45.02it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 45.08it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 45.18it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 45.07it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 44.95it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 44.73it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 44.67it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 44.73it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.81it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.95it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 45.02it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 45.06it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 45.08it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.77it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.65it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 44.68it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 44.67it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.75it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.87it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.96it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.01it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.13it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.12it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 44.90it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 44.81it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 44.85it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 43.25it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 43.93it/s][A
 59%|█████▉    | 638/1083 [00:14<00:10, 44.21it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.56it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.78it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.87it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 43.30it/s][A
 61%|██████    | 663/1083 [00:15<00:09, 43.85it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 43.95it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.17it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.45it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.63it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.83it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.93it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.80it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.87it/s][A
 65%|██████▌   | 708/1083 [00:16<00:08, 44.84it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.90it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.85it/s][A
 67%|██████▋   | 723/1083 [00:16<00:07, 45.02it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.86it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.05it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.03it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.88it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.87it/s][A
 70%|██████▉   | 753/1083 [00:17<00:07, 44.84it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.78it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.77it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.99it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.91it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.00it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.98it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.88it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 44.17it/s][A
 74%|███████▎  | 798/1083 [00:18<00:06, 44.42it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.40it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.65it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.82it/s][A
 76%|███████▌  | 818/1083 [00:18<00:07, 37.06it/s][A
 76%|███████▌  | 823/1083 [00:18<00:06, 39.69it/s][A
 76%|███████▋  | 828/1083 [00:18<00:06, 41.25it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 42.46it/s][A
 77%|███████▋  | 838/1083 [00:19<00:05, 43.36it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 43.94it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.39it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.65it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.39it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.15it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.18it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.36it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.67it/s][A
 82%|████████▏ | 883/1083 [00:20<00:04, 44.87it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 45.10it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 45.22it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 45.27it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.86it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.56it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.42it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.56it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.72it/s][A
 86%|████████▌ | 928/1083 [00:21<00:03, 44.93it/s][A
 86%|████████▌ | 933/1083 [00:21<00:03, 45.01it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 45.22it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 45.26it/s][A
 88%|████████▊ | 948/1083 [00:21<00:02, 45.05it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.52it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.59it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.74it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.73it/s][A
 90%|████████▉ | 973/1083 [00:22<00:02, 44.92it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 44.97it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 45.11it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 45.27it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.96it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.76it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.64it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.72it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.83it/s][A
 94%|█████████▍| 1018/1083 [00:23<00:01, 44.90it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 44.71it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 44.86it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 45.03it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.94it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.79it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.59it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.68it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.65it/s][A
 98%|█████████▊| 1063/1083 [00:24<00:00, 44.79it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 44.95it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 45.10it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 45.14it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.99it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.99it/s][A 80%|████████  | 328/410 [03:22<00:21,  3.77it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 06:43:20,392 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:43:20,546 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:43:23,196 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:43:23,419 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:43:23,514 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [03:27<11:55,  8.84s/it] 80%|████████  | 330/410 [03:27<08:21,  6.27s/it] 81%|████████  | 331/410 [03:27<05:53,  4.48s/it] 81%|████████  | 332/410 [03:28<04:10,  3.22s/it] 81%|████████  | 333/410 [03:28<02:59,  2.34s/it] 81%|████████▏ | 334/410 [03:28<02:10,  1.72s/it] 82%|████████▏ | 335/410 [03:28<01:36,  1.29s/it] 82%|████████▏ | 336/410 [03:29<01:12,  1.01it/s] 82%|████████▏ | 337/410 [03:29<00:56,  1.29it/s] 82%|████████▏ | 338/410 [03:29<00:45,  1.60it/s] 83%|████████▎ | 339/410 [03:29<00:36,  1.92it/s] 83%|████████▎ | 340/410 [03:30<00:31,  2.23it/s] 83%|████████▎ | 341/410 [03:30<00:27,  2.48it/s] 83%|████████▎ | 342/410 [03:30<00:24,  2.74it/s] 84%|████████▎ | 343/410 [03:31<00:22,  2.95it/s] 84%|████████▍ | 344/410 [03:31<00:21,  3.13it/s] 84%|████████▍ | 345/410 [03:31<00:19,  3.26it/s] 84%|████████▍ | 346/410 [03:31<00:19,  3.36it/s] 85%|████████▍ | 347/410 [03:32<00:18,  3.44it/s] 85%|████████▍ | 348/410 [03:32<00:17,  3.49it/s] 85%|████████▌ | 349/410 [03:32<00:17,  3.53it/s] 85%|████████▌ | 350/410 [03:33<00:16,  3.56it/s] 86%|████████▌ | 351/410 [03:33<00:16,  3.58it/s] 86%|████████▌ | 352/410 [03:33<00:17,  3.37it/s] 86%|████████▌ | 353/410 [03:33<00:16,  3.44it/s] 86%|████████▋ | 354/410 [03:34<00:16,  3.48it/s] 87%|████████▋ | 355/410 [03:34<00:15,  3.53it/s] 87%|████████▋ | 356/410 [03:34<00:15,  3.55it/s] 87%|████████▋ | 357/410 [03:35<00:14,  3.57it/s] 87%|████████▋ | 358/410 [03:35<00:14,  3.58it/s] 88%|████████▊ | 359/410 [03:35<00:14,  3.59it/s] 88%|████████▊ | 360/410 [03:35<00:13,  3.60it/s] 88%|████████▊ | 361/410 [03:36<00:13,  3.61it/s] 88%|████████▊ | 362/410 [03:36<00:13,  3.61it/s] 89%|████████▊ | 363/410 [03:36<00:13,  3.47it/s] 89%|████████▉ | 364/410 [03:36<00:13,  3.51it/s] 89%|████████▉ | 365/410 [03:37<00:12,  3.54it/s] 89%|████████▉ | 366/410 [03:37<00:12,  3.57it/s] 90%|████████▉ | 367/410 [03:37<00:11,  3.59it/s] 90%|████████▉ | 368/410 [03:38<00:11,  3.60it/s] 90%|█████████ | 369/410 [03:38<00:11,  3.60it/s] 90%|█████████ | 370/410 [03:38<00:11,  3.61it/s] 90%|█████████ | 371/410 [03:38<00:10,  3.61it/s] 91%|█████████ | 372/410 [03:39<00:10,  3.62it/s] 91%|█████████ | 373/410 [03:39<00:10,  3.62it/s] 91%|█████████ | 374/410 [03:39<00:10,  3.42it/s] 91%|█████████▏| 375/410 [03:40<00:10,  3.48it/s] 92%|█████████▏| 376/410 [03:40<00:09,  3.52it/s] 92%|█████████▏| 377/410 [03:40<00:09,  3.55it/s] 92%|█████████▏| 378/410 [03:40<00:08,  3.57it/s] 92%|█████████▏| 379/410 [03:41<00:08,  3.59it/s] 93%|█████████▎| 380/410 [03:41<00:08,  3.60it/s] 93%|█████████▎| 381/410 [03:41<00:08,  3.61it/s] 93%|█████████▎| 382/410 [03:42<00:07,  3.61it/s] 93%|█████████▎| 383/410 [03:42<00:07,  3.61it/s] 94%|█████████▎| 384/410 [03:42<00:07,  3.62it/s] 94%|█████████▍| 385/410 [03:42<00:07,  3.53it/s] 94%|█████████▍| 386/410 [03:43<00:06,  3.56it/s] 94%|█████████▍| 387/410 [03:43<00:06,  3.58it/s] 95%|█████████▍| 388/410 [03:43<00:06,  3.59it/s] 95%|█████████▍| 389/410 [03:43<00:05,  3.60it/s] 95%|█████████▌| 390/410 [03:44<00:05,  3.61it/s] 95%|█████████▌| 391/410 [03:44<00:05,  3.61it/s] 96%|█████████▌| 392/410 [03:44<00:04,  3.62it/s] 96%|█████████▌| 393/410 [03:45<00:04,  3.62it/s] 96%|█████████▌| 394/410 [03:45<00:04,  3.62it/s] 96%|█████████▋| 395/410 [03:45<00:04,  3.62it/s] 97%|█████████▋| 396/410 [03:46<00:04,  3.26it/s] 97%|█████████▋| 397/410 [03:46<00:03,  3.36it/s] 97%|█████████▋| 398/410 [03:46<00:03,  3.44it/s] 97%|█████████▋| 399/410 [03:46<00:03,  3.49it/s] 98%|█████████▊| 400/410 [03:47<00:02,  3.53it/s] 98%|█████████▊| 401/410 [03:47<00:02,  3.56it/s] 98%|█████████▊| 402/410 [03:47<00:02,  3.58it/s] 98%|█████████▊| 403/410 [03:47<00:01,  3.59it/s] 99%|█████████▊| 404/410 [03:48<00:01,  3.60it/s] 99%|█████████▉| 405/410 [03:48<00:01,  3.61it/s] 99%|█████████▉| 406/410 [03:48<00:01,  3.61it/s] 99%|█████████▉| 407/410 [03:49<00:01,  2.80it/s]100%|█████████▉| 408/410 [03:49<00:00,  3.00it/s]100%|█████████▉| 409/410 [03:49<00:00,  3.17it/s]100%|██████████| 410/410 [03:50<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 06:43:47,553 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:43:47,554 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 06:43:47,554 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.5022, 'eval_samples_per_second': 353.355, 'eval_steps_per_second': 44.2, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.58it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.48it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.40it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.17it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.74it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.32it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.01it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.88it/s][A
  4%|▍         | 48/1083 [00:01<00:25, 39.98it/s][A
  5%|▍         | 53/1083 [00:01<00:24, 41.76it/s][A
  5%|▌         | 58/1083 [00:01<00:23, 42.79it/s][A
  6%|▌         | 63/1083 [00:01<00:23, 43.66it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 44.19it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 44.61it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 44.66it/s][A
  8%|▊         | 83/1083 [00:01<00:23, 43.09it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 43.51it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 43.57it/s][A
  9%|▉         | 98/1083 [00:02<00:22, 44.06it/s][A
 10%|▉         | 103/1083 [00:02<00:22, 44.28it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 44.59it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.79it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.98it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.96it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.86it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.73it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.50it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 44.65it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.74it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.93it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.05it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.05it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.09it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.90it/s][A
 16%|█▋        | 178/1083 [00:04<00:20, 44.81it/s][A
 17%|█▋        | 183/1083 [00:04<00:21, 42.76it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 43.45it/s][A
 18%|█▊        | 193/1083 [00:04<00:20, 44.03it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 44.38it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 44.57it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 44.77it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 44.90it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.98it/s][A
 21%|██        | 223/1083 [00:05<00:19, 44.61it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.60it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.72it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.84it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.88it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.02it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.07it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.03it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 44.91it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 44.65it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.65it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 44.64it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.84it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.03it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.08it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.99it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.04it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.95it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 44.84it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 42.88it/s][A
 30%|██▉       | 323/1083 [00:07<00:17, 43.45it/s][A
 30%|███       | 328/1083 [00:07<00:17, 44.00it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.36it/s][A
 31%|███       | 338/1083 [00:07<00:16, 44.62it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 44.87it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 44.90it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.73it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 44.45it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.50it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 44.60it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.84it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.94it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.02it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.07it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.12it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 44.91it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.67it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.60it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 44.66it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.75it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.85it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.96it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.15it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.10it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.95it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 44.77it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.70it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 44.66it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.66it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.78it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 44.97it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.11it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.13it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.66it/s][A
 46%|████▌     | 493/1083 [00:11<00:13, 44.69it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.69it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.72it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.74it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.84it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 44.84it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.04it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 45.09it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 44.97it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.81it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.75it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 43.87it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.28it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.53it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.74it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.95it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 45.01it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 44.88it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.78it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.55it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.60it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.72it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.84it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.94it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.10it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.10it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.03it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.88it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.71it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.65it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.62it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.80it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.95it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.11it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 45.09it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 45.03it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.82it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.78it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.72it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.67it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.70it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.82it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.90it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.98it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.14it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 45.05it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.96it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.89it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.78it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.86it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.82it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.87it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.93it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 45.02it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.94it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.74it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.82it/s][A
 72%|███████▏  | 778/1083 [00:17<00:07, 43.50it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 43.99it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.32it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 44.53it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.67it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 44.77it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.72it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.72it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.60it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.60it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.75it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.94it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.95it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 45.03it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 45.01it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.93it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.82it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.54it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.61it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.70it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.85it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.99it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 45.03it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 44.98it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.95it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.85it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.66it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.68it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.76it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.93it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.09it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 45.04it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 44.99it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.91it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.74it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.67it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.66it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.69it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.81it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.88it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 45.06it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 45.03it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.99it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.90it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.82it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.65it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.26it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.55it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.77it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 44.97it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 44.99it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.86it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.83it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.78it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.77it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.77it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.69it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.83it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 45.00it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 45.06it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.97it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 45.02it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 45.02it/s][A100%|██████████| 410/410 [04:14<00:00,  3.41it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 06:44:11,843 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 06:44:11,943 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:44:14,818 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:44:14,948 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:44:15,010 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:44:16,119 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:44:16,144 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82 (score: 0.9504339694976807).
                                                 100%|██████████| 410/410 [04:26<00:00,  3.41it/s]100%|██████████| 410/410 [04:26<00:00,  1.54it/s]
[INFO|trainer.py:1894] 2023-08-28 06:44:24,089 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 06:44:24,235 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:44:27,209 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:44:27,360 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:44:27,436 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:44:28,024 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:28,024 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:28,024 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:28,024 >>   train_runtime            = 0:04:26.49
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:28,024 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:28,024 >>   train_samples_per_second =     98.296
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:28,024 >>   train_steps_per_second   =      1.539
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.2224, 'eval_samples_per_second': 357.438, 'eval_steps_per_second': 44.711, 'epoch': 5.0}
{'train_runtime': 266.4911, 'train_samples_per_second': 98.296, 'train_steps_per_second': 1.539, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:44:28 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:44:28,287 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:44:28,287 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 06:44:28,287 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.39it/s]  1%|          | 12/1083 [00:00<00:21, 49.45it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.74it/s]  2%|▏         | 22/1083 [00:00<00:22, 47.05it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.37it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.18it/s]  3%|▎         | 37/1083 [00:00<00:22, 46.03it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.63it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.05it/s]  5%|▍         | 52/1083 [00:01<00:23, 44.61it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.67it/s]  6%|▌         | 62/1083 [00:01<00:22, 44.86it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.08it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.23it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.31it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.29it/s]  8%|▊         | 87/1083 [00:01<00:22, 45.19it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.91it/s]  9%|▉         | 97/1083 [00:02<00:22, 42.96it/s]  9%|▉         | 102/1083 [00:02<00:22, 43.51it/s] 10%|▉         | 107/1083 [00:02<00:22, 44.02it/s] 10%|█         | 112/1083 [00:02<00:21, 44.46it/s] 11%|█         | 117/1083 [00:02<00:21, 44.85it/s] 11%|█▏        | 122/1083 [00:02<00:21, 44.99it/s] 12%|█▏        | 127/1083 [00:02<00:21, 45.11it/s] 12%|█▏        | 132/1083 [00:02<00:21, 44.95it/s] 13%|█▎        | 137/1083 [00:03<00:21, 44.67it/s] 13%|█▎        | 142/1083 [00:03<00:21, 44.55it/s] 14%|█▎        | 147/1083 [00:03<00:21, 44.49it/s] 14%|█▍        | 152/1083 [00:03<00:20, 44.69it/s] 14%|█▍        | 157/1083 [00:03<00:20, 44.93it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.14it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.19it/s] 16%|█▌        | 172/1083 [00:03<00:20, 45.23it/s] 16%|█▋        | 177/1083 [00:03<00:20, 45.07it/s] 17%|█▋        | 182/1083 [00:04<00:20, 44.79it/s] 17%|█▋        | 187/1083 [00:04<00:20, 44.66it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.67it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.70it/s] 19%|█▊        | 202/1083 [00:04<00:19, 44.93it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.04it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.23it/s] 20%|██        | 217/1083 [00:04<00:19, 45.18it/s] 20%|██        | 222/1083 [00:04<00:19, 45.13it/s] 21%|██        | 227/1083 [00:05<00:19, 44.79it/s] 21%|██▏       | 232/1083 [00:05<00:21, 40.41it/s] 22%|██▏       | 237/1083 [00:05<00:20, 41.80it/s] 22%|██▏       | 242/1083 [00:05<00:19, 42.73it/s] 23%|██▎       | 247/1083 [00:05<00:19, 43.46it/s] 23%|██▎       | 252/1083 [00:05<00:18, 43.96it/s] 24%|██▎       | 257/1083 [00:05<00:18, 44.33it/s] 24%|██▍       | 262/1083 [00:05<00:18, 44.67it/s] 25%|██▍       | 267/1083 [00:05<00:18, 44.77it/s] 25%|██▌       | 272/1083 [00:06<00:18, 44.53it/s] 26%|██▌       | 277/1083 [00:06<00:18, 44.43it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.61it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.88it/s] 27%|██▋       | 292/1083 [00:06<00:17, 44.96it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.03it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.07it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.09it/s] 29%|██▉       | 312/1083 [00:06<00:17, 44.91it/s] 29%|██▉       | 317/1083 [00:07<00:17, 44.66it/s] 30%|██▉       | 322/1083 [00:07<00:17, 44.53it/s] 30%|███       | 327/1083 [00:07<00:16, 44.75it/s] 31%|███       | 332/1083 [00:07<00:16, 44.78it/s] 31%|███       | 337/1083 [00:07<00:16, 45.00it/s] 32%|███▏      | 342/1083 [00:07<00:16, 45.07it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.17it/s] 33%|███▎      | 352/1083 [00:07<00:16, 45.05it/s] 33%|███▎      | 357/1083 [00:07<00:16, 44.99it/s] 33%|███▎      | 362/1083 [00:08<00:16, 44.83it/s] 34%|███▍      | 367/1083 [00:08<00:16, 42.61it/s] 34%|███▍      | 372/1083 [00:08<00:16, 43.44it/s] 35%|███▍      | 377/1083 [00:08<00:16, 43.95it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.33it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.67it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.90it/s] 37%|███▋      | 397/1083 [00:08<00:15, 44.94it/s] 37%|███▋      | 402/1083 [00:08<00:15, 44.80it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.65it/s] 38%|███▊      | 412/1083 [00:09<00:15, 44.72it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.85it/s] 39%|███▉      | 422/1083 [00:09<00:14, 45.09it/s] 39%|███▉      | 427/1083 [00:09<00:14, 45.08it/s] 40%|███▉      | 432/1083 [00:09<00:14, 45.32it/s] 40%|████      | 437/1083 [00:09<00:14, 45.29it/s] 41%|████      | 442/1083 [00:09<00:14, 45.27it/s] 41%|████▏     | 447/1083 [00:09<00:14, 45.06it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.88it/s] 42%|████▏     | 457/1083 [00:10<00:13, 44.75it/s] 43%|████▎     | 462/1083 [00:10<00:13, 44.84it/s] 43%|████▎     | 467/1083 [00:10<00:13, 44.99it/s] 44%|████▎     | 472/1083 [00:10<00:13, 45.17it/s] 44%|████▍     | 477/1083 [00:10<00:13, 45.27it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.35it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.08it/s] 45%|████▌     | 492/1083 [00:10<00:13, 45.12it/s] 46%|████▌     | 497/1083 [00:11<00:12, 45.08it/s] 46%|████▋     | 502/1083 [00:11<00:14, 40.83it/s] 47%|████▋     | 507/1083 [00:11<00:13, 42.19it/s] 47%|████▋     | 512/1083 [00:11<00:13, 43.15it/s] 48%|████▊     | 517/1083 [00:11<00:12, 43.82it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.36it/s] 49%|████▊     | 527/1083 [00:11<00:12, 44.68it/s] 49%|████▉     | 532/1083 [00:11<00:12, 44.94it/s] 50%|████▉     | 537/1083 [00:12<00:12, 44.81it/s] 50%|█████     | 542/1083 [00:12<00:12, 44.51it/s] 51%|█████     | 547/1083 [00:12<00:12, 44.46it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.65it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 44.85it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 45.00it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 45.08it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 45.29it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 45.38it/s] 54%|█████▎    | 582/1083 [00:13<00:11, 45.23it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 44.95it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 44.81it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 44.75it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.96it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 45.01it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 45.18it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 45.33it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 45.33it/s] 58%|█████▊    | 627/1083 [00:14<00:10, 45.14it/s] 58%|█████▊    | 632/1083 [00:14<00:10, 44.99it/s] 59%|█████▉    | 637/1083 [00:14<00:10, 43.62it/s] 59%|█████▉    | 642/1083 [00:14<00:10, 44.02it/s] 60%|█████▉    | 647/1083 [00:14<00:09, 44.36it/s] 60%|██████    | 652/1083 [00:14<00:09, 43.94it/s] 61%|██████    | 657/1083 [00:14<00:09, 44.33it/s] 61%|██████    | 662/1083 [00:14<00:09, 44.66it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 44.97it/s] 62%|██████▏   | 672/1083 [00:15<00:09, 45.10it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.88it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.67it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.64it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.72it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 44.92it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 44.99it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 45.09it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 45.18it/s] 66%|██████▌   | 717/1083 [00:16<00:08, 45.22it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 45.00it/s] 67%|██████▋   | 727/1083 [00:16<00:07, 44.91it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.79it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.82it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.84it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 45.01it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 45.18it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 45.24it/s] 70%|███████   | 762/1083 [00:17<00:07, 45.26it/s] 71%|███████   | 767/1083 [00:17<00:07, 45.11it/s] 71%|███████▏  | 772/1083 [00:17<00:06, 44.82it/s] 72%|███████▏  | 777/1083 [00:17<00:06, 44.80it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 44.78it/s] 73%|███████▎  | 787/1083 [00:17<00:07, 41.79it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 42.85it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 43.44it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 44.13it/s] 75%|███████▍  | 807/1083 [00:18<00:06, 44.44it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 44.81it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.98it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.96it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 44.59it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 44.57it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 44.81it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 44.88it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 43.28it/s] 79%|███████▊  | 852/1083 [00:19<00:05, 44.00it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.39it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.68it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.69it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.57it/s] 81%|████████  | 877/1083 [00:19<00:04, 44.59it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 44.67it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 44.76it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 44.85it/s] 83%|████████▎ | 897/1083 [00:20<00:04, 45.05it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 45.15it/s] 84%|████████▎ | 907/1083 [00:20<00:04, 41.75it/s] 84%|████████▍ | 912/1083 [00:20<00:03, 43.32it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 43.78it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 42.17it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 43.10it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 43.77it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 44.22it/s] 87%|████████▋ | 942/1083 [00:21<00:03, 44.52it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.66it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 44.60it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 44.69it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 44.61it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 44.62it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 44.79it/s] 90%|█████████ | 977/1083 [00:21<00:02, 44.92it/s] 91%|█████████ | 982/1083 [00:21<00:02, 45.14it/s] 91%|█████████ | 987/1083 [00:22<00:02, 45.17it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 45.16it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 44.99it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 44.82it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 44.73it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 44.81it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 44.93it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 45.16it/s] 95%|█████████▍| 1027/1083 [00:22<00:01, 45.21it/s] 95%|█████████▌| 1032/1083 [00:23<00:01, 45.25it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 45.11it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 44.95it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 44.86it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 44.71it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 44.10it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 44.48it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 44.79it/s] 99%|█████████▉| 1072/1083 [00:23<00:00, 44.92it/s] 99%|█████████▉| 1077/1083 [00:24<00:00, 45.06it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 45.14it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.68it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:44:52,546 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   eval_loss               =     0.9504
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   eval_runtime            = 0:00:24.25
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   eval_samples_per_second =    356.909
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   eval_steps_per_second   =     44.645
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:52,546 >>   perplexity              =     2.5868
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:02,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:02,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:02,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:02,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:02,921 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:45:03,732 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:45:03,733 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:45:04,374 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:45:05,816 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:45:05,816 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:08,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:08,944 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:08,944 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:08,944 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:08,944 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:45:09,705 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:45:09,706 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:45:10,319 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:45:10,527 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:45:10,527 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.69it/s]Extractor Predicting: 6it [00:03,  1.70it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.71it/s]Extractor Predicting: 16it [00:09,  1.72it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:13,  1.65it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:14,  1.72it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.68it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.71it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:20,  1.67it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:22,  1.69it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:23,  1.70it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.64it/s]Extractor Predicting: 43it [00:25,  1.66it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:27,  1.66it/s]Extractor Predicting: 47it [00:28,  1.54it/s]Extractor Predicting: 48it [00:28,  1.56it/s]Extractor Predicting: 49it [00:29,  1.54it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:30,  1.65it/s]Extractor Predicting: 52it [00:31,  1.64it/s]Extractor Predicting: 53it [00:31,  1.64it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.70it/s]Extractor Predicting: 56it [00:33,  1.70it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.58it/s]Extractor Predicting: 61it [00:36,  1.63it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:37,  1.72it/s]Extractor Predicting: 64it [00:38,  1.75it/s]Extractor Predicting: 65it [00:39,  1.71it/s]Extractor Predicting: 66it [00:39,  1.71it/s]Extractor Predicting: 67it [00:40,  1.68it/s]Extractor Predicting: 68it [00:40,  1.68it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.63it/s]Extractor Predicting: 71it [00:42,  1.63it/s]Extractor Predicting: 72it [00:43,  1.67it/s]Extractor Predicting: 73it [00:43,  1.69it/s]Extractor Predicting: 74it [00:44,  1.71it/s]Extractor Predicting: 75it [00:45,  1.69it/s]Extractor Predicting: 76it [00:45,  1.71it/s]Extractor Predicting: 77it [00:46,  1.69it/s]Extractor Predicting: 78it [00:46,  1.70it/s]Extractor Predicting: 79it [00:47,  1.69it/s]Extractor Predicting: 80it [00:48,  1.70it/s]Extractor Predicting: 81it [00:48,  1.74it/s]Extractor Predicting: 82it [00:49,  1.73it/s]Extractor Predicting: 83it [00:49,  1.72it/s]Extractor Predicting: 84it [00:50,  1.74it/s]Extractor Predicting: 85it [00:50,  1.68it/s]Extractor Predicting: 86it [00:51,  1.66it/s]Extractor Predicting: 87it [00:52,  1.65it/s]Extractor Predicting: 88it [00:52,  1.66it/s]Extractor Predicting: 89it [00:53,  1.70it/s]Extractor Predicting: 90it [00:53,  1.70it/s]Extractor Predicting: 91it [00:54,  1.75it/s]Extractor Predicting: 92it [00:55,  1.74it/s]Extractor Predicting: 93it [00:55,  1.72it/s]Extractor Predicting: 94it [00:56,  1.73it/s]Extractor Predicting: 95it [00:56,  1.75it/s]Extractor Predicting: 96it [00:57,  1.73it/s]Extractor Predicting: 97it [00:57,  1.73it/s]Extractor Predicting: 98it [00:58,  1.71it/s]Extractor Predicting: 99it [00:59,  1.72it/s]Extractor Predicting: 100it [00:59,  1.71it/s]Extractor Predicting: 101it [01:00,  1.73it/s]Extractor Predicting: 102it [01:00,  1.73it/s]Extractor Predicting: 103it [01:01,  1.73it/s]Extractor Predicting: 104it [01:01,  1.73it/s]Extractor Predicting: 105it [01:02,  1.72it/s]Extractor Predicting: 106it [01:03,  1.74it/s]Extractor Predicting: 107it [01:03,  1.72it/s]Extractor Predicting: 108it [01:04,  1.73it/s]Extractor Predicting: 109it [01:04,  1.71it/s]Extractor Predicting: 110it [01:05,  1.71it/s]Extractor Predicting: 111it [01:06,  1.71it/s]Extractor Predicting: 112it [01:06,  1.72it/s]Extractor Predicting: 113it [01:07,  1.73it/s]Extractor Predicting: 114it [01:07,  1.74it/s]Extractor Predicting: 115it [01:08,  1.67it/s]Extractor Predicting: 116it [01:09,  1.66it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:10,  1.72it/s]Extractor Predicting: 119it [01:10,  1.73it/s]Extractor Predicting: 120it [01:11,  1.73it/s]Extractor Predicting: 121it [01:11,  1.68it/s]Extractor Predicting: 122it [01:12,  1.81it/s]Extractor Predicting: 123it [01:12,  1.80it/s]Extractor Predicting: 124it [01:13,  1.77it/s]Extractor Predicting: 125it [01:14,  1.71it/s]Extractor Predicting: 126it [01:14,  1.68it/s]Extractor Predicting: 127it [01:15,  1.69it/s]Extractor Predicting: 128it [01:15,  1.70it/s]Extractor Predicting: 129it [01:16,  1.71it/s]Extractor Predicting: 130it [01:17,  1.72it/s]Extractor Predicting: 131it [01:17,  1.78it/s]Extractor Predicting: 132it [01:18,  1.54it/s]Extractor Predicting: 133it [01:19,  1.50it/s]Extractor Predicting: 134it [01:19,  1.56it/s]Extractor Predicting: 135it [01:20,  1.61it/s]Extractor Predicting: 136it [01:20,  1.63it/s]Extractor Predicting: 137it [01:21,  1.65it/s]Extractor Predicting: 138it [01:22,  1.64it/s]Extractor Predicting: 139it [01:22,  1.66it/s]Extractor Predicting: 140it [01:23,  1.67it/s]Extractor Predicting: 141it [01:23,  1.70it/s]Extractor Predicting: 142it [01:24,  1.71it/s]Extractor Predicting: 143it [01:25,  1.70it/s]Extractor Predicting: 144it [01:25,  1.66it/s]Extractor Predicting: 145it [01:26,  1.65it/s]Extractor Predicting: 146it [01:26,  1.64it/s]Extractor Predicting: 147it [01:27,  1.68it/s]Extractor Predicting: 148it [01:28,  1.69it/s]Extractor Predicting: 149it [01:28,  1.68it/s]Extractor Predicting: 150it [01:29,  1.69it/s]Extractor Predicting: 151it [01:29,  1.71it/s]Extractor Predicting: 152it [01:30,  1.65it/s]Extractor Predicting: 153it [01:31,  1.59it/s]Extractor Predicting: 154it [01:31,  1.54it/s]Extractor Predicting: 155it [01:32,  1.52it/s]Extractor Predicting: 156it [01:33,  1.51it/s]Extractor Predicting: 157it [01:33,  1.50it/s]Extractor Predicting: 158it [01:34,  1.51it/s]Extractor Predicting: 159it [01:35,  1.51it/s]Extractor Predicting: 160it [01:35,  1.51it/s]Extractor Predicting: 161it [01:36,  1.50it/s]Extractor Predicting: 162it [01:37,  1.50it/s]Extractor Predicting: 163it [01:37,  1.50it/s]Extractor Predicting: 164it [01:38,  1.50it/s]Extractor Predicting: 165it [01:39,  1.49it/s]Extractor Predicting: 166it [01:39,  1.51it/s]Extractor Predicting: 167it [01:40,  1.51it/s]Extractor Predicting: 168it [01:41,  1.54it/s]Extractor Predicting: 169it [01:41,  1.59it/s]Extractor Predicting: 170it [01:42,  1.62it/s]Extractor Predicting: 171it [01:42,  1.64it/s]Extractor Predicting: 172it [01:43,  1.63it/s]Extractor Predicting: 173it [01:44,  1.63it/s]Extractor Predicting: 174it [01:44,  1.62it/s]Extractor Predicting: 175it [01:45,  1.59it/s]Extractor Predicting: 176it [01:46,  1.59it/s]Extractor Predicting: 177it [01:46,  1.59it/s]Extractor Predicting: 178it [01:47,  1.58it/s]Extractor Predicting: 179it [01:47,  1.61it/s]Extractor Predicting: 180it [01:48,  1.59it/s]Extractor Predicting: 181it [01:49,  1.61it/s]Extractor Predicting: 182it [01:49,  1.63it/s]Extractor Predicting: 183it [01:50,  1.68it/s]Extractor Predicting: 184it [01:50,  1.70it/s]Extractor Predicting: 185it [01:51,  1.75it/s]Extractor Predicting: 186it [01:52,  1.65it/s]Extractor Predicting: 187it [01:52,  1.68it/s]Extractor Predicting: 188it [01:53,  1.68it/s]Extractor Predicting: 189it [01:53,  1.70it/s]Extractor Predicting: 190it [01:54,  1.69it/s]Extractor Predicting: 191it [01:55,  1.66it/s]Extractor Predicting: 192it [01:55,  1.67it/s]Extractor Predicting: 193it [01:56,  1.70it/s]Extractor Predicting: 194it [01:56,  1.71it/s]Extractor Predicting: 195it [01:57,  1.69it/s]Extractor Predicting: 196it [01:58,  1.67it/s]Extractor Predicting: 197it [01:58,  1.66it/s]Extractor Predicting: 198it [01:59,  1.67it/s]Extractor Predicting: 199it [01:59,  1.73it/s]Extractor Predicting: 200it [02:00,  1.72it/s]Extractor Predicting: 201it [02:00,  1.74it/s]Extractor Predicting: 202it [02:01,  1.71it/s]Extractor Predicting: 203it [02:02,  1.68it/s]Extractor Predicting: 204it [02:02,  1.71it/s]Extractor Predicting: 205it [02:03,  1.69it/s]Extractor Predicting: 206it [02:03,  1.70it/s]Extractor Predicting: 207it [02:04,  1.70it/s]Extractor Predicting: 208it [02:05,  1.72it/s]Extractor Predicting: 209it [02:05,  1.75it/s]Extractor Predicting: 210it [02:06,  1.72it/s]Extractor Predicting: 211it [02:06,  1.72it/s]Extractor Predicting: 212it [02:07,  1.71it/s]Extractor Predicting: 213it [02:07,  1.73it/s]Extractor Predicting: 214it [02:08,  1.76it/s]Extractor Predicting: 215it [02:08,  1.80it/s]Extractor Predicting: 216it [02:09,  1.74it/s]Extractor Predicting: 217it [02:10,  1.75it/s]Extractor Predicting: 218it [02:10,  1.75it/s]Extractor Predicting: 219it [02:11,  1.75it/s]Extractor Predicting: 220it [02:11,  1.72it/s]Extractor Predicting: 221it [02:12,  1.76it/s]Extractor Predicting: 222it [02:13,  1.54it/s]Extractor Predicting: 223it [02:13,  1.59it/s]Extractor Predicting: 224it [02:14,  1.60it/s]Extractor Predicting: 225it [02:15,  1.57it/s]Extractor Predicting: 226it [02:15,  1.62it/s]Extractor Predicting: 227it [02:16,  1.62it/s]Extractor Predicting: 228it [02:16,  1.63it/s]Extractor Predicting: 229it [02:17,  1.69it/s]Extractor Predicting: 230it [02:18,  1.67it/s]Extractor Predicting: 231it [02:18,  1.65it/s]Extractor Predicting: 232it [02:19,  1.69it/s]Extractor Predicting: 233it [02:19,  1.70it/s]Extractor Predicting: 234it [02:20,  1.66it/s]Extractor Predicting: 235it [02:21,  1.65it/s]Extractor Predicting: 236it [02:21,  1.63it/s]Extractor Predicting: 237it [02:22,  1.65it/s]Extractor Predicting: 238it [02:22,  1.64it/s]Extractor Predicting: 239it [02:23,  1.66it/s]Extractor Predicting: 240it [02:24,  1.66it/s]Extractor Predicting: 241it [02:24,  1.68it/s]Extractor Predicting: 242it [02:25,  1.70it/s]Extractor Predicting: 243it [02:25,  1.65it/s]Extractor Predicting: 244it [02:26,  1.64it/s]Extractor Predicting: 245it [02:27,  1.66it/s]Extractor Predicting: 246it [02:27,  1.66it/s]Extractor Predicting: 247it [02:28,  1.67it/s]Extractor Predicting: 248it [02:28,  1.70it/s]Extractor Predicting: 249it [02:29,  1.69it/s]Extractor Predicting: 250it [02:30,  1.63it/s]Extractor Predicting: 251it [02:30,  1.65it/s]Extractor Predicting: 252it [02:31,  1.66it/s]Extractor Predicting: 253it [02:31,  1.67it/s]Extractor Predicting: 254it [02:32,  1.69it/s]Extractor Predicting: 255it [02:33,  1.65it/s]Extractor Predicting: 256it [02:33,  1.66it/s]Extractor Predicting: 257it [02:34,  1.68it/s]Extractor Predicting: 258it [02:34,  1.71it/s]Extractor Predicting: 259it [02:35,  1.69it/s]Extractor Predicting: 260it [02:36,  1.66it/s]Extractor Predicting: 261it [02:36,  1.69it/s]Extractor Predicting: 262it [02:37,  1.67it/s]Extractor Predicting: 263it [02:37,  1.64it/s]Extractor Predicting: 264it [02:38,  1.63it/s]Extractor Predicting: 265it [02:39,  1.65it/s]Extractor Predicting: 266it [02:39,  1.62it/s]Extractor Predicting: 267it [02:40,  1.64it/s]Extractor Predicting: 268it [02:40,  1.65it/s]Extractor Predicting: 269it [02:41,  1.72it/s]Extractor Predicting: 269it [02:41,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:03,871 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:03,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:03,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:03,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:03,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:48:04,691 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:48:04,692 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:48:05,297 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:48:06,385 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:48:06,386 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:09,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:09,566 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:09,566 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:09,566 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:09,566 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:48:10,292 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:48:10,293 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:48:10,903 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:48:11,104 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:48:11,104 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.51it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:24,  1.70it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:25,  1.69it/s]Extractor Predicting: 42it [00:25,  1.72it/s]Extractor Predicting: 43it [00:26,  1.75it/s]Extractor Predicting: 44it [00:27,  1.71it/s]Extractor Predicting: 45it [00:27,  1.70it/s]Extractor Predicting: 46it [00:28,  1.65it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:29,  1.64it/s]Extractor Predicting: 49it [00:30,  1.65it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:31,  1.66it/s]Extractor Predicting: 52it [00:31,  1.69it/s]Extractor Predicting: 53it [00:32,  1.71it/s]Extractor Predicting: 54it [00:33,  1.72it/s]Extractor Predicting: 55it [00:33,  1.72it/s]Extractor Predicting: 56it [00:34,  1.76it/s]Extractor Predicting: 57it [00:34,  1.74it/s]Extractor Predicting: 58it [00:35,  1.75it/s]Extractor Predicting: 59it [00:35,  1.73it/s]Extractor Predicting: 60it [00:36,  1.76it/s]Extractor Predicting: 61it [00:37,  1.75it/s]Extractor Predicting: 62it [00:37,  1.73it/s]Extractor Predicting: 63it [00:38,  1.70it/s]Extractor Predicting: 64it [00:38,  1.67it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:40,  1.66it/s]Extractor Predicting: 67it [00:40,  1.67it/s]Extractor Predicting: 68it [00:41,  1.67it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.72it/s]Extractor Predicting: 71it [00:42,  1.76it/s]Extractor Predicting: 72it [00:43,  1.68it/s]Extractor Predicting: 73it [00:44,  1.70it/s]Extractor Predicting: 74it [00:44,  1.71it/s]Extractor Predicting: 75it [00:45,  1.71it/s]Extractor Predicting: 76it [00:45,  1.74it/s]Extractor Predicting: 77it [00:46,  1.62it/s]Extractor Predicting: 78it [00:47,  1.66it/s]Extractor Predicting: 79it [00:47,  1.71it/s]Extractor Predicting: 80it [00:48,  1.70it/s]Extractor Predicting: 81it [00:48,  1.77it/s]Extractor Predicting: 82it [00:49,  1.78it/s]Extractor Predicting: 83it [00:49,  1.78it/s]Extractor Predicting: 84it [00:50,  1.79it/s]Extractor Predicting: 85it [00:50,  1.82it/s]Extractor Predicting: 86it [00:51,  1.78it/s]Extractor Predicting: 87it [00:52,  1.78it/s]Extractor Predicting: 88it [00:52,  1.78it/s]Extractor Predicting: 89it [00:53,  1.74it/s]Extractor Predicting: 90it [00:53,  1.75it/s]Extractor Predicting: 91it [00:54,  1.75it/s]Extractor Predicting: 92it [00:55,  1.74it/s]Extractor Predicting: 93it [00:55,  1.72it/s]Extractor Predicting: 94it [00:56,  1.72it/s]Extractor Predicting: 95it [00:56,  1.81it/s]Extractor Predicting: 96it [00:57,  1.76it/s]Extractor Predicting: 97it [00:57,  1.75it/s]Extractor Predicting: 98it [00:58,  1.73it/s]Extractor Predicting: 99it [00:59,  1.73it/s]Extractor Predicting: 100it [00:59,  1.75it/s]Extractor Predicting: 101it [01:00,  1.79it/s]Extractor Predicting: 102it [01:00,  1.79it/s]Extractor Predicting: 103it [01:01,  1.78it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.80it/s]Extractor Predicting: 106it [01:02,  1.86it/s]Extractor Predicting: 107it [01:03,  1.85it/s]Extractor Predicting: 108it [01:04,  1.68it/s]Extractor Predicting: 109it [01:04,  1.76it/s]Extractor Predicting: 110it [01:05,  1.77it/s]Extractor Predicting: 111it [01:05,  1.82it/s]Extractor Predicting: 112it [01:06,  1.81it/s]Extractor Predicting: 113it [01:06,  1.80it/s]Extractor Predicting: 114it [01:07,  1.72it/s]Extractor Predicting: 115it [01:08,  1.68it/s]Extractor Predicting: 116it [01:08,  1.66it/s]Extractor Predicting: 117it [01:09,  1.61it/s]Extractor Predicting: 118it [01:09,  1.62it/s]Extractor Predicting: 119it [01:10,  1.62it/s]Extractor Predicting: 120it [01:11,  1.64it/s]Extractor Predicting: 121it [01:11,  1.63it/s]Extractor Predicting: 122it [01:12,  1.65it/s]Extractor Predicting: 123it [01:13,  1.61it/s]Extractor Predicting: 124it [01:13,  1.63it/s]Extractor Predicting: 125it [01:14,  1.64it/s]Extractor Predicting: 126it [01:14,  1.61it/s]Extractor Predicting: 127it [01:15,  1.58it/s]Extractor Predicting: 128it [01:16,  1.60it/s]Extractor Predicting: 129it [01:16,  1.63it/s]Extractor Predicting: 130it [01:17,  1.66it/s]Extractor Predicting: 131it [01:17,  1.64it/s]Extractor Predicting: 132it [01:18,  1.62it/s]Extractor Predicting: 133it [01:19,  1.62it/s]Extractor Predicting: 134it [01:19,  1.61it/s]Extractor Predicting: 135it [01:20,  1.61it/s]Extractor Predicting: 136it [01:21,  1.66it/s]Extractor Predicting: 137it [01:21,  1.64it/s]Extractor Predicting: 138it [01:22,  1.69it/s]Extractor Predicting: 139it [01:22,  1.72it/s]Extractor Predicting: 140it [01:23,  1.68it/s]Extractor Predicting: 141it [01:24,  1.66it/s]Extractor Predicting: 142it [01:24,  1.68it/s]Extractor Predicting: 143it [01:24,  2.04it/s]Extractor Predicting: 143it [01:24,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:52,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:52,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:52,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:52,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:52,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:49:53,050 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:49:53,081 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:49:53,693 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:49:54,788 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:49:54,788 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:57,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:57,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:57,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:57,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:57,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:49:58,536 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:49:58,537 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:49:59,144 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:49:59,400 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:49:59,400 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:15,  1.66it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:18,  1.66it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.69it/s]Extractor Predicting: 39it [00:24,  1.77it/s]Extractor Predicting: 40it [00:24,  1.88it/s]Extractor Predicting: 41it [00:24,  1.98it/s]Extractor Predicting: 42it [00:25,  2.02it/s]Extractor Predicting: 43it [00:25,  2.04it/s]Extractor Predicting: 44it [00:26,  2.07it/s]Extractor Predicting: 45it [00:26,  2.07it/s]Extractor Predicting: 46it [00:27,  2.04it/s]Extractor Predicting: 47it [00:27,  2.02it/s]Extractor Predicting: 48it [00:28,  2.03it/s]Extractor Predicting: 49it [00:28,  2.02it/s]Extractor Predicting: 50it [00:29,  2.07it/s]Extractor Predicting: 51it [00:29,  2.02it/s]Extractor Predicting: 52it [00:30,  2.01it/s]Extractor Predicting: 53it [00:30,  2.04it/s]Extractor Predicting: 54it [00:31,  2.10it/s]Extractor Predicting: 55it [00:31,  2.12it/s]Extractor Predicting: 56it [00:32,  2.12it/s]Extractor Predicting: 57it [00:32,  2.12it/s]Extractor Predicting: 58it [00:33,  2.10it/s]Extractor Predicting: 59it [00:33,  2.07it/s]Extractor Predicting: 60it [00:34,  2.02it/s]Extractor Predicting: 61it [00:34,  1.99it/s]Extractor Predicting: 62it [00:35,  2.07it/s]Extractor Predicting: 63it [00:35,  2.07it/s]Extractor Predicting: 64it [00:36,  2.08it/s]Extractor Predicting: 65it [00:36,  2.08it/s]Extractor Predicting: 66it [00:37,  2.09it/s]Extractor Predicting: 67it [00:37,  1.98it/s]Extractor Predicting: 68it [00:38,  1.86it/s]Extractor Predicting: 69it [00:38,  1.79it/s]Extractor Predicting: 70it [00:39,  1.73it/s]Extractor Predicting: 71it [00:40,  1.68it/s]Extractor Predicting: 72it [00:40,  1.62it/s]Extractor Predicting: 72it [00:40,  1.77it/s]
[INFO|configuration_utils.py:515] 2023-08-28 06:50:43,320 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:50:43,321 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:50:43,408 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:50:43,409 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 06:50:43,463 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:50:57,195 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 06:50:57,220 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 06:50:57,298 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:50:57,299 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:50:57,330 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:50:57,359 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:50:57,359 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:50:57,359 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:50:57,359 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:50:57,359 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:50:57,359 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 06:50:57,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:50:58,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:50:58,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:50:59,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:00,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:00,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:01,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:01,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:02,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:03,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:03,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:04,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:04,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:05,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:06,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:06,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:07,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:08,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:08,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:09,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:10,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:10,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:11,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:10, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-28 06:51:12,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:12,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:13,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:14,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:14,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:15,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:15,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:16,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:17,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:17,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:18,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:18,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:19,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:20,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:20,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:21,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:22,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:22,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:23,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:24,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:24,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:25,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:26,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:26,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:29<01:59, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-28 06:51:27,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:28,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:28,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:29,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:30,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:30,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:31,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:32,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:32,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:33,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:33,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:34,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:35,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:35,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:36,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:37,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:38,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:38,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:39,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:40,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:41,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:42, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-28 06:51:41,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:42,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:43,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:43,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:44,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:45,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:45,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:46,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:47,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:47,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:48,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:49,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:49,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:50,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:51,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:51,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:52,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:53,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:53,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:54,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:54,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:55,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:56,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:56,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:57,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:57,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:58,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:59,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:59,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:00,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:01,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:01,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:02,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:02,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:03,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:04,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:04,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:05,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:05,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:08<01:51, 18.61s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:06,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:06,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:07,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:08,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:08,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:09,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:09,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:10,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:11,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:11,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:12,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:13,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:13,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:14,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:15,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:15,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:16,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:17,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:17,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:18,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:18,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:19,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:20,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:20,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:21,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:27, 17.54s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:22,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:22,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:23,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:23,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:24,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:25,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:25,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:26,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:26,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:27,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:28,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:28,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:29,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:29,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:30,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:30,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:31,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:32,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:32,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:33,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:34,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:37<01:03, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:34,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:35,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:35,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:36,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:37,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:37,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:38,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:38,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:39,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:39,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:40,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:40,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:41,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:42,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:42,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:43,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:44,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:44,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:45,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:45,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:46,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:46,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:47,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:48,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:48,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:49,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:49,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:52<00:47, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:50,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:50,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:51,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:52,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:52,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:53,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:53,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:54,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:54,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:55,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:56,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:56,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:57,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:57,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:58,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:59,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:59,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:00,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:01,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:01,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:02,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:02,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:03,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:06<00:30, 15.11s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:04,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:04,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:05,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:05,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:06,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:07,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:07,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:08,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:09,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:09,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:10,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:11,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:11,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:12,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:12,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:13,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:14,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:14,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:15,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:16,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:16,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:17,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:18,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:21<00:15, 15.05s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:18,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:19,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:20,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:21,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:21,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:22,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:23,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:23,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:24,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:25,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:25,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:26,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:26,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:27,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:28,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:28,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:29,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:30,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:31,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:31,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:32,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:33,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:33,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:37<00:00, 15.28s/it]Generating: 100%|██████████| 10/10 [02:37<00:00, 15.71s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:41,459 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:41,475 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:41,475 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:41,475 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:41,475 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:53:41,784 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:53:41,785 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:53:42,469 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:53:43,548 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:53:43,548 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:45,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:45,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:45,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:45,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:53:45,252 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:53:46,024 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:53:46,025 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:53:46,641 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:53:46,874 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:53:46,874 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')", "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.37it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:04,  1.53it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.56it/s]Extractor Estimating: 14it [00:09,  1.53it/s]Extractor Estimating: 15it [00:09,  1.53it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:11,  1.56it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.51it/s]Extractor Estimating: 20it [00:13,  1.56it/s]Extractor Estimating: 21it [00:13,  1.52it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:15,  1.50it/s]Extractor Estimating: 25it [00:16,  1.56it/s]Extractor Estimating: 26it [00:17,  1.53it/s]Extractor Estimating: 27it [00:17,  1.50it/s]Extractor Estimating: 28it [00:18,  1.51it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.58it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:20,  1.59it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:22,  1.57it/s]Extractor Estimating: 35it [00:22,  1.57it/s]Extractor Estimating: 36it [00:23,  1.60it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:24,  1.68it/s]Extractor Estimating: 39it [00:25,  1.63it/s]Extractor Estimating: 40it [00:25,  1.59it/s]Extractor Estimating: 41it [00:26,  1.56it/s]Extractor Estimating: 42it [00:27,  1.52it/s]Extractor Estimating: 43it [00:27,  1.50it/s]Extractor Estimating: 44it [00:28,  1.50it/s]Extractor Estimating: 45it [00:29,  1.51it/s]Extractor Estimating: 46it [00:29,  1.55it/s]Extractor Estimating: 47it [00:30,  1.56it/s]Extractor Estimating: 48it [00:31,  1.53it/s]Extractor Estimating: 49it [00:31,  1.55it/s]Extractor Estimating: 50it [00:32,  1.57it/s]Extractor Estimating: 51it [00:32,  1.58it/s]Extractor Estimating: 52it [00:33,  1.65it/s]Extractor Estimating: 53it [00:34,  1.67it/s]Extractor Estimating: 54it [00:34,  1.65it/s]Extractor Estimating: 55it [00:35,  1.68it/s]Extractor Estimating: 56it [00:35,  1.70it/s]Extractor Estimating: 57it [00:36,  1.75it/s]Extractor Estimating: 58it [00:36,  1.75it/s]Extractor Estimating: 59it [00:37,  1.78it/s]Extractor Estimating: 60it [00:38,  1.75it/s]Extractor Estimating: 61it [00:38,  1.75it/s]Extractor Estimating: 62it [00:39,  1.77it/s]Extractor Estimating: 63it [00:39,  1.71it/s]Extractor Estimating: 64it [00:40,  1.71it/s]Extractor Estimating: 65it [00:41,  1.56it/s]Extractor Estimating: 66it [00:41,  1.61it/s]Extractor Estimating: 67it [00:42,  1.65it/s]Extractor Estimating: 68it [00:42,  1.67it/s]Extractor Estimating: 69it [00:43,  1.65it/s]Extractor Estimating: 70it [00:44,  1.66it/s]Extractor Estimating: 71it [00:44,  1.67it/s]Extractor Estimating: 72it [00:45,  1.65it/s]Extractor Estimating: 73it [00:45,  1.68it/s]Extractor Estimating: 74it [00:46,  1.62it/s]Extractor Estimating: 75it [00:47,  1.69it/s]Extractor Estimating: 76it [00:48,  1.43it/s]Extractor Estimating: 77it [00:48,  1.43it/s]Extractor Estimating: 78it [00:49,  1.50it/s]Extractor Estimating: 79it [00:50,  1.51it/s]Extractor Estimating: 80it [00:50,  1.54it/s]Extractor Estimating: 81it [00:51,  1.54it/s]Extractor Estimating: 82it [00:52,  1.45it/s]Extractor Estimating: 83it [00:52,  1.44it/s]Extractor Estimating: 84it [00:53,  1.44it/s]Extractor Estimating: 85it [00:54,  1.49it/s]Extractor Estimating: 86it [00:54,  1.50it/s]Extractor Estimating: 87it [00:55,  1.53it/s]Extractor Estimating: 88it [00:56,  1.54it/s]Extractor Estimating: 89it [00:56,  1.57it/s]Extractor Estimating: 90it [00:57,  1.53it/s]Extractor Estimating: 91it [00:57,  1.57it/s]Extractor Estimating: 92it [00:58,  1.53it/s]Extractor Estimating: 93it [00:59,  1.55it/s]Extractor Estimating: 94it [00:59,  1.54it/s]Extractor Estimating: 95it [01:00,  1.52it/s]Extractor Estimating: 96it [01:01,  1.51it/s]Extractor Estimating: 97it [01:01,  1.46it/s]Extractor Estimating: 98it [01:02,  1.49it/s]Extractor Estimating: 99it [01:03,  1.53it/s]Extractor Estimating: 100it [01:03,  1.53it/s]Extractor Estimating: 101it [01:04,  1.58it/s]Extractor Estimating: 102it [01:05,  1.55it/s]Extractor Estimating: 103it [01:05,  1.62it/s]Extractor Estimating: 104it [01:06,  1.64it/s]Extractor Estimating: 105it [01:06,  1.71it/s]Extractor Estimating: 106it [01:07,  1.70it/s]Extractor Estimating: 107it [01:08,  1.67it/s]Extractor Estimating: 108it [01:08,  1.67it/s]Extractor Estimating: 109it [01:09,  1.65it/s]Extractor Estimating: 110it [01:09,  1.63it/s]Extractor Estimating: 111it [01:10,  1.63it/s]Extractor Estimating: 112it [01:11,  1.63it/s]Extractor Estimating: 113it [01:11,  1.65it/s]Extractor Estimating: 114it [01:12,  1.70it/s]Extractor Estimating: 115it [01:12,  1.70it/s]Extractor Estimating: 116it [01:13,  1.67it/s]Extractor Estimating: 117it [01:14,  1.64it/s]Extractor Estimating: 118it [01:14,  1.65it/s]Extractor Estimating: 119it [01:15,  1.67it/s]Extractor Estimating: 120it [01:15,  1.67it/s]Extractor Estimating: 121it [01:16,  1.62it/s]Extractor Estimating: 122it [01:17,  1.67it/s]Extractor Estimating: 123it [01:17,  1.70it/s]Extractor Estimating: 124it [01:18,  1.66it/s]Extractor Estimating: 125it [01:18,  1.61it/s]Extractor Estimating: 126it [01:19,  1.59it/s]Extractor Estimating: 127it [01:20,  1.58it/s]Extractor Estimating: 128it [01:20,  1.56it/s]Extractor Estimating: 129it [01:21,  1.58it/s]Extractor Estimating: 130it [01:22,  1.63it/s]Extractor Estimating: 131it [01:22,  1.60it/s]Extractor Estimating: 132it [01:23,  1.59it/s]Extractor Estimating: 133it [01:23,  1.58it/s]Extractor Estimating: 134it [01:24,  1.58it/s]Extractor Estimating: 135it [01:25,  1.59it/s]Extractor Estimating: 136it [01:25,  1.55it/s]Extractor Estimating: 137it [01:26,  1.53it/s]Extractor Estimating: 138it [01:27,  1.49it/s]Extractor Estimating: 139it [01:27,  1.51it/s]Extractor Estimating: 140it [01:28,  1.53it/s]Extractor Estimating: 141it [01:29,  1.54it/s]Extractor Estimating: 142it [01:29,  1.56it/s]Extractor Estimating: 143it [01:30,  1.55it/s]Extractor Estimating: 144it [01:31,  1.56it/s]Extractor Estimating: 145it [01:31,  1.60it/s]Extractor Estimating: 146it [01:32,  1.58it/s]Extractor Estimating: 147it [01:32,  1.60it/s]Extractor Estimating: 148it [01:33,  1.52it/s]Extractor Estimating: 149it [01:34,  1.50it/s]Extractor Estimating: 150it [01:35,  1.40it/s]Extractor Estimating: 151it [01:35,  1.41it/s]Extractor Estimating: 152it [01:36,  1.46it/s]Extractor Estimating: 153it [01:37,  1.49it/s]Extractor Estimating: 154it [01:37,  1.55it/s]Extractor Estimating: 155it [01:38,  1.57it/s]Extractor Estimating: 156it [01:39,  1.51it/s]Extractor Estimating: 157it [01:39,  1.53it/s]Extractor Estimating: 158it [01:40,  1.56it/s]Extractor Estimating: 159it [01:40,  1.60it/s]Extractor Estimating: 160it [01:41,  1.64it/s]Extractor Estimating: 161it [01:42,  1.64it/s]Extractor Estimating: 162it [01:42,  1.56it/s]Extractor Estimating: 163it [01:43,  1.55it/s]Extractor Estimating: 164it [01:44,  1.51it/s]Extractor Estimating: 165it [01:44,  1.54it/s]Extractor Estimating: 166it [01:45,  1.54it/s]Extractor Estimating: 167it [01:46,  1.52it/s]Extractor Estimating: 168it [01:46,  1.56it/s]Extractor Estimating: 169it [01:47,  1.61it/s]Extractor Estimating: 170it [01:47,  1.64it/s]Extractor Estimating: 171it [01:48,  1.67it/s]Extractor Estimating: 172it [01:49,  1.64it/s]Extractor Estimating: 173it [01:49,  1.59it/s]Extractor Estimating: 174it [01:50,  1.57it/s]Extractor Estimating: 175it [01:51,  1.56it/s]Extractor Estimating: 176it [01:51,  1.57it/s]Extractor Estimating: 177it [01:52,  1.60it/s]Extractor Estimating: 178it [01:52,  1.63it/s]Extractor Estimating: 179it [01:53,  1.66it/s]Extractor Estimating: 180it [01:54,  1.68it/s]Extractor Estimating: 181it [01:54,  1.68it/s]Extractor Estimating: 182it [01:55,  1.68it/s]Extractor Estimating: 183it [01:55,  1.62it/s]Extractor Estimating: 184it [01:56,  1.63it/s]Extractor Estimating: 185it [01:57,  1.58it/s]Extractor Estimating: 186it [01:57,  1.62it/s]Extractor Estimating: 187it [01:58,  1.57it/s]Extractor Estimating: 188it [01:59,  1.61it/s]Extractor Estimating: 189it [01:59,  1.58it/s]Extractor Estimating: 190it [02:00,  1.60it/s]Extractor Estimating: 191it [02:00,  1.65it/s]Extractor Estimating: 192it [02:01,  1.65it/s]Extractor Estimating: 193it [02:02,  1.64it/s]Extractor Estimating: 194it [02:02,  1.54it/s]Extractor Estimating: 195it [02:03,  1.60it/s]Extractor Estimating: 196it [02:03,  1.63it/s]Extractor Estimating: 197it [02:04,  1.64it/s]Extractor Estimating: 198it [02:05,  1.64it/s]Extractor Estimating: 199it [02:05,  1.63it/s]Extractor Estimating: 200it [02:06,  1.61it/s]Extractor Estimating: 201it [02:07,  1.55it/s]Extractor Estimating: 202it [02:07,  1.57it/s]Extractor Estimating: 203it [02:08,  1.57it/s]Extractor Estimating: 204it [02:09,  1.60it/s]Extractor Estimating: 205it [02:09,  1.63it/s]Extractor Estimating: 206it [02:10,  1.63it/s]Extractor Estimating: 207it [02:10,  1.59it/s]Extractor Estimating: 208it [02:11,  1.60it/s]Extractor Estimating: 209it [02:12,  1.59it/s]Extractor Estimating: 210it [02:12,  1.55it/s]Extractor Estimating: 211it [02:13,  1.58it/s]Extractor Estimating: 212it [02:14,  1.50it/s]Extractor Estimating: 213it [02:14,  1.52it/s]Extractor Estimating: 214it [02:15,  1.55it/s]Extractor Estimating: 215it [02:16,  1.57it/s]Extractor Estimating: 216it [02:16,  1.60it/s]Extractor Estimating: 217it [02:17,  1.61it/s]Extractor Estimating: 218it [02:17,  1.62it/s]Extractor Estimating: 219it [02:18,  1.62it/s]Extractor Estimating: 220it [02:19,  1.59it/s]Extractor Estimating: 221it [02:19,  1.58it/s]Extractor Estimating: 222it [02:20,  1.60it/s]Extractor Estimating: 223it [02:21,  1.55it/s]Extractor Estimating: 224it [02:21,  1.52it/s]Extractor Estimating: 225it [02:22,  1.56it/s]Extractor Estimating: 226it [02:23,  1.52it/s]Extractor Estimating: 227it [02:23,  1.55it/s]Extractor Estimating: 228it [02:24,  1.57it/s]Extractor Estimating: 229it [02:24,  1.60it/s]Extractor Estimating: 230it [02:25,  1.62it/s]Extractor Estimating: 231it [02:26,  1.59it/s]Extractor Estimating: 232it [02:26,  1.59it/s]Extractor Estimating: 233it [02:27,  1.59it/s]Extractor Estimating: 234it [02:28,  1.59it/s]Extractor Estimating: 235it [02:28,  1.42it/s]Extractor Estimating: 236it [02:29,  1.50it/s]Extractor Estimating: 237it [02:30,  1.51it/s]Extractor Estimating: 238it [02:30,  1.51it/s]Extractor Estimating: 239it [02:31,  1.54it/s]Extractor Estimating: 240it [02:32,  1.54it/s]Extractor Estimating: 241it [02:32,  1.52it/s]Extractor Estimating: 242it [02:33,  1.56it/s]Extractor Estimating: 243it [02:33,  1.56it/s]Extractor Estimating: 244it [02:34,  1.57it/s]Extractor Estimating: 245it [02:35,  1.57it/s]Extractor Estimating: 246it [02:35,  1.55it/s]Extractor Estimating: 247it [02:36,  1.54it/s]Extractor Estimating: 248it [02:37,  1.57it/s]Extractor Estimating: 249it [02:37,  1.57it/s]Extractor Estimating: 250it [02:38,  1.46it/s]Extractor Estimating: 250it [02:38,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:46,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:46,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:46,198 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:46,198 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:46,198 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:56:46,801 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:56:46,802 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:56:47,387 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:56:48,433 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:56:48,433 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:51,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:51,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:51,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:51,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:51,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:56:52,545 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:56:52,546 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:56:53,181 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:56:53,422 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:56:53,422 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 08:39:00,629 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 08:39:00,653 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5099 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 22226
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22326, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22326, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.043, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.073, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.060, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.049, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 3.081, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.077, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.049, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.067, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.063, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 3.038, loss:nan
g_step 1200, step 135, avg_time 1.056, loss:nan
g_step 1300, step 22, avg_time 1.063, loss:nan
g_step 1400, step 122, avg_time 1.069, loss:nan
g_step 1500, step 9, avg_time 1.059, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 3.057, loss:nan
g_step 1700, step 209, avg_time 1.070, loss:nan
g_step 1800, step 96, avg_time 1.068, loss:nan
g_step 1900, step 196, avg_time 1.055, loss:nan
g_step 2000, step 83, avg_time 1.077, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 3.051, loss:nan
g_step 2200, step 70, avg_time 1.054, loss:nan
g_step 2300, step 170, avg_time 1.081, loss:nan
g_step 2400, step 57, avg_time 1.057, loss:nan
g_step 2500, step 157, avg_time 1.066, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 3.052, loss:nan
g_step 2700, step 144, avg_time 1.066, loss:nan
g_step 2800, step 31, avg_time 1.047, loss:nan
g_step 2900, step 131, avg_time 1.049, loss:nan
g_step 3000, step 18, avg_time 1.076, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 3.052, loss:nan
g_step 3200, step 5, avg_time 1.069, loss:nan
g_step 3300, step 105, avg_time 1.067, loss:nan
g_step 3400, step 205, avg_time 1.050, loss:nan
g_step 3500, step 92, avg_time 1.055, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 3.064, loss:nan
g_step 3700, step 79, avg_time 1.063, loss:nan
g_step 3800, step 179, avg_time 1.058, loss:nan
g_step 3900, step 66, avg_time 1.062, loss:nan
g_step 4000, step 166, avg_time 1.040, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 3.079, loss:nan
g_step 4200, step 153, avg_time 1.068, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 08:39:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 08:39:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_08-39-00_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 08:39:01 - WARNING - datasets.builder -   Using custom data configuration default-d32a49066224bee5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d32a49066224bee5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 08:39:02,797 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:39:02,798 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 08:39:02,798 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:39:02,799 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 08:39:02,903 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:39:02,968 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:39:02,969 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:39:02,969 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:39:02,969 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:39:02,969 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:39:02,969 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 08:39:03,387 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 08:39:06,522 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 08:39:06,540 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d32a49066224bee5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.64ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.58ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.22ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.67ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.97ba/s]100%|██████████| 6/6 [00:01<00:00,  4.31ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.30ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.95ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.20ba/s] 44%|████▍     | 4/9 [00:00<00:01,  4.34ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.43ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.46ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.38ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.44ba/s]100%|██████████| 9/9 [00:02<00:00,  4.99ba/s]100%|██████████| 9/9 [00:02<00:00,  4.50ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.18ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.20ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.15ba/s]100%|██████████| 6/6 [00:00<00:00, 11.31ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  5.48ba/s] 33%|███▎      | 3/9 [00:00<00:00,  8.66ba/s] 56%|█████▌    | 5/9 [00:00<00:00,  9.70ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.18ba/s]100%|██████████| 9/9 [00:00<00:00, 11.03ba/s]100%|██████████| 9/9 [00:00<00:00, 10.12ba/s]
[INFO|trainer.py:414] 2023-08-28 08:39:12,553 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 08:39:12,619 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 08:39:12,677 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 08:39:12,677 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 08:39:12,677 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 08:39:12,677 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 08:39:12,677 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 08:39:12,677 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:01<07:33,  1.14s/it]  0%|          | 2/400 [00:01<05:24,  1.22it/s]  1%|          | 3/400 [00:02<04:04,  1.62it/s]  1%|          | 4/400 [00:02<03:22,  1.96it/s]  1%|▏         | 5/400 [00:02<02:48,  2.35it/s]  2%|▏         | 6/400 [00:03<02:27,  2.67it/s]  2%|▏         | 7/400 [00:03<02:46,  2.37it/s]  2%|▏         | 8/400 [00:03<02:27,  2.65it/s]  2%|▏         | 9/400 [00:04<02:35,  2.52it/s]  2%|▎         | 10/400 [00:04<02:21,  2.77it/s]  3%|▎         | 11/400 [00:04<02:10,  2.98it/s]  3%|▎         | 12/400 [00:05<02:03,  3.15it/s]  3%|▎         | 13/400 [00:05<01:58,  3.28it/s]  4%|▎         | 14/400 [00:05<01:54,  3.38it/s]  4%|▍         | 15/400 [00:05<01:51,  3.45it/s]  4%|▍         | 16/400 [00:06<01:49,  3.50it/s]  4%|▍         | 17/400 [00:06<01:54,  3.34it/s]  4%|▍         | 18/400 [00:06<01:51,  3.42it/s]  5%|▍         | 19/400 [00:07<01:49,  3.48it/s]  5%|▌         | 20/400 [00:07<01:47,  3.52it/s]  5%|▌         | 21/400 [00:07<01:54,  3.32it/s]  6%|▌         | 22/400 [00:07<01:51,  3.39it/s]  6%|▌         | 23/400 [00:08<01:49,  3.45it/s]  6%|▌         | 24/400 [00:08<01:47,  3.50it/s]  6%|▋         | 25/400 [00:08<01:45,  3.54it/s]  6%|▋         | 26/400 [00:09<01:44,  3.57it/s]  7%|▋         | 27/400 [00:09<01:44,  3.59it/s]  7%|▋         | 28/400 [00:09<01:47,  3.46it/s]  7%|▋         | 29/400 [00:09<01:45,  3.51it/s]  8%|▊         | 30/400 [00:10<01:44,  3.54it/s]  8%|▊         | 31/400 [00:10<01:43,  3.57it/s]  8%|▊         | 32/400 [00:10<01:42,  3.58it/s]  8%|▊         | 33/400 [00:11<01:42,  3.59it/s]  8%|▊         | 34/400 [00:11<01:41,  3.60it/s]  9%|▉         | 35/400 [00:11<01:41,  3.61it/s]  9%|▉         | 36/400 [00:11<01:40,  3.61it/s]  9%|▉         | 37/400 [00:12<01:40,  3.61it/s] 10%|▉         | 38/400 [00:12<01:40,  3.62it/s] 10%|▉         | 39/400 [00:12<01:43,  3.49it/s] 10%|█         | 40/400 [00:13<01:42,  3.53it/s] 10%|█         | 41/400 [00:13<01:41,  3.55it/s] 10%|█         | 42/400 [00:13<01:40,  3.57it/s] 11%|█         | 43/400 [00:13<01:39,  3.59it/s] 11%|█         | 44/400 [00:14<01:38,  3.60it/s] 11%|█▏        | 45/400 [00:14<01:38,  3.61it/s] 12%|█▏        | 46/400 [00:14<01:38,  3.61it/s] 12%|█▏        | 47/400 [00:14<01:37,  3.61it/s] 12%|█▏        | 48/400 [00:15<01:37,  3.62it/s] 12%|█▏        | 49/400 [00:15<01:36,  3.62it/s] 12%|█▎        | 50/400 [00:15<01:40,  3.48it/s] 13%|█▎        | 51/400 [00:16<01:39,  3.52it/s] 13%|█▎        | 52/400 [00:16<01:38,  3.55it/s] 13%|█▎        | 53/400 [00:16<01:37,  3.57it/s] 14%|█▎        | 54/400 [00:16<01:36,  3.59it/s] 14%|█▍        | 55/400 [00:17<01:35,  3.60it/s] 14%|█▍        | 56/400 [00:17<01:35,  3.60it/s] 14%|█▍        | 57/400 [00:17<01:35,  3.61it/s] 14%|█▍        | 58/400 [00:18<01:34,  3.62it/s] 15%|█▍        | 59/400 [00:18<01:34,  3.62it/s] 15%|█▌        | 60/400 [00:18<01:34,  3.61it/s] 15%|█▌        | 61/400 [00:18<01:37,  3.48it/s] 16%|█▌        | 62/400 [00:19<01:36,  3.52it/s] 16%|█▌        | 63/400 [00:19<01:34,  3.55it/s] 16%|█▌        | 64/400 [00:19<01:34,  3.57it/s] 16%|█▋        | 65/400 [00:19<01:33,  3.59it/s] 16%|█▋        | 66/400 [00:20<01:32,  3.59it/s] 17%|█▋        | 67/400 [00:20<01:32,  3.60it/s] 17%|█▋        | 68/400 [00:20<01:32,  3.60it/s] 17%|█▋        | 69/400 [00:21<01:31,  3.61it/s] 18%|█▊        | 70/400 [00:21<01:31,  3.61it/s] 18%|█▊        | 71/400 [00:21<01:31,  3.61it/s] 18%|█▊        | 72/400 [00:21<01:34,  3.46it/s] 18%|█▊        | 73/400 [00:22<01:33,  3.50it/s] 18%|█▊        | 74/400 [00:22<01:32,  3.53it/s] 19%|█▉        | 75/400 [00:22<01:31,  3.56it/s] 19%|█▉        | 76/400 [00:23<01:30,  3.58it/s] 19%|█▉        | 77/400 [00:23<01:30,  3.59it/s] 20%|█▉        | 78/400 [00:23<01:29,  3.60it/s] 20%|█▉        | 79/400 [00:23<01:29,  3.60it/s] 20%|██        | 80/400 [00:24<01:21,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 08:39:36,778 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:39:36,778 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 08:39:36,778 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.95it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.68it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.67it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.86it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.13it/s][A
  3%|▎         | 33/1083 [00:00<00:22, 45.74it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.20it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 45.03it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.11it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.28it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.38it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.43it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.51it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.33it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.23it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 45.08it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.10it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.46it/s][A
  9%|▉         | 98/1083 [00:02<00:22, 44.77it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 44.92it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.14it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.21it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.29it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.11it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.82it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.95it/s][A
 13%|█▎        | 138/1083 [00:03<00:20, 45.03it/s][A
 13%|█▎        | 143/1083 [00:03<00:20, 45.18it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 45.21it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 45.35it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.33it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.31it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.14it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 45.03it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.91it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.96it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 45.19it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 45.17it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.33it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.29it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.29it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.11it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.06it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.87it/s][A
 21%|██        | 228/1083 [00:05<00:19, 43.52it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.11it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.58it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.82it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.94it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.01it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.88it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 44.88it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 44.75it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.86it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 45.03it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 45.14it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.25it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.35it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.25it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.11it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.99it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 44.94it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.94it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.05it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.21it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.30it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.30it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.31it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.08it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.97it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 44.88it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.26it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 44.57it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.80it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.93it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.10it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.04it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.95it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 44.87it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 44.81it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.94it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 45.01it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 45.19it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 45.24it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 45.33it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.15it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.04it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.96it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.89it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.99it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 44.97it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.00it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.10it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.16it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.01it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 44.98it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.95it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 44.82it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 41.99it/s][A
 46%|████▋     | 503/1083 [00:11<00:13, 42.99it/s][A
 47%|████▋     | 508/1083 [00:11<00:13, 43.79it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.35it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 44.62it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 44.92it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 44.86it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 44.83it/s][A
 50%|████▉     | 538/1083 [00:11<00:12, 44.56it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.68it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 44.74it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.96it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 45.18it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 45.24it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 45.27it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 45.28it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 45.15it/s][A
 54%|█████▍    | 583/1083 [00:12<00:11, 43.27it/s][A
 54%|█████▍    | 588/1083 [00:13<00:10, 45.03it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.98it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.12it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.11it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.25it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.25it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.26it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.05it/s][A
 58%|█████▊    | 628/1083 [00:13<00:10, 44.85it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.24it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.58it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.84it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.91it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.12it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.17it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 45.09it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 44.92it/s][A
 62%|██████▏   | 673/1083 [00:14<00:09, 44.71it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.80it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.88it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 45.07it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 45.17it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 45.27it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 45.32it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 45.19it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.02it/s][A
 66%|██████▋   | 718/1083 [00:15<00:08, 44.76it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.74it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.88it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.95it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.09it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.17it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.36it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.31it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 45.18it/s][A
 70%|███████   | 763/1083 [00:16<00:07, 45.03it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 43.12it/s][A
 71%|███████▏  | 773/1083 [00:17<00:07, 43.89it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.26it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.63it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.67it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.04it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 45.09it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 45.00it/s][A
 75%|███████▍  | 808/1083 [00:17<00:06, 44.83it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.80it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.92it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 45.04it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 45.20it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.22it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 45.26it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 45.16it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 45.06it/s][A
 79%|███████▉  | 853/1083 [00:18<00:05, 44.90it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.78it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.90it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.97it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 45.20it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.26it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 45.37it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 45.15it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 45.06it/s][A
 83%|████████▎ | 898/1083 [00:19<00:04, 44.91it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 42.02it/s][A
 84%|████████▍ | 908/1083 [00:20<00:04, 42.94it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 43.65it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.18it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.59it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 44.92it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 45.04it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 45.00it/s][A
 87%|████████▋ | 943/1083 [00:20<00:03, 44.77it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.67it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.77it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.91it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 45.06it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 45.17it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 45.31it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 45.31it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 45.17it/s][A
 91%|█████████ | 988/1083 [00:21<00:02, 44.91it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.69it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.72it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.81it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 45.01it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 45.13it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 45.27it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 45.35it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 45.20it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 45.02it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 43.28it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 43.76it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.22it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.46it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.79it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.95it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 45.18it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 45.04it/s][A
100%|█████████▉| 1078/1083 [00:23<00:00, 44.74it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.64it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:24<00:00, 44.64it/s][A 20%|██        | 80/400 [00:48<01:21,  3.94it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 08:40:01,161 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 08:40:01,323 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:40:04,127 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:40:04,294 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:40:04,358 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:52<46:35,  8.76s/it] 20%|██        | 82/400 [00:52<32:57,  6.22s/it] 21%|██        | 83/400 [00:53<23:26,  4.44s/it] 21%|██        | 84/400 [00:53<16:48,  3.19s/it] 21%|██▏       | 85/400 [00:53<12:09,  2.32s/it] 22%|██▏       | 86/400 [00:54<08:55,  1.71s/it] 22%|██▏       | 87/400 [00:54<06:40,  1.28s/it] 22%|██▏       | 88/400 [00:54<05:05,  1.02it/s] 22%|██▏       | 89/400 [00:54<03:59,  1.30it/s] 22%|██▎       | 90/400 [00:55<03:13,  1.61it/s] 23%|██▎       | 91/400 [00:55<02:40,  1.92it/s] 23%|██▎       | 92/400 [00:55<02:18,  2.23it/s] 23%|██▎       | 93/400 [00:56<02:02,  2.51it/s] 24%|██▎       | 94/400 [00:56<01:50,  2.76it/s] 24%|██▍       | 95/400 [00:56<01:43,  2.96it/s] 24%|██▍       | 96/400 [00:56<01:41,  3.00it/s] 24%|██▍       | 97/400 [00:57<01:35,  3.16it/s] 24%|██▍       | 98/400 [00:57<01:31,  3.29it/s] 25%|██▍       | 99/400 [00:57<01:29,  3.38it/s] 25%|██▌       | 100/400 [00:58<01:27,  3.45it/s] 25%|██▌       | 101/400 [00:58<01:25,  3.49it/s] 26%|██▌       | 102/400 [00:58<01:24,  3.51it/s] 26%|██▌       | 103/400 [00:58<01:24,  3.52it/s] 26%|██▌       | 104/400 [00:59<01:23,  3.53it/s] 26%|██▋       | 105/400 [00:59<01:23,  3.54it/s] 26%|██▋       | 106/400 [00:59<01:22,  3.55it/s] 27%|██▋       | 107/400 [01:00<01:24,  3.47it/s] 27%|██▋       | 108/400 [01:00<01:23,  3.50it/s] 27%|██▋       | 109/400 [01:00<01:22,  3.52it/s] 28%|██▊       | 110/400 [01:00<01:22,  3.53it/s] 28%|██▊       | 111/400 [01:01<01:21,  3.54it/s] 28%|██▊       | 112/400 [01:01<01:21,  3.55it/s] 28%|██▊       | 113/400 [01:01<01:20,  3.57it/s] 28%|██▊       | 114/400 [01:02<01:19,  3.58it/s] 29%|██▉       | 115/400 [01:02<01:19,  3.59it/s] 29%|██▉       | 116/400 [01:02<01:18,  3.60it/s] 29%|██▉       | 117/400 [01:02<01:18,  3.61it/s] 30%|██▉       | 118/400 [01:03<01:20,  3.49it/s] 30%|██▉       | 119/400 [01:03<01:32,  3.04it/s] 30%|███       | 120/400 [01:03<01:27,  3.20it/s] 30%|███       | 121/400 [01:04<01:37,  2.86it/s] 30%|███       | 122/400 [01:04<01:31,  3.04it/s] 31%|███       | 123/400 [01:04<01:26,  3.19it/s] 31%|███       | 124/400 [01:05<01:23,  3.31it/s] 31%|███▏      | 125/400 [01:05<01:20,  3.40it/s] 32%|███▏      | 126/400 [01:05<01:19,  3.46it/s] 32%|███▏      | 127/400 [01:05<01:17,  3.51it/s] 32%|███▏      | 128/400 [01:06<01:19,  3.42it/s] 32%|███▏      | 129/400 [01:06<01:17,  3.48it/s] 32%|███▎      | 130/400 [01:06<01:16,  3.52it/s] 33%|███▎      | 131/400 [01:07<01:15,  3.55it/s] 33%|███▎      | 132/400 [01:07<01:15,  3.57it/s] 33%|███▎      | 133/400 [01:07<01:22,  3.25it/s] 34%|███▎      | 134/400 [01:08<01:19,  3.33it/s] 34%|███▍      | 135/400 [01:08<01:17,  3.42it/s] 34%|███▍      | 136/400 [01:08<01:16,  3.47it/s] 34%|███▍      | 137/400 [01:08<01:14,  3.52it/s] 34%|███▍      | 138/400 [01:09<01:13,  3.54it/s] 35%|███▍      | 139/400 [01:09<01:16,  3.42it/s] 35%|███▌      | 140/400 [01:09<01:14,  3.48it/s] 35%|███▌      | 141/400 [01:09<01:13,  3.52it/s] 36%|███▌      | 142/400 [01:10<01:12,  3.55it/s] 36%|███▌      | 143/400 [01:10<01:12,  3.57it/s] 36%|███▌      | 144/400 [01:10<01:11,  3.58it/s] 36%|███▋      | 145/400 [01:11<01:11,  3.59it/s] 36%|███▋      | 146/400 [01:11<01:10,  3.60it/s] 37%|███▋      | 147/400 [01:11<01:10,  3.61it/s] 37%|███▋      | 148/400 [01:11<01:09,  3.61it/s] 37%|███▋      | 149/400 [01:12<01:09,  3.61it/s] 38%|███▊      | 150/400 [01:12<01:12,  3.43it/s] 38%|███▊      | 151/400 [01:12<01:11,  3.48it/s] 38%|███▊      | 152/400 [01:13<01:10,  3.52it/s] 38%|███▊      | 153/400 [01:13<01:09,  3.55it/s] 38%|███▊      | 154/400 [01:13<01:08,  3.57it/s] 39%|███▉      | 155/400 [01:13<01:08,  3.58it/s] 39%|███▉      | 156/400 [01:14<01:07,  3.59it/s] 39%|███▉      | 157/400 [01:14<01:07,  3.60it/s] 40%|███▉      | 158/400 [01:14<01:07,  3.60it/s] 40%|███▉      | 159/400 [01:15<01:06,  3.61it/s] 40%|████      | 160/400 [01:15<01:00,  3.95it/s][INFO|trainer.py:2140] 2023-08-28 08:40:27,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:40:27,934 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 08:40:27,934 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.1148, 'eval_samples_per_second': 359.033, 'eval_steps_per_second': 44.91, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 57.08it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.68it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.77it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 47.03it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.59it/s][A
  3%|▎         | 33/1083 [00:00<00:22, 46.07it/s][A
  4%|▎         | 38/1083 [00:00<00:22, 45.70it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 45.15it/s][A
  4%|▍         | 48/1083 [00:01<00:23, 44.81it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 44.82it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 44.84it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.14it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.28it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.40it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.41it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 45.28it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.88it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.80it/s][A
  9%|▉         | 98/1083 [00:02<00:22, 44.75it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 44.88it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.02it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.22it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.33it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.31it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 45.10it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.88it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.76it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 43.04it/s][A
 14%|█▎        | 148/1083 [00:03<00:21, 43.78it/s][A
 14%|█▍        | 153/1083 [00:03<00:21, 44.18it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 44.53it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 44.75it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.02it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 45.10it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 45.08it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.76it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 44.76it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.98it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.02it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.04it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.08it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.17it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.18it/s][A
 21%|██        | 223/1083 [00:04<00:19, 45.09it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.78it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.79it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.83it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.98it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.06it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.15it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.22it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.25it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 45.22it/s][A
 25%|██▌       | 273/1083 [00:06<00:17, 45.06it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 43.59it/s][A
 26%|██▌       | 283/1083 [00:06<00:18, 44.06it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.49it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.71it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.90it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 44.97it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.08it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 45.07it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.85it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.90it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.00it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.08it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.14it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 45.19it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 45.00it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.82it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 44.75it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.67it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.76it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.85it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.08it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.15it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 45.14it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 45.05it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.89it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 43.09it/s][A
 39%|███▊      | 418/1083 [00:09<00:15, 43.77it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.23it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.55it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.83it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.02it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.98it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.98it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.69it/s][A
 42%|████▏     | 458/1083 [00:10<00:14, 44.61it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.80it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.97it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.15it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.14it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.27it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 45.14it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 45.00it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.86it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.72it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.92it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.08it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.27it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 45.28it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 45.22it/s][A
 50%|████▉     | 538/1083 [00:11<00:12, 44.98it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.91it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 43.54it/s][A
 51%|█████     | 553/1083 [00:12<00:12, 44.07it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.42it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.72it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.95it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 45.03it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 45.14it/s][A
 54%|█████▍    | 583/1083 [00:12<00:11, 45.02it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.84it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.78it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.78it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.00it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.15it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.14it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.11it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.18it/s][A
 58%|█████▊    | 628/1083 [00:13<00:10, 45.07it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.88it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.82it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.87it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.99it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.06it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.14it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 45.16it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 45.19it/s][A
 62%|██████▏   | 673/1083 [00:14<00:09, 45.07it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.97it/s][A
 63%|██████▎   | 683/1083 [00:15<00:09, 43.17it/s][A
 64%|██████▎   | 688/1083 [00:15<00:09, 43.86it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.34it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.73it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.85it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 45.02it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.09it/s][A
 66%|██████▋   | 718/1083 [00:15<00:08, 45.06it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.82it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.70it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.95it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.04it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.24it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.25it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.28it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 45.14it/s][A
 70%|███████   | 763/1083 [00:16<00:07, 45.00it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.82it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.80it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.87it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.98it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.17it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.26it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 45.24it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 45.13it/s][A
 75%|███████▍  | 808/1083 [00:17<00:06, 45.02it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.87it/s][A
 76%|███████▌  | 818/1083 [00:18<00:06, 43.17it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 43.78it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.33it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.62it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.90it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.97it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 45.09it/s][A
 79%|███████▉  | 853/1083 [00:18<00:05, 45.02it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.69it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.75it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.77it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.99it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.09it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 45.22it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 45.19it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 45.24it/s][A
 83%|████████▎ | 898/1083 [00:19<00:04, 45.07it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.73it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.80it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.83it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.95it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.08it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.12it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 45.17it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 45.06it/s][A
 87%|████████▋ | 943/1083 [00:20<00:03, 45.00it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.73it/s][A
 88%|████████▊ | 953/1083 [00:21<00:03, 41.87it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 42.90it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 43.66it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.14it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.46it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 44.76it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 41.46it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 42.46it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 43.20it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 43.80it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.30it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.62it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.85it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 45.03it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 44.81it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 44.85it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.97it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.98it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 45.03it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 45.07it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 45.13it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 45.21it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 45.12it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.91it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 44.85it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.88it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.96it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.96it/s][A 40%|████      | 160/400 [01:39<01:00,  3.95it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 08:40:52,220 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 08:40:52,366 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:40:55,194 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:40:55,317 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:40:55,382 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:43<34:45,  8.72s/it] 40%|████      | 162/400 [01:43<24:33,  6.19s/it] 41%|████      | 163/400 [01:44<17:26,  4.42s/it] 41%|████      | 164/400 [01:44<12:29,  3.18s/it] 41%|████▏     | 165/400 [01:44<09:01,  2.31s/it] 42%|████▏     | 166/400 [01:45<06:37,  1.70s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:45<04:56,  1.27s/it] 42%|████▏     | 168/400 [01:45<03:45,  1.03it/s] 42%|████▏     | 169/400 [01:45<02:57,  1.30it/s] 42%|████▎     | 170/400 [01:46<02:22,  1.61it/s] 43%|████▎     | 171/400 [01:46<01:58,  1.93it/s] 43%|████▎     | 172/400 [01:46<01:41,  2.24it/s] 43%|████▎     | 173/400 [01:47<01:29,  2.53it/s] 44%|████▎     | 174/400 [01:47<01:21,  2.78it/s] 44%|████▍     | 175/400 [01:47<01:15,  2.99it/s] 44%|████▍     | 176/400 [01:47<01:11,  3.15it/s] 44%|████▍     | 177/400 [01:48<01:08,  3.28it/s] 44%|████▍     | 178/400 [01:48<01:05,  3.37it/s] 45%|████▍     | 179/400 [01:48<01:04,  3.44it/s] 45%|████▌     | 180/400 [01:49<01:03,  3.45it/s] 45%|████▌     | 181/400 [01:49<01:02,  3.50it/s] 46%|████▌     | 182/400 [01:49<01:01,  3.53it/s] 46%|████▌     | 183/400 [01:49<01:01,  3.56it/s] 46%|████▌     | 184/400 [01:50<01:00,  3.58it/s] 46%|████▋     | 185/400 [01:50<00:59,  3.59it/s] 46%|████▋     | 186/400 [01:50<00:59,  3.60it/s] 47%|████▋     | 187/400 [01:50<00:59,  3.60it/s] 47%|████▋     | 188/400 [01:51<00:58,  3.61it/s] 47%|████▋     | 189/400 [01:51<00:58,  3.61it/s] 48%|████▊     | 190/400 [01:51<00:58,  3.62it/s] 48%|████▊     | 191/400 [01:52<00:58,  3.58it/s] 48%|████▊     | 192/400 [01:52<00:57,  3.59it/s] 48%|████▊     | 193/400 [01:52<00:57,  3.60it/s] 48%|████▊     | 194/400 [01:52<00:57,  3.61it/s] 49%|████▉     | 195/400 [01:53<00:56,  3.61it/s] 49%|████▉     | 196/400 [01:53<00:56,  3.61it/s] 49%|████▉     | 197/400 [01:53<00:56,  3.61it/s] 50%|████▉     | 198/400 [01:53<00:55,  3.61it/s] 50%|████▉     | 199/400 [01:54<00:55,  3.62it/s] 50%|█████     | 200/400 [01:54<00:55,  3.62it/s] 50%|█████     | 201/400 [01:54<00:55,  3.62it/s] 50%|█████     | 202/400 [01:55<00:55,  3.59it/s] 51%|█████     | 203/400 [01:55<00:54,  3.60it/s] 51%|█████     | 204/400 [01:55<00:54,  3.60it/s] 51%|█████▏    | 205/400 [01:55<00:54,  3.60it/s] 52%|█████▏    | 206/400 [01:56<00:53,  3.61it/s] 52%|█████▏    | 207/400 [01:56<00:53,  3.61it/s] 52%|█████▏    | 208/400 [01:56<00:53,  3.61it/s] 52%|█████▏    | 209/400 [01:57<00:52,  3.61it/s] 52%|█████▎    | 210/400 [01:57<00:52,  3.61it/s] 53%|█████▎    | 211/400 [01:57<00:52,  3.61it/s] 53%|█████▎    | 212/400 [01:57<00:52,  3.61it/s] 53%|█████▎    | 213/400 [01:58<00:51,  3.61it/s] 54%|█████▎    | 214/400 [01:58<00:51,  3.61it/s] 54%|█████▍    | 215/400 [01:58<00:51,  3.61it/s] 54%|█████▍    | 216/400 [01:58<00:50,  3.61it/s] 54%|█████▍    | 217/400 [01:59<00:50,  3.60it/s] 55%|█████▍    | 218/400 [01:59<00:50,  3.61it/s] 55%|█████▍    | 219/400 [01:59<00:50,  3.61it/s] 55%|█████▌    | 220/400 [02:00<00:49,  3.61it/s] 55%|█████▌    | 221/400 [02:00<00:49,  3.62it/s] 56%|█████▌    | 222/400 [02:00<00:49,  3.62it/s] 56%|█████▌    | 223/400 [02:00<00:48,  3.62it/s] 56%|█████▌    | 224/400 [02:01<00:48,  3.61it/s] 56%|█████▋    | 225/400 [02:01<00:48,  3.60it/s] 56%|█████▋    | 226/400 [02:01<00:48,  3.61it/s] 57%|█████▋    | 227/400 [02:02<00:47,  3.61it/s] 57%|█████▋    | 228/400 [02:02<00:47,  3.61it/s] 57%|█████▋    | 229/400 [02:02<00:47,  3.60it/s] 57%|█████▊    | 230/400 [02:02<00:47,  3.60it/s] 58%|█████▊    | 231/400 [02:03<00:46,  3.60it/s] 58%|█████▊    | 232/400 [02:03<00:46,  3.60it/s] 58%|█████▊    | 233/400 [02:03<00:51,  3.26it/s] 58%|█████▊    | 234/400 [02:04<00:51,  3.23it/s] 59%|█████▉    | 235/400 [02:04<00:56,  2.94it/s] 59%|█████▉    | 236/400 [02:04<00:54,  3.01it/s] 59%|█████▉    | 237/400 [02:05<00:51,  3.17it/s] 60%|█████▉    | 238/400 [02:05<00:49,  3.29it/s] 60%|█████▉    | 239/400 [02:05<00:47,  3.38it/s] 60%|██████    | 240/400 [02:05<00:42,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 08:41:18,536 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:41:18,536 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 08:41:18,536 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.1756, 'eval_samples_per_second': 358.13, 'eval_steps_per_second': 44.797, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.33it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.61it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.46it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.49it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.80it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.40it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.20it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 45.02it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.02it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.08it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.24it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.33it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.42it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.18it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.09it/s][A
  8%|▊         | 83/1083 [00:01<00:27, 36.45it/s][A
  8%|▊         | 88/1083 [00:02<00:25, 39.65it/s][A
  9%|▊         | 93/1083 [00:02<00:24, 41.18it/s][A
  9%|▉         | 98/1083 [00:02<00:23, 42.46it/s][A
 10%|▉         | 103/1083 [00:02<00:22, 43.33it/s][A
 10%|▉         | 108/1083 [00:02<00:22, 44.10it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.51it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.75it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.73it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.41it/s][A
 12%|█▏        | 133/1083 [00:03<00:21, 44.29it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.44it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 44.64it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.93it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 45.19it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.24it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.35it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.23it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.92it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.72it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.60it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.68it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.85it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.03it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.19it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.32it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.31it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.67it/s][A
 21%|██        | 223/1083 [00:05<00:19, 44.54it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.53it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.60it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.73it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.89it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.02it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.02it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.20it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.22it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 45.10it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 45.00it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 44.85it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.87it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.05it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.10it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.18it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.17it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 44.95it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.95it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.81it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.85it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.97it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.08it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.20it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.25it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 43.62it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 44.10it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.36it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 44.47it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.55it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.76it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.88it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.04it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.89it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 44.85it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 45.00it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.92it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 44.92it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.90it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.90it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.92it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.06it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.04it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.89it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 45.08it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.95it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.03it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.92it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.01it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.00it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.08it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.06it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.57it/s][A
 46%|████▌     | 493/1083 [00:11<00:13, 44.76it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.80it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.87it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.89it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.84it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 44.85it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 44.96it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 44.95it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 44.92it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 45.03it/s][A
 50%|█████     | 543/1083 [00:12<00:11, 45.04it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 44.92it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 45.01it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.99it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 45.02it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 45.04it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 44.90it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 44.93it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.97it/s][A
 54%|█████▍    | 588/1083 [00:13<00:10, 45.07it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 45.07it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.04it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.03it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.99it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.02it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.01it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 44.40it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.55it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.80it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.82it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.99it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.98it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.89it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 44.86it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 44.90it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 44.97it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.87it/s][A
 63%|██████▎   | 678/1083 [00:15<00:08, 45.06it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 45.04it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 45.15it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 45.09it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 45.06it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.83it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.88it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 44.95it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.94it/s][A
 67%|██████▋   | 723/1083 [00:16<00:07, 45.13it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.10it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.16it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.16it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.20it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.09it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.97it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 43.06it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 43.65it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.18it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.47it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.76it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.94it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.07it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.02it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.73it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 44.74it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.87it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.95it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 45.11it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 45.20it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 45.12it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.18it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.98it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.87it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 44.84it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.87it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.90it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.07it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 45.16it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 45.27it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.22it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.97it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.88it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 44.77it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.79it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.91it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.96it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 45.09it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 45.22it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.23it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.09it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 44.95it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 44.95it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.95it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.93it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 45.07it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 45.06it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 45.11it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 45.20it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 45.07it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 45.01it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 44.90it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.90it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.96it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 45.04it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 45.10it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 45.13it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 45.14it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 45.01it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 45.01it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 44.97it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 42.18it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 43.09it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 43.77it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.22it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.54it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.81it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.85it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.94it/s][A
 99%|█████████▉| 1073/1083 [00:23<00:00, 44.70it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.65it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.70it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.70it/s][A 60%|██████    | 240/400 [02:30<00:42,  3.76it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 08:41:42,789 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 08:41:42,901 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:41:45,440 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:41:45,539 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:41:45,600 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [02:34<22:56,  8.66s/it] 60%|██████    | 242/400 [02:34<16:10,  6.15s/it] 61%|██████    | 243/400 [02:34<11:28,  4.39s/it] 61%|██████    | 244/400 [02:34<08:12,  3.15s/it] 61%|██████▏   | 245/400 [02:35<05:55,  2.29s/it] 62%|██████▏   | 246/400 [02:35<04:20,  1.69s/it] 62%|██████▏   | 247/400 [02:35<03:13,  1.27s/it] 62%|██████▏   | 248/400 [02:36<02:28,  1.03it/s] 62%|██████▏   | 249/400 [02:36<01:55,  1.30it/s] 62%|██████▎   | 250/400 [02:36<01:33,  1.61it/s] 63%|██████▎   | 251/400 [02:36<01:17,  1.93it/s] 63%|██████▎   | 252/400 [02:37<01:06,  2.23it/s] 63%|██████▎   | 253/400 [02:37<00:58,  2.52it/s] 64%|██████▎   | 254/400 [02:37<00:52,  2.76it/s] 64%|██████▍   | 255/400 [02:38<00:49,  2.96it/s] 64%|██████▍   | 256/400 [02:38<00:46,  3.12it/s] 64%|██████▍   | 257/400 [02:38<00:44,  3.24it/s] 64%|██████▍   | 258/400 [02:38<00:42,  3.33it/s] 65%|██████▍   | 259/400 [02:39<00:42,  3.31it/s] 65%|██████▌   | 260/400 [02:39<00:41,  3.39it/s] 65%|██████▌   | 261/400 [02:39<00:40,  3.44it/s] 66%|██████▌   | 262/400 [02:40<00:39,  3.48it/s] 66%|██████▌   | 263/400 [02:40<00:39,  3.50it/s] 66%|██████▌   | 264/400 [02:40<00:38,  3.52it/s] 66%|██████▋   | 265/400 [02:40<00:38,  3.54it/s] 66%|██████▋   | 266/400 [02:41<00:37,  3.55it/s] 67%|██████▋   | 267/400 [02:41<00:37,  3.56it/s] 67%|██████▋   | 268/400 [02:41<00:37,  3.56it/s] 67%|██████▋   | 269/400 [02:41<00:36,  3.56it/s] 68%|██████▊   | 270/400 [02:42<00:37,  3.43it/s] 68%|██████▊   | 271/400 [02:42<00:37,  3.47it/s] 68%|██████▊   | 272/400 [02:42<00:36,  3.50it/s] 68%|██████▊   | 273/400 [02:43<00:36,  3.52it/s] 68%|██████▊   | 274/400 [02:43<00:35,  3.53it/s] 69%|██████▉   | 275/400 [02:43<00:35,  3.54it/s] 69%|██████▉   | 276/400 [02:43<00:34,  3.55it/s] 69%|██████▉   | 277/400 [02:44<00:34,  3.56it/s] 70%|██████▉   | 278/400 [02:44<00:34,  3.56it/s] 70%|██████▉   | 279/400 [02:44<00:33,  3.56it/s] 70%|███████   | 280/400 [02:45<00:33,  3.56it/s] 70%|███████   | 281/400 [02:45<00:34,  3.43it/s] 70%|███████   | 282/400 [02:45<00:34,  3.47it/s] 71%|███████   | 283/400 [02:45<00:33,  3.50it/s] 71%|███████   | 284/400 [02:46<00:32,  3.52it/s] 71%|███████▏  | 285/400 [02:46<00:32,  3.54it/s] 72%|███████▏  | 286/400 [02:46<00:32,  3.55it/s] 72%|███████▏  | 287/400 [02:47<00:31,  3.55it/s] 72%|███████▏  | 288/400 [02:47<00:31,  3.56it/s] 72%|███████▏  | 289/400 [02:47<00:31,  3.56it/s] 72%|███████▎  | 290/400 [02:47<00:30,  3.57it/s] 73%|███████▎  | 291/400 [02:48<00:30,  3.57it/s] 73%|███████▎  | 292/400 [02:48<00:31,  3.48it/s] 73%|███████▎  | 293/400 [02:48<00:30,  3.50it/s] 74%|███████▎  | 294/400 [02:49<00:30,  3.52it/s] 74%|███████▍  | 295/400 [02:49<00:29,  3.53it/s] 74%|███████▍  | 296/400 [02:49<00:29,  3.54it/s] 74%|███████▍  | 297/400 [02:49<00:29,  3.55it/s] 74%|███████▍  | 298/400 [02:50<00:28,  3.55it/s] 75%|███████▍  | 299/400 [02:50<00:28,  3.55it/s] 75%|███████▌  | 300/400 [02:50<00:28,  3.55it/s] 75%|███████▌  | 301/400 [02:51<00:27,  3.55it/s] 76%|███████▌  | 302/400 [02:51<00:27,  3.55it/s] 76%|███████▌  | 303/400 [02:51<00:28,  3.41it/s] 76%|███████▌  | 304/400 [02:51<00:27,  3.45it/s] 76%|███████▋  | 305/400 [02:52<00:27,  3.49it/s] 76%|███████▋  | 306/400 [02:52<00:26,  3.51it/s] 77%|███████▋  | 307/400 [02:52<00:26,  3.53it/s] 77%|███████▋  | 308/400 [02:53<00:26,  3.54it/s] 77%|███████▋  | 309/400 [02:53<00:25,  3.55it/s] 78%|███████▊  | 310/400 [02:53<00:25,  3.55it/s] 78%|███████▊  | 311/400 [02:53<00:25,  3.56it/s] 78%|███████▊  | 312/400 [02:54<00:24,  3.56it/s] 78%|███████▊  | 313/400 [02:54<00:24,  3.56it/s] 78%|███████▊  | 314/400 [02:54<00:24,  3.50it/s] 79%|███████▉  | 315/400 [02:55<00:24,  3.52it/s] 79%|███████▉  | 316/400 [02:55<00:23,  3.54it/s] 79%|███████▉  | 317/400 [02:55<00:23,  3.54it/s] 80%|███████▉  | 318/400 [02:55<00:23,  3.55it/s] 80%|███████▉  | 319/400 [02:56<00:22,  3.56it/s] 80%|████████  | 320/400 [02:56<00:20,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 08:42:09,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:42:09,047 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 08:42:09,047 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.1797, 'eval_samples_per_second': 358.07, 'eval_steps_per_second': 44.79, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 55.97it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.24it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.41it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.48it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.88it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.43it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.14it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.87it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 45.01it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.25it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.41it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.38it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.27it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.07it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.93it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.84it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.77it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.88it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.01it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.11it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.29it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.23it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.24it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.91it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.84it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.82it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.79it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 45.07it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.56it/s][A
 14%|█▍        | 152/1083 [00:03<00:21, 44.14it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.50it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.73it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.78it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.62it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.66it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.75it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.74it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.86it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.08it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.20it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.27it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.05it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.94it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.82it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.84it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.87it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.91it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.09it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.19it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.29it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.13it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.99it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.84it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.82it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.83it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 43.90it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.35it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.60it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.79it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.95it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.07it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 45.10it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 45.00it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 43.65it/s][A
 30%|███       | 327/1083 [00:07<00:18, 41.99it/s][A
 31%|███       | 332/1083 [00:07<00:18, 40.01it/s][A
 31%|███       | 337/1083 [00:07<00:17, 41.61it/s][A
 32%|███▏      | 342/1083 [00:07<00:17, 42.67it/s][A
 32%|███▏      | 347/1083 [00:07<00:18, 39.65it/s][A
 33%|███▎      | 352/1083 [00:07<00:17, 41.27it/s][A
 33%|███▎      | 357/1083 [00:08<00:25, 28.68it/s][A
 33%|███▎      | 362/1083 [00:08<00:21, 32.88it/s][A
 34%|███▍      | 367/1083 [00:08<00:19, 35.85it/s][A
 34%|███▍      | 372/1083 [00:08<00:18, 38.31it/s][A
 35%|███▍      | 377/1083 [00:08<00:17, 40.22it/s][A
 35%|███▌      | 382/1083 [00:08<00:16, 41.72it/s][A
 36%|███▌      | 387/1083 [00:08<00:16, 42.82it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 43.64it/s][A
 37%|███▋      | 397/1083 [00:09<00:15, 44.04it/s][A
 37%|███▋      | 402/1083 [00:09<00:15, 43.96it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 42.35it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 43.13it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 43.73it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.27it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.52it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.85it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.94it/s][A
 41%|████      | 442/1083 [00:10<00:14, 45.12it/s][A
 41%|████▏     | 447/1083 [00:10<00:14, 44.79it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.71it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.80it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.85it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.01it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.11it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.22it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.20it/s][A
 45%|████▍     | 487/1083 [00:11<00:13, 45.19it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.88it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.79it/s][A
 46%|████▋     | 502/1083 [00:11<00:15, 37.67it/s][A
 47%|████▋     | 507/1083 [00:11<00:14, 39.71it/s][A
 47%|████▋     | 512/1083 [00:11<00:13, 41.23it/s][A
 48%|████▊     | 517/1083 [00:11<00:13, 42.38it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 43.31it/s][A
 49%|████▊     | 527/1083 [00:12<00:12, 43.83it/s][A
 49%|████▉     | 532/1083 [00:12<00:12, 44.08it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 44.54it/s][A
 50%|█████     | 542/1083 [00:12<00:13, 40.66it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 42.04it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 42.91it/s][A
 51%|█████▏    | 557/1083 [00:12<00:12, 43.66it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.20it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.61it/s][A
 53%|█████▎    | 572/1083 [00:13<00:11, 44.88it/s][A
 53%|█████▎    | 577/1083 [00:13<00:11, 44.88it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.62it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.57it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.70it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.80it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.03it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.23it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.26it/s][A
 57%|█████▋    | 617/1083 [00:14<00:10, 45.28it/s][A
 57%|█████▋    | 622/1083 [00:14<00:10, 45.12it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.78it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.83it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.81it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.87it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.02it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.24it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.27it/s][A
 61%|██████    | 662/1083 [00:15<00:09, 45.28it/s][A
 62%|██████▏   | 667/1083 [00:15<00:09, 45.07it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.79it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 42.47it/s][A
 63%|██████▎   | 682/1083 [00:15<00:09, 43.28it/s][A
 63%|██████▎   | 687/1083 [00:15<00:09, 43.79it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.45it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.70it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.88it/s][A
 65%|██████▌   | 707/1083 [00:16<00:08, 44.91it/s][A
 66%|██████▌   | 712/1083 [00:16<00:08, 44.85it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.63it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.62it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.72it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.88it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.06it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.11it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.27it/s][A
 69%|██████▉   | 752/1083 [00:17<00:07, 45.35it/s][A
 70%|██████▉   | 757/1083 [00:17<00:07, 45.14it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.96it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.96it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.98it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.94it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.98it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.18it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.25it/s][A
 74%|███████▎  | 797/1083 [00:18<00:06, 45.23it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 45.02it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.87it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 43.41it/s][A
 75%|███████▌  | 817/1083 [00:18<00:06, 44.04it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.35it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.67it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.82it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.04it/s][A
 78%|███████▊  | 842/1083 [00:19<00:05, 44.92it/s][A
 78%|███████▊  | 847/1083 [00:19<00:05, 44.85it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.69it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.70it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.85it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.94it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.10it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.25it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.25it/s][A
 82%|████████▏ | 887/1083 [00:20<00:04, 45.22it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 45.02it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.92it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.84it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.90it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.98it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.06it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.20it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.14it/s][A
 86%|████████▌ | 932/1083 [00:21<00:03, 45.10it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 44.88it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.95it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 43.76it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.14it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.44it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.60it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.90it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.04it/s][A
 90%|█████████ | 977/1083 [00:22<00:02, 45.06it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 44.87it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.69it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.76it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.86it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 45.05it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.99it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.13it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 45.15it/s][A
 94%|█████████▍| 1022/1083 [00:23<00:01, 45.16it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 45.00it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.82it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.81it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.80it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.97it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 45.11it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.12it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 45.18it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 45.06it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.93it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.89it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 43.50it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 43.50it/s][A 80%|████████  | 320/400 [03:20<00:20,  3.89it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 08:42:33,665 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 08:42:33,763 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:42:37,007 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:42:37,173 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:42:37,262 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [03:26<12:02,  9.14s/it] 80%|████████  | 322/400 [03:26<08:25,  6.48s/it] 81%|████████  | 323/400 [03:26<05:55,  4.62s/it] 81%|████████  | 324/400 [03:27<04:12,  3.32s/it] 81%|████████▏ | 325/400 [03:27<03:00,  2.41s/it] 82%|████████▏ | 326/400 [03:27<02:11,  1.77s/it] 82%|████████▏ | 327/400 [03:27<01:36,  1.32s/it] 82%|████████▏ | 328/400 [03:28<01:12,  1.01s/it] 82%|████████▏ | 329/400 [03:28<00:56,  1.26it/s] 82%|████████▎ | 330/400 [03:28<00:44,  1.57it/s] 83%|████████▎ | 331/400 [03:29<00:36,  1.88it/s] 83%|████████▎ | 332/400 [03:29<00:31,  2.19it/s] 83%|████████▎ | 333/400 [03:29<00:27,  2.48it/s] 84%|████████▎ | 334/400 [03:29<00:24,  2.73it/s] 84%|████████▍ | 335/400 [03:30<00:22,  2.94it/s] 84%|████████▍ | 336/400 [03:30<00:20,  3.10it/s] 84%|████████▍ | 337/400 [03:30<00:19,  3.17it/s] 84%|████████▍ | 338/400 [03:31<00:18,  3.28it/s] 85%|████████▍ | 339/400 [03:31<00:18,  3.36it/s] 85%|████████▌ | 340/400 [03:31<00:17,  3.42it/s] 85%|████████▌ | 341/400 [03:31<00:17,  3.46it/s] 86%|████████▌ | 342/400 [03:32<00:16,  3.50it/s] 86%|████████▌ | 343/400 [03:32<00:16,  3.52it/s] 86%|████████▌ | 344/400 [03:32<00:15,  3.53it/s] 86%|████████▋ | 345/400 [03:32<00:15,  3.54it/s] 86%|████████▋ | 346/400 [03:33<00:15,  3.55it/s] 87%|████████▋ | 347/400 [03:33<00:14,  3.56it/s] 87%|████████▋ | 348/400 [03:33<00:15,  3.40it/s] 87%|████████▋ | 349/400 [03:34<00:14,  3.45it/s] 88%|████████▊ | 350/400 [03:34<00:14,  3.48it/s] 88%|████████▊ | 351/400 [03:34<00:13,  3.50it/s] 88%|████████▊ | 352/400 [03:35<00:13,  3.52it/s] 88%|████████▊ | 353/400 [03:35<00:13,  3.53it/s] 88%|████████▊ | 354/400 [03:35<00:13,  3.54it/s] 89%|████████▉ | 355/400 [03:35<00:12,  3.54it/s] 89%|████████▉ | 356/400 [03:36<00:12,  3.55it/s] 89%|████████▉ | 357/400 [03:36<00:12,  3.55it/s] 90%|████████▉ | 358/400 [03:36<00:11,  3.55it/s] 90%|████████▉ | 359/400 [03:37<00:11,  3.42it/s] 90%|█████████ | 360/400 [03:37<00:11,  3.46it/s] 90%|█████████ | 361/400 [03:37<00:11,  3.49it/s] 90%|█████████ | 362/400 [03:37<00:10,  3.51it/s] 91%|█████████ | 363/400 [03:38<00:10,  3.53it/s] 91%|█████████ | 364/400 [03:38<00:10,  3.54it/s] 91%|█████████▏| 365/400 [03:38<00:09,  3.55it/s] 92%|█████████▏| 366/400 [03:38<00:09,  3.55it/s] 92%|█████████▏| 367/400 [03:39<00:09,  3.56it/s] 92%|█████████▏| 368/400 [03:39<00:08,  3.56it/s] 92%|█████████▏| 369/400 [03:39<00:08,  3.56it/s] 92%|█████████▎| 370/400 [03:40<00:08,  3.43it/s] 93%|█████████▎| 371/400 [03:40<00:08,  3.47it/s] 93%|█████████▎| 372/400 [03:40<00:08,  3.50it/s] 93%|█████████▎| 373/400 [03:40<00:07,  3.52it/s] 94%|█████████▎| 374/400 [03:41<00:07,  3.53it/s] 94%|█████████▍| 375/400 [03:41<00:07,  3.54it/s] 94%|█████████▍| 376/400 [03:41<00:06,  3.55it/s] 94%|█████████▍| 377/400 [03:42<00:06,  3.55it/s] 94%|█████████▍| 378/400 [03:42<00:06,  3.56it/s] 95%|█████████▍| 379/400 [03:42<00:05,  3.56it/s] 95%|█████████▌| 380/400 [03:42<00:05,  3.57it/s] 95%|█████████▌| 381/400 [03:43<00:05,  3.50it/s] 96%|█████████▌| 382/400 [03:43<00:05,  3.52it/s] 96%|█████████▌| 383/400 [03:43<00:04,  3.54it/s] 96%|█████████▌| 384/400 [03:44<00:04,  3.54it/s] 96%|█████████▋| 385/400 [03:44<00:04,  3.55it/s] 96%|█████████▋| 386/400 [03:44<00:03,  3.56it/s] 97%|█████████▋| 387/400 [03:44<00:03,  3.56it/s] 97%|█████████▋| 388/400 [03:45<00:03,  3.56it/s] 97%|█████████▋| 389/400 [03:45<00:03,  3.57it/s] 98%|█████████▊| 390/400 [03:45<00:02,  3.57it/s] 98%|█████████▊| 391/400 [03:46<00:02,  3.57it/s] 98%|█████████▊| 392/400 [03:46<00:02,  3.52it/s] 98%|█████████▊| 393/400 [03:46<00:01,  3.53it/s] 98%|█████████▊| 394/400 [03:46<00:01,  3.54it/s] 99%|█████████▉| 395/400 [03:47<00:01,  3.55it/s] 99%|█████████▉| 396/400 [03:47<00:01,  3.55it/s] 99%|█████████▉| 397/400 [03:47<00:00,  3.56it/s]100%|█████████▉| 398/400 [03:48<00:00,  3.56it/s]100%|█████████▉| 399/400 [03:48<00:00,  3.56it/s]100%|██████████| 400/400 [03:48<00:00,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 08:43:01,169 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:43:01,169 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 08:43:01,169 >>   Batch size = 8
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.4919, 'eval_samples_per_second': 353.504, 'eval_steps_per_second': 44.219, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 55.72it/s][A
  1%|          | 12/1083 [00:00<00:21, 48.93it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.40it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.41it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.85it/s][A
  3%|▎         | 32/1083 [00:00<00:24, 43.40it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 43.95it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.02it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.33it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.71it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.96it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.15it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.15it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.84it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.84it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.86it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.82it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.91it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.04it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.22it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.30it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.28it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.01it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.87it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.82it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.85it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.90it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 45.05it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.12it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.18it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.33it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.16it/s][A
 15%|█▌        | 167/1083 [00:03<00:21, 42.60it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 43.50it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 43.97it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.40it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.55it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.87it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.05it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.10it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.80it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.72it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.93it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.98it/s][A
 21%|██        | 227/1083 [00:05<00:19, 45.00it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 45.07it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.15it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.20it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.17it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.91it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.70it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.94it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.95it/s][A
 25%|██▌       | 272/1083 [00:06<00:17, 45.10it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 45.16it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.17it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.20it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.13it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.08it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.08it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.46it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.68it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.81it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 45.01it/s][A
 30%|███       | 327/1083 [00:07<00:16, 45.03it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.04it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.91it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.78it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.85it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.94it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.95it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.90it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 45.06it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.10it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.07it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.88it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.82it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.84it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.97it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.97it/s][A
 38%|███▊      | 407/1083 [00:09<00:14, 45.11it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.24it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.21it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.14it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.95it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.86it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.85it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.90it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.99it/s][A
 42%|████▏     | 452/1083 [00:10<00:13, 45.09it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.20it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.22it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.10it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.95it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.86it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.80it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.92it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.93it/s][A
 46%|████▌     | 497/1083 [00:11<00:12, 45.08it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.22it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.16it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.17it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.97it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.85it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.86it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.88it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.92it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 45.02it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 45.12it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.18it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.18it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.03it/s][A
 52%|█████▏    | 567/1083 [00:12<00:12, 42.32it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 43.09it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 43.73it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.22it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.57it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.74it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.92it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.89it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.59it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.51it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.72it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.99it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 45.10it/s][A
 58%|█████▊    | 632/1083 [00:14<00:09, 45.21it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 45.20it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 45.18it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.08it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.83it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.70it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.72it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.86it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 41.42it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 43.97it/s][A
 63%|██████▎   | 683/1083 [00:15<00:10, 39.29it/s][A
 64%|██████▎   | 688/1083 [00:15<00:09, 41.67it/s][A
 64%|██████▍   | 693/1083 [00:15<00:09, 42.79it/s][A
 64%|██████▍   | 698/1083 [00:15<00:09, 42.02it/s][A
 65%|██████▍   | 703/1083 [00:15<00:09, 41.81it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 42.93it/s][A
 66%|██████▌   | 713/1083 [00:15<00:09, 40.93it/s][A
 66%|██████▋   | 718/1083 [00:16<00:09, 38.98it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 40.72it/s][A
 67%|██████▋   | 728/1083 [00:16<00:08, 42.10it/s][A
 68%|██████▊   | 733/1083 [00:16<00:08, 43.17it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 43.71it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.12it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.40it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.43it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.33it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.47it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.77it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.99it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.09it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.18it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.16it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.14it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.89it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.71it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.74it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.96it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 45.09it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 45.10it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 45.25it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.24it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 42.42it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 43.20it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 43.71it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.16it/s][A
 79%|███████▉  | 858/1083 [00:19<00:06, 36.28it/s][A
 80%|███████▉  | 863/1083 [00:19<00:05, 39.20it/s][A
 80%|████████  | 868/1083 [00:19<00:05, 40.87it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 42.19it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 43.12it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 43.90it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.34it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.69it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.79it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.42it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.26it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.39it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.70it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.99it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.13it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 45.25it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 45.34it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 45.23it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.94it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.59it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.66it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.73it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 45.01it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 39.77it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 41.38it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 42.53it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 43.42it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.01it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.45it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.65it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.81it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.55it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.41it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 44.38it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 44.69it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.87it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.97it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 45.09it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 45.28it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 45.18it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 45.04it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.83it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 44.87it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 44.98it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 45.04it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 45.18it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 45.18it/s][A100%|██████████| 400/400 [04:12<00:00,  3.90it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 08:43:25,725 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 08:43:25,985 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:43:29,615 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:43:29,790 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:43:29,866 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 08:43:31,006 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 08:43:31,007 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80 (score: 0.9504339694976807).
                                                 100%|██████████| 400/400 [04:27<00:00,  3.90it/s]100%|██████████| 400/400 [04:27<00:00,  1.50it/s]
[INFO|trainer.py:1894] 2023-08-28 08:43:40,337 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 08:43:40,471 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:43:43,348 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:43:43,435 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:43:43,495 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:43:43,947 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:43:43,947 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:43:43,947 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:43:43,947 >>   train_runtime            = 0:04:27.52
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:43:43,947 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:43:43,947 >>   train_samples_per_second =     95.318
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:43:43,947 >>   train_steps_per_second   =      1.495
{'eval_loss': 0.9504339694976807, 'eval_runtime': 24.379, 'eval_samples_per_second': 355.141, 'eval_steps_per_second': 44.423, 'epoch': 5.0}
{'train_runtime': 267.5245, 'train_samples_per_second': 95.318, 'train_steps_per_second': 1.495, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 08:43:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 08:43:44,102 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:43:44,103 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 08:43:44,103 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.49it/s]  1%|          | 12/1083 [00:00<00:21, 49.22it/s]  2%|▏         | 17/1083 [00:00<00:23, 46.33it/s]  2%|▏         | 22/1083 [00:00<00:22, 46.20it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.00it/s]  3%|▎         | 32/1083 [00:00<00:22, 45.89it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.83it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.44it/s]  4%|▍         | 47/1083 [00:01<00:23, 44.99it/s]  5%|▍         | 52/1083 [00:01<00:22, 44.84it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.90it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.00it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.20it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.28it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.44it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.43it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.37it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.99it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.75it/s]  9%|▉         | 102/1083 [00:02<00:21, 44.85it/s] 10%|▉         | 107/1083 [00:02<00:21, 44.94it/s] 10%|█         | 112/1083 [00:02<00:21, 45.09it/s] 11%|█         | 117/1083 [00:02<00:21, 45.11it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.26it/s] 12%|█▏        | 127/1083 [00:02<00:21, 45.29it/s] 12%|█▏        | 132/1083 [00:02<00:20, 45.36it/s] 13%|█▎        | 137/1083 [00:03<00:20, 45.23it/s] 13%|█▎        | 142/1083 [00:03<00:20, 45.10it/s] 14%|█▎        | 147/1083 [00:03<00:20, 44.98it/s] 14%|█▍        | 152/1083 [00:03<00:20, 45.04it/s] 14%|█▍        | 157/1083 [00:03<00:21, 43.78it/s] 15%|█▍        | 162/1083 [00:03<00:20, 44.37it/s] 15%|█▌        | 167/1083 [00:03<00:20, 44.55it/s] 16%|█▌        | 172/1083 [00:03<00:20, 44.82it/s] 16%|█▋        | 177/1083 [00:03<00:20, 44.84it/s] 17%|█▋        | 182/1083 [00:04<00:20, 44.83it/s] 17%|█▋        | 187/1083 [00:04<00:20, 44.75it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.79it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.79it/s] 19%|█▊        | 202/1083 [00:04<00:19, 44.80it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.08it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.13it/s] 20%|██        | 217/1083 [00:04<00:19, 45.30it/s] 20%|██        | 222/1083 [00:04<00:19, 45.13it/s] 21%|██        | 227/1083 [00:05<00:18, 45.14it/s] 21%|██▏       | 232/1083 [00:05<00:18, 44.96it/s] 22%|██▏       | 237/1083 [00:05<00:18, 44.88it/s] 22%|██▏       | 242/1083 [00:05<00:18, 44.96it/s] 23%|██▎       | 247/1083 [00:05<00:18, 44.78it/s] 23%|██▎       | 252/1083 [00:05<00:18, 45.00it/s] 24%|██▎       | 257/1083 [00:05<00:18, 45.09it/s] 24%|██▍       | 262/1083 [00:05<00:18, 43.72it/s] 25%|██▍       | 267/1083 [00:05<00:18, 44.29it/s] 25%|██▌       | 272/1083 [00:06<00:18, 44.55it/s] 26%|██▌       | 277/1083 [00:06<00:18, 44.55it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.65it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.68it/s] 27%|██▋       | 292/1083 [00:06<00:18, 43.41it/s] 27%|██▋       | 297/1083 [00:06<00:17, 44.03it/s] 28%|██▊       | 302/1083 [00:06<00:17, 44.22it/s] 28%|██▊       | 307/1083 [00:06<00:17, 44.67it/s] 29%|██▉       | 312/1083 [00:06<00:17, 44.87it/s] 29%|██▉       | 317/1083 [00:07<00:17, 45.03it/s] 30%|██▉       | 322/1083 [00:07<00:16, 44.87it/s] 30%|███       | 327/1083 [00:07<00:16, 44.83it/s] 31%|███       | 332/1083 [00:07<00:16, 44.62it/s] 31%|███       | 337/1083 [00:07<00:16, 44.71it/s] 32%|███▏      | 342/1083 [00:07<00:16, 44.89it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.00it/s] 33%|███▎      | 352/1083 [00:07<00:16, 45.18it/s] 33%|███▎      | 357/1083 [00:07<00:16, 45.20it/s] 33%|███▎      | 362/1083 [00:08<00:15, 45.16it/s] 34%|███▍      | 367/1083 [00:08<00:15, 45.03it/s] 34%|███▍      | 372/1083 [00:08<00:15, 44.90it/s] 35%|███▍      | 377/1083 [00:08<00:15, 44.84it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.76it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.90it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.99it/s] 37%|███▋      | 397/1083 [00:08<00:15, 45.14it/s] 37%|███▋      | 402/1083 [00:08<00:15, 45.23it/s] 38%|███▊      | 407/1083 [00:09<00:14, 45.14it/s] 38%|███▊      | 412/1083 [00:09<00:14, 45.12it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.90it/s] 39%|███▉      | 422/1083 [00:09<00:14, 44.83it/s] 39%|███▉      | 427/1083 [00:09<00:14, 43.95it/s] 40%|███▉      | 432/1083 [00:09<00:14, 44.45it/s] 40%|████      | 437/1083 [00:09<00:14, 44.70it/s] 41%|████      | 442/1083 [00:09<00:14, 44.82it/s] 41%|████▏     | 447/1083 [00:09<00:14, 44.96it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.62it/s] 42%|████▏     | 457/1083 [00:10<00:13, 45.04it/s] 43%|████▎     | 462/1083 [00:10<00:13, 44.98it/s] 43%|████▎     | 467/1083 [00:10<00:13, 44.67it/s] 44%|████▎     | 472/1083 [00:10<00:13, 44.75it/s] 44%|████▍     | 477/1083 [00:10<00:13, 44.93it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.09it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.13it/s] 45%|████▌     | 492/1083 [00:10<00:13, 45.18it/s] 46%|████▌     | 497/1083 [00:11<00:12, 45.14it/s] 46%|████▋     | 502/1083 [00:11<00:12, 45.08it/s] 47%|████▋     | 507/1083 [00:11<00:12, 44.93it/s] 47%|████▋     | 512/1083 [00:11<00:12, 44.75it/s] 48%|████▊     | 517/1083 [00:11<00:12, 44.84it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.86it/s] 49%|████▊     | 527/1083 [00:11<00:12, 45.04it/s] 49%|████▉     | 532/1083 [00:11<00:12, 45.06it/s] 50%|████▉     | 537/1083 [00:11<00:12, 45.19it/s] 50%|█████     | 542/1083 [00:12<00:11, 45.11it/s] 51%|█████     | 547/1083 [00:12<00:11, 45.04it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.96it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 44.73it/s] 52%|█████▏    | 562/1083 [00:12<00:12, 42.99it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 43.71it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 44.26it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 44.50it/s] 54%|█████▎    | 582/1083 [00:12<00:11, 44.78it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 44.91it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 44.92it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 44.92it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.70it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 44.76it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 44.88it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 45.05it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 45.18it/s] 58%|█████▊    | 627/1083 [00:13<00:10, 45.13it/s] 58%|█████▊    | 632/1083 [00:14<00:09, 45.23it/s] 59%|█████▉    | 637/1083 [00:14<00:09, 45.13it/s] 59%|█████▉    | 642/1083 [00:14<00:09, 44.85it/s] 60%|█████▉    | 647/1083 [00:14<00:09, 44.80it/s] 60%|██████    | 652/1083 [00:14<00:09, 44.90it/s] 61%|██████    | 657/1083 [00:14<00:09, 45.02it/s] 61%|██████    | 662/1083 [00:14<00:09, 45.10it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 45.05it/s] 62%|██████▏   | 672/1083 [00:14<00:09, 45.12it/s] 63%|██████▎   | 677/1083 [00:15<00:08, 45.12it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.96it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.85it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.83it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 43.90it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 44.34it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 44.69it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 44.91it/s] 66%|██████▌   | 717/1083 [00:15<00:08, 44.93it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 44.70it/s] 67%|██████▋   | 727/1083 [00:16<00:07, 44.80it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.67it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.64it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.81it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 44.89it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 44.98it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 45.03it/s] 70%|███████   | 762/1083 [00:16<00:07, 45.07it/s] 71%|███████   | 767/1083 [00:17<00:07, 45.05it/s] 71%|███████▏  | 772/1083 [00:17<00:06, 45.01it/s] 72%|███████▏  | 777/1083 [00:17<00:06, 44.90it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 44.76it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 44.75it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.85it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 45.09it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 45.09it/s] 75%|███████▍  | 807/1083 [00:17<00:06, 45.11it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 45.02it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.99it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.97it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 44.74it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 43.00it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 43.67it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 44.16it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 44.53it/s] 79%|███████▊  | 852/1083 [00:18<00:05, 44.75it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.95it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 45.05it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.98it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.58it/s] 81%|████████  | 877/1083 [00:19<00:04, 44.84it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 45.00it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 45.10it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 45.19it/s] 83%|████████▎ | 897/1083 [00:19<00:04, 45.23it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 45.21it/s] 84%|████████▎ | 907/1083 [00:20<00:03, 45.24it/s] 84%|████████▍ | 912/1083 [00:20<00:03, 45.08it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 44.99it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 44.97it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 44.98it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 45.02it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 45.13it/s] 87%|████████▋ | 942/1083 [00:20<00:03, 45.27it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 45.31it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 45.13it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 45.19it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 45.19it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 44.42it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 44.70it/s] 90%|█████████ | 977/1083 [00:21<00:02, 44.86it/s] 91%|█████████ | 982/1083 [00:21<00:02, 44.89it/s] 91%|█████████ | 987/1083 [00:21<00:02, 45.03it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 45.18it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 45.16it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 45.05it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 44.92it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 44.86it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 45.00it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 45.03it/s] 95%|█████████▍| 1027/1083 [00:22<00:01, 45.20it/s] 95%|█████████▌| 1032/1083 [00:22<00:01, 45.28it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 45.29it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 45.19it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 45.05it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 45.10it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 45.00it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 44.96it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 44.92it/s] 99%|█████████▉| 1072/1083 [00:23<00:00, 45.12it/s] 99%|█████████▉| 1077/1083 [00:23<00:00, 45.08it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 45.01it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:44:08,225 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   eval_loss               =     0.9504
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   eval_runtime            = 0:00:24.12
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   eval_samples_per_second =    358.917
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   eval_steps_per_second   =     44.896
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:44:08,226 >>   perplexity              =     2.5868
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:17,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:17,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:17,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:17,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:17,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:44:18,462 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:44:18,463 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:44:19,035 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:44:20,082 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:44:20,082 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:23,114 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:23,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:23,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:23,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:23,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:44:23,870 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:44:23,871 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:44:24,449 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:44:24,624 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:44:24,624 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-80
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-160
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-400
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-240
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/generator/iter5/model/checkpoint-320
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.69it/s]Extractor Predicting: 6it [00:03,  1.70it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.74it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.72it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:11,  1.61it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:15,  1.75it/s]Extractor Predicting: 27it [00:15,  1.74it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:17,  1.72it/s]Extractor Predicting: 31it [00:18,  1.73it/s]Extractor Predicting: 32it [00:18,  1.74it/s]Extractor Predicting: 33it [00:19,  1.73it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:21,  1.69it/s]Extractor Predicting: 37it [00:21,  1.70it/s]Extractor Predicting: 38it [00:22,  1.72it/s]Extractor Predicting: 39it [00:22,  1.72it/s]Extractor Predicting: 40it [00:23,  1.73it/s]Extractor Predicting: 41it [00:24,  1.61it/s]Extractor Predicting: 42it [00:24,  1.66it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:25,  1.71it/s]Extractor Predicting: 45it [00:26,  1.68it/s]Extractor Predicting: 46it [00:27,  1.66it/s]Extractor Predicting: 47it [00:27,  1.68it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:29,  1.63it/s]Extractor Predicting: 50it [00:29,  1.66it/s]Extractor Predicting: 51it [00:30,  1.68it/s]Extractor Predicting: 52it [00:30,  1.67it/s]Extractor Predicting: 53it [00:31,  1.65it/s]Extractor Predicting: 54it [00:32,  1.68it/s]Extractor Predicting: 55it [00:32,  1.71it/s]Extractor Predicting: 56it [00:33,  1.72it/s]Extractor Predicting: 57it [00:33,  1.69it/s]Extractor Predicting: 58it [00:34,  1.68it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:35,  1.60it/s]Extractor Predicting: 61it [00:36,  1.64it/s]Extractor Predicting: 62it [00:36,  1.66it/s]Extractor Predicting: 63it [00:37,  1.75it/s]Extractor Predicting: 64it [00:37,  1.78it/s]Extractor Predicting: 65it [00:38,  1.73it/s]Extractor Predicting: 66it [00:39,  1.74it/s]Extractor Predicting: 67it [00:39,  1.71it/s]Extractor Predicting: 68it [00:40,  1.72it/s]Extractor Predicting: 69it [00:40,  1.74it/s]Extractor Predicting: 70it [00:41,  1.67it/s]Extractor Predicting: 71it [00:42,  1.67it/s]Extractor Predicting: 72it [00:42,  1.70it/s]Extractor Predicting: 73it [00:43,  1.75it/s]Extractor Predicting: 74it [00:43,  1.58it/s]Extractor Predicting: 75it [00:44,  1.61it/s]Extractor Predicting: 76it [00:45,  1.65it/s]Extractor Predicting: 77it [00:45,  1.65it/s]Extractor Predicting: 78it [00:46,  1.68it/s]Extractor Predicting: 79it [00:46,  1.69it/s]Extractor Predicting: 80it [00:47,  1.67it/s]Extractor Predicting: 81it [00:48,  1.72it/s]Extractor Predicting: 82it [00:48,  1.73it/s]Extractor Predicting: 83it [00:49,  1.72it/s]Extractor Predicting: 84it [00:49,  1.74it/s]Extractor Predicting: 85it [00:50,  1.70it/s]Extractor Predicting: 86it [00:50,  1.66it/s]Extractor Predicting: 87it [00:51,  1.65it/s]Extractor Predicting: 88it [00:52,  1.65it/s]Extractor Predicting: 89it [00:52,  1.69it/s]Extractor Predicting: 90it [00:53,  1.70it/s]Extractor Predicting: 91it [00:53,  1.74it/s]Extractor Predicting: 92it [00:54,  1.69it/s]Extractor Predicting: 93it [00:55,  1.67it/s]Extractor Predicting: 94it [00:55,  1.69it/s]Extractor Predicting: 95it [00:56,  1.72it/s]Extractor Predicting: 96it [00:56,  1.71it/s]Extractor Predicting: 97it [00:57,  1.71it/s]Extractor Predicting: 98it [00:58,  1.67it/s]Extractor Predicting: 99it [00:58,  1.69it/s]Extractor Predicting: 100it [00:59,  1.68it/s]Extractor Predicting: 101it [00:59,  1.71it/s]Extractor Predicting: 102it [01:00,  1.74it/s]Extractor Predicting: 103it [01:00,  1.73it/s]Extractor Predicting: 104it [01:01,  1.68it/s]Extractor Predicting: 105it [01:02,  1.69it/s]Extractor Predicting: 106it [01:02,  1.71it/s]Extractor Predicting: 107it [01:03,  1.69it/s]Extractor Predicting: 108it [01:03,  1.70it/s]Extractor Predicting: 109it [01:04,  1.71it/s]Extractor Predicting: 110it [01:05,  1.68it/s]Extractor Predicting: 111it [01:05,  1.69it/s]Extractor Predicting: 112it [01:06,  1.70it/s]Extractor Predicting: 113it [01:06,  1.72it/s]Extractor Predicting: 114it [01:07,  1.74it/s]Extractor Predicting: 115it [01:08,  1.69it/s]Extractor Predicting: 116it [01:08,  1.65it/s]Extractor Predicting: 117it [01:09,  1.70it/s]Extractor Predicting: 118it [01:09,  1.70it/s]Extractor Predicting: 119it [01:10,  1.73it/s]Extractor Predicting: 120it [01:10,  1.73it/s]Extractor Predicting: 121it [01:11,  1.69it/s]Extractor Predicting: 122it [01:12,  1.80it/s]Extractor Predicting: 123it [01:12,  1.78it/s]Extractor Predicting: 124it [01:13,  1.75it/s]Extractor Predicting: 125it [01:13,  1.69it/s]Extractor Predicting: 126it [01:14,  1.67it/s]Extractor Predicting: 127it [01:15,  1.71it/s]Extractor Predicting: 128it [01:15,  1.71it/s]Extractor Predicting: 129it [01:16,  1.71it/s]Extractor Predicting: 130it [01:16,  1.72it/s]Extractor Predicting: 131it [01:17,  1.78it/s]Extractor Predicting: 132it [01:17,  1.75it/s]Extractor Predicting: 133it [01:18,  1.68it/s]Extractor Predicting: 134it [01:19,  1.70it/s]Extractor Predicting: 135it [01:19,  1.72it/s]Extractor Predicting: 136it [01:20,  1.70it/s]Extractor Predicting: 137it [01:20,  1.71it/s]Extractor Predicting: 138it [01:21,  1.71it/s]Extractor Predicting: 139it [01:22,  1.68it/s]Extractor Predicting: 140it [01:22,  1.70it/s]Extractor Predicting: 141it [01:23,  1.72it/s]Extractor Predicting: 142it [01:23,  1.73it/s]Extractor Predicting: 143it [01:24,  1.52it/s]Extractor Predicting: 144it [01:25,  1.56it/s]Extractor Predicting: 145it [01:25,  1.57it/s]Extractor Predicting: 146it [01:26,  1.59it/s]Extractor Predicting: 147it [01:26,  1.65it/s]Extractor Predicting: 148it [01:27,  1.69it/s]Extractor Predicting: 149it [01:28,  1.74it/s]Extractor Predicting: 150it [01:28,  1.71it/s]Extractor Predicting: 151it [01:29,  1.74it/s]Extractor Predicting: 152it [01:29,  1.67it/s]Extractor Predicting: 153it [01:30,  1.62it/s]Extractor Predicting: 154it [01:31,  1.59it/s]Extractor Predicting: 155it [01:31,  1.53it/s]Extractor Predicting: 156it [01:32,  1.53it/s]Extractor Predicting: 157it [01:33,  1.52it/s]Extractor Predicting: 158it [01:33,  1.52it/s]Extractor Predicting: 159it [01:34,  1.53it/s]Extractor Predicting: 160it [01:35,  1.52it/s]Extractor Predicting: 161it [01:35,  1.51it/s]Extractor Predicting: 162it [01:36,  1.51it/s]Extractor Predicting: 163it [01:37,  1.51it/s]Extractor Predicting: 164it [01:37,  1.51it/s]Extractor Predicting: 165it [01:38,  1.50it/s]Extractor Predicting: 166it [01:39,  1.52it/s]Extractor Predicting: 167it [01:39,  1.51it/s]Extractor Predicting: 168it [01:40,  1.54it/s]Extractor Predicting: 169it [01:41,  1.60it/s]Extractor Predicting: 170it [01:41,  1.62it/s]Extractor Predicting: 171it [01:42,  1.64it/s]Extractor Predicting: 172it [01:42,  1.63it/s]Extractor Predicting: 173it [01:43,  1.63it/s]Extractor Predicting: 174it [01:44,  1.62it/s]Extractor Predicting: 175it [01:44,  1.59it/s]Extractor Predicting: 176it [01:45,  1.59it/s]Extractor Predicting: 177it [01:46,  1.58it/s]Extractor Predicting: 178it [01:46,  1.57it/s]Extractor Predicting: 179it [01:47,  1.60it/s]Extractor Predicting: 180it [01:47,  1.59it/s]Extractor Predicting: 181it [01:48,  1.61it/s]Extractor Predicting: 182it [01:49,  1.61it/s]Extractor Predicting: 183it [01:49,  1.67it/s]Extractor Predicting: 184it [01:50,  1.68it/s]Extractor Predicting: 185it [01:50,  1.73it/s]Extractor Predicting: 186it [01:51,  1.68it/s]Extractor Predicting: 187it [01:51,  1.70it/s]Extractor Predicting: 188it [01:52,  1.68it/s]Extractor Predicting: 189it [01:53,  1.70it/s]Extractor Predicting: 190it [01:53,  1.69it/s]Extractor Predicting: 191it [01:54,  1.67it/s]Extractor Predicting: 192it [01:54,  1.68it/s]Extractor Predicting: 193it [01:55,  1.71it/s]Extractor Predicting: 194it [01:56,  1.67it/s]Extractor Predicting: 195it [01:56,  1.66it/s]Extractor Predicting: 196it [01:57,  1.65it/s]Extractor Predicting: 197it [01:57,  1.68it/s]Extractor Predicting: 198it [01:58,  1.67it/s]Extractor Predicting: 199it [01:59,  1.70it/s]Extractor Predicting: 200it [01:59,  1.70it/s]Extractor Predicting: 201it [02:00,  1.72it/s]Extractor Predicting: 202it [02:00,  1.69it/s]Extractor Predicting: 203it [02:01,  1.68it/s]Extractor Predicting: 204it [02:02,  1.70it/s]Extractor Predicting: 205it [02:02,  1.65it/s]Extractor Predicting: 206it [02:03,  1.67it/s]Extractor Predicting: 207it [02:03,  1.67it/s]Extractor Predicting: 208it [02:04,  1.70it/s]Extractor Predicting: 209it [02:05,  1.73it/s]Extractor Predicting: 210it [02:05,  1.72it/s]Extractor Predicting: 211it [02:06,  1.70it/s]Extractor Predicting: 212it [02:06,  1.70it/s]Extractor Predicting: 213it [02:07,  1.72it/s]Extractor Predicting: 214it [02:07,  1.75it/s]Extractor Predicting: 215it [02:08,  1.79it/s]Extractor Predicting: 216it [02:09,  1.77it/s]Extractor Predicting: 217it [02:09,  1.74it/s]Extractor Predicting: 218it [02:10,  1.75it/s]Extractor Predicting: 219it [02:10,  1.75it/s]Extractor Predicting: 220it [02:11,  1.73it/s]Extractor Predicting: 221it [02:11,  1.76it/s]Extractor Predicting: 222it [02:12,  1.76it/s]Extractor Predicting: 223it [02:13,  1.72it/s]Extractor Predicting: 224it [02:13,  1.70it/s]Extractor Predicting: 225it [02:14,  1.65it/s]Extractor Predicting: 226it [02:14,  1.68it/s]Extractor Predicting: 227it [02:15,  1.69it/s]Extractor Predicting: 228it [02:16,  1.67it/s]Extractor Predicting: 229it [02:16,  1.72it/s]Extractor Predicting: 230it [02:17,  1.70it/s]Extractor Predicting: 231it [02:17,  1.68it/s]Extractor Predicting: 232it [02:18,  1.71it/s]Extractor Predicting: 233it [02:18,  1.75it/s]Extractor Predicting: 234it [02:19,  1.70it/s]Extractor Predicting: 235it [02:20,  1.69it/s]Extractor Predicting: 236it [02:20,  1.67it/s]Extractor Predicting: 237it [02:21,  1.68it/s]Extractor Predicting: 238it [02:21,  1.71it/s]Extractor Predicting: 239it [02:22,  1.74it/s]Extractor Predicting: 240it [02:23,  1.70it/s]Extractor Predicting: 241it [02:23,  1.72it/s]Extractor Predicting: 242it [02:24,  1.53it/s]Extractor Predicting: 243it [02:25,  1.54it/s]Extractor Predicting: 244it [02:25,  1.60it/s]Extractor Predicting: 245it [02:26,  1.60it/s]Extractor Predicting: 246it [02:26,  1.62it/s]Extractor Predicting: 247it [02:27,  1.65it/s]Extractor Predicting: 248it [02:28,  1.69it/s]Extractor Predicting: 249it [02:28,  1.69it/s]Extractor Predicting: 250it [02:29,  1.67it/s]Extractor Predicting: 251it [02:29,  1.66it/s]Extractor Predicting: 252it [02:30,  1.68it/s]Extractor Predicting: 253it [02:31,  1.69it/s]Extractor Predicting: 254it [02:31,  1.70it/s]Extractor Predicting: 255it [02:32,  1.68it/s]Extractor Predicting: 256it [02:32,  1.70it/s]Extractor Predicting: 257it [02:33,  1.70it/s]Extractor Predicting: 258it [02:33,  1.73it/s]Extractor Predicting: 259it [02:34,  1.70it/s]Extractor Predicting: 260it [02:35,  1.68it/s]Extractor Predicting: 261it [02:35,  1.70it/s]Extractor Predicting: 262it [02:36,  1.71it/s]Extractor Predicting: 263it [02:36,  1.65it/s]Extractor Predicting: 264it [02:37,  1.65it/s]Extractor Predicting: 265it [02:38,  1.66it/s]Extractor Predicting: 266it [02:38,  1.63it/s]Extractor Predicting: 267it [02:39,  1.65it/s]Extractor Predicting: 268it [02:40,  1.63it/s]Extractor Predicting: 269it [02:40,  1.70it/s]Extractor Predicting: 269it [02:40,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:18,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:18,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:18,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:18,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:18,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:47:18,379 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:47:18,380 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:47:19,074 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:47:20,105 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:47:20,121 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:23,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:23,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:23,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:23,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:23,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:47:23,749 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:47:23,765 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:47:24,402 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:47:24,626 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:47:24,626 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:21,  1.65it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.71it/s]Extractor Predicting: 40it [00:24,  1.72it/s]Extractor Predicting: 41it [00:25,  1.72it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.65it/s]Extractor Predicting: 44it [00:27,  1.65it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:29,  1.66it/s]Extractor Predicting: 49it [00:30,  1.65it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:31,  1.71it/s]Extractor Predicting: 52it [00:31,  1.72it/s]Extractor Predicting: 53it [00:32,  1.73it/s]Extractor Predicting: 54it [00:32,  1.75it/s]Extractor Predicting: 55it [00:33,  1.72it/s]Extractor Predicting: 56it [00:34,  1.76it/s]Extractor Predicting: 57it [00:34,  1.75it/s]Extractor Predicting: 58it [00:35,  1.76it/s]Extractor Predicting: 59it [00:35,  1.74it/s]Extractor Predicting: 60it [00:36,  1.76it/s]Extractor Predicting: 61it [00:36,  1.73it/s]Extractor Predicting: 62it [00:37,  1.72it/s]Extractor Predicting: 63it [00:38,  1.69it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:40,  1.67it/s]Extractor Predicting: 68it [00:41,  1.67it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.72it/s]Extractor Predicting: 71it [00:42,  1.75it/s]Extractor Predicting: 72it [00:43,  1.68it/s]Extractor Predicting: 73it [00:44,  1.69it/s]Extractor Predicting: 74it [00:44,  1.70it/s]Extractor Predicting: 75it [00:45,  1.70it/s]Extractor Predicting: 76it [00:45,  1.74it/s]Extractor Predicting: 77it [00:46,  1.61it/s]Extractor Predicting: 78it [00:47,  1.67it/s]Extractor Predicting: 79it [00:47,  1.71it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:49,  1.77it/s]Extractor Predicting: 83it [00:49,  1.77it/s]Extractor Predicting: 84it [00:50,  1.79it/s]Extractor Predicting: 85it [00:50,  1.81it/s]Extractor Predicting: 86it [00:51,  1.77it/s]Extractor Predicting: 87it [00:52,  1.77it/s]Extractor Predicting: 88it [00:52,  1.77it/s]Extractor Predicting: 89it [00:53,  1.73it/s]Extractor Predicting: 90it [00:53,  1.75it/s]Extractor Predicting: 91it [00:54,  1.75it/s]Extractor Predicting: 92it [00:54,  1.74it/s]Extractor Predicting: 93it [00:55,  1.72it/s]Extractor Predicting: 94it [00:56,  1.72it/s]Extractor Predicting: 95it [00:56,  1.80it/s]Extractor Predicting: 96it [00:57,  1.76it/s]Extractor Predicting: 97it [00:57,  1.76it/s]Extractor Predicting: 98it [00:58,  1.73it/s]Extractor Predicting: 99it [00:58,  1.73it/s]Extractor Predicting: 100it [00:59,  1.75it/s]Extractor Predicting: 101it [01:00,  1.78it/s]Extractor Predicting: 102it [01:00,  1.78it/s]Extractor Predicting: 103it [01:01,  1.77it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.80it/s]Extractor Predicting: 106it [01:02,  1.85it/s]Extractor Predicting: 107it [01:03,  1.85it/s]Extractor Predicting: 108it [01:03,  1.86it/s]Extractor Predicting: 109it [01:04,  1.90it/s]Extractor Predicting: 110it [01:04,  1.86it/s]Extractor Predicting: 111it [01:05,  1.90it/s]Extractor Predicting: 112it [01:05,  1.88it/s]Extractor Predicting: 113it [01:06,  1.86it/s]Extractor Predicting: 114it [01:07,  1.71it/s]Extractor Predicting: 115it [01:07,  1.68it/s]Extractor Predicting: 116it [01:08,  1.67it/s]Extractor Predicting: 117it [01:09,  1.64it/s]Extractor Predicting: 118it [01:09,  1.65it/s]Extractor Predicting: 119it [01:10,  1.65it/s]Extractor Predicting: 120it [01:10,  1.67it/s]Extractor Predicting: 121it [01:11,  1.68it/s]Extractor Predicting: 122it [01:12,  1.69it/s]Extractor Predicting: 123it [01:12,  1.65it/s]Extractor Predicting: 124it [01:13,  1.52it/s]Extractor Predicting: 125it [01:14,  1.55it/s]Extractor Predicting: 126it [01:14,  1.56it/s]Extractor Predicting: 127it [01:15,  1.55it/s]Extractor Predicting: 128it [01:15,  1.58it/s]Extractor Predicting: 129it [01:16,  1.64it/s]Extractor Predicting: 130it [01:17,  1.63it/s]Extractor Predicting: 131it [01:17,  1.63it/s]Extractor Predicting: 132it [01:18,  1.61it/s]Extractor Predicting: 133it [01:18,  1.62it/s]Extractor Predicting: 134it [01:19,  1.64it/s]Extractor Predicting: 135it [01:20,  1.61it/s]Extractor Predicting: 136it [01:20,  1.66it/s]Extractor Predicting: 137it [01:21,  1.65it/s]Extractor Predicting: 138it [01:21,  1.70it/s]Extractor Predicting: 139it [01:22,  1.73it/s]Extractor Predicting: 140it [01:23,  1.71it/s]Extractor Predicting: 141it [01:23,  1.68it/s]Extractor Predicting: 142it [01:24,  1.68it/s]Extractor Predicting: 143it [01:24,  2.04it/s]Extractor Predicting: 143it [01:24,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:48:59,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:48:59,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:48:59,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:48:59,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:48:59,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:48:59,457 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:48:59,458 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:48:59,746 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:49:00,801 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:49:00,801 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:49:03,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:49:03,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:49:03,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:49:03,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:49:03,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:49:04,562 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:49:04,563 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:49:05,161 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:49:05,331 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:49:05,331 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.66it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:21,  1.66it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.73it/s]Extractor Predicting: 39it [00:23,  1.81it/s]Extractor Predicting: 40it [00:24,  1.91it/s]Extractor Predicting: 41it [00:24,  2.02it/s]Extractor Predicting: 42it [00:25,  2.06it/s]Extractor Predicting: 43it [00:25,  2.06it/s]Extractor Predicting: 44it [00:26,  2.02it/s]Extractor Predicting: 45it [00:26,  2.08it/s]Extractor Predicting: 46it [00:27,  2.05it/s]Extractor Predicting: 47it [00:27,  2.03it/s]Extractor Predicting: 48it [00:28,  2.05it/s]Extractor Predicting: 49it [00:28,  2.05it/s]Extractor Predicting: 50it [00:29,  2.10it/s]Extractor Predicting: 51it [00:29,  1.99it/s]Extractor Predicting: 52it [00:30,  2.02it/s]Extractor Predicting: 53it [00:30,  2.05it/s]Extractor Predicting: 54it [00:31,  2.10it/s]Extractor Predicting: 55it [00:31,  2.12it/s]Extractor Predicting: 56it [00:32,  2.13it/s]Extractor Predicting: 57it [00:32,  2.13it/s]Extractor Predicting: 58it [00:33,  2.07it/s]Extractor Predicting: 59it [00:33,  2.09it/s]Extractor Predicting: 60it [00:34,  2.03it/s]Extractor Predicting: 61it [00:34,  1.99it/s]Extractor Predicting: 62it [00:34,  2.07it/s]Extractor Predicting: 63it [00:35,  2.07it/s]Extractor Predicting: 64it [00:35,  2.07it/s]Extractor Predicting: 65it [00:36,  2.04it/s]Extractor Predicting: 66it [00:36,  2.09it/s]Extractor Predicting: 67it [00:37,  1.96it/s]Extractor Predicting: 68it [00:38,  1.85it/s]Extractor Predicting: 69it [00:38,  1.78it/s]Extractor Predicting: 70it [00:39,  1.71it/s]Extractor Predicting: 71it [00:40,  1.64it/s]Extractor Predicting: 72it [00:40,  1.62it/s]Extractor Predicting: 72it [00:40,  1.77it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_0', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:21, 15.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:04, 15.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:45<01:46, 15.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:10<01:54, 19.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:26<01:29, 17.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:39<01:04, 16.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:55<00:47, 15.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:08<00:30, 15.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:23<00:15, 15.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:39<00:00, 15.45s/it]Generating: 100%|██████████| 10/10 [02:39<00:00, 15.99s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', "('Venice', 'located in or next to body of water', '', 'After the rise to power of his brother Ferdinand I in 1387 , Ferdinand left for Venice , where he joined the Medici movement .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : The city of Maribor is under the rule of the Marisian dynasty at the end of the third millennium BC , after the first Diodorus of Syracuse , who ruled from 650 BC through 5 BC . Head Entity : Maribor , Tail Entity : fourth millennium BC .\n']
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 44, 'raw': 96}
{'target': 600, 'success': 63, 'raw': 128}
{'target': 600, 'success': 74, 'raw': 160}
{'target': 600, 'success': 89, 'raw': 192}
{'target': 600, 'success': 104, 'raw': 224}
{'target': 600, 'success': 118, 'raw': 256}
{'target': 600, 'success': 134, 'raw': 288}
{'target': 600, 'success': 146, 'raw': 320}
{'target': 600, 'success': 164, 'raw': 352}
{'target': 600, 'success': 189, 'raw': 384}
{'target': 600, 'success': 205, 'raw': 416}
{'target': 600, 'success': 218, 'raw': 448}
{'target': 600, 'success': 233, 'raw': 480}
{'target': 600, 'success': 247, 'raw': 512}
{'target': 600, 'success': 266, 'raw': 544}
{'target': 600, 'success': 280, 'raw': 576}
{'target': 600, 'success': 297, 'raw': 608}
{'target': 600, 'success': 311, 'raw': 640}
{'target': 600, 'success': 325, 'raw': 672}
{'target': 600, 'success': 342, 'raw': 704}
{'target': 600, 'success': 358, 'raw': 736}
{'target': 600, 'success': 375, 'raw': 768}
{'target': 600, 'success': 394, 'raw': 800}
{'target': 600, 'success': 403, 'raw': 832}
{'target': 600, 'success': 422, 'raw': 864}
{'target': 600, 'success': 443, 'raw': 896}
{'target': 600, 'success': 463, 'raw': 928}
{'target': 600, 'success': 477, 'raw': 960}
{'target': 600, 'success': 498, 'raw': 992}
{'target': 600, 'success': 509, 'raw': 1024}
{'target': 600, 'success': 529, 'raw': 1056}
{'target': 600, 'success': 541, 'raw': 1088}
{'target': 600, 'success': 554, 'raw': 1120}
{'target': 600, 'success': 569, 'raw': 1152}
{'target': 600, 'success': 579, 'raw': 1184}
{'target': 600, 'success': 596, 'raw': 1216}
{'target': 600, 'success': 614, 'raw': 1248}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.49198717948717946, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sport .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional ice hockey defenceman . Head Entity : Steve Cagan , Tail Entity : American Hockey .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 540, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : league .', 'success_rate': 0.7106481481481481, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8247282608695652, 'errors': {'', "('Charon', 'located on astronomical body', '', 'M38 S. R. M38 S. R. is a globular cluster orbiting a binary star in the constellation Charon .')", "('', 'located on astronomical body', 'lunar', 'The sun is the opposite of a lunar or lunar eclipse and is the opposite of the Earth appearing as a cloud .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8315217391304348, 'errors': {'', "('Pope Innocent XIII', 'twinned administrative body', '', 'In 1213 , Pope Innocent XIII appointed him archbishop ( c.')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 12579
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12679, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_5_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.73s/it]Extractor Estimating: 2it [00:18,  7.90s/it]Extractor Estimating: 3it [00:18,  4.58s/it]Extractor Estimating: 4it [00:20,  3.44s/it]Extractor Estimating: 5it [00:21,  2.44s/it]Extractor Estimating: 6it [00:21,  1.80s/it]Extractor Estimating: 7it [00:22,  1.42s/it]Extractor Estimating: 8it [00:22,  1.15s/it]Extractor Estimating: 9it [00:23,  1.01it/s]Extractor Estimating: 10it [00:24,  1.14it/s]Extractor Estimating: 11it [00:24,  1.27it/s]Extractor Estimating: 12it [00:25,  1.34it/s]Extractor Estimating: 13it [00:26,  1.41it/s]Extractor Estimating: 14it [00:26,  1.45it/s]Extractor Estimating: 15it [00:27,  1.48it/s]Extractor Estimating: 16it [00:27,  1.51it/s]Extractor Estimating: 17it [00:28,  1.58it/s]Extractor Estimating: 18it [00:29,  1.60it/s]Extractor Estimating: 19it [00:29,  1.54it/s]Extractor Estimating: 20it [00:30,  1.57it/s]Extractor Estimating: 21it [00:31,  1.54it/s]Extractor Estimating: 22it [00:31,  1.52it/s]Extractor Estimating: 23it [00:32,  1.53it/s]Extractor Estimating: 24it [00:33,  1.54it/s]Extractor Estimating: 25it [00:33,  1.60it/s]Extractor Estimating: 26it [00:34,  1.57it/s]Extractor Estimating: 27it [00:34,  1.54it/s]Extractor Estimating: 28it [00:35,  1.54it/s]Extractor Estimating: 29it [00:36,  1.60it/s]Extractor Estimating: 30it [00:36,  1.62it/s]Extractor Estimating: 31it [00:37,  1.62it/s]Extractor Estimating: 32it [00:38,  1.62it/s]Extractor Estimating: 33it [00:38,  1.65it/s]Extractor Estimating: 34it [00:39,  1.61it/s]Extractor Estimating: 35it [00:39,  1.62it/s]Extractor Estimating: 36it [00:40,  1.63it/s]Extractor Estimating: 37it [00:41,  1.68it/s]Extractor Estimating: 38it [00:41,  1.72it/s]Extractor Estimating: 39it [00:42,  1.67it/s]Extractor Estimating: 40it [00:42,  1.62it/s]Extractor Estimating: 41it [00:43,  1.59it/s]Extractor Estimating: 42it [00:44,  1.56it/s]Extractor Estimating: 43it [00:44,  1.54it/s]Extractor Estimating: 44it [00:45,  1.54it/s]Extractor Estimating: 45it [00:46,  1.54it/s]Extractor Estimating: 46it [00:46,  1.57it/s]Extractor Estimating: 47it [00:47,  1.58it/s]Extractor Estimating: 48it [00:48,  1.57it/s]Extractor Estimating: 49it [00:48,  1.58it/s]Extractor Estimating: 50it [00:49,  1.59it/s]Extractor Estimating: 51it [00:49,  1.60it/s]Extractor Estimating: 52it [00:50,  1.66it/s]Extractor Estimating: 53it [00:51,  1.67it/s]Extractor Estimating: 54it [00:51,  1.56it/s]Extractor Estimating: 55it [00:52,  1.61it/s]Extractor Estimating: 56it [00:52,  1.66it/s]Extractor Estimating: 57it [00:53,  1.73it/s]Extractor Estimating: 58it [00:54,  1.74it/s]Extractor Estimating: 59it [00:54,  1.79it/s]Extractor Estimating: 60it [00:55,  1.76it/s]Extractor Estimating: 61it [00:55,  1.77it/s]Extractor Estimating: 62it [00:56,  1.77it/s]Extractor Estimating: 63it [00:56,  1.71it/s]Extractor Estimating: 64it [00:57,  1.72it/s]Extractor Estimating: 65it [00:58,  1.74it/s]Extractor Estimating: 66it [00:58,  1.75it/s]Extractor Estimating: 67it [00:59,  1.77it/s]Extractor Estimating: 68it [00:59,  1.76it/s]Extractor Estimating: 69it [01:00,  1.72it/s]Extractor Estimating: 70it [01:00,  1.71it/s]Extractor Estimating: 71it [01:01,  1.72it/s]Extractor Estimating: 72it [01:02,  1.73it/s]Extractor Estimating: 73it [01:02,  1.75it/s]Extractor Estimating: 74it [01:03,  1.67it/s]Extractor Estimating: 75it [01:03,  1.75it/s]Extractor Estimating: 76it [01:07,  1.64s/it]Extractor Estimating: 77it [01:08,  1.34s/it]Extractor Estimating: 78it [01:09,  1.11s/it]Extractor Estimating: 79it [01:09,  1.03it/s]Extractor Estimating: 80it [01:10,  1.17it/s]Extractor Estimating: 81it [01:10,  1.27it/s]Extractor Estimating: 82it [01:11,  1.29it/s]Extractor Estimating: 83it [01:12,  1.34it/s]Extractor Estimating: 84it [01:13,  1.38it/s]Extractor Estimating: 85it [01:13,  1.46it/s]Extractor Estimating: 86it [01:14,  1.49it/s]Extractor Estimating: 87it [01:14,  1.54it/s]Extractor Estimating: 88it [01:15,  1.56it/s]Extractor Estimating: 89it [01:16,  1.60it/s]Extractor Estimating: 90it [01:16,  1.53it/s]Extractor Estimating: 91it [01:17,  1.58it/s]Extractor Estimating: 92it [01:18,  1.58it/s]Extractor Estimating: 93it [01:18,  1.59it/s]Extractor Estimating: 94it [01:19,  1.59it/s]Extractor Estimating: 95it [01:19,  1.57it/s]Extractor Estimating: 96it [01:20,  1.56it/s]Extractor Estimating: 97it [01:21,  1.53it/s]Extractor Estimating: 98it [01:21,  1.53it/s]Extractor Estimating: 99it [01:22,  1.56it/s]Extractor Estimating: 100it [01:23,  1.56it/s]Extractor Estimating: 101it [01:23,  1.62it/s]Extractor Estimating: 102it [01:24,  1.62it/s]Extractor Estimating: 103it [01:24,  1.68it/s]Extractor Estimating: 104it [01:25,  1.69it/s]Extractor Estimating: 105it [01:26,  1.75it/s]Extractor Estimating: 106it [01:26,  1.69it/s]Extractor Estimating: 107it [01:27,  1.67it/s]Extractor Estimating: 108it [01:27,  1.70it/s]Extractor Estimating: 109it [01:28,  1.67it/s]Extractor Estimating: 110it [01:29,  1.65it/s]Extractor Estimating: 111it [01:29,  1.61it/s]Extractor Estimating: 112it [01:30,  1.62it/s]Extractor Estimating: 113it [01:30,  1.66it/s]Extractor Estimating: 114it [01:31,  1.72it/s]Extractor Estimating: 115it [01:32,  1.72it/s]Extractor Estimating: 116it [01:32,  1.55it/s]Extractor Estimating: 117it [01:33,  1.54it/s]Extractor Estimating: 118it [01:34,  1.59it/s]Extractor Estimating: 119it [01:34,  1.63it/s]Extractor Estimating: 120it [01:35,  1.65it/s]Extractor Estimating: 121it [01:35,  1.63it/s]Extractor Estimating: 122it [01:36,  1.64it/s]Extractor Estimating: 123it [01:37,  1.69it/s]Extractor Estimating: 124it [01:37,  1.67it/s]Extractor Estimating: 125it [01:38,  1.62it/s]Extractor Estimating: 126it [01:38,  1.64it/s]Extractor Estimating: 127it [01:39,  1.60it/s]Extractor Estimating: 128it [01:40,  1.59it/s]Extractor Estimating: 129it [01:40,  1.61it/s]Extractor Estimating: 130it [01:41,  1.66it/s]Extractor Estimating: 131it [01:41,  1.64it/s]Extractor Estimating: 132it [01:42,  1.61it/s]Extractor Estimating: 133it [01:43,  1.59it/s]Extractor Estimating: 134it [01:43,  1.60it/s]Extractor Estimating: 135it [01:44,  1.60it/s]Extractor Estimating: 136it [01:45,  1.58it/s]Extractor Estimating: 137it [01:45,  1.53it/s]Extractor Estimating: 138it [01:46,  1.50it/s]Extractor Estimating: 139it [01:47,  1.53it/s]Extractor Estimating: 140it [01:47,  1.55it/s]Extractor Estimating: 141it [01:48,  1.58it/s]Extractor Estimating: 142it [01:49,  1.55it/s]Extractor Estimating: 143it [01:49,  1.56it/s]Extractor Estimating: 144it [01:50,  1.57it/s]Extractor Estimating: 145it [01:50,  1.61it/s]Extractor Estimating: 146it [01:51,  1.61it/s]Extractor Estimating: 147it [01:52,  1.60it/s]Extractor Estimating: 148it [01:52,  1.54it/s]Extractor Estimating: 149it [01:53,  1.52it/s]Extractor Estimating: 150it [01:54,  1.56it/s]Extractor Estimating: 151it [01:54,  1.55it/s]Extractor Estimating: 152it [01:55,  1.57it/s]Extractor Estimating: 153it [01:56,  1.58it/s]Extractor Estimating: 154it [01:56,  1.64it/s]Extractor Estimating: 155it [01:57,  1.62it/s]Extractor Estimating: 156it [01:57,  1.57it/s]Extractor Estimating: 157it [01:58,  1.59it/s]Extractor Estimating: 158it [01:59,  1.62it/s]Extractor Estimating: 159it [01:59,  1.65it/s]Extractor Estimating: 160it [02:00,  1.68it/s]Extractor Estimating: 161it [02:00,  1.70it/s]Extractor Estimating: 162it [02:01,  1.62it/s]Extractor Estimating: 163it [02:02,  1.60it/s]Extractor Estimating: 164it [02:02,  1.56it/s]Extractor Estimating: 165it [02:03,  1.58it/s]Extractor Estimating: 166it [02:04,  1.57it/s]Extractor Estimating: 167it [02:04,  1.56it/s]Extractor Estimating: 168it [02:05,  1.62it/s]Extractor Estimating: 169it [02:05,  1.67it/s]Extractor Estimating: 170it [02:06,  1.69it/s]Extractor Estimating: 171it [02:07,  1.72it/s]Extractor Estimating: 172it [02:07,  1.68it/s]Extractor Estimating: 173it [02:08,  1.64it/s]Extractor Estimating: 174it [02:08,  1.64it/s]Extractor Estimating: 175it [02:09,  1.60it/s]Extractor Estimating: 176it [02:10,  1.62it/s]Extractor Estimating: 177it [02:10,  1.65it/s]Extractor Estimating: 178it [02:11,  1.67it/s]Extractor Estimating: 179it [02:11,  1.72it/s]Extractor Estimating: 180it [02:12,  1.74it/s]Extractor Estimating: 181it [02:13,  1.73it/s]Extractor Estimating: 182it [02:13,  1.72it/s]Extractor Estimating: 183it [02:14,  1.66it/s]Extractor Estimating: 184it [02:14,  1.68it/s]Extractor Estimating: 185it [02:15,  1.63it/s]Extractor Estimating: 186it [02:16,  1.65it/s]Extractor Estimating: 187it [02:16,  1.61it/s]Extractor Estimating: 188it [02:17,  1.64it/s]Extractor Estimating: 189it [02:17,  1.66it/s]Extractor Estimating: 190it [02:18,  1.66it/s]Extractor Estimating: 191it [02:19,  1.66it/s]Extractor Estimating: 192it [02:19,  1.53it/s]Extractor Estimating: 193it [02:20,  1.56it/s]Extractor Estimating: 194it [02:21,  1.53it/s]Extractor Estimating: 195it [02:21,  1.61it/s]Extractor Estimating: 196it [02:22,  1.61it/s]Extractor Estimating: 197it [02:22,  1.64it/s]Extractor Estimating: 198it [02:23,  1.65it/s]Extractor Estimating: 199it [02:24,  1.68it/s]Extractor Estimating: 200it [02:24,  1.66it/s]Extractor Estimating: 201it [02:25,  1.60it/s]Extractor Estimating: 202it [02:26,  1.60it/s]Extractor Estimating: 203it [02:26,  1.59it/s]Extractor Estimating: 204it [02:27,  1.66it/s]Extractor Estimating: 205it [02:27,  1.69it/s]Extractor Estimating: 206it [02:28,  1.68it/s]Extractor Estimating: 207it [02:29,  1.64it/s]Extractor Estimating: 208it [02:29,  1.63it/s]Extractor Estimating: 209it [02:30,  1.64it/s]Extractor Estimating: 210it [02:30,  1.60it/s]Extractor Estimating: 211it [02:31,  1.62it/s]Extractor Estimating: 212it [02:32,  1.54it/s]Extractor Estimating: 213it [02:32,  1.55it/s]Extractor Estimating: 214it [02:33,  1.59it/s]Extractor Estimating: 215it [02:34,  1.60it/s]Extractor Estimating: 216it [02:34,  1.62it/s]Extractor Estimating: 217it [02:35,  1.63it/s]Extractor Estimating: 218it [02:35,  1.67it/s]Extractor Estimating: 219it [02:36,  1.66it/s]Extractor Estimating: 220it [02:37,  1.63it/s]Extractor Estimating: 221it [02:37,  1.58it/s]Extractor Estimating: 222it [02:38,  1.61it/s]Extractor Estimating: 223it [02:39,  1.57it/s]Extractor Estimating: 224it [02:39,  1.54it/s]Extractor Estimating: 225it [02:40,  1.57it/s]Extractor Estimating: 226it [02:41,  1.52it/s]Extractor Estimating: 227it [02:41,  1.57it/s]Extractor Estimating: 228it [02:42,  1.60it/s]Extractor Estimating: 229it [02:42,  1.63it/s]Extractor Estimating: 230it [02:43,  1.64it/s]Extractor Estimating: 231it [02:44,  1.58it/s]Extractor Estimating: 232it [02:44,  1.58it/s]Extractor Estimating: 233it [02:45,  1.61it/s]Extractor Estimating: 234it [02:45,  1.61it/s]Extractor Estimating: 235it [02:46,  1.58it/s]Extractor Estimating: 236it [02:47,  1.62it/s]Extractor Estimating: 237it [02:47,  1.61it/s]Extractor Estimating: 238it [02:48,  1.60it/s]Extractor Estimating: 239it [02:49,  1.63it/s]Extractor Estimating: 240it [02:49,  1.61it/s]Extractor Estimating: 241it [02:50,  1.56it/s]Extractor Estimating: 242it [02:50,  1.60it/s]Extractor Estimating: 243it [02:51,  1.62it/s]Extractor Estimating: 244it [02:52,  1.64it/s]Extractor Estimating: 245it [02:52,  1.64it/s]Extractor Estimating: 246it [02:53,  1.59it/s]Extractor Estimating: 247it [02:54,  1.58it/s]Extractor Estimating: 248it [02:54,  1.62it/s]Extractor Estimating: 249it [02:55,  1.62it/s]Extractor Estimating: 250it [02:55,  1.54it/s]Extractor Estimating: 250it [02:55,  1.42it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4995 mean pseudo reward: 0.9144202043025711
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 26311
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26411, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_5_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26411, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.248, loss:1375.0914
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.966, loss:1275.9530
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.965, loss:1208.1552
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.983, loss:1222.9842
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.964, loss:1170.2235
>> valid entity prec:0.5281, rec:0.4642, f1:0.4941
>> valid relation prec:0.2753, rec:0.0101, f1:0.0194
>> valid relation with NER prec:0.2753, rec:0.0101, f1:0.0194
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.342, loss:1146.9901
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.974, loss:1132.1512
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.973, loss:1130.7232
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.972, loss:1059.9483
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.971, loss:1090.5923
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5261, rec:0.3662, f1:0.4318
>> valid relation prec:0.2985, rec:0.0023, f1:0.0046
>> valid relation with NER prec:0.2985, rec:0.0023, f1:0.0046
g_step 1100, step 55, avg_time 3.318, loss:1054.3938
g_step 1200, step 155, avg_time 0.976, loss:1061.3882
g_step 1300, step 46, avg_time 0.977, loss:1022.5555
g_step 1400, step 146, avg_time 0.969, loss:963.8097
g_step 1500, step 37, avg_time 0.966, loss:967.5595
>> valid entity prec:0.4814, rec:0.4683, f1:0.4748
>> valid relation prec:0.1772, rec:0.0101, f1:0.0190
>> valid relation with NER prec:0.1772, rec:0.0101, f1:0.0190
g_step 1600, step 137, avg_time 3.326, loss:929.6505
g_step 1700, step 28, avg_time 0.980, loss:984.2023
g_step 1800, step 128, avg_time 0.971, loss:909.0795
g_step 1900, step 19, avg_time 0.961, loss:880.1489
g_step 2000, step 119, avg_time 0.967, loss:859.9023
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5070, rec:0.4045, f1:0.4500
>> valid relation prec:0.2478, rec:0.0097, f1:0.0187
>> valid relation with NER prec:0.2478, rec:0.0097, f1:0.0187
g_step 2100, step 10, avg_time 3.327, loss:904.7143
g_step 2200, step 110, avg_time 0.966, loss:830.4077
g_step 2300, step 1, avg_time 0.968, loss:851.2447
g_step 2400, step 101, avg_time 0.968, loss:812.9429
g_step 2500, step 201, avg_time 0.964, loss:824.1050
>> valid entity prec:0.5157, rec:0.5364, f1:0.5258
>> valid relation prec:0.1171, rec:0.0153, f1:0.0270
>> valid relation with NER prec:0.1171, rec:0.0153, f1:0.0270
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 92, avg_time 3.344, loss:744.3014
g_step 2700, step 192, avg_time 0.967, loss:821.0473
g_step 2800, step 83, avg_time 0.977, loss:757.5419
g_step 2900, step 183, avg_time 0.961, loss:748.0336
g_step 3000, step 74, avg_time 0.968, loss:710.0138
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5001, rec:0.4417, f1:0.4691
>> valid relation prec:0.1147, rec:0.0119, f1:0.0216
>> valid relation with NER prec:0.1147, rec:0.0119, f1:0.0216
g_step 3100, step 174, avg_time 3.326, loss:735.4054
g_step 3200, step 65, avg_time 0.962, loss:703.5612
g_step 3300, step 165, avg_time 0.970, loss:670.4161
g_step 3400, step 56, avg_time 0.959, loss:674.0523
g_step 3500, step 156, avg_time 0.974, loss:681.5299
>> valid entity prec:0.5325, rec:0.2996, f1:0.3835
>> valid relation prec:0.0967, rec:0.0044, f1:0.0084
>> valid relation with NER prec:0.0967, rec:0.0044, f1:0.0084
g_step 3600, step 47, avg_time 3.285, loss:650.3321
g_step 3700, step 147, avg_time 0.972, loss:641.2081
g_step 3800, step 38, avg_time 0.964, loss:620.0995
g_step 3900, step 138, avg_time 0.970, loss:632.8161
g_step 4000, step 29, avg_time 0.960, loss:617.6787
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4310, rec:0.4096, f1:0.4200
>> valid relation prec:0.1217, rec:0.0194, f1:0.0335
>> valid relation with NER prec:0.1217, rec:0.0194, f1:0.0335
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 129, avg_time 3.322, loss:590.3700
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:40:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:40:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-40-18_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:40:20 - WARNING - datasets.builder -   Using custom data configuration default-43dc74ced24d19b8
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-43dc74ced24d19b8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:40:22,136 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:40:22,145 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:40:22,145 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:40:22,146 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:40:22,185 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:40:22,202 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:40:22,202 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:40:22,202 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:40:22,202 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:40:22,202 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:40:22,203 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:40:22,435 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:40:25,533 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:40:25,533 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-43dc74ced24d19b8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 10:40:25 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14b562fab3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:03,  1.61ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.63ba/s] 50%|█████     | 3/6 [00:01<00:00,  3.27ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.69ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.95ba/s]100%|██████████| 6/6 [00:01<00:00,  3.99ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.21ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.87ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.14ba/s] 44%|████▍     | 4/9 [00:00<00:01,  4.29ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.37ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.44ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.45ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.47ba/s]100%|██████████| 9/9 [00:02<00:00,  5.02ba/s]100%|██████████| 9/9 [00:02<00:00,  4.49ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.59ba/s] 50%|█████     | 3/6 [00:00<00:00,  6.12ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  7.91ba/s]100%|██████████| 6/6 [00:00<00:00,  8.78ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  5.72ba/s] 33%|███▎      | 3/9 [00:00<00:00,  8.76ba/s] 56%|█████▌    | 5/9 [00:00<00:00,  9.53ba/s] 67%|██████▋   | 6/9 [00:00<00:00,  9.40ba/s] 89%|████████▉ | 8/9 [00:00<00:00,  9.96ba/s]100%|██████████| 9/9 [00:00<00:00,  9.88ba/s]
[INFO|trainer.py:414] 2023-08-28 10:40:31,720 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:40:31,848 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:40:31,848 >>   Num examples = 5036
[INFO|trainer.py:1149] 2023-08-28 10:40:31,848 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:40:31,848 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:40:31,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:40:31,848 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:40:31,848 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<01:59,  3.30it/s]  1%|          | 2/395 [00:00<01:55,  3.40it/s]  1%|          | 3/395 [00:00<01:54,  3.43it/s]  1%|          | 4/395 [00:01<01:53,  3.45it/s]  1%|▏         | 5/395 [00:01<01:53,  3.43it/s]  2%|▏         | 6/395 [00:01<01:53,  3.43it/s]  2%|▏         | 7/395 [00:02<01:53,  3.42it/s]  2%|▏         | 8/395 [00:02<01:53,  3.42it/s]  2%|▏         | 9/395 [00:02<01:52,  3.42it/s]  3%|▎         | 10/395 [00:02<01:52,  3.42it/s]  3%|▎         | 11/395 [00:03<01:52,  3.42it/s]  3%|▎         | 12/395 [00:03<01:51,  3.42it/s]  3%|▎         | 13/395 [00:03<01:51,  3.42it/s]  4%|▎         | 14/395 [00:04<01:51,  3.42it/s]  4%|▍         | 15/395 [00:04<01:51,  3.41it/s]  4%|▍         | 16/395 [00:04<01:50,  3.41it/s]  4%|▍         | 17/395 [00:04<01:50,  3.42it/s]  5%|▍         | 18/395 [00:05<01:50,  3.41it/s]  5%|▍         | 19/395 [00:05<01:50,  3.42it/s]  5%|▌         | 20/395 [00:05<01:49,  3.42it/s]  5%|▌         | 21/395 [00:06<01:49,  3.42it/s]  6%|▌         | 22/395 [00:06<01:49,  3.42it/s]  6%|▌         | 23/395 [00:06<01:48,  3.42it/s]  6%|▌         | 24/395 [00:07<01:48,  3.42it/s]  6%|▋         | 25/395 [00:07<01:48,  3.42it/s]  7%|▋         | 26/395 [00:07<01:48,  3.41it/s]  7%|▋         | 27/395 [00:07<01:47,  3.41it/s]  7%|▋         | 28/395 [00:08<01:47,  3.42it/s]  7%|▋         | 29/395 [00:08<01:47,  3.41it/s]  8%|▊         | 30/395 [00:08<01:46,  3.41it/s]  8%|▊         | 31/395 [00:09<01:46,  3.42it/s]  8%|▊         | 32/395 [00:09<01:46,  3.41it/s]  8%|▊         | 33/395 [00:09<01:46,  3.41it/s]  9%|▊         | 34/395 [00:09<01:45,  3.41it/s]  9%|▉         | 35/395 [00:10<01:45,  3.41it/s]  9%|▉         | 36/395 [00:10<01:45,  3.41it/s]  9%|▉         | 37/395 [00:10<01:44,  3.41it/s] 10%|▉         | 38/395 [00:11<01:44,  3.41it/s] 10%|▉         | 39/395 [00:11<01:44,  3.41it/s] 10%|█         | 40/395 [00:11<01:44,  3.41it/s] 10%|█         | 41/395 [00:12<01:43,  3.41it/s] 11%|█         | 42/395 [00:12<01:43,  3.41it/s] 11%|█         | 43/395 [00:12<01:43,  3.41it/s] 11%|█         | 44/395 [00:12<01:42,  3.41it/s] 11%|█▏        | 45/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 46/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 47/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 48/395 [00:14<01:41,  3.41it/s] 12%|█▏        | 49/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 50/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 51/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 52/395 [00:15<01:40,  3.41it/s] 13%|█▎        | 53/395 [00:15<01:40,  3.41it/s] 14%|█▎        | 54/395 [00:15<01:40,  3.40it/s] 14%|█▍        | 55/395 [00:16<01:39,  3.41it/s] 14%|█▍        | 56/395 [00:16<01:39,  3.41it/s] 14%|█▍        | 57/395 [00:16<01:39,  3.41it/s] 15%|█▍        | 58/395 [00:16<01:38,  3.41it/s] 15%|█▍        | 59/395 [00:17<01:38,  3.41it/s] 15%|█▌        | 60/395 [00:17<01:38,  3.40it/s] 15%|█▌        | 61/395 [00:17<01:38,  3.40it/s] 16%|█▌        | 62/395 [00:18<01:37,  3.41it/s] 16%|█▌        | 63/395 [00:18<01:37,  3.41it/s] 16%|█▌        | 64/395 [00:18<01:37,  3.41it/s] 16%|█▋        | 65/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 66/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 67/395 [00:19<01:36,  3.41it/s] 17%|█▋        | 68/395 [00:19<01:35,  3.41it/s] 17%|█▋        | 69/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 70/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 71/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 72/395 [00:21<01:34,  3.41it/s] 18%|█▊        | 73/395 [00:21<01:34,  3.41it/s] 19%|█▊        | 74/395 [00:21<01:34,  3.41it/s] 19%|█▉        | 75/395 [00:21<01:33,  3.41it/s] 19%|█▉        | 76/395 [00:22<01:33,  3.41it/s] 19%|█▉        | 77/395 [00:22<01:33,  3.41it/s] 20%|█▉        | 78/395 [00:22<01:33,  3.41it/s] 20%|██        | 79/395 [00:23<01:25,  3.71it/s][INFO|trainer.py:2140] 2023-08-28 10:40:54,931 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:40:54,931 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 10:40:54,931 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.63it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.08it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.57it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.41it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.87it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.50it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.08it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.93it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.99it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.18it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.20it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.27it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.17it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.08it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.94it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.77it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.74it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.83it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.88it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.03it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.15it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.20it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.06it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.96it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.79it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.72it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.82it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.85it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.07it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.08it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.24it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.18it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.93it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.87it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.72it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.84it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.93it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.11it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.09it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.22it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.07it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.01it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.89it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.66it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.77it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.82it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.96it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.14it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.23it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.05it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.03it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.81it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.78it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.76it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.88it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.96it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.15it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.19it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.17it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.04it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.91it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.80it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.70it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.86it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.98it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.14it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.14it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.17it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.98it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.87it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.80it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.77it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.80it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.78it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.10it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.17it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.14it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.87it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.87it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.74it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.84it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.93it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.01it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.99it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.05it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.12it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.96it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.86it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.76it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.76it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.87it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.96it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.12it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.11it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.08it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.99it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.71it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.69it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.64it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.91it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.92it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.11it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.06it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.06it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.94it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.80it/s][A
 50%|████▉     | 537/1083 [00:11<00:13, 39.36it/s][A
 50%|█████     | 542/1083 [00:12<00:13, 41.06it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 42.24it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 43.19it/s][A
 51%|█████▏    | 557/1083 [00:12<00:12, 43.78it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.24it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.51it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.71it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.36it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.35it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.52it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.67it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.93it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.10it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.17it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.27it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.03it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.70it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.54it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.58it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.71it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.86it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.96it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.05it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.19it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.05it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.84it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.66it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.62it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.72it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.90it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.01it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.13it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.25it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.03it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.94it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 44.66it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.70it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.78it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.01it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.14it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.12it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 45.03it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.84it/s][A
 70%|███████   | 762/1083 [00:17<00:08, 36.99it/s][A
 71%|███████   | 767/1083 [00:17<00:08, 39.19it/s][A
 71%|███████▏  | 772/1083 [00:17<00:07, 40.77it/s][A
 72%|███████▏  | 777/1083 [00:17<00:07, 42.08it/s][A
 72%|███████▏  | 782/1083 [00:17<00:07, 43.00it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 43.65it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.14it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.41it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.20it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.32it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.33it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.62it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.86it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.03it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.12it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.14it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.85it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.66it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.60it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.65it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.78it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.89it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.08it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.12it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.13it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.95it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.69it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.69it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.75it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.84it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.95it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.10it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.09it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.04it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.91it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.78it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.67it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.76it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.77it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.95it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.06it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 45.17it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.09it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.88it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.75it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 40.07it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 41.62it/s][A
 92%|█████████▏| 997/1083 [00:22<00:02, 42.67it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 43.46it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 43.92it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.46it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.70it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.81it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.46it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.19it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.35it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.55it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.75it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.92it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.03it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 45.05it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 28.54it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 32.22it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 35.31it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 37.90it/s][A                                                
                                                   [A 20%|██        | 79/395 [00:47<01:25,  3.71it/s]
100%|██████████| 1083/1083 [00:24<00:00, 37.90it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 10:41:19,643 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-28 10:41:19,743 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:41:23,590 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:41:23,839 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:41:23,955 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [01:00<59:44, 11.38s/it] 21%|██        | 81/395 [01:00<42:12,  8.06s/it] 21%|██        | 82/395 [01:01<29:54,  5.73s/it] 21%|██        | 83/395 [01:01<21:19,  4.10s/it] 21%|██▏       | 84/395 [01:01<15:20,  2.96s/it] 22%|██▏       | 85/395 [01:01<11:09,  2.16s/it] 22%|██▏       | 86/395 [01:02<08:14,  1.60s/it] 22%|██▏       | 87/395 [01:02<06:11,  1.21s/it] 22%|██▏       | 88/395 [01:02<04:46,  1.07it/s] 23%|██▎       | 89/395 [01:03<03:46,  1.35it/s] 23%|██▎       | 90/395 [01:03<03:05,  1.65it/s] 23%|██▎       | 91/395 [01:03<02:35,  1.95it/s] 23%|██▎       | 92/395 [01:03<02:19,  2.17it/s] 24%|██▎       | 93/395 [01:04<02:04,  2.43it/s] 24%|██▍       | 94/395 [01:04<01:53,  2.66it/s] 24%|██▍       | 95/395 [01:04<01:45,  2.85it/s] 24%|██▍       | 96/395 [01:05<01:39,  3.00it/s] 25%|██▍       | 97/395 [01:05<01:35,  3.11it/s] 25%|██▍       | 98/395 [01:05<01:32,  3.19it/s] 25%|██▌       | 99/395 [01:06<01:30,  3.26it/s] 25%|██▌       | 100/395 [01:06<01:29,  3.30it/s] 26%|██▌       | 101/395 [01:06<01:28,  3.33it/s] 26%|██▌       | 102/395 [01:06<01:27,  3.35it/s] 26%|██▌       | 103/395 [01:07<01:29,  3.25it/s] 26%|██▋       | 104/395 [01:07<01:28,  3.30it/s] 27%|██▋       | 105/395 [01:07<01:27,  3.33it/s] 27%|██▋       | 106/395 [01:08<01:26,  3.36it/s] 27%|██▋       | 107/395 [01:08<01:25,  3.37it/s] 27%|██▋       | 108/395 [01:08<01:24,  3.38it/s] 28%|██▊       | 109/395 [01:09<01:24,  3.39it/s] 28%|██▊       | 110/395 [01:09<01:24,  3.39it/s] 28%|██▊       | 111/395 [01:09<01:23,  3.40it/s] 28%|██▊       | 112/395 [01:09<01:23,  3.40it/s] 29%|██▊       | 113/395 [01:10<01:22,  3.40it/s] 29%|██▉       | 114/395 [01:10<01:26,  3.26it/s] 29%|██▉       | 115/395 [01:10<01:24,  3.31it/s] 29%|██▉       | 116/395 [01:11<01:23,  3.34it/s] 30%|██▉       | 117/395 [01:11<01:22,  3.36it/s] 30%|██▉       | 118/395 [01:11<01:22,  3.37it/s] 30%|███       | 119/395 [01:11<01:21,  3.38it/s] 30%|███       | 120/395 [01:12<01:21,  3.39it/s] 31%|███       | 121/395 [01:12<01:20,  3.40it/s] 31%|███       | 122/395 [01:12<01:20,  3.40it/s] 31%|███       | 123/395 [01:13<01:19,  3.40it/s] 31%|███▏      | 124/395 [01:13<01:19,  3.40it/s] 32%|███▏      | 125/395 [01:13<01:22,  3.28it/s] 32%|███▏      | 126/395 [01:14<01:21,  3.31it/s] 32%|███▏      | 127/395 [01:14<01:20,  3.34it/s] 32%|███▏      | 128/395 [01:14<01:19,  3.36it/s] 33%|███▎      | 129/395 [01:14<01:18,  3.37it/s] 33%|███▎      | 130/395 [01:15<01:18,  3.38it/s] 33%|███▎      | 131/395 [01:15<01:17,  3.39it/s] 33%|███▎      | 132/395 [01:15<01:17,  3.40it/s] 34%|███▎      | 133/395 [01:16<01:21,  3.22it/s] 34%|███▍      | 134/395 [01:16<01:19,  3.26it/s] 34%|███▍      | 135/395 [01:16<01:18,  3.30it/s] 34%|███▍      | 136/395 [01:17<01:17,  3.33it/s] 35%|███▍      | 137/395 [01:17<01:16,  3.35it/s] 35%|███▍      | 138/395 [01:17<01:16,  3.37it/s] 35%|███▌      | 139/395 [01:17<01:15,  3.38it/s] 35%|███▌      | 140/395 [01:18<01:15,  3.39it/s] 36%|███▌      | 141/395 [01:18<01:14,  3.39it/s] 36%|███▌      | 142/395 [01:18<01:14,  3.40it/s] 36%|███▌      | 143/395 [01:19<01:16,  3.30it/s] 36%|███▋      | 144/395 [01:19<01:15,  3.33it/s] 37%|███▋      | 145/395 [01:19<01:14,  3.35it/s] 37%|███▋      | 146/395 [01:20<01:13,  3.36it/s] 37%|███▋      | 147/395 [01:20<01:13,  3.38it/s] 37%|███▋      | 148/395 [01:20<01:13,  3.38it/s] 38%|███▊      | 149/395 [01:20<01:12,  3.38it/s] 38%|███▊      | 150/395 [01:21<01:12,  3.39it/s] 38%|███▊      | 151/395 [01:21<01:11,  3.40it/s] 38%|███▊      | 152/395 [01:21<01:11,  3.40it/s] 39%|███▊      | 153/395 [01:22<01:11,  3.40it/s] 39%|███▉      | 154/395 [01:22<01:12,  3.31it/s] 39%|███▉      | 155/395 [01:22<01:11,  3.34it/s] 39%|███▉      | 156/395 [01:23<01:11,  3.36it/s] 40%|███▉      | 157/395 [01:23<01:10,  3.37it/s] 40%|████      | 158/395 [01:23<01:04,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 10:41:55,369 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:41:55,369 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 10:41:55,369 >>   Batch size = 8
{'eval_loss': 0.880391001701355, 'eval_runtime': 24.4647, 'eval_samples_per_second': 353.897, 'eval_steps_per_second': 44.268, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.31it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.11it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.40it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.61it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.03it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.61it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.13it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.90it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.91it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.17it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.30it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.30it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.21it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.09it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.19it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.25it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.46it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.70it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.83it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.07it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.18it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.17it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.92it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.82it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.66it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.67it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.84it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 45.01it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.26it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.22it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.18it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.01it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.80it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.78it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.62it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.78it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.89it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.12it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.18it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.17it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.01it/s][A
 20%|█▉        | 212/1083 [00:04<00:20, 43.43it/s][A
 20%|██        | 217/1083 [00:04<00:19, 43.91it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.18it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.36it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.54it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.82it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.96it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.14it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.92it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.88it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.77it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.74it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.83it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.92it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.10it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.23it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.92it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.98it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.82it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.79it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.78it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.80it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.88it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.98it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.11it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.14it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.05it/s][A
 32%|███▏      | 347/1083 [00:07<00:17, 42.15it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 43.07it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 43.62it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.10it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.44it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.58it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.79it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.92it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.71it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.72it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.71it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.79it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.92it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.91it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.05it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.00it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.97it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.75it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.67it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.73it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.85it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 45.01it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.02it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.14it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.14it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.01it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.99it/s][A
 45%|████▍     | 482/1083 [00:10<00:14, 41.24it/s][A
 45%|████▍     | 487/1083 [00:10<00:14, 42.47it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 43.31it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 43.89it/s][A
 46%|████▋     | 502/1083 [00:11<00:13, 44.32it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.57it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.78it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.81it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.55it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.54it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.65it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.78it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.78it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.98it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.11it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.16it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.04it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.78it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.62it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.65it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.80it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.95it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.13it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 45.19it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.09it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.99it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 44.80it/s][A
 57%|█████▋    | 617/1083 [00:13<00:11, 42.11it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 43.02it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 43.59it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.01it/s][A
 59%|█████▉    | 637/1083 [00:14<00:10, 44.53it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.80it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.85it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.79it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.44it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.43it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.63it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.74it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.98it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 45.16it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.17it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.13it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.89it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.68it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.63it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.73it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.81it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.80it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.90it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 45.10it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.15it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.01it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.82it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 41.55it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 42.59it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 43.38it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 43.97it/s][A
 71%|███████▏  | 772/1083 [00:17<00:07, 44.35it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.55it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.87it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.73it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.53it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.57it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.76it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.83it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.00it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 45.02it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.04it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.13it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.92it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.75it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.61it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.73it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.77it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.94it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.09it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 45.21it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.09it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.04it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.81it/s][A
 82%|████████▏ | 887/1083 [00:19<00:05, 38.46it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 40.30it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 41.63it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 42.74it/s][A
 84%|████████▎ | 907/1083 [00:20<00:04, 43.51it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 43.87it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.29it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.56it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.27it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.32it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 44.52it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.70it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.91it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 45.04it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 45.08it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 41.07it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 43.39it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 43.84it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.04it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 44.28it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.53it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.83it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.99it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 45.07it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.67it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.60it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.69it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 43.88it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 44.31it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.65it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.72it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.90it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:01, 27.37it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 31.21it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 34.40it/s][A
 98%|█████████▊| 1062/1083 [00:24<00:00, 37.17it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 39.36it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 41.03it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 42.17it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 43.19it/s][A                                                 
                                                   [A 40%|████      | 158/395 [01:48<01:04,  3.68it/s]
100%|██████████| 1083/1083 [00:24<00:00, 43.19it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 10:42:20,340 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-28 10:42:20,589 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:42:24,108 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:42:24,212 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:42:24,266 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:59<42:48, 10.88s/it] 41%|████      | 160/395 [01:59<30:12,  7.71s/it] 41%|████      | 161/395 [01:59<21:24,  5.49s/it] 41%|████      | 162/395 [02:00<15:15,  3.93s/it] 41%|████▏     | 163/395 [02:00<10:58,  2.84s/it] 42%|████▏     | 164/395 [02:00<07:59,  2.08s/it] 42%|████▏     | 165/395 [02:00<05:54,  1.54s/it] 42%|████▏     | 166/395 [02:01<04:27,  1.17s/it] 42%|████▏     | 167/395 [02:01<03:26,  1.11it/s] 43%|████▎     | 168/395 [02:01<02:43,  1.39it/s] 43%|████▎     | 169/395 [02:02<02:13,  1.69it/s] 43%|████▎     | 170/395 [02:02<01:53,  1.99it/s] 43%|████▎     | 171/395 [02:02<01:40,  2.23it/s] 44%|████▎     | 172/395 [02:03<01:29,  2.49it/s] 44%|████▍     | 173/395 [02:03<01:21,  2.71it/s] 44%|████▍     | 174/395 [02:03<01:16,  2.89it/s] 44%|████▍     | 175/395 [02:03<01:12,  3.02it/s] 45%|████▍     | 176/395 [02:04<01:09,  3.13it/s] 45%|████▍     | 177/395 [02:04<01:07,  3.21it/s] 45%|████▌     | 178/395 [02:04<01:06,  3.27it/s] 45%|████▌     | 179/395 [02:05<01:05,  3.31it/s] 46%|████▌     | 180/395 [02:05<01:04,  3.34it/s] 46%|████▌     | 181/395 [02:05<01:03,  3.36it/s] 46%|████▌     | 182/395 [02:05<01:04,  3.29it/s] 46%|████▋     | 183/395 [02:06<01:03,  3.33it/s] 47%|████▋     | 184/395 [02:06<01:02,  3.35it/s] 47%|████▋     | 185/395 [02:06<01:02,  3.36it/s] 47%|████▋     | 186/395 [02:07<01:01,  3.38it/s] 47%|████▋     | 187/395 [02:07<01:01,  3.39it/s] 48%|████▊     | 188/395 [02:07<01:01,  3.39it/s] 48%|████▊     | 189/395 [02:08<01:00,  3.40it/s] 48%|████▊     | 190/395 [02:08<01:00,  3.40it/s] 48%|████▊     | 191/395 [02:08<01:00,  3.40it/s] 49%|████▊     | 192/395 [02:08<00:59,  3.40it/s] 49%|████▉     | 193/395 [02:09<01:01,  3.30it/s] 49%|████▉     | 194/395 [02:09<01:00,  3.33it/s] 49%|████▉     | 195/395 [02:09<00:59,  3.35it/s] 50%|████▉     | 196/395 [02:10<00:59,  3.37it/s] 50%|████▉     | 197/395 [02:10<00:58,  3.38it/s] 50%|█████     | 198/395 [02:10<00:58,  3.39it/s] 50%|█████     | 199/395 [02:11<00:57,  3.39it/s] 51%|█████     | 200/395 [02:11<00:57,  3.39it/s] 51%|█████     | 201/395 [02:11<00:57,  3.40it/s] 51%|█████     | 202/395 [02:11<00:56,  3.40it/s] 51%|█████▏    | 203/395 [02:12<00:56,  3.40it/s] 52%|█████▏    | 204/395 [02:12<00:57,  3.32it/s] 52%|█████▏    | 205/395 [02:12<00:56,  3.35it/s] 52%|█████▏    | 206/395 [02:13<00:56,  3.36it/s] 52%|█████▏    | 207/395 [02:13<00:55,  3.38it/s] 53%|█████▎    | 208/395 [02:13<00:55,  3.38it/s] 53%|█████▎    | 209/395 [02:13<00:54,  3.39it/s] 53%|█████▎    | 210/395 [02:14<00:54,  3.39it/s] 53%|█████▎    | 211/395 [02:14<00:54,  3.39it/s] 54%|█████▎    | 212/395 [02:14<00:53,  3.40it/s] 54%|█████▍    | 213/395 [02:15<00:53,  3.40it/s] 54%|█████▍    | 214/395 [02:15<00:53,  3.40it/s] 54%|█████▍    | 215/395 [02:15<00:54,  3.32it/s] 55%|█████▍    | 216/395 [02:16<00:53,  3.35it/s] 55%|█████▍    | 217/395 [02:16<00:52,  3.36it/s] 55%|█████▌    | 218/395 [02:16<00:52,  3.37it/s] 55%|█████▌    | 219/395 [02:16<00:51,  3.39it/s] 56%|█████▌    | 220/395 [02:17<00:51,  3.39it/s] 56%|█████▌    | 221/395 [02:17<00:51,  3.39it/s] 56%|█████▌    | 222/395 [02:17<00:50,  3.40it/s] 56%|█████▋    | 223/395 [02:18<00:50,  3.40it/s] 57%|█████▋    | 224/395 [02:18<00:50,  3.40it/s] 57%|█████▋    | 225/395 [02:18<00:49,  3.40it/s] 57%|█████▋    | 226/395 [02:18<00:49,  3.40it/s] 57%|█████▋    | 227/395 [02:19<00:49,  3.40it/s] 58%|█████▊    | 228/395 [02:19<00:49,  3.40it/s] 58%|█████▊    | 229/395 [02:19<00:48,  3.41it/s] 58%|█████▊    | 230/395 [02:20<00:48,  3.40it/s] 58%|█████▊    | 231/395 [02:20<00:48,  3.40it/s] 59%|█████▊    | 232/395 [02:20<00:49,  3.28it/s] 59%|█████▉    | 233/395 [02:21<00:48,  3.32it/s] 59%|█████▉    | 234/395 [02:21<00:48,  3.34it/s] 59%|█████▉    | 235/395 [02:21<00:47,  3.36it/s] 60%|█████▉    | 236/395 [02:21<00:47,  3.37it/s] 60%|██████    | 237/395 [02:22<00:42,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 10:42:54,032 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:42:54,032 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 10:42:54,032 >>   Batch size = 8
{'eval_loss': 0.8650857210159302, 'eval_runtime': 24.5202, 'eval_samples_per_second': 353.096, 'eval_steps_per_second': 44.168, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.11it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.08it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.51it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.32it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.87it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.51it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.20it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.08it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.93it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.99it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.11it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.21it/s][A
  6%|▌         | 67/1083 [00:01<00:23, 43.66it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 44.10it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.36it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.47it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.54it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.59it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.75it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.91it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.83it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.88it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.03it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.10it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.14it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.91it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.92it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.89it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.87it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.91it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.79it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.94it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.99it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 45.09it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.97it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.95it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.93it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.88it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.95it/s][A
 19%|█▊        | 202/1083 [00:04<00:20, 43.93it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.36it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.61it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.67it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.70it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.77it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.89it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.91it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 44.74it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.87it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.01it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.05it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.98it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.82it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.73it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.64it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.62it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.49it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.45it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.60it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.61it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.65it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.81it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.92it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.85it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.78it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.77it/s][A
 31%|███       | 337/1083 [00:07<00:17, 42.74it/s][A
 32%|███▏      | 342/1083 [00:07<00:17, 43.57it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.11it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.34it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.65it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.92it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.97it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.96it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.65it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.58it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.79it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.88it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.93it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.05it/s][A
 38%|███▊      | 407/1083 [00:09<00:14, 45.08it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.03it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.94it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.83it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.64it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.86it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.83it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.85it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.02it/s][A
 42%|████▏     | 452/1083 [00:10<00:13, 45.10it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.10it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.93it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.76it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.03it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.31it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.61it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.64it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.78it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.77it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.92it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.85it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.69it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.79it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.85it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.87it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.95it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.98it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 45.02it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.91it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.85it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.71it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.78it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.84it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.94it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.01it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.87it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.93it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.98it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.91it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.84it/s][A
 56%|█████▌    | 607/1083 [00:13<00:11, 42.63it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 43.45it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 43.92it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.30it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.53it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.70it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.70it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.75it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.58it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.66it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.88it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.84it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.94it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.96it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 45.00it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.89it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.80it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.70it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.61it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.82it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.03it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 45.08it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 45.01it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 45.11it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 45.02it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.93it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.82it/s][A
 69%|██████▊   | 742/1083 [00:16<00:08, 41.45it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 42.62it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 43.49it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.00it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.43it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.63it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.65it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.56it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.35it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.37it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.60it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.87it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 45.00it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 45.15it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.16it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 45.05it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.84it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.60it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.54it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.58it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.75it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.94it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 45.10it/s][A
 79%|███████▉  | 857/1083 [00:19<00:04, 45.27it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.20it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.92it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.78it/s][A
 81%|████████  | 877/1083 [00:19<00:05, 39.84it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 41.40it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 42.55it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 43.32it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 43.87it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.27it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.59it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.78it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.50it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.46it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.48it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.67it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.84it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 45.04it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 45.12it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 45.20it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.91it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.74it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.61it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.63it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.72it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.88it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.94it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 45.08it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.19it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 45.16it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.97it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 43.10it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 43.61it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.01it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 43.89it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.34it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.65it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.80it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.85it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.56it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.55it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.62it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.66it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.77it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.84it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.07it/s][A                                                 
                                                   [A 60%|██████    | 237/395 [02:46<00:42,  3.68it/s]
100%|██████████| 1083/1083 [00:24<00:00, 45.07it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 10:43:18,328 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-28 10:43:18,427 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:43:21,589 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:43:21,935 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:43:22,018 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:57<28:10, 10.77s/it] 61%|██████    | 239/395 [02:57<19:50,  7.63s/it] 61%|██████    | 240/395 [02:58<14:01,  5.43s/it] 61%|██████    | 241/395 [02:58<09:58,  3.89s/it] 61%|██████▏   | 242/395 [02:58<07:09,  2.81s/it] 62%|██████▏   | 243/395 [02:58<05:12,  2.06s/it] 62%|██████▏   | 244/395 [02:59<03:50,  1.53s/it] 62%|██████▏   | 245/395 [02:59<02:53,  1.16s/it] 62%|██████▏   | 246/395 [02:59<02:13,  1.11it/s] 63%|██████▎   | 247/395 [03:00<01:46,  1.40it/s] 63%|██████▎   | 248/395 [03:00<01:26,  1.70it/s] 63%|██████▎   | 249/395 [03:00<01:13,  2.00it/s] 63%|██████▎   | 250/395 [03:01<01:04,  2.24it/s] 64%|██████▎   | 251/395 [03:01<00:57,  2.50it/s] 64%|██████▍   | 252/395 [03:01<00:52,  2.73it/s] 64%|██████▍   | 253/395 [03:01<00:48,  2.91it/s] 64%|██████▍   | 254/395 [03:02<00:46,  3.05it/s] 65%|██████▍   | 255/395 [03:02<00:44,  3.17it/s] 65%|██████▍   | 256/395 [03:02<00:42,  3.25it/s] 65%|██████▌   | 257/395 [03:03<00:41,  3.31it/s] 65%|██████▌   | 258/395 [03:03<00:40,  3.35it/s] 66%|██████▌   | 259/395 [03:03<00:40,  3.38it/s] 66%|██████▌   | 260/395 [03:03<00:39,  3.40it/s] 66%|██████▌   | 261/395 [03:04<00:40,  3.33it/s] 66%|██████▋   | 262/395 [03:04<00:39,  3.37it/s] 67%|██████▋   | 263/395 [03:04<00:38,  3.39it/s] 67%|██████▋   | 264/395 [03:05<00:38,  3.41it/s] 67%|██████▋   | 265/395 [03:05<00:37,  3.42it/s] 67%|██████▋   | 266/395 [03:05<00:37,  3.43it/s] 68%|██████▊   | 267/395 [03:05<00:37,  3.44it/s] 68%|██████▊   | 268/395 [03:06<00:36,  3.45it/s] 68%|██████▊   | 269/395 [03:06<00:36,  3.45it/s] 68%|██████▊   | 270/395 [03:06<00:36,  3.45it/s] 69%|██████▊   | 271/395 [03:07<00:35,  3.45it/s] 69%|██████▉   | 272/395 [03:07<00:36,  3.36it/s] 69%|██████▉   | 273/395 [03:07<00:35,  3.39it/s] 69%|██████▉   | 274/395 [03:08<00:35,  3.41it/s] 70%|██████▉   | 275/395 [03:08<00:35,  3.42it/s] 70%|██████▉   | 276/395 [03:08<00:34,  3.43it/s] 70%|███████   | 277/395 [03:08<00:34,  3.44it/s] 70%|███████   | 278/395 [03:09<00:34,  3.44it/s] 71%|███████   | 279/395 [03:09<00:33,  3.44it/s] 71%|███████   | 280/395 [03:09<00:33,  3.44it/s] 71%|███████   | 281/395 [03:10<00:33,  3.45it/s] 71%|███████▏  | 282/395 [03:10<00:32,  3.45it/s] 72%|███████▏  | 283/395 [03:10<00:33,  3.34it/s] 72%|███████▏  | 284/395 [03:10<00:32,  3.38it/s] 72%|███████▏  | 285/395 [03:11<00:32,  3.40it/s] 72%|███████▏  | 286/395 [03:11<00:31,  3.42it/s] 73%|███████▎  | 287/395 [03:11<00:31,  3.43it/s] 73%|███████▎  | 288/395 [03:12<00:31,  3.44it/s] 73%|███████▎  | 289/395 [03:12<00:30,  3.44it/s] 73%|███████▎  | 290/395 [03:12<00:30,  3.45it/s] 74%|███████▎  | 291/395 [03:12<00:30,  3.45it/s] 74%|███████▍  | 292/395 [03:13<00:29,  3.45it/s] 74%|███████▍  | 293/395 [03:13<00:29,  3.45it/s] 74%|███████▍  | 294/395 [03:13<00:29,  3.38it/s] 75%|███████▍  | 295/395 [03:14<00:29,  3.40it/s] 75%|███████▍  | 296/395 [03:14<00:29,  3.41it/s] 75%|███████▌  | 297/395 [03:14<00:28,  3.42it/s] 75%|███████▌  | 298/395 [03:15<00:28,  3.43it/s] 76%|███████▌  | 299/395 [03:15<00:27,  3.44it/s] 76%|███████▌  | 300/395 [03:15<00:27,  3.44it/s] 76%|███████▌  | 301/395 [03:15<00:27,  3.45it/s] 76%|███████▋  | 302/395 [03:16<00:26,  3.45it/s] 77%|███████▋  | 303/395 [03:16<00:26,  3.45it/s] 77%|███████▋  | 304/395 [03:16<00:26,  3.45it/s] 77%|███████▋  | 305/395 [03:17<00:26,  3.37it/s] 77%|███████▋  | 306/395 [03:17<00:26,  3.39it/s] 78%|███████▊  | 307/395 [03:17<00:25,  3.41it/s] 78%|███████▊  | 308/395 [03:17<00:25,  3.42it/s] 78%|███████▊  | 309/395 [03:18<00:25,  3.43it/s] 78%|███████▊  | 310/395 [03:18<00:24,  3.44it/s] 79%|███████▊  | 311/395 [03:18<00:24,  3.44it/s] 79%|███████▉  | 312/395 [03:19<00:24,  3.45it/s] 79%|███████▉  | 313/395 [03:19<00:23,  3.45it/s] 79%|███████▉  | 314/395 [03:19<00:23,  3.45it/s] 80%|███████▉  | 315/395 [03:19<00:23,  3.45it/s] 80%|████████  | 316/395 [03:20<00:21,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 10:43:52,022 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:43:52,022 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 10:43:52,022 >>   Batch size = 8
{'eval_loss': 0.8689820170402527, 'eval_runtime': 24.2515, 'eval_samples_per_second': 357.009, 'eval_steps_per_second': 44.657, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.45it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.11it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.34it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.48it/s][A
  2%|▏         | 27/1083 [00:00<00:26, 40.12it/s][A
  3%|▎         | 32/1083 [00:00<00:25, 41.81it/s][A
  3%|▎         | 37/1083 [00:00<00:24, 42.96it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 43.56it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.09it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 44.48it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 44.83it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 44.99it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 44.62it/s][A
  7%|▋         | 72/1083 [00:01<00:23, 43.17it/s][A
  7%|▋         | 77/1083 [00:01<00:23, 43.71it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.04it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.47it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.76it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.77it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.90it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.01it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.61it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.63it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.61it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.62it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.76it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.94it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 45.10it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.20it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.90it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.87it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.62it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.70it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.79it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.84it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.99it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 45.13it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.12it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.10it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.82it/s][A
 19%|█▉        | 207/1083 [00:04<00:20, 43.49it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.02it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.30it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.45it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.55it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.73it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.97it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.04it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.85it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.75it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.84it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.84it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.92it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.88it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 45.00it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.92it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.91it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.75it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.77it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.93it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.94it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.92it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.91it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.92it/s][A
 30%|███       | 327/1083 [00:07<00:16, 45.04it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.97it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.75it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.01it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.36it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.57it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.67it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.85it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.83it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.07it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.89it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.69it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.84it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.86it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.90it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.93it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.96it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.92it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.05it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.88it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.75it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.83it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.84it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.85it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.97it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.95it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.92it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.00it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.86it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.78it/s][A
 44%|████▍     | 477/1083 [00:10<00:14, 42.34it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 43.20it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 43.80it/s][A
 45%|████▌     | 492/1083 [00:11<00:13, 44.18it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.46it/s][A
 46%|████▋     | 502/1083 [00:11<00:13, 44.67it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.61it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.68it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.45it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.40it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.73it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.91it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 45.00it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 45.01it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.92it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.93it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.86it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.68it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.62it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.74it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.80it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 45.00it/s][A
 54%|█████▍    | 587/1083 [00:13<00:10, 45.14it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.00it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.91it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.81it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.68it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 43.19it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 43.80it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.14it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.57it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.76it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.92it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.83it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.92it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.55it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.56it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.79it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.90it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 45.05it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.97it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.95it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.99it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.87it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.74it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.64it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.78it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.82it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 45.04it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 45.06it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 45.06it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 45.11it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.93it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.82it/s][A
 69%|██████▉   | 747/1083 [00:16<00:08, 41.98it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 42.90it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 43.64it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.17it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.21it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.62it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.78it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.77it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.59it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.63it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.70it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.82it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.97it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.02it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.99it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.00it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.73it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.54it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.59it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.76it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.93it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.97it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 45.04it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.04it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.86it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.79it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.67it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 42.50it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 43.36it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 43.75it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.25it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.41it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.77it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.84it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.87it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.52it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.47it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.69it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.73it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.98it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.98it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.95it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.91it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.88it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.64it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.62it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.70it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.82it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.83it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.95it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.03it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 45.01it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.94it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.72it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 42.67it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 43.41it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 43.97it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.35it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.60it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.71it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.73it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.60it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.43it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.43it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.56it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.86it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.86it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.10it/s][A                                                 
                                                   [A 80%|████████  | 316/395 [03:44<00:21,  3.76it/s]
100%|██████████| 1083/1083 [00:24<00:00, 45.10it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 10:44:16,413 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-28 10:44:16,733 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:44:19,187 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:44:19,289 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:44:19,319 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [03:54<13:26, 10.34s/it] 81%|████████  | 318/395 [03:54<09:25,  7.34s/it] 81%|████████  | 319/395 [03:54<06:37,  5.23s/it] 81%|████████  | 320/395 [03:54<04:40,  3.75s/it] 81%|████████▏ | 321/395 [03:55<03:20,  2.71s/it] 82%|████████▏ | 322/395 [03:55<02:24,  1.99s/it] 82%|████████▏ | 323/395 [03:55<01:46,  1.48s/it] 82%|████████▏ | 324/395 [03:56<01:19,  1.12s/it] 82%|████████▏ | 325/395 [03:56<01:01,  1.14it/s] 83%|████████▎ | 326/395 [03:56<00:48,  1.43it/s] 83%|████████▎ | 327/395 [03:56<00:39,  1.73it/s] 83%|████████▎ | 328/395 [03:57<00:33,  2.03it/s] 83%|████████▎ | 329/395 [03:57<00:29,  2.27it/s] 84%|████████▎ | 330/395 [03:57<00:25,  2.53it/s] 84%|████████▍ | 331/395 [03:58<00:23,  2.74it/s] 84%|████████▍ | 332/395 [03:58<00:21,  2.91it/s] 84%|████████▍ | 333/395 [03:58<00:20,  3.04it/s] 85%|████████▍ | 334/395 [03:59<00:19,  3.14it/s] 85%|████████▍ | 335/395 [03:59<00:18,  3.21it/s] 85%|████████▌ | 336/395 [03:59<00:18,  3.27it/s] 85%|████████▌ | 337/395 [03:59<00:17,  3.31it/s] 86%|████████▌ | 338/395 [04:00<00:17,  3.34it/s] 86%|████████▌ | 339/395 [04:00<00:16,  3.36it/s] 86%|████████▌ | 340/395 [04:00<00:16,  3.31it/s] 86%|████████▋ | 341/395 [04:01<00:16,  3.33it/s] 87%|████████▋ | 342/395 [04:01<00:15,  3.35it/s] 87%|████████▋ | 343/395 [04:01<00:15,  3.37it/s] 87%|████████▋ | 344/395 [04:02<00:15,  3.38it/s] 87%|████████▋ | 345/395 [04:02<00:14,  3.38it/s] 88%|████████▊ | 346/395 [04:02<00:14,  3.39it/s] 88%|████████▊ | 347/395 [04:02<00:14,  3.40it/s] 88%|████████▊ | 348/395 [04:03<00:13,  3.40it/s] 88%|████████▊ | 349/395 [04:03<00:13,  3.40it/s] 89%|████████▊ | 350/395 [04:03<00:13,  3.40it/s] 89%|████████▉ | 351/395 [04:04<00:13,  3.34it/s] 89%|████████▉ | 352/395 [04:04<00:12,  3.36it/s] 89%|████████▉ | 353/395 [04:04<00:12,  3.37it/s] 90%|████████▉ | 354/395 [04:04<00:12,  3.38it/s] 90%|████████▉ | 355/395 [04:05<00:11,  3.38it/s] 90%|█████████ | 356/395 [04:05<00:11,  3.39it/s] 90%|█████████ | 357/395 [04:05<00:11,  3.39it/s] 91%|█████████ | 358/395 [04:06<00:10,  3.39it/s] 91%|█████████ | 359/395 [04:06<00:10,  3.40it/s] 91%|█████████ | 360/395 [04:06<00:10,  3.40it/s] 91%|█████████▏| 361/395 [04:07<00:10,  3.40it/s] 92%|█████████▏| 362/395 [04:07<00:09,  3.32it/s] 92%|█████████▏| 363/395 [04:07<00:09,  3.34it/s] 92%|█████████▏| 364/395 [04:07<00:09,  3.36it/s] 92%|█████████▏| 365/395 [04:08<00:08,  3.37it/s] 93%|█████████▎| 366/395 [04:08<00:08,  3.38it/s] 93%|█████████▎| 367/395 [04:08<00:08,  3.39it/s] 93%|█████████▎| 368/395 [04:09<00:07,  3.39it/s] 93%|█████████▎| 369/395 [04:09<00:07,  3.39it/s] 94%|█████████▎| 370/395 [04:09<00:07,  3.40it/s] 94%|█████████▍| 371/395 [04:10<00:07,  3.40it/s] 94%|█████████▍| 372/395 [04:10<00:06,  3.40it/s] 94%|█████████▍| 373/395 [04:10<00:06,  3.29it/s] 95%|█████████▍| 374/395 [04:10<00:06,  3.32it/s] 95%|█████████▍| 375/395 [04:11<00:05,  3.34it/s] 95%|█████████▌| 376/395 [04:11<00:05,  3.36it/s] 95%|█████████▌| 377/395 [04:11<00:05,  3.37it/s] 96%|█████████▌| 378/395 [04:12<00:05,  3.38it/s] 96%|█████████▌| 379/395 [04:12<00:04,  3.38it/s] 96%|█████████▌| 380/395 [04:12<00:04,  3.39it/s] 96%|█████████▋| 381/395 [04:12<00:04,  3.39it/s] 97%|█████████▋| 382/395 [04:13<00:03,  3.41it/s] 97%|█████████▋| 383/395 [04:13<00:03,  3.42it/s] 97%|█████████▋| 384/395 [04:13<00:03,  3.33it/s] 97%|█████████▋| 385/395 [04:14<00:02,  3.36it/s] 98%|█████████▊| 386/395 [04:14<00:02,  3.39it/s] 98%|█████████▊| 387/395 [04:14<00:02,  3.41it/s] 98%|█████████▊| 388/395 [04:15<00:02,  3.42it/s] 98%|█████████▊| 389/395 [04:15<00:01,  3.43it/s] 99%|█████████▊| 390/395 [04:15<00:01,  3.43it/s] 99%|█████████▉| 391/395 [04:15<00:01,  3.44it/s] 99%|█████████▉| 392/395 [04:16<00:00,  3.44it/s] 99%|█████████▉| 393/395 [04:16<00:00,  3.45it/s]100%|█████████▉| 394/395 [04:16<00:00,  3.45it/s]100%|██████████| 395/395 [04:17<00:00,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 10:44:48,899 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:44:48,900 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 10:44:48,900 >>   Batch size = 8
{'eval_loss': 0.8739299178123474, 'eval_runtime': 24.2699, 'eval_samples_per_second': 356.738, 'eval_steps_per_second': 44.623, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.41it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.25it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.51it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.76it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.15it/s][A
  3%|▎         | 32/1083 [00:00<00:22, 45.73it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.04it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.76it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.85it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 44.98it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.06it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.19it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.25it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.07it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.11it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.77it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.87it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.78it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 44.81it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 44.88it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.06it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.19it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.09it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.03it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.65it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 43.78it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.19it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.46it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.71it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.83it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.95it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.11it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 43.93it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.28it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.51it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.49it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.69it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.86it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.04it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.06it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.93it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.79it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.75it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.75it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.82it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.91it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.73it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.10it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.10it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.80it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.87it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.78it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.76it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.77it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.87it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 43.26it/s][A
 27%|██▋       | 287/1083 [00:06<00:18, 43.98it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.40it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.44it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.47it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.57it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.71it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.75it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.53it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.72it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.88it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.99it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.01it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 44.98it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 44.82it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.79it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.71it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.72it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.78it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.75it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.96it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.06it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.01it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.94it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.83it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.77it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.77it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 42.98it/s][A
 39%|███▉      | 422/1083 [00:09<00:15, 43.65it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.21it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.46it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.66it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.64it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.63it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.61it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 44.44it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.57it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.65it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.81it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.93it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.02it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.79it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.76it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.66it/s][A
 46%|████▋     | 502/1083 [00:11<00:13, 44.58it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.68it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.86it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.82it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.00it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.97it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 45.05it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.89it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.78it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 44.63it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 43.40it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 43.93it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.42it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.70it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.73it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.87it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.90it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.86it/s][A
 55%|█████▍    | 592/1083 [00:13<00:11, 44.61it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.66it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.74it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.97it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.04it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.05it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.94it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.91it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.88it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.65it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.50it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.74it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.93it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.10it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.05it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 45.06it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.95it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.77it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.70it/s][A
 63%|██████▎   | 687/1083 [00:15<00:09, 42.44it/s][A
 64%|██████▍   | 692/1083 [00:15<00:09, 43.28it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 43.86it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.27it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.44it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.75it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.90it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.91it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.53it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.53it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.64it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.76it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.91it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.99it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.05it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.97it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.85it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.72it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.60it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.72it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.82it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.89it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.02it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 45.00it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 45.01it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.78it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.59it/s][A
 76%|███████▌  | 822/1083 [00:18<00:06, 43.37it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 43.92it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.25it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.36it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.42it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.73it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.71it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.72it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.39it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.41it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.57it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.77it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.02it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.92it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 45.01it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.85it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.66it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.43it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.53it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.42it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.87it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.01it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.13it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 45.13it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 45.03it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.74it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.65it/s][A
 88%|████████▊ | 957/1083 [00:21<00:03, 39.60it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 41.24it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 42.45it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 43.29it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 43.76it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.32it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 44.67it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.72it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.45it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.14it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.18it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.60it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.85it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 45.05it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 45.04it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 45.12it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.93it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.69it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.44it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.47it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.40it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.71it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.87it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 44.97it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 45.11it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.92it/s][A                                                 
                                                   [A100%|██████████| 395/395 [04:41<00:00,  3.57it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.92it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 10:45:13,276 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-28 10:45:13,596 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:45:18,171 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:45:18,550 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:45:18,729 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:45:30,198 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:45:30,220 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158 (score: 0.8650857210159302).
                                                 100%|██████████| 395/395 [05:07<00:00,  3.57it/s]100%|██████████| 395/395 [05:07<00:00,  1.28it/s]
[INFO|trainer.py:1894] 2023-08-28 10:45:39,412 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 10:45:39,540 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:45:43,068 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:45:43,239 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:45:43,316 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:45:43,774 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:45:43,774 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:45:43,774 >>   train_loss               =     0.7303
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:45:43,774 >>   train_runtime            = 0:05:07.52
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:45:43,774 >>   train_samples            =       5036
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:45:43,774 >>   train_samples_per_second =     81.881
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:45:43,775 >>   train_steps_per_second   =      1.284
{'eval_loss': 0.8749673962593079, 'eval_runtime': 24.2528, 'eval_samples_per_second': 356.989, 'eval_steps_per_second': 44.655, 'epoch': 5.0}
{'train_runtime': 307.5203, 'train_samples_per_second': 81.881, 'train_steps_per_second': 1.284, 'train_loss': 0.7303249697142009, 'epoch': 5.0}
08/28/2023 10:45:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:45:44,009 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:45:44,009 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 10:45:44,009 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.27it/s]  1%|          | 12/1083 [00:00<00:21, 49.37it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.92it/s]  2%|▏         | 22/1083 [00:00<00:22, 46.93it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.48it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.25it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.98it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.68it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.11it/s]  5%|▍         | 52/1083 [00:01<00:23, 44.79it/s]  5%|▌         | 57/1083 [00:01<00:22, 44.88it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.00it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.10it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.30it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.41it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.39it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.32it/s]  8%|▊         | 92/1083 [00:02<00:22, 45.01it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.71it/s]  9%|▉         | 102/1083 [00:02<00:23, 42.47it/s] 10%|▉         | 107/1083 [00:02<00:22, 43.33it/s] 10%|█         | 112/1083 [00:02<00:22, 43.93it/s] 11%|█         | 117/1083 [00:02<00:21, 44.44it/s] 11%|█▏        | 122/1083 [00:02<00:21, 44.78it/s] 12%|█▏        | 127/1083 [00:02<00:21, 44.94it/s] 12%|█▏        | 132/1083 [00:02<00:21, 45.09it/s] 13%|█▎        | 137/1083 [00:03<00:20, 45.09it/s] 13%|█▎        | 142/1083 [00:03<00:20, 44.84it/s] 14%|█▎        | 147/1083 [00:03<00:20, 44.75it/s] 14%|█▍        | 152/1083 [00:03<00:20, 44.87it/s] 14%|█▍        | 157/1083 [00:03<00:20, 44.98it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.21it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.31it/s] 16%|█▌        | 172/1083 [00:03<00:20, 45.38it/s] 16%|█▋        | 177/1083 [00:03<00:20, 45.30it/s] 17%|█▋        | 182/1083 [00:04<00:20, 45.03it/s] 17%|█▋        | 187/1083 [00:04<00:19, 44.82it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.75it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.83it/s] 19%|█▊        | 202/1083 [00:04<00:19, 44.98it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.16it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.21it/s] 20%|██        | 217/1083 [00:04<00:19, 45.34it/s] 20%|██        | 222/1083 [00:04<00:18, 45.32it/s] 21%|██        | 227/1083 [00:05<00:18, 45.26it/s] 21%|██▏       | 232/1083 [00:05<00:18, 45.11it/s] 22%|██▏       | 237/1083 [00:05<00:18, 44.67it/s] 22%|██▏       | 242/1083 [00:05<00:18, 44.67it/s] 23%|██▎       | 247/1083 [00:05<00:18, 44.90it/s] 23%|██▎       | 252/1083 [00:05<00:18, 45.06it/s] 24%|██▎       | 257/1083 [00:05<00:18, 45.05it/s] 24%|██▍       | 262/1083 [00:05<00:18, 45.15it/s] 25%|██▍       | 267/1083 [00:05<00:18, 45.18it/s] 25%|██▌       | 272/1083 [00:06<00:17, 45.08it/s] 26%|██▌       | 277/1083 [00:06<00:17, 44.94it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.76it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.79it/s] 27%|██▋       | 292/1083 [00:06<00:17, 44.92it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.03it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.11it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.17it/s] 29%|██▉       | 312/1083 [00:06<00:17, 45.14it/s] 29%|██▉       | 317/1083 [00:07<00:17, 45.00it/s] 30%|██▉       | 322/1083 [00:07<00:16, 44.89it/s] 30%|███       | 327/1083 [00:07<00:16, 44.78it/s] 31%|███       | 332/1083 [00:07<00:16, 44.74it/s] 31%|███       | 337/1083 [00:07<00:16, 44.83it/s] 32%|███▏      | 342/1083 [00:07<00:16, 44.94it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.08it/s] 33%|███▎      | 352/1083 [00:07<00:16, 45.10it/s] 33%|███▎      | 357/1083 [00:07<00:16, 45.12it/s] 33%|███▎      | 362/1083 [00:08<00:16, 44.99it/s] 34%|███▍      | 367/1083 [00:08<00:15, 44.98it/s] 34%|███▍      | 372/1083 [00:08<00:16, 43.32it/s] 35%|███▍      | 377/1083 [00:08<00:16, 43.74it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.23it/s] 36%|███▌      | 387/1083 [00:08<00:15, 43.99it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.46it/s] 37%|███▋      | 397/1083 [00:08<00:15, 44.75it/s] 37%|███▋      | 402/1083 [00:08<00:15, 44.75it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.91it/s] 38%|███▊      | 412/1083 [00:09<00:14, 44.79it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.76it/s] 39%|███▉      | 422/1083 [00:09<00:14, 44.83it/s] 39%|███▉      | 427/1083 [00:09<00:14, 44.96it/s] 40%|███▉      | 432/1083 [00:09<00:14, 44.86it/s] 40%|████      | 437/1083 [00:09<00:14, 45.04it/s] 41%|████      | 442/1083 [00:09<00:14, 45.07it/s] 41%|████▏     | 447/1083 [00:09<00:14, 45.05it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.96it/s] 42%|████▏     | 457/1083 [00:10<00:14, 44.62it/s] 43%|████▎     | 462/1083 [00:10<00:13, 44.78it/s] 43%|████▎     | 467/1083 [00:10<00:13, 44.80it/s] 44%|████▎     | 472/1083 [00:10<00:13, 44.84it/s] 44%|████▍     | 477/1083 [00:10<00:13, 45.06it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.04it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.02it/s] 45%|████▌     | 492/1083 [00:10<00:13, 45.01it/s] 46%|████▌     | 497/1083 [00:11<00:13, 44.91it/s] 46%|████▋     | 502/1083 [00:11<00:12, 44.84it/s] 47%|████▋     | 507/1083 [00:11<00:12, 44.81it/s] 47%|████▋     | 512/1083 [00:11<00:12, 44.85it/s] 48%|████▊     | 517/1083 [00:11<00:12, 44.98it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.91it/s] 49%|████▊     | 527/1083 [00:11<00:12, 44.97it/s] 49%|████▉     | 532/1083 [00:11<00:12, 44.96it/s] 50%|████▉     | 537/1083 [00:11<00:12, 45.03it/s] 50%|█████     | 542/1083 [00:12<00:12, 44.94it/s] 51%|█████     | 547/1083 [00:12<00:11, 44.76it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.76it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 44.76it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 43.98it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 44.43it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 44.68it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 44.67it/s] 54%|█████▎    | 582/1083 [00:12<00:11, 44.78it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 44.78it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 44.76it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 44.81it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.68it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 44.63it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 44.86it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 45.02it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 45.10it/s] 58%|█████▊    | 627/1083 [00:13<00:10, 45.05it/s] 58%|█████▊    | 632/1083 [00:14<00:10, 44.99it/s] 59%|█████▉    | 637/1083 [00:14<00:09, 44.84it/s] 59%|█████▉    | 642/1083 [00:14<00:09, 44.66it/s] 60%|█████▉    | 647/1083 [00:14<00:09, 44.70it/s] 60%|██████    | 652/1083 [00:14<00:09, 44.76it/s] 61%|██████    | 657/1083 [00:14<00:09, 44.92it/s] 61%|██████    | 662/1083 [00:14<00:09, 45.01it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 45.01it/s] 62%|██████▏   | 672/1083 [00:14<00:09, 45.03it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.84it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.84it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.65it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.64it/s] 64%|██████▍   | 697/1083 [00:15<00:09, 42.49it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 43.29it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 44.01it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 44.36it/s] 66%|██████▌   | 717/1083 [00:15<00:08, 44.50it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 44.58it/s] 67%|██████▋   | 727/1083 [00:16<00:08, 44.48it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.63it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.32it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.54it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 44.77it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 44.99it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 44.97it/s] 70%|███████   | 762/1083 [00:16<00:07, 45.12it/s] 71%|███████   | 767/1083 [00:17<00:07, 45.02it/s] 71%|███████▏  | 772/1083 [00:17<00:06, 44.76it/s] 72%|███████▏  | 777/1083 [00:17<00:06, 44.72it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 44.58it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 44.71it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.72it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 44.98it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 44.99it/s] 75%|███████▍  | 807/1083 [00:17<00:06, 45.09it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 45.02it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.67it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.65it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 44.69it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 43.90it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 44.38it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 44.55it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 44.76it/s] 79%|███████▊  | 852/1083 [00:18<00:05, 44.82it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.72it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.66it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.60it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.55it/s] 81%|████████  | 877/1083 [00:19<00:04, 44.69it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 44.84it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 44.92it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 44.98it/s] 83%|████████▎ | 897/1083 [00:19<00:04, 45.09it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 44.93it/s] 84%|████████▎ | 907/1083 [00:20<00:03, 44.81it/s] 84%|████████▍ | 912/1083 [00:20<00:03, 44.63it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 44.50it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 44.70it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 44.87it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 45.03it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 45.08it/s] 87%|████████▋ | 942/1083 [00:20<00:03, 44.95it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.96it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 44.67it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 44.65it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 44.62it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 41.88it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 42.89it/s] 90%|█████████ | 977/1083 [00:21<00:02, 43.58it/s] 91%|█████████ | 982/1083 [00:21<00:02, 44.02it/s] 91%|█████████ | 987/1083 [00:22<00:02, 44.45it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 44.71it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 44.67it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 44.49it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 44.22it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 44.35it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 44.60it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 44.88it/s] 95%|█████████▍| 1027/1083 [00:22<00:01, 45.00it/s] 95%|█████████▌| 1032/1083 [00:23<00:01, 45.00it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 44.67it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 45.10it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 44.76it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 44.51it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 44.59it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 44.68it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 44.76it/s] 99%|█████████▉| 1072/1083 [00:23<00:00, 45.01it/s] 99%|█████████▉| 1077/1083 [00:24<00:00, 45.15it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 45.07it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:46:08,192 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   eval_loss               =     0.8651
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   eval_runtime            = 0:00:24.18
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   eval_samples_per_second =    358.021
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   eval_steps_per_second   =     44.784
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:46:08,192 >>   perplexity              =     2.3752
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:21,837 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:21,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:21,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:21,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:21,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:46:23,040 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:46:23,041 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:46:23,731 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:46:24,862 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:46:24,863 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:27,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:27,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:27,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:27,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:46:27,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:46:28,765 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:46:28,766 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:46:29,414 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:46:29,638 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:46:29,638 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-316
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-237
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-395
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-158
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/checkpoint-79
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:12,  1.41it/s]Extractor Predicting: 21it [00:13,  1.45it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.60it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:21,  1.60it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.57it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:32,  1.52it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:33,  1.54it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:37,  1.52it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:40,  1.62it/s]Extractor Predicting: 65it [00:41,  1.60it/s]Extractor Predicting: 66it [00:42,  1.61it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:44,  1.59it/s]Extractor Predicting: 70it [00:44,  1.54it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:47,  1.59it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:48,  1.59it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:49,  1.58it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:51,  1.58it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.61it/s]Extractor Predicting: 83it [00:52,  1.60it/s]Extractor Predicting: 84it [00:53,  1.61it/s]Extractor Predicting: 85it [00:54,  1.58it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:56,  1.57it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.62it/s]Extractor Predicting: 92it [00:58,  1.59it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [00:59,  1.58it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:01,  1.60it/s]Extractor Predicting: 97it [01:01,  1.58it/s]Extractor Predicting: 98it [01:02,  1.57it/s]Extractor Predicting: 99it [01:03,  1.58it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:04,  1.59it/s]Extractor Predicting: 102it [01:04,  1.60it/s]Extractor Predicting: 103it [01:05,  1.59it/s]Extractor Predicting: 104it [01:06,  1.59it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:07,  1.59it/s]Extractor Predicting: 107it [01:08,  1.55it/s]Extractor Predicting: 108it [01:08,  1.56it/s]Extractor Predicting: 109it [01:09,  1.58it/s]Extractor Predicting: 110it [01:10,  1.57it/s]Extractor Predicting: 111it [01:10,  1.58it/s]Extractor Predicting: 112it [01:11,  1.57it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:12,  1.43it/s]Extractor Predicting: 115it [01:13,  1.44it/s]Extractor Predicting: 116it [01:14,  1.46it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:15,  1.52it/s]Extractor Predicting: 119it [01:16,  1.55it/s]Extractor Predicting: 120it [01:16,  1.56it/s]Extractor Predicting: 121it [01:17,  1.54it/s]Extractor Predicting: 122it [01:17,  1.63it/s]Extractor Predicting: 123it [01:18,  1.63it/s]Extractor Predicting: 124it [01:19,  1.60it/s]Extractor Predicting: 125it [01:19,  1.55it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.55it/s]Extractor Predicting: 128it [01:21,  1.56it/s]Extractor Predicting: 129it [01:22,  1.57it/s]Extractor Predicting: 130it [01:23,  1.57it/s]Extractor Predicting: 131it [01:23,  1.62it/s]Extractor Predicting: 132it [01:24,  1.60it/s]Extractor Predicting: 133it [01:24,  1.57it/s]Extractor Predicting: 134it [01:25,  1.58it/s]Extractor Predicting: 135it [01:26,  1.58it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:27,  1.57it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:28,  1.57it/s]Extractor Predicting: 140it [01:29,  1.57it/s]Extractor Predicting: 141it [01:29,  1.59it/s]Extractor Predicting: 142it [01:30,  1.60it/s]Extractor Predicting: 143it [01:31,  1.59it/s]Extractor Predicting: 144it [01:31,  1.60it/s]Extractor Predicting: 145it [01:32,  1.57it/s]Extractor Predicting: 146it [01:33,  1.56it/s]Extractor Predicting: 147it [01:33,  1.59it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:34,  1.64it/s]Extractor Predicting: 150it [01:35,  1.61it/s]Extractor Predicting: 151it [01:36,  1.62it/s]Extractor Predicting: 152it [01:36,  1.55it/s]Extractor Predicting: 153it [01:37,  1.49it/s]Extractor Predicting: 154it [01:38,  1.47it/s]Extractor Predicting: 155it [01:39,  1.42it/s]Extractor Predicting: 156it [01:39,  1.41it/s]Extractor Predicting: 157it [01:40,  1.40it/s]Extractor Predicting: 158it [01:41,  1.40it/s]Extractor Predicting: 159it [01:41,  1.41it/s]Extractor Predicting: 160it [01:42,  1.39it/s]Extractor Predicting: 161it [01:43,  1.39it/s]Extractor Predicting: 162it [01:44,  1.39it/s]Extractor Predicting: 163it [01:44,  1.39it/s]Extractor Predicting: 164it [01:45,  1.39it/s]Extractor Predicting: 165it [01:46,  1.37it/s]Extractor Predicting: 166it [01:47,  1.39it/s]Extractor Predicting: 167it [01:47,  1.38it/s]Extractor Predicting: 168it [01:48,  1.41it/s]Extractor Predicting: 169it [01:49,  1.46it/s]Extractor Predicting: 170it [01:49,  1.49it/s]Extractor Predicting: 171it [01:50,  1.51it/s]Extractor Predicting: 172it [01:51,  1.50it/s]Extractor Predicting: 173it [01:51,  1.50it/s]Extractor Predicting: 174it [01:52,  1.50it/s]Extractor Predicting: 175it [01:53,  1.48it/s]Extractor Predicting: 176it [01:53,  1.48it/s]Extractor Predicting: 177it [01:54,  1.48it/s]Extractor Predicting: 178it [01:55,  1.47it/s]Extractor Predicting: 179it [01:55,  1.49it/s]Extractor Predicting: 180it [01:56,  1.48it/s]Extractor Predicting: 181it [01:57,  1.49it/s]Extractor Predicting: 182it [01:57,  1.50it/s]Extractor Predicting: 183it [01:58,  1.54it/s]Extractor Predicting: 184it [01:58,  1.56it/s]Extractor Predicting: 185it [01:59,  1.60it/s]Extractor Predicting: 186it [02:00,  1.55it/s]Extractor Predicting: 187it [02:00,  1.56it/s]Extractor Predicting: 188it [02:01,  1.56it/s]Extractor Predicting: 189it [02:02,  1.57it/s]Extractor Predicting: 190it [02:02,  1.55it/s]Extractor Predicting: 191it [02:03,  1.53it/s]Extractor Predicting: 192it [02:04,  1.54it/s]Extractor Predicting: 193it [02:04,  1.56it/s]Extractor Predicting: 194it [02:05,  1.57it/s]Extractor Predicting: 195it [02:06,  1.55it/s]Extractor Predicting: 196it [02:06,  1.51it/s]Extractor Predicting: 197it [02:07,  1.54it/s]Extractor Predicting: 198it [02:08,  1.54it/s]Extractor Predicting: 199it [02:08,  1.59it/s]Extractor Predicting: 200it [02:09,  1.58it/s]Extractor Predicting: 201it [02:09,  1.56it/s]Extractor Predicting: 202it [02:10,  1.39it/s]Extractor Predicting: 203it [02:11,  1.42it/s]Extractor Predicting: 204it [02:12,  1.47it/s]Extractor Predicting: 205it [02:12,  1.48it/s]Extractor Predicting: 206it [02:13,  1.51it/s]Extractor Predicting: 207it [02:14,  1.52it/s]Extractor Predicting: 208it [02:14,  1.55it/s]Extractor Predicting: 209it [02:15,  1.57it/s]Extractor Predicting: 210it [02:15,  1.57it/s]Extractor Predicting: 211it [02:16,  1.55it/s]Extractor Predicting: 212it [02:17,  1.56it/s]Extractor Predicting: 213it [02:17,  1.57it/s]Extractor Predicting: 214it [02:18,  1.59it/s]Extractor Predicting: 215it [02:19,  1.63it/s]Extractor Predicting: 216it [02:19,  1.60it/s]Extractor Predicting: 217it [02:20,  1.60it/s]Extractor Predicting: 218it [02:20,  1.60it/s]Extractor Predicting: 219it [02:21,  1.61it/s]Extractor Predicting: 220it [02:22,  1.58it/s]Extractor Predicting: 221it [02:22,  1.60it/s]Extractor Predicting: 222it [02:23,  1.60it/s]Extractor Predicting: 223it [02:24,  1.60it/s]Extractor Predicting: 224it [02:24,  1.57it/s]Extractor Predicting: 225it [02:25,  1.52it/s]Extractor Predicting: 226it [02:26,  1.55it/s]Extractor Predicting: 227it [02:26,  1.56it/s]Extractor Predicting: 228it [02:27,  1.56it/s]Extractor Predicting: 229it [02:27,  1.59it/s]Extractor Predicting: 230it [02:28,  1.57it/s]Extractor Predicting: 231it [02:29,  1.55it/s]Extractor Predicting: 232it [02:29,  1.59it/s]Extractor Predicting: 233it [02:30,  1.61it/s]Extractor Predicting: 234it [02:31,  1.57it/s]Extractor Predicting: 235it [02:31,  1.56it/s]Extractor Predicting: 236it [02:32,  1.54it/s]Extractor Predicting: 237it [02:33,  1.55it/s]Extractor Predicting: 238it [02:33,  1.57it/s]Extractor Predicting: 239it [02:34,  1.58it/s]Extractor Predicting: 240it [02:34,  1.57it/s]Extractor Predicting: 241it [02:35,  1.59it/s]Extractor Predicting: 242it [02:36,  1.61it/s]Extractor Predicting: 243it [02:36,  1.57it/s]Extractor Predicting: 244it [02:37,  1.57it/s]Extractor Predicting: 245it [02:38,  1.58it/s]Extractor Predicting: 246it [02:38,  1.57it/s]Extractor Predicting: 247it [02:39,  1.58it/s]Extractor Predicting: 248it [02:39,  1.60it/s]Extractor Predicting: 249it [02:40,  1.58it/s]Extractor Predicting: 250it [02:41,  1.56it/s]Extractor Predicting: 251it [02:41,  1.56it/s]Extractor Predicting: 252it [02:42,  1.57it/s]Extractor Predicting: 253it [02:43,  1.58it/s]Extractor Predicting: 254it [02:43,  1.55it/s]Extractor Predicting: 255it [02:44,  1.55it/s]Extractor Predicting: 256it [02:45,  1.58it/s]Extractor Predicting: 257it [02:45,  1.58it/s]Extractor Predicting: 258it [02:46,  1.61it/s]Extractor Predicting: 259it [02:47,  1.56it/s]Extractor Predicting: 260it [02:47,  1.55it/s]Extractor Predicting: 261it [02:48,  1.56it/s]Extractor Predicting: 262it [02:48,  1.58it/s]Extractor Predicting: 263it [02:49,  1.54it/s]Extractor Predicting: 264it [02:50,  1.51it/s]Extractor Predicting: 265it [02:50,  1.52it/s]Extractor Predicting: 266it [02:51,  1.51it/s]Extractor Predicting: 267it [02:52,  1.53it/s]Extractor Predicting: 268it [02:52,  1.54it/s]Extractor Predicting: 269it [02:53,  1.60it/s]Extractor Predicting: 269it [02:53,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:35,778 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:35,809 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:35,809 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:35,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:35,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:49:36,454 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:49:36,455 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:49:37,056 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:49:38,151 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:49:38,151 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:40,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:40,484 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:40,484 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:40,484 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:40,484 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:49:40,820 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:49:40,821 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:49:41,107 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:49:41,280 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:49:41,280 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.37122557726465366,
  "recall": 0.02413952413952414,
  "score": 0.0453313089686585,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:20,  1.50it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.58it/s]Extractor Predicting: 42it [00:27,  1.61it/s]Extractor Predicting: 43it [00:28,  1.63it/s]Extractor Predicting: 44it [00:28,  1.61it/s]Extractor Predicting: 45it [00:29,  1.59it/s]Extractor Predicting: 46it [00:30,  1.57it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.55it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:33,  1.59it/s]Extractor Predicting: 52it [00:33,  1.60it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.62it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.63it/s]Extractor Predicting: 57it [00:37,  1.62it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:38,  1.61it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:39,  1.60it/s]Extractor Predicting: 62it [00:40,  1.59it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:44,  1.57it/s]Extractor Predicting: 70it [00:45,  1.58it/s]Extractor Predicting: 71it [00:45,  1.60it/s]Extractor Predicting: 72it [00:46,  1.55it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:47,  1.57it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.59it/s]Extractor Predicting: 77it [00:49,  1.50it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:51,  1.58it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:52,  1.62it/s]Extractor Predicting: 82it [00:52,  1.62it/s]Extractor Predicting: 83it [00:53,  1.62it/s]Extractor Predicting: 84it [00:54,  1.65it/s]Extractor Predicting: 85it [00:54,  1.67it/s]Extractor Predicting: 86it [00:55,  1.62it/s]Extractor Predicting: 87it [00:56,  1.61it/s]Extractor Predicting: 88it [00:56,  1.61it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:57,  1.62it/s]Extractor Predicting: 91it [00:58,  1.62it/s]Extractor Predicting: 92it [00:59,  1.60it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:00,  1.65it/s]Extractor Predicting: 96it [01:01,  1.63it/s]Extractor Predicting: 97it [01:02,  1.61it/s]Extractor Predicting: 98it [01:02,  1.58it/s]Extractor Predicting: 99it [01:03,  1.59it/s]Extractor Predicting: 100it [01:04,  1.60it/s]Extractor Predicting: 101it [01:04,  1.63it/s]Extractor Predicting: 102it [01:05,  1.66it/s]Extractor Predicting: 103it [01:05,  1.65it/s]Extractor Predicting: 104it [01:06,  1.66it/s]Extractor Predicting: 105it [01:07,  1.66it/s]Extractor Predicting: 106it [01:07,  1.70it/s]Extractor Predicting: 107it [01:08,  1.70it/s]Extractor Predicting: 108it [01:08,  1.60it/s]Extractor Predicting: 109it [01:09,  1.65it/s]Extractor Predicting: 110it [01:10,  1.65it/s]Extractor Predicting: 111it [01:10,  1.69it/s]Extractor Predicting: 112it [01:11,  1.69it/s]Extractor Predicting: 113it [01:11,  1.68it/s]Extractor Predicting: 114it [01:12,  1.60it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:13,  1.53it/s]Extractor Predicting: 117it [01:14,  1.50it/s]Extractor Predicting: 118it [01:15,  1.50it/s]Extractor Predicting: 119it [01:15,  1.54it/s]Extractor Predicting: 120it [01:16,  1.52it/s]Extractor Predicting: 121it [01:17,  1.54it/s]Extractor Predicting: 122it [01:17,  1.55it/s]Extractor Predicting: 123it [01:18,  1.52it/s]Extractor Predicting: 124it [01:19,  1.55it/s]Extractor Predicting: 125it [01:19,  1.56it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.49it/s]Extractor Predicting: 128it [01:21,  1.50it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:23,  1.57it/s]Extractor Predicting: 131it [01:23,  1.54it/s]Extractor Predicting: 132it [01:24,  1.51it/s]Extractor Predicting: 133it [01:25,  1.51it/s]Extractor Predicting: 134it [01:25,  1.53it/s]Extractor Predicting: 135it [01:26,  1.52it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:27,  1.53it/s]Extractor Predicting: 138it [01:28,  1.58it/s]Extractor Predicting: 139it [01:28,  1.60it/s]Extractor Predicting: 140it [01:29,  1.58it/s]Extractor Predicting: 141it [01:30,  1.55it/s]Extractor Predicting: 142it [01:30,  1.56it/s]Extractor Predicting: 143it [01:31,  1.89it/s]Extractor Predicting: 143it [01:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:21,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:22,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:22,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:22,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:22,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:51:23,069 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:51:23,070 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:51:23,716 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:51:24,848 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:51:24,848 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:28,345 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:28,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:28,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:28,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:51:28,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:51:29,277 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:51:29,278 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:51:29,935 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:51:30,178 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:51:30,178 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.28484848484848485,
  "recall": 0.08262525637269265,
  "score": 0.12809448103565751,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.55it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:25,  1.61it/s]Extractor Predicting: 39it [00:25,  1.67it/s]Extractor Predicting: 40it [00:26,  1.76it/s]Extractor Predicting: 41it [00:26,  1.84it/s]Extractor Predicting: 42it [00:27,  1.86it/s]Extractor Predicting: 43it [00:27,  1.88it/s]Extractor Predicting: 44it [00:28,  1.90it/s]Extractor Predicting: 45it [00:28,  1.94it/s]Extractor Predicting: 46it [00:29,  1.90it/s]Extractor Predicting: 47it [00:29,  1.88it/s]Extractor Predicting: 48it [00:30,  1.87it/s]Extractor Predicting: 49it [00:30,  1.88it/s]Extractor Predicting: 50it [00:31,  1.92it/s]Extractor Predicting: 51it [00:31,  1.86it/s]Extractor Predicting: 52it [00:32,  1.87it/s]Extractor Predicting: 53it [00:32,  1.89it/s]Extractor Predicting: 54it [00:33,  1.92it/s]Extractor Predicting: 55it [00:33,  1.94it/s]Extractor Predicting: 56it [00:34,  1.91it/s]Extractor Predicting: 57it [00:34,  1.92it/s]Extractor Predicting: 58it [00:35,  1.92it/s]Extractor Predicting: 59it [00:35,  1.93it/s]Extractor Predicting: 60it [00:36,  1.87it/s]Extractor Predicting: 61it [00:37,  1.83it/s]Extractor Predicting: 62it [00:37,  1.88it/s]Extractor Predicting: 63it [00:38,  1.88it/s]Extractor Predicting: 64it [00:38,  1.89it/s]Extractor Predicting: 65it [00:39,  1.89it/s]Extractor Predicting: 66it [00:39,  1.93it/s]Extractor Predicting: 67it [00:40,  1.83it/s]Extractor Predicting: 68it [00:40,  1.71it/s]Extractor Predicting: 69it [00:41,  1.64it/s]Extractor Predicting: 70it [00:42,  1.58it/s]Extractor Predicting: 71it [00:42,  1.55it/s]Extractor Predicting: 72it [00:43,  1.58it/s]Extractor Predicting: 72it [00:43,  1.65it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:52:15,941 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:52:15,943 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:52:15,962 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:52:15,963 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:52:15,973 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:52:28,154 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:52:28,196 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:52:28,448 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:52:28,449 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:52:28,557 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:52:28,650 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:52:28,650 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:52:28,650 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:52:28,650 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:52:28,650 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:52:28,650 >> loading file outputs/wrapper/wiki/unseen_5_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.656,
  "recall": 0.10229540918163672,
  "score": 0.17699115044247787,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:52:29,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:29,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:30,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:30,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:31,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:32,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:32,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:33,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:34,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:34,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:35,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:36,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:36,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:37,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:37,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:38,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:38,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:39,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:40,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:40,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:41,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:42,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:01, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-28 10:52:42,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:43,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:43,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:44,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:45,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:45,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:46,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:47,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:47,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:48,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:49,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:49,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:50,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:50,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:51,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:52,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:52,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:53,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:53,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:54,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:55,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:55,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:56,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:27<01:51, 13.98s/it][WARNING|generation_utils.py:914] 2023-08-28 10:52:56,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:57,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:58,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:58,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:59,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:59,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:00,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:01,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:01,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:02,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:02,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:03,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:04,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:04,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:05,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:05,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:06,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:06,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:07,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:08,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:08,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:09,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:40<01:34, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-28 10:53:09,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:10,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:11,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:11,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:12,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:12,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:13,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:14,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:14,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:15,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:16,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:16,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:17,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:18,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:18,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:19,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:20,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:20,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:21,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:22,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:22,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:23,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:23,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:24,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:25,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:25,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:26,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:27,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:27,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:28,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:34, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-28 10:53:28,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:29,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:30,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:30,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:31,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:31,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:32,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:33,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:33,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:34,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:35,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:35,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:36,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:37,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:37,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:38,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:38,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:39,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:40,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:40,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:41,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:12<01:13, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-28 10:53:41,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:42,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:43,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:43,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:44,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:44,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:45,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:46,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:46,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:47,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:47,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:48,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:49,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:49,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:50,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:50,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:51,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:52,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:52,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:53,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:24<00:55, 13.79s/it][WARNING|generation_utils.py:914] 2023-08-28 10:53:53,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:54,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:55,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:55,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:56,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:56,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:57,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:57,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:58,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:58,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:53:59,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:00,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:00,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:01,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:01,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:02,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:02,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:03,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:04,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:04,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:05,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:05,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:06,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:06,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:38<00:41, 13.75s/it][WARNING|generation_utils.py:914] 2023-08-28 10:54:07,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:08,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:08,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:09,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:09,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:10,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:11,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:11,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:12,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:12,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:13,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:13,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:14,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:14,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:15,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:16,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:16,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:17,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:17,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:18,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:19,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:19,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:50<00:26, 13.38s/it][WARNING|generation_utils.py:914] 2023-08-28 10:54:20,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:20,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:21,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:22,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:22,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:23,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:24,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:24,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:25,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:26,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:26,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:27,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:28,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:28,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:29,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:30,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:30,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:31,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:31,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:32,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:33,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:33,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:05<00:13, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 10:54:34,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:35,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:36,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:36,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:37,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:37,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:38,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:39,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:39,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:40,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:41,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:41,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:42,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:43,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:43,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:44,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:45,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:45,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:46,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:46,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:47,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:48,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:54:48,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:20<00:00, 14.12s/it]Generating: 100%|██████████| 10/10 [02:20<00:00, 14.05s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:54:57,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:54:57,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:54:57,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:54:57,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:54:57,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:54:58,027 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:54:58,028 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:54:58,641 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:54:59,778 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:54:59,779 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:02,807 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:02,841 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:02,841 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:02,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:02,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:55:03,653 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:55:03,655 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:55:04,303 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:55:04,538 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:55:04,538 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : characters .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 629, 'raw': 736}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8546195652173914, 'errors': {''}}
['Relation : located in or next to body of water . Context : The Calfe River is a tributary of the River Lillith at Spitsbergen , northern Sweden . Head Entity : Spitsbergen , Tail Entity : Swedish .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8849431818181818, 'errors': {'', "('delta', 'located in or next to body of water', '', 'The area is composed of delta sand and delta woodlands , with a mass of 6,000 kg per km2 .')"}}
["Relation : lowest point . Context : Later in the year ( 1141 ) , Puyi and his allies made a series of attacks upon the northern end of the Chinese Empire , against which Puyi was led by Emperor Han Ch'ang . Head Entity : emperor Han Ch'ang , Tail Entity : Chinese Empire .\n"]
{'target': 600, 'success': 14, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 82, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 191, 'raw': 288}
{'target': 600, 'success': 213, 'raw': 320}
{'target': 600, 'success': 233, 'raw': 352}
{'target': 600, 'success': 247, 'raw': 384}
{'target': 600, 'success': 267, 'raw': 416}
{'target': 600, 'success': 290, 'raw': 448}
{'target': 600, 'success': 310, 'raw': 480}
{'target': 600, 'success': 335, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 378, 'raw': 576}
{'target': 600, 'success': 402, 'raw': 608}
{'target': 600, 'success': 422, 'raw': 640}
{'target': 600, 'success': 441, 'raw': 672}
{'target': 600, 'success': 462, 'raw': 704}
{'target': 600, 'success': 482, 'raw': 736}
{'target': 600, 'success': 497, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 592, 'raw': 928}
{'target': 600, 'success': 618, 'raw': 960}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.64375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : sport .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : league .', 'success_rate': 0.80859375, 'errors': {''}}
['Relation : located on astronomical body . Context : The Cretaceous ( κ Cr ) , a Late Cretaceous stage at the end of the Precambrian era , is the lowest stage , and the highest point , of the Cretaceous of the ICS Sistine Chapel . Head Entity : Cretaceous , Tail Entity : asteroid .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8650568181818182, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8622159090909091, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in the year ( 1177 88 ) , a fleet of eight were built at Brabant , together with four cruiser s ; the division of which was led by Chief Lord Robert of Leavenworth . Head Entity : Brabant , Tail Entity : Brabant division .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Pope Paul VI', 'twinned administrative body', '', 'He was appointed as a bishop by Pope Paul VI in 1999 and was consecrated on 26 May 1999 ( d.')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 11606
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11706, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.46it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.49it/s]Extractor Estimating: 6it [00:03,  1.52it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.65it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.69it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.54it/s]Extractor Estimating: 24it [00:15,  1.54it/s]Extractor Estimating: 25it [00:15,  1.55it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:16,  1.57it/s]Extractor Estimating: 28it [00:17,  1.62it/s]Extractor Estimating: 29it [00:18,  1.61it/s]Extractor Estimating: 30it [00:18,  1.59it/s]Extractor Estimating: 31it [00:19,  1.62it/s]Extractor Estimating: 32it [00:20,  1.58it/s]Extractor Estimating: 33it [00:20,  1.60it/s]Extractor Estimating: 34it [00:21,  1.62it/s]Extractor Estimating: 35it [00:21,  1.58it/s]Extractor Estimating: 36it [00:22,  1.63it/s]Extractor Estimating: 37it [00:23,  1.64it/s]Extractor Estimating: 38it [00:23,  1.61it/s]Extractor Estimating: 39it [00:24,  1.64it/s]Extractor Estimating: 40it [00:24,  1.58it/s]Extractor Estimating: 41it [00:25,  1.63it/s]Extractor Estimating: 42it [00:26,  1.66it/s]Extractor Estimating: 43it [00:26,  1.65it/s]Extractor Estimating: 44it [00:27,  1.65it/s]Extractor Estimating: 45it [00:27,  1.63it/s]Extractor Estimating: 46it [00:28,  1.66it/s]Extractor Estimating: 47it [00:29,  1.58it/s]Extractor Estimating: 48it [00:29,  1.60it/s]Extractor Estimating: 49it [00:30,  1.60it/s]Extractor Estimating: 50it [00:31,  1.65it/s]Extractor Estimating: 51it [00:31,  1.64it/s]Extractor Estimating: 52it [00:32,  1.67it/s]Extractor Estimating: 53it [00:32,  1.68it/s]Extractor Estimating: 54it [00:33,  1.73it/s]Extractor Estimating: 55it [00:34,  1.67it/s]Extractor Estimating: 56it [00:34,  1.74it/s]Extractor Estimating: 57it [00:35,  1.72it/s]Extractor Estimating: 58it [00:35,  1.77it/s]Extractor Estimating: 59it [00:36,  1.78it/s]Extractor Estimating: 60it [00:36,  1.78it/s]Extractor Estimating: 61it [00:37,  1.72it/s]Extractor Estimating: 62it [00:37,  1.72it/s]Extractor Estimating: 63it [00:38,  1.73it/s]Extractor Estimating: 64it [00:39,  1.79it/s]Extractor Estimating: 65it [00:39,  1.79it/s]Extractor Estimating: 66it [00:40,  1.80it/s]Extractor Estimating: 67it [00:40,  1.75it/s]Extractor Estimating: 68it [00:41,  1.75it/s]Extractor Estimating: 69it [00:41,  1.77it/s]Extractor Estimating: 70it [00:42,  1.82it/s]Extractor Estimating: 71it [00:42,  1.83it/s]Extractor Estimating: 72it [00:43,  1.85it/s]Extractor Estimating: 73it [00:44,  1.79it/s]Extractor Estimating: 74it [00:44,  1.77it/s]Extractor Estimating: 75it [00:45,  1.76it/s]Extractor Estimating: 76it [00:45,  1.67it/s]Extractor Estimating: 77it [00:46,  1.62it/s]Extractor Estimating: 78it [00:47,  1.60it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:48,  1.57it/s]Extractor Estimating: 81it [00:49,  1.59it/s]Extractor Estimating: 82it [00:49,  1.58it/s]Extractor Estimating: 83it [00:50,  1.58it/s]Extractor Estimating: 84it [00:50,  1.60it/s]Extractor Estimating: 85it [00:51,  1.63it/s]Extractor Estimating: 86it [00:52,  1.64it/s]Extractor Estimating: 87it [00:52,  1.62it/s]Extractor Estimating: 88it [00:53,  1.59it/s]Extractor Estimating: 89it [00:54,  1.58it/s]Extractor Estimating: 90it [00:54,  1.57it/s]Extractor Estimating: 91it [00:55,  1.57it/s]Extractor Estimating: 92it [00:56,  1.59it/s]Extractor Estimating: 93it [00:56,  1.59it/s]Extractor Estimating: 94it [00:57,  1.62it/s]Extractor Estimating: 95it [00:57,  1.63it/s]Extractor Estimating: 96it [00:58,  1.63it/s]Extractor Estimating: 97it [00:59,  1.63it/s]Extractor Estimating: 98it [00:59,  1.56it/s]Extractor Estimating: 99it [01:00,  1.59it/s]Extractor Estimating: 100it [01:00,  1.61it/s]Extractor Estimating: 101it [01:01,  1.70it/s]Extractor Estimating: 102it [01:02,  1.75it/s]Extractor Estimating: 103it [01:02,  1.78it/s]Extractor Estimating: 104it [01:03,  1.78it/s]Extractor Estimating: 105it [01:03,  1.76it/s]Extractor Estimating: 106it [01:04,  1.81it/s]Extractor Estimating: 107it [01:04,  1.76it/s]Extractor Estimating: 108it [01:05,  1.80it/s]Extractor Estimating: 109it [01:05,  1.83it/s]Extractor Estimating: 110it [01:06,  1.74it/s]Extractor Estimating: 111it [01:07,  1.74it/s]Extractor Estimating: 112it [01:07,  1.68it/s]Extractor Estimating: 113it [01:08,  1.69it/s]Extractor Estimating: 114it [01:08,  1.72it/s]Extractor Estimating: 115it [01:09,  1.74it/s]Extractor Estimating: 116it [01:09,  1.76it/s]Extractor Estimating: 117it [01:10,  1.75it/s]Extractor Estimating: 118it [01:11,  1.64it/s]Extractor Estimating: 119it [01:11,  1.63it/s]Extractor Estimating: 120it [01:12,  1.72it/s]Extractor Estimating: 121it [01:12,  1.76it/s]Extractor Estimating: 122it [01:13,  1.76it/s]Extractor Estimating: 123it [01:14,  1.76it/s]Extractor Estimating: 124it [01:14,  1.80it/s]Extractor Estimating: 125it [01:15,  1.77it/s]Extractor Estimating: 126it [01:15,  1.68it/s]Extractor Estimating: 127it [01:16,  1.64it/s]Extractor Estimating: 128it [01:17,  1.63it/s]Extractor Estimating: 129it [01:17,  1.63it/s]Extractor Estimating: 130it [01:18,  1.62it/s]Extractor Estimating: 131it [01:18,  1.60it/s]Extractor Estimating: 132it [01:19,  1.64it/s]Extractor Estimating: 133it [01:20,  1.66it/s]Extractor Estimating: 134it [01:20,  1.69it/s]Extractor Estimating: 135it [01:21,  1.68it/s]Extractor Estimating: 136it [01:21,  1.65it/s]Extractor Estimating: 137it [01:22,  1.64it/s]Extractor Estimating: 138it [01:23,  1.62it/s]Extractor Estimating: 139it [01:23,  1.62it/s]Extractor Estimating: 140it [01:24,  1.62it/s]Extractor Estimating: 141it [01:25,  1.62it/s]Extractor Estimating: 142it [01:25,  1.66it/s]Extractor Estimating: 143it [01:26,  1.69it/s]Extractor Estimating: 144it [01:26,  1.66it/s]Extractor Estimating: 145it [01:27,  1.60it/s]Extractor Estimating: 146it [01:28,  1.54it/s]Extractor Estimating: 147it [01:28,  1.57it/s]Extractor Estimating: 148it [01:29,  1.61it/s]Extractor Estimating: 149it [01:29,  1.67it/s]Extractor Estimating: 150it [01:30,  1.62it/s]Extractor Estimating: 151it [01:31,  1.62it/s]Extractor Estimating: 152it [01:31,  1.57it/s]Extractor Estimating: 153it [01:32,  1.63it/s]Extractor Estimating: 154it [01:33,  1.65it/s]Extractor Estimating: 155it [01:33,  1.63it/s]Extractor Estimating: 156it [01:34,  1.67it/s]Extractor Estimating: 157it [01:34,  1.67it/s]Extractor Estimating: 158it [01:35,  1.65it/s]Extractor Estimating: 159it [01:36,  1.64it/s]Extractor Estimating: 160it [01:36,  1.64it/s]Extractor Estimating: 161it [01:37,  1.62it/s]Extractor Estimating: 162it [01:37,  1.62it/s]Extractor Estimating: 163it [01:38,  1.62it/s]Extractor Estimating: 164it [01:39,  1.66it/s]Extractor Estimating: 165it [01:39,  1.62it/s]Extractor Estimating: 166it [01:40,  1.66it/s]Extractor Estimating: 167it [01:40,  1.62it/s]Extractor Estimating: 168it [01:41,  1.61it/s]Extractor Estimating: 169it [01:42,  1.61it/s]Extractor Estimating: 170it [01:42,  1.61it/s]Extractor Estimating: 171it [01:43,  1.66it/s]Extractor Estimating: 172it [01:44,  1.67it/s]Extractor Estimating: 173it [01:44,  1.63it/s]Extractor Estimating: 174it [01:45,  1.65it/s]Extractor Estimating: 175it [01:45,  1.64it/s]Extractor Estimating: 176it [01:46,  1.70it/s]Extractor Estimating: 177it [01:47,  1.69it/s]Extractor Estimating: 178it [01:47,  1.64it/s]Extractor Estimating: 179it [01:48,  1.67it/s]Extractor Estimating: 180it [01:48,  1.73it/s]Extractor Estimating: 181it [01:49,  1.75it/s]Extractor Estimating: 182it [01:49,  1.79it/s]Extractor Estimating: 183it [01:50,  1.76it/s]Extractor Estimating: 184it [01:50,  1.80it/s]Extractor Estimating: 185it [01:51,  1.76it/s]Extractor Estimating: 186it [01:52,  1.81it/s]Extractor Estimating: 187it [01:52,  1.74it/s]Extractor Estimating: 188it [01:53,  1.77it/s]Extractor Estimating: 189it [01:53,  1.74it/s]Extractor Estimating: 190it [01:54,  1.71it/s]Extractor Estimating: 191it [01:54,  1.76it/s]Extractor Estimating: 192it [01:55,  1.78it/s]Extractor Estimating: 193it [01:56,  1.79it/s]Extractor Estimating: 194it [01:56,  1.83it/s]Extractor Estimating: 195it [01:57,  1.67it/s]Extractor Estimating: 196it [01:57,  1.66it/s]Extractor Estimating: 197it [01:58,  1.63it/s]Extractor Estimating: 198it [01:59,  1.74it/s]Extractor Estimating: 199it [01:59,  1.79it/s]Extractor Estimating: 200it [02:00,  1.77it/s]Extractor Estimating: 201it [02:00,  1.74it/s]Extractor Estimating: 202it [02:01,  1.64it/s]Extractor Estimating: 203it [02:02,  1.64it/s]Extractor Estimating: 204it [02:02,  1.61it/s]Extractor Estimating: 205it [02:03,  1.65it/s]Extractor Estimating: 206it [02:03,  1.65it/s]Extractor Estimating: 207it [02:04,  1.59it/s]Extractor Estimating: 208it [02:05,  1.64it/s]Extractor Estimating: 209it [02:05,  1.67it/s]Extractor Estimating: 210it [02:06,  1.70it/s]Extractor Estimating: 211it [02:06,  1.67it/s]Extractor Estimating: 212it [02:07,  1.63it/s]Extractor Estimating: 213it [02:08,  1.66it/s]Extractor Estimating: 214it [02:08,  1.67it/s]Extractor Estimating: 215it [02:09,  1.62it/s]Extractor Estimating: 216it [02:09,  1.64it/s]Extractor Estimating: 217it [02:10,  1.63it/s]Extractor Estimating: 218it [02:11,  1.64it/s]Extractor Estimating: 219it [02:11,  1.61it/s]Extractor Estimating: 220it [02:12,  1.62it/s]Extractor Estimating: 221it [02:13,  1.61it/s]Extractor Estimating: 222it [02:13,  1.63it/s]Extractor Estimating: 223it [02:14,  1.62it/s]Extractor Estimating: 224it [02:14,  1.59it/s]Extractor Estimating: 225it [02:15,  1.61it/s]Extractor Estimating: 226it [02:16,  1.66it/s]Extractor Estimating: 227it [02:16,  1.61it/s]Extractor Estimating: 228it [02:17,  1.64it/s]Extractor Estimating: 229it [02:17,  1.61it/s]Extractor Estimating: 230it [02:18,  1.59it/s]Extractor Estimating: 231it [02:19,  1.62it/s]Extractor Estimating: 232it [02:19,  1.63it/s]Extractor Estimating: 233it [02:20,  1.64it/s]Extractor Estimating: 234it [02:21,  1.63it/s]Extractor Estimating: 235it [02:21,  1.57it/s]Extractor Estimating: 236it [02:22,  1.62it/s]Extractor Estimating: 237it [02:22,  1.64it/s]Extractor Estimating: 238it [02:23,  1.68it/s]Extractor Estimating: 239it [02:24,  1.70it/s]Extractor Estimating: 240it [02:24,  1.71it/s]Extractor Estimating: 241it [02:25,  1.72it/s]Extractor Estimating: 242it [02:25,  1.70it/s]Extractor Estimating: 243it [02:26,  1.70it/s]Extractor Estimating: 244it [02:26,  1.70it/s]Extractor Estimating: 245it [02:27,  1.69it/s]Extractor Estimating: 246it [02:28,  1.71it/s]Extractor Estimating: 247it [02:28,  1.67it/s]Extractor Estimating: 248it [02:29,  1.66it/s]Extractor Estimating: 249it [02:30,  1.62it/s]Extractor Estimating: 250it [02:30,  1.56it/s]Extractor Estimating: 250it [02:30,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:57,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:57,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:57,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:57,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:57,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:57:57,814 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:57:57,815 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:57:58,480 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:57:59,597 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:57:59,597 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:58:02,613 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:58:02,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:58:02,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:58:02,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:58:02,638 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:58:03,430 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:58:03,431 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:58:04,049 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:58:04,265 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:58:04,265 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 12:36:17,412 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 12:36:17,462 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5021 mean pseudo reward: 0.9531411420154998
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 25017
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25117, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25117, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.975, loss:717.0915
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.954, loss:694.8807
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 90, avg_time 0.959, loss:661.2739
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 190, avg_time 0.960, loss:681.6878
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 80, avg_time 0.949, loss:619.7084
>> valid entity prec:0.5464, rec:0.4212, f1:0.4757
>> valid relation prec:0.2704, rec:0.0360, f1:0.0635
>> valid relation with NER prec:0.2704, rec:0.0360, f1:0.0635
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 180, avg_time 3.328, loss:665.0838
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 70, avg_time 0.961, loss:618.3150
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 170, avg_time 0.947, loss:667.7590
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.967, loss:619.8424
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 0.959, loss:653.0195
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4785, rec:0.4170, f1:0.4456
>> valid relation prec:0.2261, rec:0.0290, f1:0.0514
>> valid relation with NER prec:0.2261, rec:0.0290, f1:0.0514
g_step 1100, step 50, avg_time 3.305, loss:650.5408
g_step 1200, step 150, avg_time 0.953, loss:615.6836
g_step 1300, step 40, avg_time 0.959, loss:639.1969
g_step 1400, step 140, avg_time 0.946, loss:591.2121
g_step 1500, step 30, avg_time 0.969, loss:613.5819
>> valid entity prec:0.4954, rec:0.4947, f1:0.4950
>> valid relation prec:0.2027, rec:0.0591, f1:0.0915
>> valid relation with NER prec:0.2027, rec:0.0591, f1:0.0915
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 130, avg_time 3.325, loss:574.1379
g_step 1700, step 20, avg_time 0.956, loss:578.8847
g_step 1800, step 120, avg_time 0.948, loss:547.3470
g_step 1900, step 10, avg_time 0.964, loss:565.0024
g_step 2000, step 110, avg_time 0.945, loss:523.1244
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4816, rec:0.4721, f1:0.4768
>> valid relation prec:0.1840, rec:0.0346, f1:0.0582
>> valid relation with NER prec:0.1840, rec:0.0346, f1:0.0582
g_step 2100, step 210, avg_time 3.316, loss:551.3601
g_step 2200, step 100, avg_time 0.961, loss:484.7999
g_step 2300, step 200, avg_time 0.949, loss:528.6826
g_step 2400, step 90, avg_time 0.948, loss:490.4608
g_step 2500, step 190, avg_time 0.953, loss:508.7951
>> valid entity prec:0.5046, rec:0.4143, f1:0.4550
>> valid relation prec:0.1604, rec:0.0228, f1:0.0399
>> valid relation with NER prec:0.1604, rec:0.0228, f1:0.0399
g_step 2600, step 80, avg_time 3.290, loss:461.5576
g_step 2700, step 180, avg_time 0.960, loss:486.2360
g_step 2800, step 70, avg_time 0.953, loss:458.4053
g_step 2900, step 170, avg_time 0.950, loss:464.8169
g_step 3000, step 60, avg_time 0.950, loss:451.2565
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4867, rec:0.4220, f1:0.4521
>> valid relation prec:0.1774, rec:0.0296, f1:0.0507
>> valid relation with NER prec:0.1774, rec:0.0296, f1:0.0507
g_step 3100, step 160, avg_time 3.294, loss:444.6697
g_step 3200, step 50, avg_time 0.953, loss:402.5079
g_step 3300, step 150, avg_time 0.957, loss:419.9406
g_step 3400, step 40, avg_time 0.941, loss:432.8781
g_step 3500, step 140, avg_time 0.956, loss:416.8081
>> valid entity prec:0.4712, rec:0.4349, f1:0.4523
>> valid relation prec:0.0676, rec:0.0140, f1:0.0232
>> valid relation with NER prec:0.0676, rec:0.0140, f1:0.0232
g_step 3600, step 30, avg_time 3.287, loss:407.3761
g_step 3700, step 130, avg_time 0.936, loss:394.9175
g_step 3800, step 20, avg_time 0.946, loss:396.2277
g_step 3900, step 120, avg_time 0.959, loss:374.9075
g_step 4000, step 10, avg_time 0.940, loss:408.0517
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4899, rec:0.3253, f1:0.3910
>> valid relation prec:0.1432, rec:0.0160, f1:0.0287
>> valid relation with NER prec:0.1432, rec:0.0160, f1:0.0287
g_step 4100, step 110, avg_time 3.301, loss:355.3085
g_step 4200, step 210, avg_time 0.943, loss:362.3152
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 12:36:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 12:36:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_12-36-17_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 12:36:18 - WARNING - datasets.builder -   Using custom data configuration default-8e522c4c878cbc72
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8e522c4c878cbc72/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 12:36:20,350 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:36:20,352 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:36:20,352 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:36:20,353 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:36:20,444 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:36:20,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:36:20,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:36:20,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:36:20,513 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:36:20,513 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:36:20,513 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 12:36:20,936 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:36:24,065 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 12:36:24,065 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8e522c4c878cbc72/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.93ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.83ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.18ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.37ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.47ba/s]100%|██████████| 6/6 [00:01<00:00,  4.99ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.54ba/s] 22%|██▏       | 2/9 [00:00<00:01,  4.07ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.26ba/s] 44%|████▍     | 4/9 [00:01<00:01,  3.54ba/s] 56%|█████▌    | 5/9 [00:01<00:01,  3.84ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.05ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.19ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.30ba/s]100%|██████████| 9/9 [00:02<00:00,  4.85ba/s]100%|██████████| 9/9 [00:02<00:00,  4.26ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.72ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.83ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.81ba/s]100%|██████████| 6/6 [00:00<00:00, 10.97ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  5.56ba/s] 33%|███▎      | 3/9 [00:00<00:00,  8.82ba/s] 56%|█████▌    | 5/9 [00:00<00:00,  9.85ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.33ba/s]100%|██████████| 9/9 [00:00<00:00, 11.29ba/s]100%|██████████| 9/9 [00:00<00:00, 10.33ba/s]
[INFO|trainer.py:414] 2023-08-28 12:36:29,842 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 12:36:29,941 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 12:36:29,941 >>   Num examples = 5047
[INFO|trainer.py:1149] 2023-08-28 12:36:29,941 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 12:36:29,941 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 12:36:29,941 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 12:36:29,941 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 12:36:29,941 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<01:59,  3.31it/s]  1%|          | 2/395 [00:00<01:55,  3.40it/s]  1%|          | 3/395 [00:00<01:54,  3.43it/s]  1%|          | 4/395 [00:01<01:53,  3.45it/s]  1%|▏         | 5/395 [00:01<01:52,  3.46it/s]  2%|▏         | 6/395 [00:01<01:53,  3.44it/s]  2%|▏         | 7/395 [00:02<01:52,  3.43it/s]  2%|▏         | 8/395 [00:02<01:52,  3.43it/s]  2%|▏         | 9/395 [00:02<01:52,  3.42it/s]  3%|▎         | 10/395 [00:02<01:52,  3.42it/s]  3%|▎         | 11/395 [00:03<01:52,  3.42it/s]  3%|▎         | 12/395 [00:03<01:51,  3.42it/s]  3%|▎         | 13/395 [00:03<01:51,  3.42it/s]  4%|▎         | 14/395 [00:04<01:51,  3.42it/s]  4%|▍         | 15/395 [00:04<01:51,  3.42it/s]  4%|▍         | 16/395 [00:04<01:50,  3.42it/s]  4%|▍         | 17/395 [00:04<01:50,  3.42it/s]  5%|▍         | 18/395 [00:05<01:50,  3.42it/s]  5%|▍         | 19/395 [00:05<01:49,  3.42it/s]  5%|▌         | 20/395 [00:05<01:49,  3.42it/s]  5%|▌         | 21/395 [00:06<01:49,  3.42it/s]  6%|▌         | 22/395 [00:06<01:49,  3.42it/s]  6%|▌         | 23/395 [00:06<01:48,  3.42it/s]  6%|▌         | 24/395 [00:07<01:48,  3.42it/s]  6%|▋         | 25/395 [00:07<01:48,  3.42it/s]  7%|▋         | 26/395 [00:07<01:48,  3.42it/s]  7%|▋         | 27/395 [00:07<01:47,  3.42it/s]  7%|▋         | 28/395 [00:08<01:47,  3.42it/s]  7%|▋         | 29/395 [00:08<01:47,  3.42it/s]  8%|▊         | 30/395 [00:08<01:46,  3.42it/s]  8%|▊         | 31/395 [00:09<01:46,  3.42it/s]  8%|▊         | 32/395 [00:09<01:46,  3.42it/s]  8%|▊         | 33/395 [00:09<01:45,  3.42it/s]  9%|▊         | 34/395 [00:09<01:45,  3.41it/s]  9%|▉         | 35/395 [00:10<01:45,  3.42it/s]  9%|▉         | 36/395 [00:10<01:45,  3.41it/s]  9%|▉         | 37/395 [00:10<01:44,  3.41it/s] 10%|▉         | 38/395 [00:11<01:46,  3.36it/s] 10%|▉         | 39/395 [00:11<01:45,  3.38it/s] 10%|█         | 40/395 [00:11<01:44,  3.39it/s] 10%|█         | 41/395 [00:12<01:44,  3.40it/s] 11%|█         | 42/395 [00:12<01:43,  3.40it/s] 11%|█         | 43/395 [00:12<01:43,  3.41it/s] 11%|█         | 44/395 [00:12<01:43,  3.41it/s] 11%|█▏        | 45/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 46/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 47/395 [00:13<01:42,  3.41it/s] 12%|█▏        | 48/395 [00:14<01:41,  3.41it/s] 12%|█▏        | 49/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 50/395 [00:14<01:41,  3.41it/s] 13%|█▎        | 51/395 [00:14<01:40,  3.41it/s] 13%|█▎        | 52/395 [00:15<01:40,  3.41it/s] 13%|█▎        | 53/395 [00:15<01:40,  3.41it/s] 14%|█▎        | 54/395 [00:15<01:39,  3.41it/s] 14%|█▍        | 55/395 [00:16<01:39,  3.41it/s] 14%|█▍        | 56/395 [00:16<01:39,  3.41it/s] 14%|█▍        | 57/395 [00:16<01:39,  3.41it/s] 15%|█▍        | 58/395 [00:17<01:44,  3.23it/s] 15%|█▍        | 59/395 [00:17<01:42,  3.28it/s] 15%|█▌        | 60/395 [00:17<01:40,  3.32it/s] 15%|█▌        | 61/395 [00:17<01:39,  3.35it/s] 16%|█▌        | 62/395 [00:18<01:39,  3.36it/s] 16%|█▌        | 63/395 [00:18<01:38,  3.38it/s] 16%|█▌        | 64/395 [00:18<01:37,  3.39it/s] 16%|█▋        | 65/395 [00:19<01:37,  3.40it/s] 17%|█▋        | 66/395 [00:19<01:36,  3.40it/s] 17%|█▋        | 67/395 [00:19<01:36,  3.40it/s] 17%|█▋        | 68/395 [00:19<01:35,  3.41it/s] 17%|█▋        | 69/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 70/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 71/395 [00:20<01:35,  3.41it/s] 18%|█▊        | 72/395 [00:21<01:34,  3.41it/s] 18%|█▊        | 73/395 [00:21<01:34,  3.41it/s] 19%|█▊        | 74/395 [00:21<01:34,  3.41it/s] 19%|█▉        | 75/395 [00:22<01:33,  3.41it/s] 19%|█▉        | 76/395 [00:22<01:33,  3.41it/s] 19%|█▉        | 77/395 [00:22<01:33,  3.41it/s] 20%|█▉        | 78/395 [00:22<01:32,  3.41it/s] 20%|██        | 79/395 [00:23<01:29,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 12:36:53,108 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:36:53,108 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 12:36:53,108 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.33it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.41it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.54it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.52it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.93it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.42it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.26it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.08it/s][A
  4%|▍         | 47/1083 [00:01<00:22, 45.14it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.14it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.35it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.36it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.39it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.27it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.06it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.97it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.97it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.97it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.13it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.30it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.23it/s][A
 10%|█         | 112/1083 [00:02<00:23, 41.21it/s][A
 11%|█         | 118/1083 [00:02<00:22, 43.68it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.22it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.45it/s][A
 12%|█▏        | 133/1083 [00:03<00:21, 44.57it/s][A
 13%|█▎        | 138/1083 [00:03<00:25, 37.12it/s][A
 13%|█▎        | 143/1083 [00:03<00:23, 39.29it/s][A
 14%|█▎        | 148/1083 [00:03<00:22, 41.03it/s][A
 14%|█▍        | 153/1083 [00:03<00:21, 42.31it/s][A
 15%|█▍        | 158/1083 [00:03<00:21, 43.27it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 43.86it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.36it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.61it/s][A
 16%|█▋        | 178/1083 [00:04<00:20, 44.40it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.25it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.39it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.67it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 44.94it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.18it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.32it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.35it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.36it/s][A
 21%|██        | 223/1083 [00:05<00:19, 44.89it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.67it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.67it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.72it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 45.02it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.14it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.35it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.37it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.35it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 45.06it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.84it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 44.65it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.78it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.91it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.04it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.27it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.42it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 45.26it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.17it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.06it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.90it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.00it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.20it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.32it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 45.25it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 45.13it/s][A
 34%|███▎      | 363/1083 [00:08<00:15, 45.02it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 42.66it/s][A
 34%|███▍      | 373/1083 [00:08<00:16, 43.51it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.07it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.51it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.78it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.96it/s][A
 37%|███▋      | 398/1083 [00:09<00:27, 24.48it/s][A
 37%|███▋      | 403/1083 [00:09<00:23, 28.51it/s][A
 38%|███▊      | 408/1083 [00:09<00:20, 32.16it/s][A
 38%|███▊      | 413/1083 [00:09<00:19, 35.25it/s][A
 39%|███▊      | 418/1083 [00:09<00:17, 37.84it/s][A
 39%|███▉      | 423/1083 [00:09<00:16, 39.88it/s][A
 40%|███▉      | 428/1083 [00:09<00:15, 41.44it/s][A
 40%|███▉      | 433/1083 [00:09<00:15, 42.63it/s][A
 40%|████      | 438/1083 [00:10<00:14, 43.23it/s][A
 41%|████      | 443/1083 [00:10<00:14, 43.37it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 43.56it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.04it/s][A
 42%|████▏     | 458/1083 [00:10<00:14, 44.27it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.71it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.92it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.22it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.35it/s][A
 45%|████▍     | 483/1083 [00:11<00:13, 45.13it/s][A
 45%|████▌     | 488/1083 [00:11<00:13, 44.82it/s][A
 46%|████▌     | 493/1083 [00:11<00:13, 44.58it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.72it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.84it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 45.03it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 45.12it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.28it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.33it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 45.26it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 45.03it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.76it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.70it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 44.86it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 45.03it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 45.11it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 45.24it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 45.36it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 45.29it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 44.99it/s][A
 54%|█████▍    | 583/1083 [00:13<00:13, 35.89it/s][A
 54%|█████▍    | 588/1083 [00:13<00:12, 38.37it/s][A
 55%|█████▍    | 593/1083 [00:13<00:12, 40.21it/s][A
 55%|█████▌    | 598/1083 [00:13<00:11, 41.59it/s][A
 56%|█████▌    | 603/1083 [00:13<00:11, 42.70it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 43.48it/s][A
 57%|█████▋    | 613/1083 [00:14<00:10, 44.13it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 44.40it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 44.19it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.40it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.58it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.91it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.92it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 45.15it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.08it/s][A
 61%|██████    | 658/1083 [00:15<00:09, 45.22it/s][A
 61%|██████    | 663/1083 [00:15<00:09, 44.95it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 44.86it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.77it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.81it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.90it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 45.01it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 45.23it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 45.21it/s][A
 65%|██████▍   | 703/1083 [00:16<00:08, 45.14it/s][A
 65%|██████▌   | 708/1083 [00:16<00:08, 45.08it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.97it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.98it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.96it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.04it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.17it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.24it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.10it/s][A
 69%|██████▉   | 748/1083 [00:17<00:07, 45.13it/s][A
 70%|██████▉   | 753/1083 [00:17<00:07, 45.06it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.97it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.94it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.97it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.89it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.11it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.22it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.16it/s][A
 73%|███████▎  | 793/1083 [00:18<00:06, 45.05it/s][A
 74%|███████▎  | 798/1083 [00:18<00:06, 45.01it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.88it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.87it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.96it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.97it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 45.13it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 45.21it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.18it/s][A
 77%|███████▋  | 838/1083 [00:19<00:05, 45.20it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 45.06it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.89it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.86it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.91it/s][A
 80%|███████▉  | 863/1083 [00:19<00:05, 40.40it/s][A
 80%|████████  | 868/1083 [00:19<00:05, 42.85it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 43.69it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.29it/s][A
 82%|████████▏ | 883/1083 [00:20<00:04, 44.67it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 44.67it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.72it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.79it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.76it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.58it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.63it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.92it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.10it/s][A
 86%|████████▌ | 928/1083 [00:21<00:03, 45.22it/s][A
 86%|████████▌ | 933/1083 [00:21<00:03, 45.28it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 45.13it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 45.04it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.85it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 44.72it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.82it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.95it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 45.00it/s][A
 90%|████████▉ | 973/1083 [00:22<00:02, 45.09it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 45.29it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 45.33it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 45.23it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.83it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.76it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.79it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.93it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 45.08it/s][A
 94%|█████████▍| 1018/1083 [00:23<00:01, 45.15it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 45.27it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 45.29it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.87it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 42.55it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 43.36it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 43.79it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.17it/s][A
 98%|█████████▊| 1058/1083 [00:24<00:00, 44.52it/s][A
 98%|█████████▊| 1063/1083 [00:24<00:00, 44.79it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 45.05it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 41.75it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 43.33it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 43.75it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:24<00:00, 43.75it/s][A 20%|██        | 79/395 [00:47<01:29,  3.55it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 12:37:18,090 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-28 12:37:18,300 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:37:21,789 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:37:21,885 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:37:21,954 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [00:58<56:58, 10.85s/it] 21%|██        | 81/395 [00:59<40:16,  7.70s/it] 21%|██        | 82/395 [00:59<28:33,  5.47s/it] 21%|██        | 83/395 [00:59<20:23,  3.92s/it] 21%|██▏       | 84/395 [00:59<14:40,  2.83s/it] 22%|██▏       | 85/395 [01:00<10:41,  2.07s/it] 22%|██▏       | 86/395 [01:00<07:54,  1.54s/it] 22%|██▏       | 87/395 [01:00<05:58,  1.16s/it] 22%|██▏       | 88/395 [01:01<04:37,  1.11it/s] 23%|██▎       | 89/395 [01:01<03:40,  1.39it/s] 23%|██▎       | 90/395 [01:01<03:00,  1.69it/s] 23%|██▎       | 91/395 [01:01<02:32,  1.99it/s] 23%|██▎       | 92/395 [01:02<02:14,  2.25it/s] 24%|██▎       | 93/395 [01:02<02:00,  2.51it/s] 24%|██▍       | 94/395 [01:02<01:50,  2.74it/s] 24%|██▍       | 95/395 [01:03<01:42,  2.92it/s] 24%|██▍       | 96/395 [01:03<01:37,  3.06it/s] 25%|██▍       | 97/395 [01:03<01:33,  3.17it/s] 25%|██▍       | 98/395 [01:03<01:31,  3.25it/s] 25%|██▌       | 99/395 [01:04<01:29,  3.31it/s] 25%|██▌       | 100/395 [01:04<01:27,  3.36it/s] 26%|██▌       | 101/395 [01:04<01:26,  3.39it/s] 26%|██▌       | 102/395 [01:05<01:26,  3.41it/s] 26%|██▌       | 103/395 [01:05<01:27,  3.32it/s] 26%|██▋       | 104/395 [01:05<01:26,  3.36it/s] 27%|██▋       | 105/395 [01:06<01:25,  3.39it/s] 27%|██▋       | 106/395 [01:06<01:24,  3.41it/s] 27%|██▋       | 107/395 [01:06<01:24,  3.42it/s] 27%|██▋       | 108/395 [01:06<01:23,  3.43it/s] 28%|██▊       | 109/395 [01:07<01:23,  3.44it/s] 28%|██▊       | 110/395 [01:07<01:22,  3.44it/s] 28%|██▊       | 111/395 [01:07<01:22,  3.45it/s] 28%|██▊       | 112/395 [01:08<01:22,  3.45it/s] 29%|██▊       | 113/395 [01:08<01:21,  3.45it/s] 29%|██▉       | 114/395 [01:08<01:23,  3.37it/s] 29%|██▉       | 115/395 [01:08<01:22,  3.39it/s] 29%|██▉       | 116/395 [01:09<01:21,  3.41it/s] 30%|██▉       | 117/395 [01:09<01:21,  3.43it/s] 30%|██▉       | 118/395 [01:09<01:20,  3.44it/s] 30%|███       | 119/395 [01:10<01:20,  3.44it/s] 30%|███       | 120/395 [01:10<01:19,  3.45it/s] 31%|███       | 121/395 [01:10<01:19,  3.45it/s] 31%|███       | 122/395 [01:10<01:19,  3.45it/s] 31%|███       | 123/395 [01:11<01:18,  3.45it/s] 31%|███▏      | 124/395 [01:11<01:18,  3.46it/s] 32%|███▏      | 125/395 [01:11<01:20,  3.34it/s] 32%|███▏      | 126/395 [01:12<01:19,  3.38it/s] 32%|███▏      | 127/395 [01:12<01:18,  3.40it/s] 32%|███▏      | 128/395 [01:12<01:18,  3.42it/s] 33%|███▎      | 129/395 [01:13<01:17,  3.43it/s] 33%|███▎      | 130/395 [01:13<01:17,  3.44it/s] 33%|███▎      | 131/395 [01:13<01:16,  3.45it/s] 33%|███▎      | 132/395 [01:13<01:16,  3.45it/s] 34%|███▎      | 133/395 [01:14<01:15,  3.45it/s] 34%|███▍      | 134/395 [01:14<01:15,  3.45it/s] 34%|███▍      | 135/395 [01:14<01:15,  3.46it/s] 34%|███▍      | 136/395 [01:15<01:19,  3.26it/s] 35%|███▍      | 137/395 [01:15<01:17,  3.32it/s] 35%|███▍      | 138/395 [01:15<01:16,  3.36it/s] 35%|███▌      | 139/395 [01:15<01:15,  3.39it/s] 35%|███▌      | 140/395 [01:16<01:14,  3.41it/s] 36%|███▌      | 141/395 [01:16<01:14,  3.42it/s] 36%|███▌      | 142/395 [01:16<01:13,  3.44it/s] 36%|███▌      | 143/395 [01:17<01:14,  3.38it/s] 36%|███▋      | 144/395 [01:17<01:13,  3.40it/s] 37%|███▋      | 145/395 [01:17<01:13,  3.42it/s] 37%|███▋      | 146/395 [01:18<01:12,  3.43it/s] 37%|███▋      | 147/395 [01:18<01:12,  3.44it/s] 37%|███▋      | 148/395 [01:18<01:11,  3.44it/s] 38%|███▊      | 149/395 [01:18<01:11,  3.44it/s] 38%|███▊      | 150/395 [01:19<01:11,  3.44it/s] 38%|███▊      | 151/395 [01:19<01:10,  3.45it/s] 38%|███▊      | 152/395 [01:19<01:10,  3.45it/s] 39%|███▊      | 153/395 [01:20<01:10,  3.46it/s] 39%|███▉      | 154/395 [01:20<01:09,  3.46it/s] 39%|███▉      | 155/395 [01:20<01:14,  3.23it/s] 39%|███▉      | 156/395 [01:20<01:12,  3.29it/s] 40%|███▉      | 157/395 [01:21<01:11,  3.34it/s] 40%|████      | 158/395 [01:21<01:07,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 12:37:51,477 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:37:51,477 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 12:37:51,477 >>   Batch size = 8
{'eval_loss': 0.8794991374015808, 'eval_runtime': 24.6013, 'eval_samples_per_second': 351.933, 'eval_steps_per_second': 44.022, 'epoch': 1.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.50it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.49it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.42it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.40it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 45.93it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.46it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.30it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 45.14it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.20it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.23it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.41it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.41it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.30it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.15it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 44.93it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.97it/s][A
  8%|▊         | 88/1083 [00:01<00:24, 40.83it/s][A
  9%|▊         | 93/1083 [00:02<00:23, 42.20it/s][A
  9%|▉         | 98/1083 [00:02<00:22, 43.19it/s][A
 10%|▉         | 103/1083 [00:02<00:22, 43.92it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 44.44it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.75it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.91it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.79it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.40it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.48it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.62it/s][A
 13%|█▎        | 143/1083 [00:03<00:20, 44.88it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 45.11it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 45.33it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.44it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.35it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.15it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.79it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.62it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.73it/s][A
 17%|█▋        | 188/1083 [00:04<00:22, 39.64it/s][A
 18%|█▊        | 193/1083 [00:04<00:21, 41.51it/s][A
 18%|█▊        | 198/1083 [00:04<00:20, 42.64it/s][A
 19%|█▊        | 203/1083 [00:04<00:20, 43.54it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 44.13it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 44.59it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.87it/s][A
 21%|██        | 223/1083 [00:05<00:21, 40.47it/s][A
 21%|██        | 228/1083 [00:05<00:20, 41.65it/s][A
 22%|██▏       | 233/1083 [00:05<00:20, 42.44it/s][A
 22%|██▏       | 238/1083 [00:05<00:19, 43.36it/s][A
 22%|██▏       | 243/1083 [00:05<00:19, 44.02it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.52it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 44.91it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.06it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 44.77it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 44.69it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.55it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 44.67it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.87it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.00it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.20it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.34it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.37it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.23it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 44.99it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.74it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.83it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.85it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.05it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.19it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.36it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.43it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 45.33it/s][A
 33%|███▎      | 358/1083 [00:08<00:18, 40.14it/s][A
 34%|███▎      | 363/1083 [00:08<00:17, 41.68it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 42.78it/s][A
 34%|███▍      | 373/1083 [00:08<00:16, 43.52it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.13it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.50it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.87it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.93it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 44.65it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.45it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.54it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 44.78it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 45.01it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 45.17it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 45.34it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.40it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.29it/s][A
 41%|████      | 443/1083 [00:09<00:14, 45.08it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 44.70it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.70it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 44.82it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.04it/s][A
 43%|████▎     | 468/1083 [00:10<00:14, 42.01it/s][A
 44%|████▎     | 473/1083 [00:10<00:19, 31.35it/s][A
 44%|████▍     | 478/1083 [00:10<00:17, 35.30it/s][A
 45%|████▍     | 483/1083 [00:11<00:15, 37.70it/s][A
 45%|████▌     | 488/1083 [00:11<00:16, 37.03it/s][A
 46%|████▌     | 493/1083 [00:11<00:15, 39.17it/s][A
 46%|████▌     | 498/1083 [00:11<00:14, 40.90it/s][A
 46%|████▋     | 503/1083 [00:11<00:13, 42.25it/s][A
 47%|████▋     | 508/1083 [00:11<00:13, 43.18it/s][A
 47%|████▋     | 513/1083 [00:11<00:13, 43.48it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 43.75it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 44.14it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 44.25it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 44.40it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.71it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.82it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 45.04it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 45.19it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 45.17it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 45.12it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.98it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 44.92it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 44.90it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.97it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.99it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 45.14it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.20it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.14it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.09it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 44.94it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 44.86it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 42.96it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 43.76it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.27it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.51it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.77it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.89it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.79it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 44.79it/s][A
 61%|██████    | 663/1083 [00:15<00:09, 44.54it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 44.62it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 44.80it/s][A
 63%|██████▎   | 678/1083 [00:15<00:08, 45.06it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 45.17it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 45.13it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 45.13it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 45.02it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.88it/s][A
 65%|██████▌   | 708/1083 [00:16<00:08, 44.74it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.73it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.84it/s][A
 67%|██████▋   | 723/1083 [00:16<00:07, 45.02it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.14it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.32it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.26it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.09it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.99it/s][A
 70%|██████▉   | 753/1083 [00:17<00:07, 44.76it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 43.87it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.28it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.60it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.82it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.97it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.95it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.06it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 44.96it/s][A
 74%|███████▎  | 798/1083 [00:18<00:06, 44.84it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 44.82it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 44.92it/s][A
 75%|███████▌  | 813/1083 [00:18<00:05, 45.04it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 45.01it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 45.02it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 45.18it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.11it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.95it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 44.80it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.77it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.85it/s][A
 79%|███████▉  | 858/1083 [00:19<00:04, 45.02it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.08it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 45.17it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 45.15it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.13it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.99it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 44.88it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 43.51it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.03it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.45it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.72it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.87it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 45.01it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.06it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.03it/s][A
 86%|████████▌ | 933/1083 [00:21<00:03, 44.78it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 44.80it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 40.23it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 42.33it/s][A
 88%|████████▊ | 953/1083 [00:21<00:03, 43.18it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 43.89it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.34it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.67it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.66it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 44.65it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 44.65it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.46it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 44.55it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 44.78it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.99it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 45.11it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 45.24it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 45.15it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 45.10it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 43.21it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 43.71it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 43.97it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.33it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.63it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.85it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 45.05it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 45.06it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 44.82it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 44.74it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 44.75it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 44.77it/s][A                                                 
                                                   [A 40%|████      | 158/395 [01:45<01:07,  3.51it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.77it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 12:38:16,111 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-28 12:38:16,286 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:38:20,022 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:38:20,328 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:38:20,450 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [01:58<44:00, 11.19s/it] 41%|████      | 160/395 [01:58<31:03,  7.93s/it] 41%|████      | 161/395 [01:58<21:59,  5.64s/it] 41%|████      | 162/395 [01:59<15:40,  4.03s/it] 41%|████▏     | 163/395 [01:59<11:15,  2.91s/it] 42%|████▏     | 164/395 [01:59<08:11,  2.13s/it] 42%|████▏     | 165/395 [01:59<06:02,  1.58s/it] 42%|████▏     | 166/395 [02:00<04:32,  1.19s/it] 42%|████▏     | 167/395 [02:00<03:30,  1.08it/s] 43%|████▎     | 168/395 [02:00<02:46,  1.36it/s] 43%|████▎     | 169/395 [02:01<02:15,  1.66it/s] 43%|████▎     | 170/395 [02:01<01:54,  1.97it/s] 43%|████▎     | 171/395 [02:01<01:41,  2.21it/s] 44%|████▎     | 172/395 [02:02<01:30,  2.47it/s] 44%|████▍     | 173/395 [02:02<01:22,  2.69it/s] 44%|████▍     | 174/395 [02:02<01:16,  2.87it/s] 44%|████▍     | 175/395 [02:02<01:13,  3.01it/s] 45%|████▍     | 176/395 [02:03<01:10,  3.12it/s] 45%|████▍     | 177/395 [02:03<01:07,  3.21it/s] 45%|████▌     | 178/395 [02:03<01:06,  3.27it/s] 45%|████▌     | 179/395 [02:04<01:04,  3.32it/s] 46%|████▌     | 180/395 [02:04<01:03,  3.36it/s] 46%|████▌     | 181/395 [02:04<01:03,  3.39it/s] 46%|████▌     | 182/395 [02:04<01:03,  3.34it/s] 46%|████▋     | 183/395 [02:05<01:02,  3.38it/s] 47%|████▋     | 184/395 [02:05<01:02,  3.40it/s] 47%|████▋     | 185/395 [02:05<01:01,  3.42it/s] 47%|████▋     | 186/395 [02:06<01:00,  3.43it/s] 47%|████▋     | 187/395 [02:06<01:00,  3.44it/s] 48%|████▊     | 188/395 [02:06<01:00,  3.44it/s] 48%|████▊     | 189/395 [02:06<00:59,  3.45it/s] 48%|████▊     | 190/395 [02:07<00:59,  3.45it/s] 48%|████▊     | 191/395 [02:07<00:59,  3.45it/s] 49%|████▊     | 192/395 [02:07<00:58,  3.45it/s] 49%|████▉     | 193/395 [02:08<01:00,  3.35it/s] 49%|████▉     | 194/395 [02:08<00:59,  3.39it/s] 49%|████▉     | 195/395 [02:08<00:58,  3.41it/s] 50%|████▉     | 196/395 [02:09<00:58,  3.42it/s] 50%|████▉     | 197/395 [02:09<00:57,  3.43it/s] 50%|█████     | 198/395 [02:09<00:57,  3.44it/s] 50%|█████     | 199/395 [02:09<00:56,  3.45it/s] 51%|█████     | 200/395 [02:10<00:56,  3.45it/s] 51%|█████     | 201/395 [02:10<00:56,  3.45it/s] 51%|█████     | 202/395 [02:10<00:55,  3.46it/s] 51%|█████▏    | 203/395 [02:11<00:55,  3.45it/s] 52%|█████▏    | 204/395 [02:11<00:57,  3.34it/s] 52%|█████▏    | 205/395 [02:11<00:56,  3.37it/s] 52%|█████▏    | 206/395 [02:11<00:55,  3.40it/s] 52%|█████▏    | 207/395 [02:12<00:54,  3.42it/s] 53%|█████▎    | 208/395 [02:12<00:54,  3.43it/s] 53%|█████▎    | 209/395 [02:12<00:54,  3.44it/s] 53%|█████▎    | 210/395 [02:13<00:53,  3.44it/s] 53%|█████▎    | 211/395 [02:13<00:53,  3.45it/s] 54%|█████▎    | 212/395 [02:13<00:53,  3.45it/s] 54%|█████▍    | 213/395 [02:13<00:52,  3.45it/s] 54%|█████▍    | 214/395 [02:14<00:52,  3.46it/s] 54%|█████▍    | 215/395 [02:14<00:53,  3.38it/s] 55%|█████▍    | 216/395 [02:14<00:52,  3.40it/s] 55%|█████▍    | 217/395 [02:15<00:52,  3.42it/s] 55%|█████▌    | 218/395 [02:15<00:51,  3.43it/s] 55%|█████▌    | 219/395 [02:15<00:51,  3.44it/s] 56%|█████▌    | 220/395 [02:16<00:50,  3.44it/s] 56%|█████▌    | 221/395 [02:16<00:50,  3.45it/s] 56%|█████▌    | 222/395 [02:16<00:50,  3.45it/s] 56%|█████▋    | 223/395 [02:16<00:49,  3.45it/s] 57%|█████▋    | 224/395 [02:17<00:50,  3.40it/s] 57%|█████▋    | 225/395 [02:17<00:49,  3.42it/s] 57%|█████▋    | 226/395 [02:17<00:50,  3.35it/s] 57%|█████▋    | 227/395 [02:18<00:49,  3.38it/s] 58%|█████▊    | 228/395 [02:18<00:49,  3.40it/s] 58%|█████▊    | 229/395 [02:18<00:48,  3.42it/s] 58%|█████▊    | 230/395 [02:18<00:48,  3.43it/s] 58%|█████▊    | 231/395 [02:19<00:47,  3.44it/s] 59%|█████▊    | 232/395 [02:19<00:47,  3.44it/s] 59%|█████▉    | 233/395 [02:19<00:46,  3.45it/s] 59%|█████▉    | 234/395 [02:20<00:46,  3.45it/s] 59%|█████▉    | 235/395 [02:20<00:46,  3.45it/s] 60%|█████▉    | 236/395 [02:20<00:46,  3.45it/s] 60%|██████    | 237/395 [02:20<00:43,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 12:38:50,903 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:38:50,903 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 12:38:50,903 >>   Batch size = 8
{'eval_loss': 0.8808199167251587, 'eval_runtime': 24.4608, 'eval_samples_per_second': 353.954, 'eval_steps_per_second': 44.275, 'epoch': 2.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.49it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.46it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.42it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.49it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.01it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.50it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.26it/s][A
  4%|▍         | 43/1083 [00:00<00:26, 39.61it/s][A
  4%|▍         | 48/1083 [00:01<00:25, 41.31it/s][A
  5%|▍         | 53/1083 [00:01<00:24, 42.54it/s][A
  5%|▌         | 58/1083 [00:01<00:23, 43.40it/s][A
  6%|▌         | 63/1083 [00:01<00:23, 44.13it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 44.58it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 44.93it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 44.95it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.66it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.49it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.48it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 44.80it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 44.98it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.20it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.31it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.39it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.30it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 45.02it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.74it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.69it/s][A
 13%|█▎        | 143/1083 [00:03<00:20, 44.97it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 45.05it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 45.19it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.41it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.36it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.25it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 45.09it/s][A
 16%|█▋        | 178/1083 [00:04<00:22, 39.94it/s][A
 17%|█▋        | 183/1083 [00:04<00:21, 41.46it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 42.63it/s][A
 18%|█▊        | 193/1083 [00:04<00:20, 43.51it/s][A
 18%|█▊        | 198/1083 [00:04<00:20, 44.10it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 44.50it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 44.79it/s][A
 20%|█▉        | 213/1083 [00:04<00:21, 40.69it/s][A
 20%|██        | 218/1083 [00:04<00:20, 42.04it/s][A
 21%|██        | 223/1083 [00:05<00:20, 42.86it/s][A
 21%|██        | 228/1083 [00:05<00:19, 43.51it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.09it/s][A
 22%|██▏       | 238/1083 [00:05<00:19, 44.47it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.69it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.87it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 44.86it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.71it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 44.81it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 44.89it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.95it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 45.01it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 45.10it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.16it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.02it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 44.83it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.91it/s][A
 29%|██▉       | 313/1083 [00:07<00:18, 41.72it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 42.86it/s][A
 30%|██▉       | 323/1083 [00:07<00:17, 43.64it/s][A
 30%|███       | 328/1083 [00:07<00:17, 44.15it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.58it/s][A
 31%|███       | 338/1083 [00:07<00:16, 44.74it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 44.79it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 44.73it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.42it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 44.53it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.66it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 45.00it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 45.14it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 45.27it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.36it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.23it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.03it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 44.75it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.75it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.88it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 45.04it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 45.12it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 45.24it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 45.28it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.31it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.02it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.81it/s][A
 41%|████▏     | 448/1083 [00:10<00:15, 41.96it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 42.91it/s][A
 42%|████▏     | 458/1083 [00:10<00:14, 43.63it/s][A
 43%|████▎     | 463/1083 [00:10<00:14, 44.12it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.49it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 44.70it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.00it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.07it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.76it/s][A
 46%|████▌     | 493/1083 [00:11<00:14, 41.67it/s][A
 46%|████▌     | 498/1083 [00:11<00:19, 30.13it/s][A
 47%|████▋     | 504/1083 [00:11<00:16, 34.76it/s][A
 47%|████▋     | 509/1083 [00:11<00:15, 37.29it/s][A
 47%|████▋     | 514/1083 [00:11<00:14, 39.39it/s][A
 48%|████▊     | 519/1083 [00:11<00:13, 41.05it/s][A
 48%|████▊     | 524/1083 [00:11<00:13, 42.27it/s][A
 49%|████▉     | 529/1083 [00:12<00:12, 43.11it/s][A
 49%|████▉     | 534/1083 [00:12<00:12, 43.72it/s][A
 50%|████▉     | 539/1083 [00:12<00:12, 43.88it/s][A
 50%|█████     | 544/1083 [00:12<00:12, 43.99it/s][A
 51%|█████     | 549/1083 [00:12<00:12, 44.29it/s][A
 51%|█████     | 554/1083 [00:12<00:11, 44.65it/s][A
 52%|█████▏    | 559/1083 [00:12<00:11, 44.82it/s][A
 52%|█████▏    | 564/1083 [00:12<00:11, 45.09it/s][A
 53%|█████▎    | 569/1083 [00:12<00:11, 45.15it/s][A
 53%|█████▎    | 574/1083 [00:13<00:11, 45.15it/s][A
 53%|█████▎    | 579/1083 [00:13<00:12, 41.75it/s][A
 54%|█████▍    | 584/1083 [00:13<00:11, 42.87it/s][A
 54%|█████▍    | 589/1083 [00:13<00:11, 43.51it/s][A
 55%|█████▍    | 594/1083 [00:13<00:11, 44.07it/s][A
 55%|█████▌    | 599/1083 [00:13<00:10, 44.37it/s][A
 56%|█████▌    | 604/1083 [00:13<00:10, 44.69it/s][A
 56%|█████▌    | 609/1083 [00:13<00:10, 44.87it/s][A
 57%|█████▋    | 614/1083 [00:13<00:10, 45.07it/s][A
 57%|█████▋    | 619/1083 [00:14<00:10, 44.73it/s][A
 58%|█████▊    | 624/1083 [00:14<00:10, 44.82it/s][A
 58%|█████▊    | 629/1083 [00:14<00:10, 44.84it/s][A
 59%|█████▊    | 634/1083 [00:14<00:09, 45.03it/s][A
 59%|█████▉    | 639/1083 [00:14<00:09, 45.11it/s][A
 59%|█████▉    | 644/1083 [00:14<00:09, 45.12it/s][A
 60%|█████▉    | 649/1083 [00:14<00:09, 45.21it/s][A
 60%|██████    | 654/1083 [00:14<00:09, 45.16it/s][A
 61%|██████    | 659/1083 [00:14<00:09, 45.07it/s][A
 61%|██████▏   | 664/1083 [00:15<00:09, 44.90it/s][A
 62%|██████▏   | 669/1083 [00:15<00:09, 44.94it/s][A
 62%|██████▏   | 674/1083 [00:15<00:09, 44.80it/s][A
 63%|██████▎   | 679/1083 [00:15<00:08, 44.98it/s][A
 63%|██████▎   | 684/1083 [00:15<00:08, 45.07it/s][A
 64%|██████▎   | 689/1083 [00:15<00:08, 45.17it/s][A
 64%|██████▍   | 694/1083 [00:15<00:08, 45.24it/s][A
 65%|██████▍   | 699/1083 [00:15<00:08, 45.14it/s][A
 65%|██████▌   | 704/1083 [00:15<00:08, 45.04it/s][A
 65%|██████▌   | 709/1083 [00:16<00:08, 44.93it/s][A
 66%|██████▌   | 714/1083 [00:16<00:08, 42.57it/s][A
 66%|██████▋   | 719/1083 [00:16<00:08, 43.35it/s][A
 67%|██████▋   | 724/1083 [00:16<00:08, 44.02it/s][A
 67%|██████▋   | 729/1083 [00:16<00:07, 44.42it/s][A
 68%|██████▊   | 734/1083 [00:16<00:07, 44.74it/s][A
 68%|██████▊   | 739/1083 [00:16<00:07, 44.88it/s][A
 69%|██████▊   | 744/1083 [00:16<00:07, 44.93it/s][A
 69%|██████▉   | 749/1083 [00:16<00:07, 44.97it/s][A
 70%|██████▉   | 754/1083 [00:17<00:07, 44.67it/s][A
 70%|███████   | 759/1083 [00:17<00:07, 44.75it/s][A
 71%|███████   | 764/1083 [00:17<00:07, 44.83it/s][A
 71%|███████   | 769/1083 [00:17<00:06, 45.01it/s][A
 71%|███████▏  | 774/1083 [00:17<00:06, 45.10it/s][A
 72%|███████▏  | 779/1083 [00:17<00:06, 45.30it/s][A
 72%|███████▏  | 784/1083 [00:17<00:06, 45.16it/s][A
 73%|███████▎  | 789/1083 [00:17<00:06, 45.13it/s][A
 73%|███████▎  | 794/1083 [00:17<00:06, 45.03it/s][A
 74%|███████▍  | 799/1083 [00:18<00:06, 44.70it/s][A
 74%|███████▍  | 804/1083 [00:18<00:06, 44.76it/s][A
 75%|███████▍  | 809/1083 [00:18<00:06, 44.73it/s][A
 75%|███████▌  | 814/1083 [00:18<00:05, 44.92it/s][A
 76%|███████▌  | 819/1083 [00:18<00:05, 45.14it/s][A
 76%|███████▌  | 824/1083 [00:18<00:05, 45.20it/s][A
 77%|███████▋  | 829/1083 [00:18<00:05, 45.25it/s][A
 77%|███████▋  | 834/1083 [00:18<00:05, 45.18it/s][A
 77%|███████▋  | 839/1083 [00:18<00:05, 44.95it/s][A
 78%|███████▊  | 844/1083 [00:19<00:05, 44.88it/s][A
 78%|███████▊  | 849/1083 [00:19<00:05, 44.55it/s][A
 79%|███████▉  | 854/1083 [00:19<00:05, 44.73it/s][A
 79%|███████▉  | 859/1083 [00:19<00:04, 44.96it/s][A
 80%|███████▉  | 864/1083 [00:19<00:04, 44.97it/s][A
 80%|████████  | 869/1083 [00:19<00:04, 45.10it/s][A
 81%|████████  | 874/1083 [00:19<00:04, 45.22it/s][A
 81%|████████  | 879/1083 [00:19<00:04, 45.19it/s][A
 82%|████████▏ | 884/1083 [00:19<00:04, 45.06it/s][A
 82%|████████▏ | 889/1083 [00:20<00:04, 44.97it/s][A
 83%|████████▎ | 894/1083 [00:20<00:04, 44.94it/s][A
 83%|████████▎ | 899/1083 [00:20<00:04, 44.96it/s][A
 83%|████████▎ | 904/1083 [00:20<00:03, 45.05it/s][A
 84%|████████▍ | 909/1083 [00:20<00:03, 45.02it/s][A
 84%|████████▍ | 914/1083 [00:20<00:03, 45.14it/s][A
 85%|████████▍ | 919/1083 [00:20<00:03, 45.10it/s][A
 85%|████████▌ | 924/1083 [00:20<00:03, 45.09it/s][A
 86%|████████▌ | 929/1083 [00:20<00:03, 44.95it/s][A
 86%|████████▌ | 934/1083 [00:21<00:03, 44.95it/s][A
 87%|████████▋ | 939/1083 [00:21<00:03, 44.94it/s][A
 87%|████████▋ | 944/1083 [00:21<00:03, 44.93it/s][A
 88%|████████▊ | 949/1083 [00:21<00:02, 44.97it/s][A
 88%|████████▊ | 954/1083 [00:21<00:02, 45.11it/s][A
 89%|████████▊ | 959/1083 [00:21<00:02, 45.15it/s][A
 89%|████████▉ | 964/1083 [00:21<00:02, 45.10it/s][A
 89%|████████▉ | 969/1083 [00:21<00:02, 42.46it/s][A
 90%|████████▉ | 974/1083 [00:22<00:02, 43.90it/s][A
 90%|█████████ | 979/1083 [00:22<00:02, 44.33it/s][A
 91%|█████████ | 984/1083 [00:22<00:02, 43.18it/s][A
 91%|█████████▏| 989/1083 [00:22<00:02, 43.84it/s][A
 92%|█████████▏| 994/1083 [00:22<00:02, 44.16it/s][A
 92%|█████████▏| 999/1083 [00:22<00:01, 44.52it/s][A
 93%|█████████▎| 1004/1083 [00:22<00:01, 44.69it/s][A
 93%|█████████▎| 1009/1083 [00:22<00:01, 44.77it/s][A
 94%|█████████▎| 1014/1083 [00:22<00:01, 44.76it/s][A
 94%|█████████▍| 1019/1083 [00:23<00:01, 44.81it/s][A
 95%|█████████▍| 1024/1083 [00:23<00:01, 44.79it/s][A
 95%|█████████▌| 1029/1083 [00:23<00:01, 44.78it/s][A
 95%|█████████▌| 1034/1083 [00:23<00:01, 44.93it/s][A
 96%|█████████▌| 1039/1083 [00:23<00:00, 45.06it/s][A
 96%|█████████▋| 1044/1083 [00:23<00:00, 45.03it/s][A
 97%|█████████▋| 1049/1083 [00:23<00:00, 45.23it/s][A
 97%|█████████▋| 1054/1083 [00:23<00:00, 45.10it/s][A
 98%|█████████▊| 1059/1083 [00:23<00:00, 44.95it/s][A
 98%|█████████▊| 1064/1083 [00:24<00:00, 44.92it/s][A
 99%|█████████▊| 1069/1083 [00:24<00:00, 44.90it/s][A
 99%|█████████▉| 1074/1083 [00:24<00:00, 44.85it/s][A
100%|█████████▉| 1079/1083 [00:24<00:00, 44.97it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 44.97it/s][A 60%|██████    | 237/395 [02:45<00:43,  3.59it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 12:39:15,462 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-28 12:39:15,666 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:39:19,070 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:39:19,352 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:39:19,485 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [02:57<29:16, 11.19s/it] 61%|██████    | 239/395 [02:57<20:35,  7.92s/it] 61%|██████    | 240/395 [02:58<14:32,  5.63s/it] 61%|██████    | 241/395 [02:58<10:20,  4.03s/it] 61%|██████▏   | 242/395 [02:58<07:24,  2.91s/it] 62%|██████▏   | 243/395 [02:59<05:22,  2.12s/it] 62%|██████▏   | 244/395 [02:59<03:57,  1.57s/it] 62%|██████▏   | 245/395 [02:59<02:58,  1.19s/it] 62%|██████▏   | 246/395 [02:59<02:17,  1.09it/s] 63%|██████▎   | 247/395 [03:00<01:48,  1.37it/s] 63%|██████▎   | 248/395 [03:00<01:28,  1.67it/s] 63%|██████▎   | 249/395 [03:00<01:14,  1.97it/s] 63%|██████▎   | 250/395 [03:01<01:04,  2.25it/s] 64%|██████▎   | 251/395 [03:01<00:57,  2.51it/s] 64%|██████▍   | 252/395 [03:01<00:52,  2.73it/s] 64%|██████▍   | 253/395 [03:02<00:50,  2.83it/s] 64%|██████▍   | 254/395 [03:02<00:47,  2.98it/s] 65%|██████▍   | 255/395 [03:02<00:45,  3.10it/s] 65%|██████▍   | 256/395 [03:02<00:43,  3.19it/s] 65%|██████▌   | 257/395 [03:03<00:42,  3.25it/s] 65%|██████▌   | 258/395 [03:03<00:41,  3.30it/s] 66%|██████▌   | 259/395 [03:03<00:40,  3.33it/s] 66%|██████▌   | 260/395 [03:04<00:40,  3.35it/s] 66%|██████▌   | 261/395 [03:04<00:39,  3.37it/s] 66%|██████▋   | 262/395 [03:04<00:39,  3.38it/s] 67%|██████▋   | 263/395 [03:04<00:38,  3.39it/s] 67%|██████▋   | 264/395 [03:05<00:39,  3.33it/s] 67%|██████▋   | 265/395 [03:05<00:38,  3.35it/s] 67%|██████▋   | 266/395 [03:05<00:38,  3.37it/s] 68%|██████▊   | 267/395 [03:06<00:37,  3.38it/s] 68%|██████▊   | 268/395 [03:06<00:37,  3.39it/s] 68%|██████▊   | 269/395 [03:06<00:37,  3.39it/s] 68%|██████▊   | 270/395 [03:07<00:36,  3.40it/s] 69%|██████▊   | 271/395 [03:07<00:36,  3.40it/s] 69%|██████▉   | 272/395 [03:07<00:36,  3.40it/s] 69%|██████▉   | 273/395 [03:07<00:35,  3.40it/s] 69%|██████▉   | 274/395 [03:08<00:35,  3.40it/s] 70%|██████▉   | 275/395 [03:08<00:36,  3.30it/s] 70%|██████▉   | 276/395 [03:08<00:35,  3.33it/s] 70%|███████   | 277/395 [03:09<00:35,  3.35it/s] 70%|███████   | 278/395 [03:09<00:34,  3.37it/s] 71%|███████   | 279/395 [03:09<00:34,  3.38it/s] 71%|███████   | 280/395 [03:09<00:33,  3.39it/s] 71%|███████   | 281/395 [03:10<00:33,  3.40it/s] 71%|███████▏  | 282/395 [03:10<00:33,  3.40it/s] 72%|███████▏  | 283/395 [03:10<00:32,  3.40it/s] 72%|███████▏  | 284/395 [03:11<00:32,  3.40it/s] 72%|███████▏  | 285/395 [03:11<00:32,  3.40it/s] 72%|███████▏  | 286/395 [03:11<00:32,  3.32it/s] 73%|███████▎  | 287/395 [03:12<00:32,  3.34it/s] 73%|███████▎  | 288/395 [03:12<00:31,  3.37it/s] 73%|███████▎  | 289/395 [03:12<00:31,  3.38it/s] 73%|███████▎  | 290/395 [03:12<00:31,  3.38it/s] 74%|███████▎  | 291/395 [03:13<00:30,  3.39it/s] 74%|███████▍  | 292/395 [03:13<00:30,  3.40it/s] 74%|███████▍  | 293/395 [03:13<00:30,  3.40it/s] 74%|███████▍  | 294/395 [03:14<00:29,  3.40it/s] 75%|███████▍  | 295/395 [03:14<00:29,  3.41it/s] 75%|███████▍  | 296/395 [03:14<00:29,  3.41it/s] 75%|███████▌  | 297/395 [03:15<00:29,  3.33it/s] 75%|███████▌  | 298/395 [03:15<00:28,  3.36it/s] 76%|███████▌  | 299/395 [03:15<00:28,  3.37it/s] 76%|███████▌  | 300/395 [03:15<00:28,  3.38it/s] 76%|███████▌  | 301/395 [03:16<00:27,  3.39it/s] 76%|███████▋  | 302/395 [03:16<00:27,  3.39it/s] 77%|███████▋  | 303/395 [03:16<00:27,  3.40it/s] 77%|███████▋  | 304/395 [03:17<00:26,  3.40it/s] 77%|███████▋  | 305/395 [03:17<00:27,  3.31it/s] 77%|███████▋  | 306/395 [03:17<00:26,  3.34it/s] 78%|███████▊  | 307/395 [03:17<00:26,  3.36it/s] 78%|███████▊  | 308/395 [03:18<00:26,  3.28it/s] 78%|███████▊  | 309/395 [03:18<00:25,  3.32it/s] 78%|███████▊  | 310/395 [03:18<00:25,  3.35it/s] 79%|███████▊  | 311/395 [03:19<00:24,  3.38it/s] 79%|███████▉  | 312/395 [03:19<00:24,  3.40it/s] 79%|███████▉  | 313/395 [03:19<00:23,  3.42it/s] 79%|███████▉  | 314/395 [03:20<00:23,  3.43it/s] 80%|███████▉  | 315/395 [03:20<00:23,  3.44it/s] 80%|████████  | 316/395 [03:20<00:22,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 12:39:50,535 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:39:50,535 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 12:39:50,536 >>   Batch size = 8
{'eval_loss': 0.8918523788452148, 'eval_runtime': 24.447, 'eval_samples_per_second': 354.154, 'eval_steps_per_second': 44.3, 'epoch': 3.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.52it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.22it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.82it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.65it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.97it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.53it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.31it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.16it/s][A
  4%|▍         | 47/1083 [00:01<00:22, 45.14it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.34it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.44it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.36it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.27it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.16it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.03it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.88it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.89it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.93it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.16it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.16it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.29it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.25it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.07it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.03it/s][A
 12%|█▏        | 127/1083 [00:02<00:22, 42.23it/s][A
 12%|█▏        | 132/1083 [00:02<00:22, 43.19it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 43.96it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 44.43it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.75it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.89it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.91it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.86it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.57it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.51it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.80it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.92it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 45.15it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.21it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.33it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.29it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.12it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.94it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.83it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.81it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.97it/s][A
 21%|██▏       | 232/1083 [00:05<00:20, 41.15it/s][A
 22%|██▏       | 238/1083 [00:05<00:19, 43.92it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.35it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.71it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 44.91it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.87it/s][A
 24%|██▍       | 263/1083 [00:05<00:20, 40.04it/s][A
 25%|██▍       | 268/1083 [00:06<00:19, 41.59it/s][A
 25%|██▌       | 273/1083 [00:06<00:19, 42.62it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 43.40it/s][A
 26%|██▌       | 283/1083 [00:06<00:18, 44.06it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.50it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.89it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.90it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 44.60it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.43it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 44.57it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.66it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.02it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.17it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.33it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.39it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.27it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 44.92it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.75it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 44.70it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.79it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 45.00it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 45.15it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 45.23it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.24it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.28it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.23it/s][A
 37%|███▋      | 398/1083 [00:08<00:16, 40.96it/s][A
 37%|███▋      | 403/1083 [00:09<00:16, 42.15it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 43.12it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 43.78it/s][A
 39%|███▊      | 418/1083 [00:09<00:15, 44.28it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.63it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.81it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.93it/s][A
 40%|████      | 438/1083 [00:09<00:14, 44.68it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.64it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 44.79it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.91it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.09it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.20it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.23it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.20it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.15it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 44.87it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.87it/s][A
 46%|████▌     | 493/1083 [00:11<00:13, 44.88it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.99it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 45.09it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 45.26it/s][A
 47%|████▋     | 513/1083 [00:11<00:13, 43.71it/s][A
 48%|████▊     | 518/1083 [00:11<00:14, 38.69it/s][A
 48%|████▊     | 522/1083 [00:11<00:14, 37.46it/s][A
 49%|████▉     | 528/1083 [00:11<00:13, 41.03it/s][A
 49%|████▉     | 533/1083 [00:12<00:13, 41.37it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 42.52it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 43.41it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 44.07it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.47it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.59it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.44it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.40it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 44.49it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 44.66it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.79it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.95it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 45.04it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.18it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.23it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.20it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.06it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 44.94it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.00it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 45.03it/s][A
 58%|█████▊    | 633/1083 [00:14<00:09, 45.10it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 45.03it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 45.20it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 45.21it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.08it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.05it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 44.99it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 41.53it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 42.70it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 43.53it/s][A
 63%|██████▎   | 683/1083 [00:15<00:09, 44.13it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.57it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.83it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.80it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.87it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.61it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.57it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.69it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.82it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.03it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.10it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.27it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.26it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.07it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.70it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.84it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.85it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.93it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 45.11it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.17it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.34it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.19it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.19it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.93it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 43.38it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 43.83it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 44.21it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.61it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.92it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 45.02it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.09it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 45.10it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.85it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.81it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.98it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.99it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.15it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 45.20it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 45.20it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.23it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 45.05it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.93it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.85it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.92it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.98it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 45.13it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 45.22it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 45.25it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.31it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.07it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 44.97it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 44.50it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 44.70it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 44.89it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 45.01it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 45.13it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 45.19it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 45.25it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.91it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 45.16it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 44.96it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.95it/s][A
 92%|█████████▏| 993/1083 [00:22<00:02, 42.49it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 43.58it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.18it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.59it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 44.76it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 44.80it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 44.80it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 44.68it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.72it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.66it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.70it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.92it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 45.10it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 45.29it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 45.18it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 45.10it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 42.47it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 43.29it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 43.87it/s][A                                                 
                                                   [A 80%|████████  | 316/395 [03:44<00:22,  3.58it/s]
100%|██████████| 1083/1083 [00:24<00:00, 43.87it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 12:40:14,970 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-28 12:40:15,260 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:40:18,687 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:40:18,907 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:40:19,051 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [03:57<14:45, 11.35s/it] 81%|████████  | 318/395 [03:58<10:19,  8.04s/it] 81%|████████  | 319/395 [03:58<07:14,  5.72s/it] 81%|████████  | 320/395 [03:58<05:06,  4.09s/it] 81%|████████▏ | 321/395 [03:58<03:38,  2.95s/it] 82%|████████▏ | 322/395 [03:59<02:37,  2.15s/it] 82%|████████▏ | 323/395 [03:59<01:54,  1.60s/it] 82%|████████▏ | 324/395 [03:59<01:25,  1.20s/it] 82%|████████▏ | 325/395 [04:00<01:05,  1.07it/s] 83%|████████▎ | 326/395 [04:00<00:51,  1.35it/s] 83%|████████▎ | 327/395 [04:00<00:41,  1.65it/s] 83%|████████▎ | 328/395 [04:01<00:34,  1.95it/s] 83%|████████▎ | 329/395 [04:01<00:29,  2.21it/s] 84%|████████▎ | 330/395 [04:01<00:26,  2.47it/s] 84%|████████▍ | 331/395 [04:01<00:23,  2.69it/s] 84%|████████▍ | 332/395 [04:02<00:21,  2.87it/s] 84%|████████▍ | 333/395 [04:02<00:20,  3.02it/s] 85%|████████▍ | 334/395 [04:02<00:19,  3.13it/s] 85%|████████▍ | 335/395 [04:03<00:18,  3.21it/s] 85%|████████▌ | 336/395 [04:03<00:18,  3.26it/s] 85%|████████▌ | 337/395 [04:03<00:17,  3.31it/s] 86%|████████▌ | 338/395 [04:03<00:17,  3.34it/s] 86%|████████▌ | 339/395 [04:04<00:16,  3.36it/s] 86%|████████▌ | 340/395 [04:04<00:16,  3.27it/s] 86%|████████▋ | 341/395 [04:04<00:16,  3.31it/s] 87%|████████▋ | 342/395 [04:05<00:15,  3.35it/s] 87%|████████▋ | 343/395 [04:05<00:15,  3.36it/s] 87%|████████▋ | 344/395 [04:05<00:15,  3.38it/s] 87%|████████▋ | 345/395 [04:06<00:14,  3.39it/s] 88%|████████▊ | 346/395 [04:06<00:14,  3.39it/s] 88%|████████▊ | 347/395 [04:06<00:14,  3.40it/s] 88%|████████▊ | 348/395 [04:06<00:13,  3.40it/s] 88%|████████▊ | 349/395 [04:07<00:13,  3.41it/s] 89%|████████▊ | 350/395 [04:07<00:13,  3.41it/s] 89%|████████▉ | 351/395 [04:07<00:13,  3.29it/s] 89%|████████▉ | 352/395 [04:08<00:12,  3.32it/s] 89%|████████▉ | 353/395 [04:08<00:12,  3.35it/s] 90%|████████▉ | 354/395 [04:08<00:12,  3.37it/s] 90%|████████▉ | 355/395 [04:09<00:11,  3.38it/s] 90%|█████████ | 356/395 [04:09<00:11,  3.39it/s] 90%|█████████ | 357/395 [04:09<00:11,  3.39it/s] 91%|█████████ | 358/395 [04:09<00:10,  3.40it/s] 91%|█████████ | 359/395 [04:10<00:10,  3.40it/s] 91%|█████████ | 360/395 [04:10<00:10,  3.40it/s] 91%|█████████▏| 361/395 [04:10<00:09,  3.40it/s] 92%|█████████▏| 362/395 [04:11<00:10,  3.29it/s] 92%|█████████▏| 363/395 [04:11<00:09,  3.32it/s] 92%|█████████▏| 364/395 [04:11<00:09,  3.35it/s] 92%|█████████▏| 365/395 [04:11<00:08,  3.37it/s] 93%|█████████▎| 366/395 [04:12<00:08,  3.38it/s] 93%|█████████▎| 367/395 [04:12<00:08,  3.39it/s] 93%|█████████▎| 368/395 [04:12<00:07,  3.40it/s] 93%|█████████▎| 369/395 [04:13<00:07,  3.42it/s] 94%|█████████▎| 370/395 [04:13<00:07,  3.43it/s] 94%|█████████▍| 371/395 [04:13<00:06,  3.44it/s] 94%|█████████▍| 372/395 [04:14<00:06,  3.44it/s] 94%|█████████▍| 373/395 [04:14<00:06,  3.37it/s] 95%|█████████▍| 374/395 [04:14<00:06,  3.39it/s] 95%|█████████▍| 375/395 [04:14<00:05,  3.41it/s] 95%|█████████▌| 376/395 [04:15<00:05,  3.43it/s] 95%|█████████▌| 377/395 [04:15<00:05,  3.44it/s] 96%|█████████▌| 378/395 [04:15<00:04,  3.44it/s] 96%|█████████▌| 379/395 [04:16<00:04,  3.45it/s] 96%|█████████▌| 380/395 [04:16<00:04,  3.45it/s] 96%|█████████▋| 381/395 [04:16<00:04,  3.45it/s] 97%|█████████▋| 382/395 [04:16<00:03,  3.45it/s] 97%|█████████▋| 383/395 [04:17<00:03,  3.45it/s] 97%|█████████▋| 384/395 [04:17<00:03,  3.45it/s] 97%|█████████▋| 385/395 [04:17<00:02,  3.46it/s] 98%|█████████▊| 386/395 [04:18<00:02,  3.46it/s] 98%|█████████▊| 387/395 [04:18<00:02,  3.46it/s] 98%|█████████▊| 388/395 [04:18<00:02,  3.46it/s] 98%|█████████▊| 389/395 [04:18<00:01,  3.46it/s] 99%|█████████▊| 390/395 [04:19<00:01,  3.46it/s] 99%|█████████▉| 391/395 [04:19<00:01,  3.46it/s] 99%|█████████▉| 392/395 [04:19<00:00,  3.35it/s] 99%|█████████▉| 393/395 [04:20<00:00,  3.38it/s]100%|█████████▉| 394/395 [04:20<00:00,  3.40it/s]100%|██████████| 395/395 [04:20<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 12:40:50,652 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:40:50,652 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 12:40:50,652 >>   Batch size = 8
{'eval_loss': 0.896589457988739, 'eval_runtime': 24.3032, 'eval_samples_per_second': 356.249, 'eval_steps_per_second': 44.562, 'epoch': 4.0}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.53it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.51it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.46it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.59it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.06it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.63it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.33it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 45.14it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.11it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.15it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.27it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.33it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.29it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.15it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.08it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.93it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.91it/s][A
  9%|▊         | 93/1083 [00:02<00:25, 38.77it/s][A
  9%|▉         | 98/1083 [00:02<00:24, 40.68it/s][A
 10%|▉         | 103/1083 [00:02<00:23, 42.09it/s][A
 10%|▉         | 108/1083 [00:02<00:22, 43.11it/s][A
 10%|█         | 113/1083 [00:02<00:22, 43.79it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.32it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.78it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.99it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.63it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.40it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 44.49it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.75it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 45.01it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.05it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.19it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.39it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 45.32it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 45.08it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.75it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.69it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.82it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.00it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.07it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.27it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.41it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.36it/s][A
 21%|██        | 223/1083 [00:05<00:19, 45.19it/s][A
 21%|██        | 228/1083 [00:05<00:28, 29.66it/s][A
 22%|██▏       | 233/1083 [00:05<00:25, 33.14it/s][A
 22%|██▏       | 238/1083 [00:05<00:23, 36.11it/s][A
 22%|██▏       | 243/1083 [00:05<00:21, 38.52it/s][A
 23%|██▎       | 248/1083 [00:05<00:20, 40.37it/s][A
 23%|██▎       | 253/1083 [00:05<00:19, 41.84it/s][A
 24%|██▍       | 258/1083 [00:05<00:19, 42.83it/s][A
 24%|██▍       | 263/1083 [00:06<00:18, 43.57it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 43.74it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 43.94it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 44.18it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.51it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.67it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.12it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.15it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.24it/s][A
 28%|██▊       | 308/1083 [00:07<00:17, 45.12it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 44.91it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.81it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.72it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.80it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.98it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.13it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.31it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.38it/s][A
 33%|███▎      | 353/1083 [00:08<00:16, 45.26it/s][A
 33%|███▎      | 358/1083 [00:08<00:18, 38.24it/s][A
 34%|███▎      | 363/1083 [00:08<00:17, 40.24it/s][A
 34%|███▍      | 368/1083 [00:08<00:17, 41.67it/s][A
 34%|███▍      | 373/1083 [00:08<00:16, 42.85it/s][A
 35%|███▍      | 378/1083 [00:08<00:16, 43.67it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.27it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.65it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.82it/s][A
 37%|███▋      | 398/1083 [00:09<00:15, 44.57it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.28it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.43it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 44.63it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.79it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.97it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 45.16it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.17it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.25it/s][A
 41%|████      | 443/1083 [00:10<00:14, 45.09it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 45.00it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.99it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 44.98it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.98it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.07it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.19it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.15it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.12it/s][A
 45%|████▌     | 488/1083 [00:11<00:13, 44.99it/s][A
 46%|████▌     | 493/1083 [00:11<00:14, 41.32it/s][A
 46%|████▌     | 498/1083 [00:11<00:16, 35.90it/s][A
 46%|████▋     | 502/1083 [00:11<00:21, 26.45it/s][A
 47%|████▋     | 507/1083 [00:11<00:18, 30.77it/s][A
 47%|████▋     | 512/1083 [00:11<00:16, 34.22it/s][A
 48%|████▊     | 517/1083 [00:12<00:15, 37.03it/s][A
 48%|████▊     | 522/1083 [00:12<00:14, 39.28it/s][A
 49%|████▊     | 527/1083 [00:12<00:13, 40.94it/s][A
 49%|████▉     | 532/1083 [00:12<00:13, 42.20it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 43.02it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 43.79it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 43.81it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 44.03it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.46it/s][A
 52%|█████▏    | 562/1083 [00:13<00:11, 44.69it/s][A
 52%|█████▏    | 567/1083 [00:13<00:11, 44.87it/s][A
 53%|█████▎    | 572/1083 [00:13<00:11, 45.09it/s][A
 53%|█████▎    | 577/1083 [00:13<00:11, 45.18it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 45.21it/s][A
 54%|█████▍    | 587/1083 [00:13<00:10, 45.09it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.84it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.64it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.85it/s][A
 56%|█████▌    | 607/1083 [00:14<00:10, 45.01it/s][A
 57%|█████▋    | 612/1083 [00:14<00:10, 45.07it/s][A
 57%|█████▋    | 617/1083 [00:14<00:12, 38.69it/s][A
 57%|█████▋    | 622/1083 [00:14<00:11, 40.54it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 41.96it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 43.02it/s][A
 59%|█████▉    | 637/1083 [00:14<00:10, 43.79it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.33it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.64it/s][A
 60%|██████    | 652/1083 [00:15<00:09, 44.88it/s][A
 61%|██████    | 657/1083 [00:15<00:09, 44.60it/s][A
 61%|██████    | 662/1083 [00:15<00:09, 44.32it/s][A
 62%|██████▏   | 667/1083 [00:15<00:09, 44.50it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.62it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.88it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 45.03it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.24it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.29it/s][A
 64%|██████▍   | 697/1083 [00:16<00:08, 45.35it/s][A
 65%|██████▍   | 702/1083 [00:16<00:08, 45.07it/s][A
 65%|██████▌   | 707/1083 [00:16<00:08, 44.87it/s][A
 66%|██████▌   | 712/1083 [00:16<00:08, 44.68it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.73it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.85it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 45.05it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 45.09it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.20it/s][A
 69%|██████▊   | 742/1083 [00:17<00:07, 45.22it/s][A
 69%|██████▉   | 747/1083 [00:17<00:07, 45.26it/s][A
 69%|██████▉   | 752/1083 [00:17<00:07, 43.57it/s][A
 70%|██████▉   | 757/1083 [00:17<00:07, 44.02it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.25it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.56it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.78it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.89it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.06it/s][A
 73%|███████▎  | 787/1083 [00:18<00:06, 45.04it/s][A
 73%|███████▎  | 792/1083 [00:18<00:06, 44.85it/s][A
 74%|███████▎  | 797/1083 [00:18<00:06, 44.93it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 44.92it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.88it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.89it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.95it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.07it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.11it/s][A
 77%|███████▋  | 832/1083 [00:19<00:05, 45.19it/s][A
 77%|███████▋  | 837/1083 [00:19<00:05, 45.08it/s][A
 78%|███████▊  | 842/1083 [00:19<00:05, 44.94it/s][A
 78%|███████▊  | 847/1083 [00:19<00:05, 44.89it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.93it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 45.08it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.06it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 45.14it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.22it/s][A
 81%|████████  | 877/1083 [00:20<00:04, 45.16it/s][A
 81%|████████▏ | 882/1083 [00:20<00:04, 45.11it/s][A
 82%|████████▏ | 887/1083 [00:20<00:04, 43.50it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 44.03it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.33it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.55it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.63it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.83it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.99it/s][A
 85%|████████▌ | 922/1083 [00:21<00:03, 45.06it/s][A
 86%|████████▌ | 927/1083 [00:21<00:03, 44.86it/s][A
 86%|████████▌ | 932/1083 [00:21<00:03, 44.80it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 44.96it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.98it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.99it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 45.07it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 45.04it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.12it/s][A
 89%|████████▉ | 967/1083 [00:22<00:02, 45.10it/s][A
 90%|████████▉ | 972/1083 [00:22<00:02, 40.66it/s][A
 90%|█████████ | 977/1083 [00:22<00:02, 42.04it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 42.91it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 43.60it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.09it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.52it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.79it/s][A
 93%|█████████▎| 1007/1083 [00:23<00:01, 44.89it/s][A
 93%|█████████▎| 1012/1083 [00:23<00:01, 44.73it/s][A
 94%|█████████▍| 1017/1083 [00:23<00:01, 44.68it/s][A
 94%|█████████▍| 1022/1083 [00:23<00:01, 42.18it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 43.18it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 43.78it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.20it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.57it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.75it/s][A
 97%|█████████▋| 1052/1083 [00:24<00:00, 44.85it/s][A
 98%|█████████▊| 1057/1083 [00:24<00:00, 44.88it/s][A
 98%|█████████▊| 1062/1083 [00:24<00:00, 44.64it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 44.70it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.90it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 45.08it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.13it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 45.13it/s][A100%|██████████| 395/395 [04:45<00:00,  3.55it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 12:41:15,497 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-28 12:41:15,689 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:41:19,892 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:41:20,093 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:41:20,224 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 12:41:28,275 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 12:41:28,302 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79 (score: 0.8794991374015808).
                                                 100%|██████████| 395/395 [05:06<00:00,  3.55it/s]100%|██████████| 395/395 [05:06<00:00,  1.29it/s]
[INFO|trainer.py:1894] 2023-08-28 12:41:35,968 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 12:41:36,084 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:41:39,156 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:41:39,262 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:41:39,318 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:41:39,684 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:41:39,684 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:41:39,684 >>   train_loss               =     0.6385
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:41:39,685 >>   train_runtime            = 0:05:05.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:41:39,685 >>   train_samples            =       5047
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:41:39,685 >>   train_samples_per_second =     82.471
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:41:39,685 >>   train_steps_per_second   =      1.291
{'eval_loss': 0.8994749784469604, 'eval_runtime': 24.7324, 'eval_samples_per_second': 350.067, 'eval_steps_per_second': 43.789, 'epoch': 5.0}
{'train_runtime': 305.987, 'train_samples_per_second': 82.471, 'train_steps_per_second': 1.291, 'train_loss': 0.638495452494561, 'epoch': 5.0}
08/28/2023 12:41:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 12:41:39,927 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:41:39,927 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 12:41:39,927 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 56.00it/s]  1%|          | 12/1083 [00:00<00:21, 49.40it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.95it/s]  2%|▏         | 22/1083 [00:00<00:22, 47.18it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.72it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.25it/s]  3%|▎         | 37/1083 [00:00<00:22, 46.10it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.76it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.17it/s]  5%|▍         | 52/1083 [00:01<00:22, 44.86it/s]  5%|▌         | 57/1083 [00:01<00:22, 45.04it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.14it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.39it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.43it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.50it/s]  8%|▊         | 82/1083 [00:01<00:21, 45.54it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.43it/s]  8%|▊         | 92/1083 [00:02<00:21, 45.09it/s]  9%|▉         | 97/1083 [00:02<00:21, 44.83it/s]  9%|▉         | 102/1083 [00:02<00:21, 44.89it/s] 10%|▉         | 107/1083 [00:02<00:23, 41.57it/s] 10%|█         | 112/1083 [00:02<00:22, 42.75it/s] 11%|█         | 117/1083 [00:02<00:22, 43.61it/s] 11%|█▏        | 122/1083 [00:02<00:21, 44.20it/s] 12%|█▏        | 127/1083 [00:02<00:21, 44.58it/s] 12%|█▏        | 132/1083 [00:02<00:21, 44.92it/s] 13%|█▎        | 137/1083 [00:03<00:21, 45.01it/s] 13%|█▎        | 142/1083 [00:03<00:20, 44.90it/s] 14%|█▎        | 147/1083 [00:03<00:20, 44.61it/s] 14%|█▍        | 152/1083 [00:03<00:20, 44.67it/s] 14%|█▍        | 157/1083 [00:03<00:20, 44.97it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.09it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.28it/s] 16%|█▌        | 172/1083 [00:03<00:20, 45.39it/s] 16%|█▋        | 177/1083 [00:03<00:19, 45.39it/s] 17%|█▋        | 182/1083 [00:04<00:19, 45.37it/s] 17%|█▋        | 187/1083 [00:04<00:19, 45.10it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.78it/s] 18%|█▊        | 197/1083 [00:04<00:19, 44.81it/s] 19%|█▊        | 202/1083 [00:04<00:19, 44.93it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.09it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.21it/s] 20%|██        | 217/1083 [00:04<00:19, 45.36it/s] 20%|██        | 222/1083 [00:04<00:18, 45.45it/s] 21%|██        | 227/1083 [00:05<00:18, 45.40it/s] 21%|██▏       | 232/1083 [00:05<00:18, 45.24it/s] 22%|██▏       | 237/1083 [00:05<00:18, 45.07it/s] 22%|██▏       | 242/1083 [00:05<00:19, 43.19it/s] 23%|██▎       | 247/1083 [00:05<00:19, 43.80it/s] 23%|██▎       | 252/1083 [00:05<00:18, 44.32it/s] 24%|██▎       | 257/1083 [00:05<00:18, 44.68it/s] 24%|██▍       | 262/1083 [00:05<00:18, 44.97it/s] 25%|██▍       | 267/1083 [00:05<00:18, 45.15it/s] 25%|██▌       | 272/1083 [00:06<00:17, 45.19it/s] 26%|██▌       | 277/1083 [00:06<00:17, 44.99it/s] 26%|██▌       | 282/1083 [00:06<00:17, 44.74it/s] 27%|██▋       | 287/1083 [00:06<00:17, 44.82it/s] 27%|██▋       | 292/1083 [00:06<00:17, 45.06it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.11it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.28it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.31it/s] 29%|██▉       | 312/1083 [00:06<00:17, 45.35it/s] 29%|██▉       | 317/1083 [00:07<00:16, 45.28it/s] 30%|██▉       | 322/1083 [00:07<00:16, 45.01it/s] 30%|███       | 327/1083 [00:07<00:16, 44.87it/s] 31%|███       | 332/1083 [00:07<00:16, 44.83it/s] 31%|███       | 337/1083 [00:07<00:17, 42.51it/s] 32%|███▏      | 342/1083 [00:07<00:17, 43.45it/s] 32%|███▏      | 347/1083 [00:07<00:16, 44.12it/s] 33%|███▎      | 352/1083 [00:07<00:16, 44.55it/s] 33%|███▎      | 357/1083 [00:07<00:16, 44.91it/s] 33%|███▎      | 362/1083 [00:08<00:16, 45.06it/s] 34%|███▍      | 367/1083 [00:08<00:15, 44.93it/s] 34%|███▍      | 372/1083 [00:08<00:15, 44.91it/s] 35%|███▍      | 377/1083 [00:08<00:15, 44.74it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.74it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.79it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.96it/s] 37%|███▋      | 397/1083 [00:08<00:15, 45.08it/s] 37%|███▋      | 402/1083 [00:08<00:15, 45.23it/s] 38%|███▊      | 407/1083 [00:09<00:14, 45.35it/s] 38%|███▊      | 412/1083 [00:09<00:14, 45.27it/s] 39%|███▊      | 417/1083 [00:09<00:14, 45.14it/s] 39%|███▉      | 422/1083 [00:09<00:14, 44.92it/s] 39%|███▉      | 427/1083 [00:09<00:14, 44.85it/s] 40%|███▉      | 432/1083 [00:09<00:14, 44.78it/s] 40%|████      | 437/1083 [00:09<00:14, 44.97it/s] 41%|████      | 442/1083 [00:09<00:14, 45.10it/s] 41%|████▏     | 447/1083 [00:09<00:14, 45.30it/s] 42%|████▏     | 452/1083 [00:10<00:13, 45.28it/s] 42%|████▏     | 457/1083 [00:10<00:13, 45.29it/s] 43%|████▎     | 462/1083 [00:10<00:13, 45.17it/s] 43%|████▎     | 467/1083 [00:10<00:13, 44.93it/s] 44%|████▎     | 472/1083 [00:10<00:13, 44.86it/s] 44%|████▍     | 477/1083 [00:10<00:13, 44.86it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.06it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.11it/s] 45%|████▌     | 492/1083 [00:10<00:13, 45.21it/s] 46%|████▌     | 497/1083 [00:11<00:12, 45.17it/s] 46%|████▋     | 502/1083 [00:11<00:13, 41.54it/s] 47%|████▋     | 507/1083 [00:11<00:13, 42.74it/s] 47%|████▋     | 512/1083 [00:11<00:13, 43.44it/s] 48%|████▊     | 517/1083 [00:11<00:12, 43.92it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.18it/s] 49%|████▊     | 527/1083 [00:11<00:12, 44.59it/s] 49%|████▉     | 532/1083 [00:11<00:12, 44.81it/s] 50%|████▉     | 537/1083 [00:11<00:12, 44.97it/s] 50%|█████     | 542/1083 [00:12<00:12, 44.61it/s] 51%|█████     | 547/1083 [00:12<00:11, 44.76it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.99it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 45.11it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 45.14it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 45.05it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 45.15it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 45.14it/s] 54%|█████▎    | 582/1083 [00:12<00:11, 44.99it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 44.89it/s] 55%|█████▍    | 592/1083 [00:13<00:10, 44.78it/s] 55%|█████▌    | 597/1083 [00:13<00:10, 45.01it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 45.14it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 45.09it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 45.08it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 45.19it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 45.17it/s] 58%|█████▊    | 627/1083 [00:13<00:10, 45.00it/s] 58%|█████▊    | 632/1083 [00:14<00:10, 44.83it/s] 59%|█████▉    | 637/1083 [00:14<00:10, 40.92it/s] 59%|█████▉    | 642/1083 [00:14<00:10, 42.11it/s] 60%|█████▉    | 647/1083 [00:14<00:10, 43.05it/s] 60%|██████    | 652/1083 [00:14<00:09, 43.74it/s] 61%|██████    | 657/1083 [00:14<00:09, 44.33it/s] 61%|██████    | 662/1083 [00:14<00:09, 44.69it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 44.94it/s] 62%|██████▏   | 672/1083 [00:14<00:09, 45.06it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.67it/s] 63%|██████▎   | 682/1083 [00:15<00:08, 44.63it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.68it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.87it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 45.07it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 45.19it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 45.26it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 43.03it/s] 66%|██████▌   | 717/1083 [00:15<00:08, 44.05it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 44.25it/s] 67%|██████▋   | 727/1083 [00:16<00:08, 44.35it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.48it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.70it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 44.91it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 45.09it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 45.22it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 45.04it/s] 70%|███████   | 762/1083 [00:16<00:07, 44.96it/s] 71%|███████   | 767/1083 [00:17<00:07, 44.86it/s] 71%|███████▏  | 772/1083 [00:17<00:07, 40.83it/s] 72%|███████▏  | 777/1083 [00:17<00:07, 42.18it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 43.10it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 43.79it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.20it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 44.59it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 44.79it/s] 75%|███████▍  | 807/1083 [00:18<00:06, 44.90it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 44.50it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.34it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.51it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 44.78it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 45.01it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 45.11it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 45.15it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 45.33it/s] 79%|███████▊  | 852/1083 [00:19<00:05, 45.23it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.82it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.67it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.70it/s] 81%|████████  | 872/1083 [00:19<00:04, 44.79it/s] 81%|████████  | 877/1083 [00:19<00:04, 45.03it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 45.17it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 45.33it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 45.25it/s] 83%|████████▎ | 897/1083 [00:20<00:04, 45.17it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 44.93it/s] 84%|████████▎ | 907/1083 [00:20<00:04, 39.89it/s] 84%|████████▍ | 912/1083 [00:20<00:04, 41.39it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 42.50it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 43.33it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 43.93it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 44.39it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 44.70it/s] 87%|████████▋ | 942/1083 [00:21<00:03, 44.72it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.52it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 44.61it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 44.81it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 44.84it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 44.97it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 45.07it/s] 90%|█████████ | 977/1083 [00:21<00:02, 45.16it/s] 91%|█████████ | 982/1083 [00:21<00:02, 45.21it/s] 91%|█████████ | 987/1083 [00:22<00:04, 22.35it/s] 92%|█████████▏| 992/1083 [00:22<00:03, 26.78it/s] 92%|█████████▏| 997/1083 [00:22<00:02, 30.58it/s] 93%|█████████▎| 1002/1083 [00:22<00:02, 33.96it/s] 93%|█████████▎| 1007/1083 [00:22<00:02, 36.81it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 39.12it/s] 94%|█████████▍| 1017/1083 [00:23<00:01, 40.83it/s] 94%|█████████▍| 1022/1083 [00:23<00:01, 42.06it/s] 95%|█████████▍| 1027/1083 [00:23<00:01, 37.78it/s] 95%|█████████▌| 1032/1083 [00:23<00:01, 39.87it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 41.40it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 42.56it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 43.47it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 44.15it/s] 98%|█████████▊| 1057/1083 [00:24<00:00, 44.63it/s] 98%|█████████▊| 1062/1083 [00:24<00:00, 44.82it/s] 99%|█████████▊| 1067/1083 [00:24<00:00, 44.59it/s] 99%|█████████▉| 1072/1083 [00:24<00:00, 44.39it/s] 99%|█████████▉| 1077/1083 [00:24<00:00, 44.58it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 44.83it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.00it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:42:04,556 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   eval_loss               =     0.8795
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   eval_runtime            = 0:00:24.62
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   eval_samples_per_second =    351.543
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   eval_steps_per_second   =     43.973
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:42:04,556 >>   perplexity              =     2.4097
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:15,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:16,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:16,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:16,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:16,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:42:16,844 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:42:16,845 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:42:17,483 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:42:18,632 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:42:18,686 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:21,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:21,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:21,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:21,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:21,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:42:22,651 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:42:22,652 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:42:23,286 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:42:23,526 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:42:23,526 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-237
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-395
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-79
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-158
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/checkpoint-316
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.62it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:25,  1.50it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.51it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:34,  1.58it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.57it/s]Extractor Predicting: 58it [00:36,  1.56it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:38,  1.52it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:40,  1.65it/s]Extractor Predicting: 65it [00:41,  1.61it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:43,  1.61it/s]Extractor Predicting: 70it [00:44,  1.55it/s]Extractor Predicting: 71it [00:45,  1.52it/s]Extractor Predicting: 72it [00:45,  1.55it/s]Extractor Predicting: 73it [00:46,  1.58it/s]Extractor Predicting: 74it [00:47,  1.60it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:48,  1.59it/s]Extractor Predicting: 77it [00:48,  1.57it/s]Extractor Predicting: 78it [00:49,  1.58it/s]Extractor Predicting: 79it [00:50,  1.59it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.60it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.60it/s]Extractor Predicting: 85it [00:54,  1.57it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.39it/s]Extractor Predicting: 89it [00:56,  1.44it/s]Extractor Predicting: 90it [00:57,  1.48it/s]Extractor Predicting: 91it [00:58,  1.54it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.54it/s]Extractor Predicting: 94it [01:00,  1.55it/s]Extractor Predicting: 95it [01:00,  1.57it/s]Extractor Predicting: 96it [01:01,  1.58it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:02,  1.56it/s]Extractor Predicting: 99it [01:03,  1.55it/s]Extractor Predicting: 100it [01:03,  1.55it/s]Extractor Predicting: 101it [01:04,  1.58it/s]Extractor Predicting: 102it [01:05,  1.60it/s]Extractor Predicting: 103it [01:05,  1.60it/s]Extractor Predicting: 104it [01:06,  1.59it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:07,  1.59it/s]Extractor Predicting: 107it [01:08,  1.57it/s]Extractor Predicting: 108it [01:08,  1.57it/s]Extractor Predicting: 109it [01:09,  1.56it/s]Extractor Predicting: 110it [01:10,  1.57it/s]Extractor Predicting: 111it [01:10,  1.57it/s]Extractor Predicting: 112it [01:11,  1.57it/s]Extractor Predicting: 113it [01:12,  1.57it/s]Extractor Predicting: 114it [01:12,  1.57it/s]Extractor Predicting: 115it [01:13,  1.53it/s]Extractor Predicting: 116it [01:14,  1.52it/s]Extractor Predicting: 117it [01:14,  1.56it/s]Extractor Predicting: 118it [01:15,  1.57it/s]Extractor Predicting: 119it [01:15,  1.57it/s]Extractor Predicting: 120it [01:16,  1.57it/s]Extractor Predicting: 121it [01:17,  1.55it/s]Extractor Predicting: 122it [01:17,  1.66it/s]Extractor Predicting: 123it [01:18,  1.65it/s]Extractor Predicting: 124it [01:19,  1.60it/s]Extractor Predicting: 125it [01:19,  1.54it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.56it/s]Extractor Predicting: 128it [01:21,  1.57it/s]Extractor Predicting: 129it [01:22,  1.55it/s]Extractor Predicting: 130it [01:22,  1.57it/s]Extractor Predicting: 131it [01:23,  1.62it/s]Extractor Predicting: 132it [01:24,  1.61it/s]Extractor Predicting: 133it [01:24,  1.57it/s]Extractor Predicting: 134it [01:25,  1.59it/s]Extractor Predicting: 135it [01:26,  1.59it/s]Extractor Predicting: 136it [01:26,  1.57it/s]Extractor Predicting: 137it [01:27,  1.56it/s]Extractor Predicting: 138it [01:27,  1.57it/s]Extractor Predicting: 139it [01:28,  1.57it/s]Extractor Predicting: 140it [01:29,  1.58it/s]Extractor Predicting: 141it [01:29,  1.60it/s]Extractor Predicting: 142it [01:30,  1.61it/s]Extractor Predicting: 143it [01:31,  1.60it/s]Extractor Predicting: 144it [01:31,  1.61it/s]Extractor Predicting: 145it [01:32,  1.55it/s]Extractor Predicting: 146it [01:33,  1.54it/s]Extractor Predicting: 147it [01:33,  1.58it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:34,  1.63it/s]Extractor Predicting: 150it [01:35,  1.63it/s]Extractor Predicting: 151it [01:36,  1.64it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:37,  1.48it/s]Extractor Predicting: 154it [01:38,  1.46it/s]Extractor Predicting: 155it [01:38,  1.44it/s]Extractor Predicting: 156it [01:39,  1.43it/s]Extractor Predicting: 157it [01:40,  1.43it/s]Extractor Predicting: 158it [01:41,  1.42it/s]Extractor Predicting: 159it [01:41,  1.43it/s]Extractor Predicting: 160it [01:42,  1.41it/s]Extractor Predicting: 161it [01:43,  1.41it/s]Extractor Predicting: 162it [01:43,  1.41it/s]Extractor Predicting: 163it [01:44,  1.40it/s]Extractor Predicting: 164it [01:45,  1.41it/s]Extractor Predicting: 165it [01:46,  1.41it/s]Extractor Predicting: 166it [01:46,  1.43it/s]Extractor Predicting: 167it [01:47,  1.29it/s]Extractor Predicting: 168it [01:48,  1.34it/s]Extractor Predicting: 169it [01:48,  1.41it/s]Extractor Predicting: 170it [01:49,  1.47it/s]Extractor Predicting: 171it [01:50,  1.50it/s]Extractor Predicting: 172it [01:50,  1.49it/s]Extractor Predicting: 173it [01:51,  1.50it/s]Extractor Predicting: 174it [01:52,  1.50it/s]Extractor Predicting: 175it [01:52,  1.47it/s]Extractor Predicting: 176it [01:53,  1.48it/s]Extractor Predicting: 177it [01:54,  1.48it/s]Extractor Predicting: 178it [01:54,  1.47it/s]Extractor Predicting: 179it [01:55,  1.49it/s]Extractor Predicting: 180it [01:56,  1.48it/s]Extractor Predicting: 181it [01:56,  1.50it/s]Extractor Predicting: 182it [01:57,  1.51it/s]Extractor Predicting: 183it [01:58,  1.54it/s]Extractor Predicting: 184it [01:58,  1.56it/s]Extractor Predicting: 185it [01:59,  1.60it/s]Extractor Predicting: 186it [02:00,  1.56it/s]Extractor Predicting: 187it [02:00,  1.57it/s]Extractor Predicting: 188it [02:01,  1.57it/s]Extractor Predicting: 189it [02:02,  1.58it/s]Extractor Predicting: 190it [02:02,  1.57it/s]Extractor Predicting: 191it [02:03,  1.53it/s]Extractor Predicting: 192it [02:03,  1.55it/s]Extractor Predicting: 193it [02:04,  1.57it/s]Extractor Predicting: 194it [02:05,  1.58it/s]Extractor Predicting: 195it [02:05,  1.56it/s]Extractor Predicting: 196it [02:06,  1.54it/s]Extractor Predicting: 197it [02:07,  1.56it/s]Extractor Predicting: 198it [02:07,  1.56it/s]Extractor Predicting: 199it [02:08,  1.58it/s]Extractor Predicting: 200it [02:09,  1.58it/s]Extractor Predicting: 201it [02:09,  1.58it/s]Extractor Predicting: 202it [02:10,  1.56it/s]Extractor Predicting: 203it [02:10,  1.55it/s]Extractor Predicting: 204it [02:11,  1.57it/s]Extractor Predicting: 205it [02:12,  1.56it/s]Extractor Predicting: 206it [02:12,  1.57it/s]Extractor Predicting: 207it [02:13,  1.55it/s]Extractor Predicting: 208it [02:14,  1.58it/s]Extractor Predicting: 209it [02:14,  1.59it/s]Extractor Predicting: 210it [02:15,  1.60it/s]Extractor Predicting: 211it [02:16,  1.59it/s]Extractor Predicting: 212it [02:16,  1.59it/s]Extractor Predicting: 213it [02:17,  1.59it/s]Extractor Predicting: 214it [02:17,  1.61it/s]Extractor Predicting: 215it [02:18,  1.60it/s]Extractor Predicting: 216it [02:19,  1.60it/s]Extractor Predicting: 217it [02:19,  1.61it/s]Extractor Predicting: 218it [02:20,  1.61it/s]Extractor Predicting: 219it [02:20,  1.61it/s]Extractor Predicting: 220it [02:21,  1.59it/s]Extractor Predicting: 221it [02:22,  1.62it/s]Extractor Predicting: 222it [02:22,  1.62it/s]Extractor Predicting: 223it [02:23,  1.60it/s]Extractor Predicting: 224it [02:24,  1.58it/s]Extractor Predicting: 225it [02:24,  1.52it/s]Extractor Predicting: 226it [02:25,  1.55it/s]Extractor Predicting: 227it [02:26,  1.56it/s]Extractor Predicting: 228it [02:26,  1.55it/s]Extractor Predicting: 229it [02:27,  1.60it/s]Extractor Predicting: 230it [02:28,  1.57it/s]Extractor Predicting: 231it [02:28,  1.54it/s]Extractor Predicting: 232it [02:29,  1.57it/s]Extractor Predicting: 233it [02:29,  1.60it/s]Extractor Predicting: 234it [02:30,  1.58it/s]Extractor Predicting: 235it [02:31,  1.57it/s]Extractor Predicting: 236it [02:31,  1.54it/s]Extractor Predicting: 237it [02:32,  1.55it/s]Extractor Predicting: 238it [02:33,  1.57it/s]Extractor Predicting: 239it [02:33,  1.56it/s]Extractor Predicting: 240it [02:34,  1.56it/s]Extractor Predicting: 241it [02:35,  1.58it/s]Extractor Predicting: 242it [02:35,  1.57it/s]Extractor Predicting: 243it [02:36,  1.54it/s]Extractor Predicting: 244it [02:36,  1.56it/s]Extractor Predicting: 245it [02:37,  1.57it/s]Extractor Predicting: 246it [02:38,  1.57it/s]Extractor Predicting: 247it [02:38,  1.53it/s]Extractor Predicting: 248it [02:39,  1.56it/s]Extractor Predicting: 249it [02:40,  1.56it/s]Extractor Predicting: 250it [02:41,  1.38it/s]Extractor Predicting: 251it [02:41,  1.43it/s]Extractor Predicting: 252it [02:42,  1.47it/s]Extractor Predicting: 253it [02:43,  1.50it/s]Extractor Predicting: 254it [02:43,  1.53it/s]Extractor Predicting: 255it [02:44,  1.53it/s]Extractor Predicting: 256it [02:44,  1.56it/s]Extractor Predicting: 257it [02:45,  1.58it/s]Extractor Predicting: 258it [02:46,  1.61it/s]Extractor Predicting: 259it [02:46,  1.58it/s]Extractor Predicting: 260it [02:47,  1.56it/s]Extractor Predicting: 261it [02:48,  1.58it/s]Extractor Predicting: 262it [02:48,  1.59it/s]Extractor Predicting: 263it [02:49,  1.54it/s]Extractor Predicting: 264it [02:50,  1.54it/s]Extractor Predicting: 265it [02:50,  1.54it/s]Extractor Predicting: 266it [02:51,  1.52it/s]Extractor Predicting: 267it [02:51,  1.54it/s]Extractor Predicting: 268it [02:52,  1.54it/s]Extractor Predicting: 269it [02:53,  1.62it/s]Extractor Predicting: 269it [02:53,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:28,634 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:28,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:28,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:28,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:28,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:45:29,300 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:45:29,301 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:45:29,649 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:45:30,780 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:45:30,780 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:33,769 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:33,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:33,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:33,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:45:33,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:45:34,686 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:45:34,687 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:45:35,305 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:45:35,570 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:45:35,570 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.29249394673123486,
  "recall": 0.06976206976206976,
  "score": 0.1126550405670055,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.56it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:21,  1.47it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.58it/s]Extractor Predicting: 41it [00:27,  1.59it/s]Extractor Predicting: 42it [00:27,  1.62it/s]Extractor Predicting: 43it [00:28,  1.64it/s]Extractor Predicting: 44it [00:28,  1.61it/s]Extractor Predicting: 45it [00:29,  1.57it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.55it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:33,  1.60it/s]Extractor Predicting: 52it [00:33,  1.60it/s]Extractor Predicting: 53it [00:34,  1.55it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.62it/s]Extractor Predicting: 57it [00:37,  1.62it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:38,  1.61it/s]Extractor Predicting: 60it [00:38,  1.63it/s]Extractor Predicting: 61it [00:39,  1.59it/s]Extractor Predicting: 62it [00:40,  1.59it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.55it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:44,  1.56it/s]Extractor Predicting: 70it [00:45,  1.58it/s]Extractor Predicting: 71it [00:46,  1.61it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:47,  1.58it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.60it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.58it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:52,  1.62it/s]Extractor Predicting: 82it [00:52,  1.63it/s]Extractor Predicting: 83it [00:53,  1.62it/s]Extractor Predicting: 84it [00:54,  1.66it/s]Extractor Predicting: 85it [00:54,  1.65it/s]Extractor Predicting: 86it [00:55,  1.62it/s]Extractor Predicting: 87it [00:56,  1.62it/s]Extractor Predicting: 88it [00:56,  1.62it/s]Extractor Predicting: 89it [00:57,  1.59it/s]Extractor Predicting: 90it [00:57,  1.63it/s]Extractor Predicting: 91it [00:58,  1.63it/s]Extractor Predicting: 92it [00:59,  1.60it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:01,  1.64it/s]Extractor Predicting: 96it [01:01,  1.63it/s]Extractor Predicting: 97it [01:02,  1.62it/s]Extractor Predicting: 98it [01:02,  1.59it/s]Extractor Predicting: 99it [01:03,  1.59it/s]Extractor Predicting: 100it [01:04,  1.60it/s]Extractor Predicting: 101it [01:04,  1.64it/s]Extractor Predicting: 102it [01:05,  1.64it/s]Extractor Predicting: 103it [01:05,  1.63it/s]Extractor Predicting: 104it [01:06,  1.65it/s]Extractor Predicting: 105it [01:07,  1.65it/s]Extractor Predicting: 106it [01:07,  1.70it/s]Extractor Predicting: 107it [01:08,  1.70it/s]Extractor Predicting: 108it [01:08,  1.76it/s]Extractor Predicting: 109it [01:09,  1.78it/s]Extractor Predicting: 110it [01:10,  1.72it/s]Extractor Predicting: 111it [01:10,  1.74it/s]Extractor Predicting: 112it [01:11,  1.73it/s]Extractor Predicting: 113it [01:11,  1.71it/s]Extractor Predicting: 114it [01:12,  1.63it/s]Extractor Predicting: 115it [01:13,  1.59it/s]Extractor Predicting: 116it [01:13,  1.56it/s]Extractor Predicting: 117it [01:14,  1.52it/s]Extractor Predicting: 118it [01:15,  1.38it/s]Extractor Predicting: 119it [01:15,  1.44it/s]Extractor Predicting: 120it [01:16,  1.48it/s]Extractor Predicting: 121it [01:17,  1.47it/s]Extractor Predicting: 122it [01:17,  1.50it/s]Extractor Predicting: 123it [01:18,  1.49it/s]Extractor Predicting: 124it [01:19,  1.53it/s]Extractor Predicting: 125it [01:19,  1.55it/s]Extractor Predicting: 126it [01:20,  1.51it/s]Extractor Predicting: 127it [01:21,  1.49it/s]Extractor Predicting: 128it [01:21,  1.50it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:23,  1.57it/s]Extractor Predicting: 131it [01:23,  1.55it/s]Extractor Predicting: 132it [01:24,  1.53it/s]Extractor Predicting: 133it [01:25,  1.52it/s]Extractor Predicting: 134it [01:25,  1.49it/s]Extractor Predicting: 135it [01:26,  1.49it/s]Extractor Predicting: 136it [01:27,  1.54it/s]Extractor Predicting: 137it [01:27,  1.53it/s]Extractor Predicting: 138it [01:28,  1.58it/s]Extractor Predicting: 139it [01:28,  1.60it/s]Extractor Predicting: 140it [01:29,  1.58it/s]Extractor Predicting: 141it [01:30,  1.55it/s]Extractor Predicting: 142it [01:30,  1.52it/s]Extractor Predicting: 143it [01:31,  1.85it/s]Extractor Predicting: 143it [01:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:19,556 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:19,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:19,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:19,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:19,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:47:20,393 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:47:20,394 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:47:21,007 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:47:22,142 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:47:22,142 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:25,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:25,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:25,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:25,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:25,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:47:26,049 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:47:26,050 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:47:26,673 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:47:26,930 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:47:26,931 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.284789644012945,
  "recall": 0.07735130383826545,
  "score": 0.1216589861751152,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.42it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:25,  1.66it/s]Extractor Predicting: 40it [00:26,  1.74it/s]Extractor Predicting: 41it [00:26,  1.84it/s]Extractor Predicting: 42it [00:27,  1.88it/s]Extractor Predicting: 43it [00:27,  1.88it/s]Extractor Predicting: 44it [00:28,  1.88it/s]Extractor Predicting: 45it [00:28,  1.93it/s]Extractor Predicting: 46it [00:29,  1.90it/s]Extractor Predicting: 47it [00:29,  1.88it/s]Extractor Predicting: 48it [00:30,  1.89it/s]Extractor Predicting: 49it [00:30,  1.89it/s]Extractor Predicting: 50it [00:31,  1.91it/s]Extractor Predicting: 51it [00:31,  1.87it/s]Extractor Predicting: 52it [00:32,  1.88it/s]Extractor Predicting: 53it [00:32,  1.90it/s]Extractor Predicting: 54it [00:33,  1.94it/s]Extractor Predicting: 55it [00:33,  1.96it/s]Extractor Predicting: 56it [00:34,  1.90it/s]Extractor Predicting: 57it [00:34,  1.92it/s]Extractor Predicting: 58it [00:35,  1.92it/s]Extractor Predicting: 59it [00:35,  1.93it/s]Extractor Predicting: 60it [00:36,  1.88it/s]Extractor Predicting: 61it [00:37,  1.84it/s]Extractor Predicting: 62it [00:37,  1.87it/s]Extractor Predicting: 63it [00:38,  1.89it/s]Extractor Predicting: 64it [00:38,  1.87it/s]Extractor Predicting: 65it [00:39,  1.88it/s]Extractor Predicting: 66it [00:39,  1.92it/s]Extractor Predicting: 67it [00:40,  1.82it/s]Extractor Predicting: 68it [00:40,  1.71it/s]Extractor Predicting: 69it [00:41,  1.65it/s]Extractor Predicting: 70it [00:42,  1.57it/s]Extractor Predicting: 71it [00:42,  1.54it/s]Extractor Predicting: 72it [00:43,  1.56it/s]Extractor Predicting: 72it [00:43,  1.65it/s]
[INFO|configuration_utils.py:515] 2023-08-28 12:48:13,589 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:48:13,591 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:48:13,645 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:48:13,646 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 12:48:13,681 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:48:26,608 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 12:48:26,643 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 12:48:26,780 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:48:26,781 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:48:26,862 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:48:26,929 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:48:26,929 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:48:26,929 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:48:26,929 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:48:26,929 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:48:26,929 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7745664739884393,
  "recall": 0.16716566866267465,
  "score": 0.27498460907038785,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 12:48:27,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:27,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:28,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:28,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:29,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:30,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:30,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:31,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:31,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:32,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:33,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:33,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:34,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:34,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:35,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:36,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:36,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:37,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:38,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:38,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:39,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:39,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<01:59, 13.26s/it][WARNING|generation_utils.py:914] 2023-08-28 12:48:40,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:41,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:41,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:42,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:42,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:43,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:43,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:44,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:45,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:45,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:46,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:46,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:47,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:47,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:48,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:49,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:49,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:50,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:50,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:51,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:52,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:52,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:26<01:45, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-28 12:48:53,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:54,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:54,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:55,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:55,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:56,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:57,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:57,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:58,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:58,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:59,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:48:59,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:00,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:00,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:01,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:01,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:02,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:02,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:03,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:04,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:04,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:37<01:27, 12.44s/it][WARNING|generation_utils.py:914] 2023-08-28 12:49:05,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:05,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:06,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:07,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:07,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:08,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:09,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:09,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:10,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:11,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:11,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:12,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:13,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:13,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:14,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:15,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:16,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:16,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:17,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:18,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:18,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:19,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:19,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:20,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:20,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:21,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:54<01:25, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-28 12:49:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:22,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:23,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:23,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:24,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:25,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:25,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:26,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:26,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:27,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:27,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:28,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:29,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:29,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:30,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:30,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:31,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:31,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:32,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:33,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:06<01:05, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-28 12:49:33,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:34,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:34,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:35,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:36,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:36,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:37,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:37,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:38,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:38,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:39,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:39,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:40,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:41,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:41,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:42,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:43,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:43,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:44,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:44,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:45,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:18<00:51, 12.91s/it][WARNING|generation_utils.py:914] 2023-08-28 12:49:45,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:46,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:46,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:47,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:47,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:48,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:49,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:49,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:50,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:50,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:51,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:51,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:52,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:53,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:53,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:54,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:54,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:55,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:56,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:56,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:57,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:57,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:30<00:38, 12.69s/it][WARNING|generation_utils.py:914] 2023-08-28 12:49:58,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:58,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:59,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:49:59,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:00,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:00,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:01,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:01,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:02,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:02,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:03,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:04,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:04,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:05,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:05,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:06,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:06,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:07,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:07,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:08,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:08,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:42<00:24, 12.23s/it][WARNING|generation_utils.py:914] 2023-08-28 12:50:09,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:10,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:10,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:11,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:11,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:12,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:13,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:13,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:14,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:14,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:15,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:16,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:16,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:17,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:17,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:18,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:19,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:19,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:20,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:21,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:22,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:22,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:56<00:12, 12.76s/it][WARNING|generation_utils.py:914] 2023-08-28 12:50:23,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:23,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:24,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:25,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:25,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:26,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:27,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:28,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:28,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:29,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:29,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:30,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:31,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:31,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:32,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:33,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:33,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:34,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:35,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:35,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:36,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:37,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:50:37,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:11<00:00, 13.46s/it]Generating: 100%|██████████| 10/10 [02:11<00:00, 13.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:50:46,653 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:50:46,654 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:50:47,275 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:50:48,443 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:50:48,443 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:51,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:51,474 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:51,474 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:51,474 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:51,474 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:50:52,333 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:50:52,334 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:50:52,993 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:50:53,267 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:50:53,268 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : characters .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 395, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 468, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 538, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 589, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.7379807692307693, 'errors': {'', 'too many values to unpack (expected 2)', "('Cape Iria', 'lowest point', '', 'On the coast of Cape Iria , the line is interrupted due to the arrival of the German cruiser Oder .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 596, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : sport .', 'success_rate': 0.978125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : league .', 'success_rate': 0.8650568181818182, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8478260869565217, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 10374
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10474, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.65it/s]Extractor Estimating: 4it [00:02,  1.67it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:04,  1.61it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.64it/s]Extractor Estimating: 12it [00:07,  1.64it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.55it/s]Extractor Estimating: 15it [00:09,  1.51it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:10,  1.55it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:11,  1.61it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.61it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:16,  1.68it/s]Extractor Estimating: 27it [00:16,  1.71it/s]Extractor Estimating: 28it [00:17,  1.75it/s]Extractor Estimating: 29it [00:17,  1.75it/s]Extractor Estimating: 30it [00:18,  1.69it/s]Extractor Estimating: 31it [00:19,  1.69it/s]Extractor Estimating: 32it [00:19,  1.72it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:20,  1.77it/s]Extractor Estimating: 35it [00:21,  1.79it/s]Extractor Estimating: 36it [00:21,  1.78it/s]Extractor Estimating: 37it [00:22,  1.76it/s]Extractor Estimating: 38it [00:22,  1.76it/s]Extractor Estimating: 39it [00:23,  1.73it/s]Extractor Estimating: 40it [00:24,  1.70it/s]Extractor Estimating: 41it [00:24,  1.69it/s]Extractor Estimating: 42it [00:25,  1.66it/s]Extractor Estimating: 43it [00:25,  1.67it/s]Extractor Estimating: 44it [00:26,  1.72it/s]Extractor Estimating: 45it [00:27,  1.71it/s]Extractor Estimating: 46it [00:27,  1.74it/s]Extractor Estimating: 47it [00:28,  1.66it/s]Extractor Estimating: 48it [00:28,  1.66it/s]Extractor Estimating: 49it [00:29,  1.64it/s]Extractor Estimating: 50it [00:30,  1.66it/s]Extractor Estimating: 51it [00:30,  1.71it/s]Extractor Estimating: 52it [00:31,  1.75it/s]Extractor Estimating: 53it [00:31,  1.79it/s]Extractor Estimating: 54it [00:32,  1.82it/s]Extractor Estimating: 55it [00:32,  1.92it/s]Extractor Estimating: 56it [00:33,  1.96it/s]Extractor Estimating: 57it [00:33,  1.94it/s]Extractor Estimating: 58it [00:34,  1.94it/s]Extractor Estimating: 59it [00:34,  1.89it/s]Extractor Estimating: 60it [00:35,  1.93it/s]Extractor Estimating: 61it [00:35,  1.92it/s]Extractor Estimating: 62it [00:36,  1.91it/s]Extractor Estimating: 63it [00:36,  1.93it/s]Extractor Estimating: 64it [00:37,  1.95it/s]Extractor Estimating: 65it [00:37,  1.97it/s]Extractor Estimating: 66it [00:38,  1.91it/s]Extractor Estimating: 67it [00:39,  1.82it/s]Extractor Estimating: 68it [00:39,  1.83it/s]Extractor Estimating: 69it [00:40,  1.81it/s]Extractor Estimating: 70it [00:40,  1.80it/s]Extractor Estimating: 71it [00:41,  1.86it/s]Extractor Estimating: 72it [00:41,  1.82it/s]Extractor Estimating: 73it [00:42,  1.87it/s]Extractor Estimating: 74it [00:42,  1.82it/s]Extractor Estimating: 75it [00:43,  1.79it/s]Extractor Estimating: 76it [00:44,  1.72it/s]Extractor Estimating: 77it [00:44,  1.66it/s]Extractor Estimating: 78it [00:45,  1.69it/s]Extractor Estimating: 79it [00:45,  1.64it/s]Extractor Estimating: 80it [00:46,  1.60it/s]Extractor Estimating: 81it [00:47,  1.61it/s]Extractor Estimating: 82it [00:47,  1.62it/s]Extractor Estimating: 83it [00:48,  1.58it/s]Extractor Estimating: 84it [00:49,  1.56it/s]Extractor Estimating: 85it [00:49,  1.60it/s]Extractor Estimating: 86it [00:50,  1.61it/s]Extractor Estimating: 87it [00:51,  1.54it/s]Extractor Estimating: 88it [00:51,  1.54it/s]Extractor Estimating: 89it [00:52,  1.53it/s]Extractor Estimating: 90it [00:53,  1.54it/s]Extractor Estimating: 91it [00:53,  1.58it/s]Extractor Estimating: 92it [00:54,  1.56it/s]Extractor Estimating: 93it [00:54,  1.62it/s]Extractor Estimating: 94it [00:55,  1.64it/s]Extractor Estimating: 95it [00:56,  1.62it/s]Extractor Estimating: 96it [00:56,  1.63it/s]Extractor Estimating: 97it [00:57,  1.63it/s]Extractor Estimating: 98it [00:57,  1.69it/s]Extractor Estimating: 99it [00:58,  1.71it/s]Extractor Estimating: 100it [00:59,  1.66it/s]Extractor Estimating: 101it [00:59,  1.69it/s]Extractor Estimating: 102it [01:00,  1.72it/s]Extractor Estimating: 103it [01:00,  1.83it/s]Extractor Estimating: 104it [01:01,  1.68it/s]Extractor Estimating: 105it [01:01,  1.72it/s]Extractor Estimating: 106it [01:02,  1.78it/s]Extractor Estimating: 107it [01:02,  1.84it/s]Extractor Estimating: 108it [01:03,  1.84it/s]Extractor Estimating: 109it [01:03,  1.92it/s]Extractor Estimating: 110it [01:04,  1.94it/s]Extractor Estimating: 111it [01:04,  1.94it/s]Extractor Estimating: 112it [01:05,  1.92it/s]Extractor Estimating: 113it [01:05,  1.97it/s]Extractor Estimating: 114it [01:06,  1.95it/s]Extractor Estimating: 115it [01:06,  1.99it/s]Extractor Estimating: 116it [01:07,  1.97it/s]Extractor Estimating: 117it [01:07,  2.00it/s]Extractor Estimating: 118it [01:08,  1.98it/s]Extractor Estimating: 119it [01:08,  1.99it/s]Extractor Estimating: 120it [01:09,  1.90it/s]Extractor Estimating: 121it [01:09,  1.96it/s]Extractor Estimating: 122it [01:10,  1.97it/s]Extractor Estimating: 123it [01:10,  1.97it/s]Extractor Estimating: 124it [01:11,  2.02it/s]Extractor Estimating: 125it [01:11,  2.00it/s]Extractor Estimating: 126it [01:12,  1.77it/s]Extractor Estimating: 127it [01:13,  1.72it/s]Extractor Estimating: 128it [01:13,  1.67it/s]Extractor Estimating: 129it [01:14,  1.62it/s]Extractor Estimating: 130it [01:15,  1.64it/s]Extractor Estimating: 131it [01:15,  1.65it/s]Extractor Estimating: 132it [01:16,  1.69it/s]Extractor Estimating: 133it [01:16,  1.71it/s]Extractor Estimating: 134it [01:17,  1.76it/s]Extractor Estimating: 135it [01:18,  1.70it/s]Extractor Estimating: 136it [01:18,  1.79it/s]Extractor Estimating: 137it [01:19,  1.76it/s]Extractor Estimating: 138it [01:19,  1.71it/s]Extractor Estimating: 139it [01:20,  1.69it/s]Extractor Estimating: 140it [01:21,  1.65it/s]Extractor Estimating: 141it [01:21,  1.66it/s]Extractor Estimating: 142it [01:22,  1.63it/s]Extractor Estimating: 143it [01:22,  1.70it/s]Extractor Estimating: 144it [01:23,  1.66it/s]Extractor Estimating: 145it [01:24,  1.66it/s]Extractor Estimating: 146it [01:24,  1.70it/s]Extractor Estimating: 147it [01:25,  1.65it/s]Extractor Estimating: 148it [01:25,  1.68it/s]Extractor Estimating: 149it [01:26,  1.71it/s]Extractor Estimating: 150it [01:27,  1.68it/s]Extractor Estimating: 151it [01:27,  1.72it/s]Extractor Estimating: 152it [01:28,  1.73it/s]Extractor Estimating: 153it [01:28,  1.71it/s]Extractor Estimating: 154it [01:29,  1.68it/s]Extractor Estimating: 155it [01:30,  1.63it/s]Extractor Estimating: 156it [01:30,  1.59it/s]Extractor Estimating: 157it [01:31,  1.59it/s]Extractor Estimating: 158it [01:31,  1.62it/s]Extractor Estimating: 159it [01:32,  1.66it/s]Extractor Estimating: 160it [01:33,  1.61it/s]Extractor Estimating: 161it [01:33,  1.61it/s]Extractor Estimating: 162it [01:34,  1.62it/s]Extractor Estimating: 163it [01:34,  1.60it/s]Extractor Estimating: 164it [01:35,  1.56it/s]Extractor Estimating: 165it [01:36,  1.60it/s]Extractor Estimating: 166it [01:36,  1.62it/s]Extractor Estimating: 167it [01:37,  1.61it/s]Extractor Estimating: 168it [01:38,  1.57it/s]Extractor Estimating: 169it [01:38,  1.44it/s]Extractor Estimating: 170it [01:39,  1.47it/s]Extractor Estimating: 171it [01:40,  1.48it/s]Extractor Estimating: 172it [01:40,  1.52it/s]Extractor Estimating: 173it [01:41,  1.54it/s]Extractor Estimating: 174it [01:42,  1.60it/s]Extractor Estimating: 175it [01:42,  1.64it/s]Extractor Estimating: 176it [01:43,  1.74it/s]Extractor Estimating: 177it [01:43,  1.80it/s]Extractor Estimating: 178it [01:44,  1.87it/s]Extractor Estimating: 179it [01:44,  1.85it/s]Extractor Estimating: 180it [01:45,  1.88it/s]Extractor Estimating: 181it [01:45,  1.79it/s]Extractor Estimating: 182it [01:46,  1.86it/s]Extractor Estimating: 183it [01:46,  1.83it/s]Extractor Estimating: 184it [01:47,  1.93it/s]Extractor Estimating: 185it [01:47,  1.90it/s]Extractor Estimating: 186it [01:48,  1.91it/s]Extractor Estimating: 187it [01:48,  1.87it/s]Extractor Estimating: 188it [01:49,  1.90it/s]Extractor Estimating: 189it [01:50,  1.89it/s]Extractor Estimating: 190it [01:50,  1.79it/s]Extractor Estimating: 191it [01:51,  1.85it/s]Extractor Estimating: 192it [01:51,  1.85it/s]Extractor Estimating: 193it [01:52,  1.88it/s]Extractor Estimating: 194it [01:52,  1.92it/s]Extractor Estimating: 195it [01:53,  1.89it/s]Extractor Estimating: 196it [01:53,  1.89it/s]Extractor Estimating: 197it [01:54,  1.88it/s]Extractor Estimating: 198it [01:54,  1.88it/s]Extractor Estimating: 199it [01:55,  1.87it/s]Extractor Estimating: 200it [01:55,  1.89it/s]Extractor Estimating: 201it [01:56,  1.85it/s]Extractor Estimating: 202it [01:57,  1.81it/s]Extractor Estimating: 203it [01:57,  1.71it/s]Extractor Estimating: 204it [01:58,  1.71it/s]Extractor Estimating: 205it [01:58,  1.65it/s]Extractor Estimating: 206it [01:59,  1.65it/s]Extractor Estimating: 207it [02:00,  1.67it/s]Extractor Estimating: 208it [02:00,  1.63it/s]Extractor Estimating: 209it [02:01,  1.67it/s]Extractor Estimating: 210it [02:01,  1.68it/s]Extractor Estimating: 211it [02:02,  1.71it/s]Extractor Estimating: 212it [02:03,  1.73it/s]Extractor Estimating: 213it [02:03,  1.72it/s]Extractor Estimating: 214it [02:04,  1.70it/s]Extractor Estimating: 215it [02:04,  1.68it/s]Extractor Estimating: 216it [02:05,  1.72it/s]Extractor Estimating: 217it [02:05,  1.74it/s]Extractor Estimating: 218it [02:06,  1.68it/s]Extractor Estimating: 219it [02:07,  1.71it/s]Extractor Estimating: 220it [02:07,  1.67it/s]Extractor Estimating: 221it [02:08,  1.68it/s]Extractor Estimating: 222it [02:08,  1.70it/s]Extractor Estimating: 223it [02:09,  1.70it/s]Extractor Estimating: 224it [02:10,  1.68it/s]Extractor Estimating: 225it [02:10,  1.67it/s]Extractor Estimating: 226it [02:11,  1.70it/s]Extractor Estimating: 227it [02:11,  1.76it/s]Extractor Estimating: 228it [02:12,  1.68it/s]Extractor Estimating: 229it [02:13,  1.66it/s]Extractor Estimating: 230it [02:13,  1.62it/s]Extractor Estimating: 231it [02:14,  1.62it/s]Extractor Estimating: 232it [02:14,  1.67it/s]Extractor Estimating: 233it [02:15,  1.67it/s]Extractor Estimating: 234it [02:16,  1.66it/s]Extractor Estimating: 235it [02:16,  1.64it/s]Extractor Estimating: 236it [02:17,  1.59it/s]Extractor Estimating: 237it [02:18,  1.60it/s]Extractor Estimating: 238it [02:18,  1.65it/s]Extractor Estimating: 239it [02:19,  1.66it/s]Extractor Estimating: 240it [02:19,  1.71it/s]Extractor Estimating: 241it [02:20,  1.55it/s]Extractor Estimating: 242it [02:21,  1.56it/s]Extractor Estimating: 243it [02:21,  1.58it/s]Extractor Estimating: 244it [02:22,  1.59it/s]Extractor Estimating: 245it [02:23,  1.63it/s]Extractor Estimating: 246it [02:23,  1.71it/s]Extractor Estimating: 247it [02:24,  1.68it/s]Extractor Estimating: 248it [02:24,  1.59it/s]Extractor Estimating: 249it [02:25,  1.59it/s]Extractor Estimating: 250it [02:26,  1.60it/s]Extractor Estimating: 250it [02:26,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:40,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:40,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:40,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:40,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:40,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:53:41,835 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:53:41,836 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:53:42,150 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:53:43,292 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:53:43,292 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:45,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:45,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:45,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:45,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:53:45,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:53:46,700 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:53:46,701 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:53:47,303 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:53:47,504 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:53:47,504 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 14:30:35,587 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 14:30:35,639 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4992 mean pseudo reward: 0.9644428376180834
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 23416
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23516, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23516, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.936, loss:579.7935
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.942, loss:531.0795
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 0.940, loss:510.1404
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 0.941, loss:516.8681
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.941, loss:480.8513
>> valid entity prec:0.5214, rec:0.4170, f1:0.4634
>> valid relation prec:0.2324, rec:0.0443, f1:0.0744
>> valid relation with NER prec:0.2324, rec:0.0443, f1:0.0744
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.312, loss:509.7880
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.942, loss:491.3363
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.943, loss:488.2898
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.949, loss:504.2167
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.936, loss:502.6240
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4799, rec:0.4902, f1:0.4850
>> valid relation prec:0.2274, rec:0.0338, f1:0.0588
>> valid relation with NER prec:0.2274, rec:0.0338, f1:0.0588
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 3.320, loss:492.2012
g_step 1200, step 160, avg_time 0.941, loss:498.3082
g_step 1300, step 52, avg_time 0.952, loss:474.1257
g_step 1400, step 152, avg_time 0.946, loss:482.7515
g_step 1500, step 44, avg_time 0.935, loss:468.0196
>> valid entity prec:0.4235, rec:0.4401, f1:0.4316
>> valid relation prec:0.1851, rec:0.0282, f1:0.0490
>> valid relation with NER prec:0.1851, rec:0.0282, f1:0.0490
g_step 1600, step 144, avg_time 3.307, loss:447.7849
g_step 1700, step 36, avg_time 0.936, loss:448.7222
g_step 1800, step 136, avg_time 0.934, loss:439.7573
g_step 1900, step 28, avg_time 0.944, loss:422.3838
g_step 2000, step 128, avg_time 0.937, loss:390.7982
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5321, rec:0.4153, f1:0.4665
>> valid relation prec:0.2243, rec:0.0571, f1:0.0911
>> valid relation with NER prec:0.2243, rec:0.0571, f1:0.0911
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 3.305, loss:425.4891
g_step 2200, step 120, avg_time 0.941, loss:371.2758
g_step 2300, step 12, avg_time 0.946, loss:405.3066
g_step 2400, step 112, avg_time 0.943, loss:366.3574
g_step 2500, step 4, avg_time 0.942, loss:382.1136
>> valid entity prec:0.5371, rec:0.3565, f1:0.4286
>> valid relation prec:0.1960, rec:0.0320, f1:0.0551
>> valid relation with NER prec:0.1960, rec:0.0320, f1:0.0551
g_step 2600, step 104, avg_time 3.273, loss:349.9208
g_step 2700, step 204, avg_time 0.945, loss:367.6279
g_step 2800, step 96, avg_time 0.941, loss:330.2314
g_step 2900, step 196, avg_time 0.944, loss:360.9789
g_step 3000, step 88, avg_time 0.940, loss:326.4741
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4652, rec:0.4088, f1:0.4352
>> valid relation prec:0.1607, rec:0.0259, f1:0.0446
>> valid relation with NER prec:0.1607, rec:0.0259, f1:0.0446
g_step 3100, step 188, avg_time 3.300, loss:319.2439
g_step 3200, step 80, avg_time 0.942, loss:309.0003
g_step 3300, step 180, avg_time 0.937, loss:324.7934
g_step 3400, step 72, avg_time 0.938, loss:302.4229
g_step 3500, step 172, avg_time 0.941, loss:311.6082
>> valid entity prec:0.5194, rec:0.4029, f1:0.4538
>> valid relation prec:0.1988, rec:0.0390, f1:0.0652
>> valid relation with NER prec:0.1988, rec:0.0390, f1:0.0652
g_step 3600, step 64, avg_time 3.289, loss:304.2183
g_step 3700, step 164, avg_time 0.939, loss:298.9207
g_step 3800, step 56, avg_time 0.931, loss:286.0630
g_step 3900, step 156, avg_time 0.947, loss:291.7314
g_step 4000, step 48, avg_time 0.941, loss:294.1559
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5014, rec:0.3807, f1:0.4328
>> valid relation prec:0.2063, rec:0.0506, f1:0.0813
>> valid relation with NER prec:0.2063, rec:0.0506, f1:0.0813
g_step 4100, step 148, avg_time 3.286, loss:272.3379
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:30:35 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:30:35 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-30-35_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:30:36 - WARNING - datasets.builder -   Using custom data configuration default-42798e982a1051fd
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-42798e982a1051fd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:30:37,757 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:37,758 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:30:37,759 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:37,760 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:30:37,851 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:37,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:37,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:37,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:37,881 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:37,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:37,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:30:38,084 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:30:41,185 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:30:41,214 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-42798e982a1051fd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.06ba/s] 40%|████      | 2/5 [00:00<00:00,  3.98ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.35ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.51ba/s]100%|██████████| 5/5 [00:01<00:00,  4.61ba/s]100%|██████████| 5/5 [00:01<00:00,  4.36ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.63ba/s] 22%|██▏       | 2/9 [00:00<00:01,  4.10ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.29ba/s] 44%|████▍     | 4/9 [00:00<00:01,  4.40ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.44ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.50ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.50ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.49ba/s]100%|██████████| 9/9 [00:01<00:00,  4.88ba/s]100%|██████████| 9/9 [00:01<00:00,  4.53ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.89ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.34ba/s]100%|██████████| 5/5 [00:00<00:00,  8.92ba/s]100%|██████████| 5/5 [00:00<00:00,  8.02ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:00,  9.18ba/s] 33%|███▎      | 3/9 [00:00<00:00, 10.50ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.50ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.55ba/s]100%|██████████| 9/9 [00:00<00:00, 11.43ba/s]100%|██████████| 9/9 [00:00<00:00, 10.98ba/s]
[INFO|trainer.py:414] 2023-08-28 14:30:46,403 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:30:46,438 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:30:46,438 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 14:30:46,438 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:30:46,438 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:30:46,438 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:30:46,438 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:30:46,438 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.30it/s]  1%|          | 2/390 [00:00<01:54,  3.39it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:54,  3.38it/s]  1%|▏         | 5/390 [00:01<01:52,  3.41it/s]  2%|▏         | 6/390 [00:01<01:52,  3.42it/s]  2%|▏         | 7/390 [00:02<01:52,  3.41it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.42it/s]  3%|▎         | 10/390 [00:02<01:51,  3.42it/s]  3%|▎         | 11/390 [00:03<01:50,  3.42it/s]  3%|▎         | 12/390 [00:03<01:50,  3.42it/s]  3%|▎         | 13/390 [00:03<01:50,  3.42it/s]  4%|▎         | 14/390 [00:04<01:49,  3.42it/s]  4%|▍         | 15/390 [00:04<01:49,  3.41it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:04<01:49,  3.41it/s]  5%|▍         | 18/390 [00:05<01:48,  3.41it/s]  5%|▍         | 19/390 [00:05<01:48,  3.41it/s]  5%|▌         | 20/390 [00:05<01:48,  3.42it/s]  5%|▌         | 21/390 [00:06<01:48,  3.42it/s]  6%|▌         | 22/390 [00:06<01:47,  3.42it/s]  6%|▌         | 23/390 [00:06<01:47,  3.42it/s]  6%|▌         | 24/390 [00:07<01:47,  3.42it/s]  6%|▋         | 25/390 [00:07<01:46,  3.42it/s]  7%|▋         | 26/390 [00:07<01:47,  3.40it/s]  7%|▋         | 27/390 [00:07<01:46,  3.40it/s]  7%|▋         | 28/390 [00:08<01:46,  3.41it/s]  7%|▋         | 29/390 [00:08<01:45,  3.41it/s]  8%|▊         | 30/390 [00:08<01:45,  3.41it/s]  8%|▊         | 31/390 [00:09<01:45,  3.41it/s]  8%|▊         | 32/390 [00:09<01:44,  3.41it/s]  8%|▊         | 33/390 [00:09<01:44,  3.41it/s]  9%|▊         | 34/390 [00:09<01:44,  3.41it/s]  9%|▉         | 35/390 [00:10<01:43,  3.42it/s]  9%|▉         | 36/390 [00:10<01:43,  3.41it/s]  9%|▉         | 37/390 [00:10<01:43,  3.40it/s] 10%|▉         | 38/390 [00:11<01:43,  3.41it/s] 10%|█         | 39/390 [00:11<01:43,  3.41it/s] 10%|█         | 40/390 [00:11<01:42,  3.41it/s] 11%|█         | 41/390 [00:12<01:42,  3.41it/s] 11%|█         | 42/390 [00:12<01:42,  3.41it/s] 11%|█         | 43/390 [00:12<01:41,  3.41it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.41it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.41it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.42it/s] 12%|█▏        | 48/390 [00:14<01:41,  3.36it/s] 13%|█▎        | 49/390 [00:14<01:41,  3.38it/s] 13%|█▎        | 50/390 [00:14<01:40,  3.39it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.40it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.40it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.41it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.41it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.41it/s] 15%|█▍        | 58/390 [00:17<01:36,  3.42it/s] 15%|█▌        | 59/390 [00:17<01:37,  3.38it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.41it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.43it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.45it/s] 17%|█▋        | 65/390 [00:19<01:34,  3.45it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.46it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.46it/s] 18%|█▊        | 69/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 70/390 [00:20<01:36,  3.31it/s] 18%|█▊        | 71/390 [00:20<01:35,  3.36it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.39it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.41it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.43it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.45it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.45it/s] 20%|██        | 78/390 [00:22<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 14:31:09,327 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:31:09,327 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 14:31:09,327 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.74it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.51it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.17it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.41it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.77it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.47it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.17it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.99it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.12it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.23it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.26it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.29it/s][A
  6%|▋         | 68/1083 [00:01<00:24, 41.82it/s][A
  7%|▋         | 73/1083 [00:01<00:23, 42.90it/s][A
  7%|▋         | 78/1083 [00:01<00:23, 43.50it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.06it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.38it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.68it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 44.86it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.01it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 44.75it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.70it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.78it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.75it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 45.04it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 45.02it/s][A
 13%|█▎        | 138/1083 [00:03<00:20, 45.16it/s][A
 13%|█▎        | 143/1083 [00:03<00:20, 45.16it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 45.14it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.98it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 44.87it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 44.88it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.86it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 45.07it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 45.13it/s][A
 17%|█▋        | 183/1083 [00:04<00:19, 45.15it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 45.20it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 45.22it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.15it/s][A
 19%|█▊        | 203/1083 [00:04<00:20, 42.94it/s][A
 19%|█▉        | 208/1083 [00:04<00:20, 43.64it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 44.03it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.37it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.64it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.73it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.88it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.89it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.70it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.74it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 44.72it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.94it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.08it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 45.12it/s][A
 25%|██▌       | 273/1083 [00:06<00:17, 45.15it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 45.13it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 45.06it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.88it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.95it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.84it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 44.95it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.96it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 45.16it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.17it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.17it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.04it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.95it/s][A
 31%|███       | 338/1083 [00:07<00:17, 42.97it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 43.62it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 44.08it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.50it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 44.72it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.94it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 45.03it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.92it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 44.77it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 44.82it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 44.95it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.99it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 45.04it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 45.04it/s][A
 38%|███▊      | 408/1083 [00:09<00:14, 45.12it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 45.13it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.98it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.86it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.87it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.93it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.00it/s][A
 41%|████      | 443/1083 [00:09<00:14, 45.16it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 45.21it/s][A
 42%|████▏     | 453/1083 [00:10<00:13, 45.17it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.20it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.00it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.95it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 44.76it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 44.81it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 44.98it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 45.03it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 45.16it/s][A
 46%|████▌     | 498/1083 [00:11<00:12, 45.15it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 45.16it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 45.03it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 44.90it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 44.91it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 44.90it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 44.98it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 45.09it/s][A
 50%|████▉     | 538/1083 [00:11<00:12, 45.18it/s][A
 50%|█████     | 543/1083 [00:12<00:11, 45.14it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 45.15it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 44.94it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.92it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.79it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.83it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 45.00it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 45.10it/s][A
 54%|█████▍    | 583/1083 [00:12<00:11, 45.20it/s][A
 54%|█████▍    | 588/1083 [00:13<00:10, 45.19it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 45.14it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.01it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.89it/s][A
 56%|█████▌    | 608/1083 [00:13<00:11, 42.32it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 43.11it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 43.79it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 44.23it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 44.53it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.79it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.89it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.92it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.73it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 44.64it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 44.80it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 44.94it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 44.98it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 45.09it/s][A
 63%|██████▎   | 678/1083 [00:15<00:08, 45.14it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 45.08it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 45.03it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.87it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.82it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.77it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.88it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.07it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 45.17it/s][A
 67%|██████▋   | 723/1083 [00:16<00:07, 45.19it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.15it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.98it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.88it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 43.12it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 43.74it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.21it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 44.51it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.73it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.95it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 45.06it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.02it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.79it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 44.73it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 44.78it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.91it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 45.12it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 45.09it/s][A
 75%|███████▌  | 813/1083 [00:18<00:05, 45.19it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 45.12it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.96it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.81it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.64it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.83it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.88it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 45.09it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 45.17it/s][A
 79%|███████▉  | 858/1083 [00:19<00:04, 45.17it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.13it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.98it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.80it/s][A
 81%|████████  | 878/1083 [00:19<00:05, 40.29it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 41.67it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 42.72it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 43.50it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.04it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.39it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 44.68it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 44.86it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 44.52it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 44.55it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 44.72it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 44.88it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 45.07it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 45.14it/s][A
 88%|████████▊ | 948/1083 [00:21<00:02, 45.13it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 45.11it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.97it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.76it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.68it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.69it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 44.83it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 44.97it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.83it/s][A
 92%|█████████▏| 993/1083 [00:22<00:01, 45.38it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 45.26it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 45.05it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.83it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 37.68it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 39.70it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 41.27it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 42.39it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 43.27it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 43.88it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 44.30it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 44.51it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 44.33it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 44.42it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 44.65it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.86it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 44.98it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 45.10it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 45.17it/s][A                                                
                                                   [A 20%|██        | 78/390 [00:47<01:30,  3.45it/s]
100%|██████████| 1083/1083 [00:24<00:00, 45.17it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 14:31:33,871 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 14:31:34,606 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:31:38,631 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:31:39,066 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:31:39,258 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:03<1:03:56, 12.34s/it] 21%|██        | 80/390 [01:03<45:09,  8.74s/it]   21%|██        | 81/390 [01:03<31:57,  6.21s/it] 21%|██        | 82/390 [01:04<22:44,  4.43s/it] 21%|██▏       | 83/390 [01:04<16:19,  3.19s/it] 22%|██▏       | 84/390 [01:04<11:50,  2.32s/it] 22%|██▏       | 85/390 [01:05<08:42,  1.71s/it] 22%|██▏       | 86/390 [01:05<06:31,  1.29s/it] 22%|██▏       | 87/390 [01:05<04:59,  1.01it/s] 23%|██▎       | 88/390 [01:05<03:55,  1.28it/s] 23%|██▎       | 89/390 [01:06<03:10,  1.58it/s] 23%|██▎       | 90/390 [01:06<02:39,  1.88it/s] 23%|██▎       | 91/390 [01:06<02:19,  2.14it/s] 24%|██▎       | 92/390 [01:07<02:03,  2.41it/s] 24%|██▍       | 93/390 [01:07<01:52,  2.65it/s] 24%|██▍       | 94/390 [01:07<01:44,  2.84it/s] 24%|██▍       | 95/390 [01:08<01:38,  2.99it/s] 25%|██▍       | 96/390 [01:08<01:34,  3.11it/s] 25%|██▍       | 97/390 [01:08<01:31,  3.19it/s] 25%|██▌       | 98/390 [01:08<01:29,  3.26it/s] 25%|██▌       | 99/390 [01:09<01:28,  3.31it/s] 26%|██▌       | 100/390 [01:09<01:26,  3.34it/s] 26%|██▌       | 101/390 [01:09<01:25,  3.36it/s] 26%|██▌       | 102/390 [01:10<01:27,  3.29it/s] 26%|██▋       | 103/390 [01:10<01:26,  3.33it/s] 27%|██▋       | 104/390 [01:10<01:25,  3.35it/s] 27%|██▋       | 105/390 [01:11<01:24,  3.37it/s] 27%|██▋       | 106/390 [01:11<01:23,  3.38it/s] 27%|██▋       | 107/390 [01:11<01:23,  3.39it/s] 28%|██▊       | 108/390 [01:11<01:22,  3.40it/s] 28%|██▊       | 109/390 [01:12<01:22,  3.40it/s] 28%|██▊       | 110/390 [01:12<01:22,  3.41it/s] 28%|██▊       | 111/390 [01:12<01:21,  3.41it/s] 29%|██▊       | 112/390 [01:13<01:21,  3.41it/s] 29%|██▉       | 113/390 [01:13<01:23,  3.30it/s] 29%|██▉       | 114/390 [01:13<01:22,  3.33it/s] 29%|██▉       | 115/390 [01:13<01:21,  3.36it/s] 30%|██▉       | 116/390 [01:14<01:21,  3.38it/s] 30%|███       | 117/390 [01:14<01:20,  3.39it/s] 30%|███       | 118/390 [01:14<01:20,  3.40it/s] 31%|███       | 119/390 [01:15<01:19,  3.40it/s] 31%|███       | 120/390 [01:15<01:19,  3.41it/s] 31%|███       | 121/390 [01:15<01:18,  3.41it/s] 31%|███▏      | 122/390 [01:16<01:18,  3.41it/s] 32%|███▏      | 123/390 [01:16<01:18,  3.41it/s] 32%|███▏      | 124/390 [01:16<01:21,  3.26it/s] 32%|███▏      | 125/390 [01:16<01:20,  3.31it/s] 32%|███▏      | 126/390 [01:17<01:19,  3.34it/s] 33%|███▎      | 127/390 [01:17<01:18,  3.37it/s] 33%|███▎      | 128/390 [01:17<01:17,  3.38it/s] 33%|███▎      | 129/390 [01:18<01:17,  3.39it/s] 33%|███▎      | 130/390 [01:18<01:16,  3.40it/s] 34%|███▎      | 131/390 [01:18<01:16,  3.40it/s] 34%|███▍      | 132/390 [01:18<01:15,  3.41it/s] 34%|███▍      | 133/390 [01:19<01:15,  3.41it/s] 34%|███▍      | 134/390 [01:19<01:15,  3.41it/s] 35%|███▍      | 135/390 [01:19<01:18,  3.26it/s] 35%|███▍      | 136/390 [01:20<01:16,  3.31it/s] 35%|███▌      | 137/390 [01:20<01:15,  3.34it/s] 35%|███▌      | 138/390 [01:20<01:14,  3.36it/s] 36%|███▌      | 139/390 [01:21<01:14,  3.38it/s] 36%|███▌      | 140/390 [01:21<01:13,  3.39it/s] 36%|███▌      | 141/390 [01:21<01:13,  3.40it/s] 36%|███▋      | 142/390 [01:21<01:12,  3.41it/s] 37%|███▋      | 143/390 [01:22<01:12,  3.41it/s] 37%|███▋      | 144/390 [01:22<01:12,  3.41it/s] 37%|███▋      | 145/390 [01:22<01:11,  3.41it/s] 37%|███▋      | 146/390 [01:23<01:11,  3.41it/s] 38%|███▊      | 147/390 [01:23<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:23<01:10,  3.41it/s] 38%|███▊      | 149/390 [01:24<01:10,  3.41it/s] 38%|███▊      | 150/390 [01:24<01:10,  3.41it/s] 39%|███▊      | 151/390 [01:24<01:10,  3.41it/s] 39%|███▉      | 152/390 [01:24<01:09,  3.41it/s] 39%|███▉      | 153/390 [01:25<01:09,  3.41it/s] 39%|███▉      | 154/390 [01:25<01:09,  3.41it/s] 40%|███▉      | 155/390 [01:25<01:08,  3.41it/s] 40%|████      | 156/390 [01:26<01:10,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 14:32:12,569 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:32:12,569 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 14:32:12,569 >>   Batch size = 8
{'eval_loss': 0.8896969556808472, 'eval_runtime': 24.2512, 'eval_samples_per_second': 357.013, 'eval_steps_per_second': 44.658, 'epoch': 0.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.21it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.46it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.73it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.91it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.25it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.45it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.02it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.74it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 44.89it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.11it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.17it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.26it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.38it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.40it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.19it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.89it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.69it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.79it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.96it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.11it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.11it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.19it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.30it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.21it/s][A
 12%|█▏        | 127/1083 [00:02<00:23, 39.86it/s][A
 12%|█▏        | 132/1083 [00:02<00:22, 41.44it/s][A
 13%|█▎        | 137/1083 [00:03<00:22, 42.64it/s][A
 13%|█▎        | 142/1083 [00:03<00:21, 43.52it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 44.17it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.47it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.77it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.86it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.54it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.36it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.58it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.82it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 45.00it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.03it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.13it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.24it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.23it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.11it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.93it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.92it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.97it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 45.12it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.08it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.14it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.17it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.07it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.06it/s][A
 24%|██▍       | 262/1083 [00:05<00:19, 41.05it/s][A
 25%|██▍       | 267/1083 [00:05<00:19, 42.39it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 43.27it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 43.93it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.39it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.70it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.81it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.80it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.45it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.51it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.67it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.85it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.99it/s][A
 30%|███       | 327/1083 [00:07<00:16, 45.07it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.17it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.19it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.19it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.06it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.03it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.96it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 45.01it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 45.06it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 45.13it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.12it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.11it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.04it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.95it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 43.72it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.17it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.48it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 44.68it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.92it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.08it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.10it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.00it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.77it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.70it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.85it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.95it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.02it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.10it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.26it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.26it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.09it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.01it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.90it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 45.02it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 45.02it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.03it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.10it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.11it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.13it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.93it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.92it/s][A
 49%|████▉     | 532/1083 [00:11<00:13, 42.18it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 43.24it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 43.83it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 44.26it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.58it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.78it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.77it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.78it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.58it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.52it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.66it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.78it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.01it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 45.10it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.28it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.20it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.02it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 44.86it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.72it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 44.91it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.90it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.92it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 45.03it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.19it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.22it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.07it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.88it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 43.25it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 43.86it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.28it/s][A
 63%|██████▎   | 682/1083 [00:15<00:09, 44.37it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.63it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.89it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.97it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.92it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.69it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.68it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 44.86it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.92it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.95it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 45.03it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.10it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.06it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.93it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.84it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.80it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.86it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.91it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 45.02it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 45.14it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.25it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.15it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.01it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.84it/s][A
 74%|███████▍  | 802/1083 [00:17<00:07, 39.74it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 41.34it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 42.56it/s][A
 75%|███████▌  | 817/1083 [00:18<00:06, 43.42it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.08it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.55it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.79it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.72it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.46it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.32it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 44.37it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.70it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.89it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 45.09it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.27it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.34it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.22it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.79it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.65it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.59it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.75it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.91it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 45.08it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.16it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.17it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.25it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.14it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 37.98it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 40.00it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 41.50it/s][A
 88%|████████▊ | 952/1083 [00:21<00:03, 42.69it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 43.55it/s][A
 89%|████████▉ | 962/1083 [00:21<00:03, 38.07it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 40.15it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 41.44it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 42.38it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 43.21it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 43.87it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.34it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.74it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.55it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.37it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.61it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.65it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.84it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 44.91it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 45.07it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 45.09it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 45.23it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 45.02it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.81it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.85it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.83it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 39.81it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 41.33it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 42.58it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 43.46it/s][A                                                 
                                                   [A 40%|████      | 156/390 [01:50<01:10,  3.31it/s]
100%|██████████| 1083/1083 [00:24<00:00, 43.46it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 14:32:37,486 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 14:32:37,890 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:32:42,722 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:32:42,889 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:32:43,004 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:09<51:22, 13.23s/it] 41%|████      | 158/390 [02:09<36:11,  9.36s/it] 41%|████      | 159/390 [02:10<25:33,  6.64s/it] 41%|████      | 160/390 [02:10<18:08,  4.73s/it] 41%|████▏     | 161/390 [02:10<12:58,  3.40s/it] 42%|████▏     | 162/390 [02:10<09:22,  2.47s/it] 42%|████▏     | 163/390 [02:11<06:51,  1.81s/it] 42%|████▏     | 164/390 [02:11<05:06,  1.36s/it] 42%|████▏     | 165/390 [02:11<03:52,  1.04s/it] 43%|████▎     | 166/390 [02:12<03:01,  1.23it/s] 43%|████▎     | 167/390 [02:12<02:25,  1.53it/s] 43%|████▎     | 168/390 [02:12<02:00,  1.84it/s] 43%|████▎     | 169/390 [02:13<01:46,  2.08it/s] 44%|████▎     | 170/390 [02:13<01:33,  2.37it/s] 44%|████▍     | 171/390 [02:13<01:28,  2.47it/s] 44%|████▍     | 172/390 [02:13<01:20,  2.70it/s] 44%|████▍     | 173/390 [02:14<01:14,  2.90it/s] 45%|████▍     | 174/390 [02:14<01:10,  3.05it/s] 45%|████▍     | 175/390 [02:14<01:07,  3.16it/s] 45%|████▌     | 176/390 [02:15<01:05,  3.25it/s] 45%|████▌     | 177/390 [02:15<01:04,  3.31it/s] 46%|████▌     | 178/390 [02:15<01:03,  3.36it/s] 46%|████▌     | 179/390 [02:16<01:05,  3.24it/s] 46%|████▌     | 180/390 [02:16<01:03,  3.31it/s] 46%|████▋     | 181/390 [02:16<01:02,  3.35it/s] 47%|████▋     | 182/390 [02:16<01:01,  3.39it/s] 47%|████▋     | 183/390 [02:17<01:00,  3.41it/s] 47%|████▋     | 184/390 [02:17<01:00,  3.43it/s] 47%|████▋     | 185/390 [02:17<00:59,  3.44it/s] 48%|████▊     | 186/390 [02:18<00:59,  3.45it/s] 48%|████▊     | 187/390 [02:18<00:58,  3.45it/s] 48%|████▊     | 188/390 [02:18<00:58,  3.46it/s] 48%|████▊     | 189/390 [02:18<00:58,  3.46it/s] 49%|████▊     | 190/390 [02:19<01:01,  3.26it/s] 49%|████▉     | 191/390 [02:19<01:00,  3.31it/s] 49%|████▉     | 192/390 [02:19<00:58,  3.36it/s] 49%|████▉     | 193/390 [02:20<00:58,  3.39it/s] 50%|████▉     | 194/390 [02:20<00:57,  3.41it/s] 50%|█████     | 195/390 [02:20<00:56,  3.43it/s] 50%|█████     | 196/390 [02:20<00:56,  3.44it/s] 51%|█████     | 197/390 [02:21<00:55,  3.45it/s] 51%|█████     | 198/390 [02:21<00:55,  3.45it/s] 51%|█████     | 199/390 [02:21<00:55,  3.46it/s] 51%|█████▏    | 200/390 [02:22<00:54,  3.46it/s] 52%|█████▏    | 201/390 [02:22<00:56,  3.37it/s] 52%|█████▏    | 202/390 [02:22<00:55,  3.40it/s] 52%|█████▏    | 203/390 [02:23<00:54,  3.42it/s] 52%|█████▏    | 204/390 [02:23<00:54,  3.43it/s] 53%|█████▎    | 205/390 [02:23<00:53,  3.44it/s] 53%|█████▎    | 206/390 [02:23<00:53,  3.45it/s] 53%|█████▎    | 207/390 [02:24<00:53,  3.45it/s] 53%|█████▎    | 208/390 [02:24<00:52,  3.46it/s] 54%|█████▎    | 209/390 [02:24<00:52,  3.46it/s] 54%|█████▍    | 210/390 [02:25<00:51,  3.46it/s] 54%|█████▍    | 211/390 [02:25<00:51,  3.46it/s] 54%|█████▍    | 212/390 [02:25<00:51,  3.46it/s] 55%|█████▍    | 213/390 [02:25<00:51,  3.46it/s] 55%|█████▍    | 214/390 [02:26<00:50,  3.46it/s] 55%|█████▌    | 215/390 [02:26<00:50,  3.46it/s] 55%|█████▌    | 216/390 [02:26<00:50,  3.47it/s] 56%|█████▌    | 217/390 [02:27<00:49,  3.47it/s] 56%|█████▌    | 218/390 [02:27<00:49,  3.46it/s] 56%|█████▌    | 219/390 [02:27<00:51,  3.33it/s] 56%|█████▋    | 220/390 [02:27<00:50,  3.37it/s] 57%|█████▋    | 221/390 [02:28<00:49,  3.40it/s] 57%|█████▋    | 222/390 [02:28<00:49,  3.42it/s] 57%|█████▋    | 223/390 [02:28<00:48,  3.43it/s] 57%|█████▋    | 224/390 [02:29<00:48,  3.44it/s] 58%|█████▊    | 225/390 [02:29<00:47,  3.45it/s] 58%|█████▊    | 226/390 [02:29<00:47,  3.45it/s] 58%|█████▊    | 227/390 [02:30<00:47,  3.46it/s] 58%|█████▊    | 228/390 [02:30<00:46,  3.46it/s] 59%|█████▊    | 229/390 [02:30<00:46,  3.46it/s] 59%|█████▉    | 230/390 [02:30<00:48,  3.32it/s] 59%|█████▉    | 231/390 [02:31<00:47,  3.36it/s] 59%|█████▉    | 232/390 [02:31<00:46,  3.39it/s] 60%|█████▉    | 233/390 [02:31<00:45,  3.41it/s] 60%|██████    | 234/390 [02:32<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 14:33:18,545 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:33:18,545 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 14:33:18,545 >>   Batch size = 8
{'eval_loss': 0.8963579535484314, 'eval_runtime': 24.3751, 'eval_samples_per_second': 355.199, 'eval_steps_per_second': 44.431, 'epoch': 1.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.71it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.12it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.18it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.25it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.72it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.51it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.25it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.10it/s][A
  4%|▍         | 47/1083 [00:01<00:22, 45.22it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.38it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.47it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.30it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.18it/s][A
  7%|▋         | 72/1083 [00:01<00:23, 42.68it/s][A
  7%|▋         | 77/1083 [00:01<00:23, 43.44it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 43.91it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.26it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.50it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.86it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.09it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.12it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.81it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.81it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.84it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.81it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.90it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 45.01it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 45.23it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.37it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.14it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.03it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.96it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.92it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.95it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.91it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.89it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 45.08it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.20it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.20it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.14it/s][A
 19%|█▉        | 207/1083 [00:04<00:20, 43.35it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 43.91it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.21it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.49it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.67it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.85it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.01it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.08it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.95it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.74it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.97it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.95it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.81it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 45.02it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 45.09it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.24it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.08it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.05it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.91it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.00it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.94it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.98it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 45.05it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 45.06it/s][A
 30%|███       | 327/1083 [00:07<00:16, 45.19it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.11it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.04it/s][A
 32%|███▏      | 342/1083 [00:07<00:17, 42.45it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 43.32it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 43.91it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.35it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.55it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.72it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.84it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.90it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.65it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.64it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.80it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.97it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.15it/s][A
 38%|███▊      | 407/1083 [00:09<00:14, 45.23it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.20it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 45.20it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.00it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.86it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.83it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.95it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.07it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.10it/s][A
 42%|████▏     | 452/1083 [00:10<00:13, 45.16it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 45.12it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 45.15it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 45.04it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.85it/s][A
 44%|████▍     | 477/1083 [00:10<00:14, 42.77it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 43.56it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.07it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.45it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.57it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.97it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.09it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.06it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.77it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.69it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.84it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.96it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 45.12it/s][A
 50%|█████     | 542/1083 [00:12<00:11, 45.09it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 45.27it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.29it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.07it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.90it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 44.76it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.89it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.90it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 45.04it/s][A
 54%|█████▍    | 587/1083 [00:13<00:10, 45.12it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.20it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 45.30it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.19it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.06it/s][A
 57%|█████▋    | 612/1083 [00:13<00:12, 37.82it/s][A
 57%|█████▋    | 617/1083 [00:13<00:11, 39.89it/s][A
 57%|█████▋    | 622/1083 [00:13<00:11, 41.34it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 42.38it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 43.32it/s][A
 59%|█████▉    | 637/1083 [00:14<00:10, 43.90it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.33it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.53it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.37it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.48it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.68it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.89it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 45.06it/s][A
 63%|██████▎   | 677/1083 [00:15<00:08, 45.14it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 45.03it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.15it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.99it/s][A
 64%|██████▍   | 697/1083 [00:15<00:10, 38.58it/s][A
 65%|██████▍   | 702/1083 [00:15<00:09, 40.48it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 41.94it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 42.93it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 43.77it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.33it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.60it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.91it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.72it/s][A
 69%|██████▊   | 742/1083 [00:16<00:08, 39.59it/s][A
 69%|██████▉   | 747/1083 [00:16<00:08, 41.14it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 42.32it/s][A
 70%|██████▉   | 757/1083 [00:17<00:07, 43.16it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 43.85it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.26it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.61it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.74it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.52it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.61it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.76it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 44.90it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 45.03it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 45.10it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 45.11it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 45.13it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.05it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.81it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.80it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.73it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.89it/s][A
 78%|███████▊  | 847/1083 [00:19<00:05, 44.94it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 45.08it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 45.16it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 45.22it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 45.17it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.00it/s][A
 81%|████████  | 877/1083 [00:19<00:05, 38.59it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 40.42it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 41.85it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 42.78it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 43.54it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.09it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.46it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.70it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.40it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 44.33it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 44.64it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.80it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 44.98it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 45.14it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 45.18it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 45.20it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 45.03it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.83it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.67it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.78it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.89it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 45.09it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 45.21it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 45.24it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.02it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.95it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.76it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:02, 30.79it/s][A
 94%|█████████▍| 1017/1083 [00:23<00:01, 34.14it/s][A
 94%|█████████▍| 1022/1083 [00:23<00:01, 36.94it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 39.08it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 40.80it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 42.17it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 43.14it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 43.76it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 43.76it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 43.80it/s][A
 98%|█████████▊| 1062/1083 [00:24<00:00, 44.08it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 44.45it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.71it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.86it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.03it/s][A                                                 
                                                   [A 60%|██████    | 234/390 [02:56<00:45,  3.43it/s]
100%|██████████| 1083/1083 [00:24<00:00, 45.03it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 14:33:43,291 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:33:43,523 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:33:47,608 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:33:47,771 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:33:47,901 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:11<31:00, 12.01s/it] 61%|██████    | 236/390 [03:11<21:48,  8.50s/it] 61%|██████    | 237/390 [03:12<15:23,  6.04s/it] 61%|██████    | 238/390 [03:12<10:55,  4.31s/it] 61%|██████▏   | 239/390 [03:12<07:49,  3.11s/it] 62%|██████▏   | 240/390 [03:12<05:39,  2.26s/it] 62%|██████▏   | 241/390 [03:13<04:09,  1.67s/it] 62%|██████▏   | 242/390 [03:13<03:06,  1.26s/it] 62%|██████▏   | 243/390 [03:13<02:22,  1.03it/s] 63%|██████▎   | 244/390 [03:14<01:51,  1.31it/s] 63%|██████▎   | 245/390 [03:14<01:30,  1.60it/s] 63%|██████▎   | 246/390 [03:14<01:15,  1.91it/s] 63%|██████▎   | 247/390 [03:14<01:06,  2.14it/s] 64%|██████▎   | 248/390 [03:15<00:58,  2.41it/s] 64%|██████▍   | 249/390 [03:15<00:53,  2.65it/s] 64%|██████▍   | 250/390 [03:15<00:49,  2.84it/s] 64%|██████▍   | 251/390 [03:16<00:46,  2.99it/s] 65%|██████▍   | 252/390 [03:16<00:44,  3.11it/s] 65%|██████▍   | 253/390 [03:16<00:42,  3.19it/s] 65%|██████▌   | 254/390 [03:17<00:41,  3.27it/s] 65%|██████▌   | 255/390 [03:17<00:40,  3.33it/s] 66%|██████▌   | 256/390 [03:17<00:39,  3.37it/s] 66%|██████▌   | 257/390 [03:17<00:39,  3.40it/s] 66%|██████▌   | 258/390 [03:18<00:39,  3.31it/s] 66%|██████▋   | 259/390 [03:18<00:39,  3.35it/s] 67%|██████▋   | 260/390 [03:18<00:38,  3.36it/s] 67%|██████▋   | 261/390 [03:19<00:38,  3.38it/s] 67%|██████▋   | 262/390 [03:19<00:37,  3.39it/s] 67%|██████▋   | 263/390 [03:19<00:37,  3.40it/s] 68%|██████▊   | 264/390 [03:19<00:37,  3.40it/s] 68%|██████▊   | 265/390 [03:20<00:36,  3.41it/s] 68%|██████▊   | 266/390 [03:20<00:36,  3.41it/s] 68%|██████▊   | 267/390 [03:20<00:36,  3.41it/s] 69%|██████▊   | 268/390 [03:21<00:35,  3.41it/s] 69%|██████▉   | 269/390 [03:21<00:36,  3.35it/s] 69%|██████▉   | 270/390 [03:21<00:35,  3.37it/s] 69%|██████▉   | 271/390 [03:22<00:35,  3.38it/s] 70%|██████▉   | 272/390 [03:22<00:34,  3.39it/s] 70%|███████   | 273/390 [03:22<00:34,  3.40it/s] 70%|███████   | 274/390 [03:22<00:34,  3.40it/s] 71%|███████   | 275/390 [03:23<00:33,  3.41it/s] 71%|███████   | 276/390 [03:23<00:33,  3.41it/s] 71%|███████   | 277/390 [03:23<00:33,  3.41it/s] 71%|███████▏  | 278/390 [03:24<00:32,  3.41it/s] 72%|███████▏  | 279/390 [03:24<00:32,  3.41it/s] 72%|███████▏  | 280/390 [03:24<00:33,  3.26it/s] 72%|███████▏  | 281/390 [03:25<00:32,  3.31it/s] 72%|███████▏  | 282/390 [03:25<00:32,  3.34it/s] 73%|███████▎  | 283/390 [03:25<00:31,  3.36it/s] 73%|███████▎  | 284/390 [03:25<00:31,  3.38it/s] 73%|███████▎  | 285/390 [03:26<00:30,  3.39it/s] 73%|███████▎  | 286/390 [03:26<00:30,  3.39it/s] 74%|███████▎  | 287/390 [03:26<00:30,  3.40it/s] 74%|███████▍  | 288/390 [03:27<00:29,  3.41it/s] 74%|███████▍  | 289/390 [03:27<00:29,  3.41it/s] 74%|███████▍  | 290/390 [03:27<00:29,  3.41it/s] 75%|███████▍  | 291/390 [03:27<00:29,  3.41it/s] 75%|███████▍  | 292/390 [03:28<00:28,  3.41it/s] 75%|███████▌  | 293/390 [03:28<00:28,  3.42it/s] 75%|███████▌  | 294/390 [03:28<00:28,  3.41it/s] 76%|███████▌  | 295/390 [03:29<00:29,  3.27it/s] 76%|███████▌  | 296/390 [03:29<00:28,  3.31it/s] 76%|███████▌  | 297/390 [03:29<00:27,  3.34it/s] 76%|███████▋  | 298/390 [03:30<00:27,  3.36it/s] 77%|███████▋  | 299/390 [03:30<00:26,  3.38it/s] 77%|███████▋  | 300/390 [03:30<00:26,  3.39it/s] 77%|███████▋  | 301/390 [03:30<00:26,  3.40it/s] 77%|███████▋  | 302/390 [03:31<00:25,  3.40it/s] 78%|███████▊  | 303/390 [03:31<00:25,  3.40it/s] 78%|███████▊  | 304/390 [03:31<00:25,  3.40it/s] 78%|███████▊  | 305/390 [03:32<00:24,  3.41it/s] 78%|███████▊  | 306/390 [03:32<00:25,  3.33it/s] 79%|███████▊  | 307/390 [03:32<00:24,  3.35it/s] 79%|███████▉  | 308/390 [03:32<00:24,  3.37it/s] 79%|███████▉  | 309/390 [03:33<00:23,  3.39it/s] 79%|███████▉  | 310/390 [03:33<00:23,  3.39it/s] 80%|███████▉  | 311/390 [03:33<00:23,  3.40it/s] 80%|████████  | 312/390 [03:34<00:22,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:34:20,634 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:34:20,634 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 14:34:20,634 >>   Batch size = 8
{'eval_loss': 0.9112064242362976, 'eval_runtime': 24.5015, 'eval_samples_per_second': 353.366, 'eval_steps_per_second': 44.201, 'epoch': 2.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.17it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.10it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.21it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.39it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.73it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.48it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.26it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.98it/s][A
  4%|▍         | 47/1083 [00:01<00:24, 43.08it/s][A
  5%|▍         | 52/1083 [00:01<00:23, 43.76it/s][A
  5%|▌         | 57/1083 [00:01<00:23, 44.28it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 44.61it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 44.85it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.02it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.11it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 45.02it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.83it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.80it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.04it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.04it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.13it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.18it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.20it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.18it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.03it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.88it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.82it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.89it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.02it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.09it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.15it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.10it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.17it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.99it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.83it/s][A
 17%|█▋        | 182/1083 [00:04<00:21, 42.16it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 43.10it/s][A
 18%|█▊        | 192/1083 [00:04<00:20, 43.88it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 44.41it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 44.64it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 44.83it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 44.88it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.80it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.50it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.59it/s][A
 21%|██▏       | 232/1083 [00:05<00:19, 44.73it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.00it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.23it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.31it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.22it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.15it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.91it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.82it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.75it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.78it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 45.02it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.26it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.30it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.32it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.00it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.77it/s][A
 29%|██▉       | 317/1083 [00:07<00:18, 42.06it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 43.02it/s][A
 30%|███       | 327/1083 [00:07<00:17, 43.75it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.22it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.57it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.85it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.02it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.06it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 44.78it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.63it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.73it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.94it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.08it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.13it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.23it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.28it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.26it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.08it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.93it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.89it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.99it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.08it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.91it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.09it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.16it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.11it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.99it/s][A
 42%|████▏     | 452/1083 [00:10<00:15, 41.97it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 43.05it/s][A
 43%|████▎     | 462/1083 [00:10<00:14, 43.72it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.13it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.50it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 44.68it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.84it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.85it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.55it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.53it/s][A
 46%|████▋     | 502/1083 [00:11<00:13, 44.69it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.95it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.08it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.24it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.16it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.21it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.95it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.72it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.65it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.69it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.94it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.09it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.24it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.29it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.21it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.00it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 44.84it/s][A
 54%|█████▍    | 587/1083 [00:13<00:13, 36.51it/s][A
 55%|█████▍    | 592/1083 [00:13<00:12, 38.82it/s][A
 55%|█████▌    | 597/1083 [00:13<00:11, 40.58it/s][A
 56%|█████▌    | 602/1083 [00:13<00:12, 37.32it/s][A
 56%|█████▌    | 607/1083 [00:13<00:12, 39.44it/s][A
 57%|█████▋    | 612/1083 [00:13<00:11, 41.02it/s][A
 57%|█████▋    | 617/1083 [00:13<00:11, 42.27it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 43.15it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 43.77it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.17it/s][A
 59%|█████▉    | 637/1083 [00:14<00:10, 44.57it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.52it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 44.51it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 44.60it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 44.75it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 44.96it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 45.11it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 45.20it/s][A
 63%|██████▎   | 677/1083 [00:15<00:08, 45.17it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 45.15it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.05it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.98it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.95it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.93it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 44.99it/s][A
 66%|██████▌   | 712/1083 [00:16<00:08, 45.14it/s][A
 66%|██████▌   | 717/1083 [00:16<00:09, 38.71it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 40.52it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 41.89it/s][A
 68%|██████▊   | 732/1083 [00:16<00:08, 42.98it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 43.71it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.19it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.63it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.73it/s][A
 70%|██████▉   | 757/1083 [00:17<00:07, 44.53it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 44.30it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.38it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.63it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 44.90it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.04it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.03it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.20it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.28it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 45.18it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.92it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.85it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.82it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.04it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.04it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.11it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.11it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.12it/s][A
 78%|███████▊  | 847/1083 [00:19<00:05, 45.07it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 40.70it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 42.09it/s][A
 80%|███████▉  | 862/1083 [00:19<00:05, 43.11it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 43.76it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.24it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 44.62it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 44.81it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.76it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 44.43it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 44.37it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.57it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.77it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.93it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.03it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.23it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.38it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.25it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 44.85it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 44.63it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.65it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.75it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.94it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.00it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 45.24it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.35it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 45.27it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 44.91it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 37.49it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 39.53it/s][A
 92%|█████████▏| 997/1083 [00:22<00:02, 41.15it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 42.25it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 43.20it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 43.78it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.27it/s][A
 94%|█████████▍| 1022/1083 [00:23<00:01, 44.51it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 44.36it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 44.48it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.56it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.77it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.75it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 45.10it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.14it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 45.10it/s][A
 99%|█████████▊| 1067/1083 [00:24<00:00, 44.94it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.82it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.75it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.69it/s][A                                                 
                                                   [A 80%|████████  | 312/390 [03:58<00:22,  3.41it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.69it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 14:34:45,336 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 14:34:45,534 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:34:49,660 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:34:49,917 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:34:50,023 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [04:14<15:51, 12.36s/it] 81%|████████  | 314/390 [04:14<11:04,  8.75s/it] 81%|████████  | 315/390 [04:15<07:45,  6.21s/it] 81%|████████  | 316/390 [04:15<05:28,  4.44s/it] 81%|████████▏ | 317/390 [04:15<03:53,  3.19s/it] 82%|████████▏ | 318/390 [04:16<02:47,  2.32s/it] 82%|████████▏ | 319/390 [04:16<02:01,  1.71s/it] 82%|████████▏ | 320/390 [04:16<01:30,  1.29s/it] 82%|████████▏ | 321/390 [04:17<01:08,  1.01it/s] 83%|████████▎ | 322/390 [04:17<00:52,  1.29it/s] 83%|████████▎ | 323/390 [04:17<00:42,  1.59it/s] 83%|████████▎ | 324/390 [04:17<00:34,  1.89it/s] 83%|████████▎ | 325/390 [04:18<00:30,  2.10it/s] 84%|████████▎ | 326/390 [04:18<00:26,  2.38it/s] 84%|████████▍ | 327/390 [04:18<00:24,  2.62it/s] 84%|████████▍ | 328/390 [04:19<00:22,  2.81it/s] 84%|████████▍ | 329/390 [04:19<00:20,  2.97it/s] 85%|████████▍ | 330/390 [04:19<00:19,  3.09it/s] 85%|████████▍ | 331/390 [04:20<00:18,  3.18it/s] 85%|████████▌ | 332/390 [04:20<00:17,  3.25it/s] 85%|████████▌ | 333/390 [04:20<00:17,  3.30it/s] 86%|████████▌ | 334/390 [04:20<00:16,  3.33it/s] 86%|████████▌ | 335/390 [04:21<00:16,  3.36it/s] 86%|████████▌ | 336/390 [04:21<00:16,  3.18it/s] 86%|████████▋ | 337/390 [04:21<00:16,  3.25it/s] 87%|████████▋ | 338/390 [04:22<00:15,  3.30it/s] 87%|████████▋ | 339/390 [04:22<00:15,  3.33it/s] 87%|████████▋ | 340/390 [04:22<00:14,  3.36it/s] 87%|████████▋ | 341/390 [04:22<00:14,  3.37it/s] 88%|████████▊ | 342/390 [04:23<00:14,  3.39it/s] 88%|████████▊ | 343/390 [04:23<00:13,  3.40it/s] 88%|████████▊ | 344/390 [04:23<00:13,  3.40it/s] 88%|████████▊ | 345/390 [04:24<00:13,  3.41it/s] 89%|████████▊ | 346/390 [04:24<00:12,  3.41it/s] 89%|████████▉ | 347/390 [04:24<00:13,  3.28it/s] 89%|████████▉ | 348/390 [04:25<00:12,  3.32it/s] 89%|████████▉ | 349/390 [04:25<00:12,  3.35it/s] 90%|████████▉ | 350/390 [04:25<00:11,  3.37it/s] 90%|█████████ | 351/390 [04:25<00:11,  3.38it/s] 90%|█████████ | 352/390 [04:26<00:11,  3.39it/s] 91%|█████████ | 353/390 [04:26<00:10,  3.40it/s] 91%|█████████ | 354/390 [04:26<00:10,  3.40it/s] 91%|█████████ | 355/390 [04:27<00:10,  3.41it/s] 91%|█████████▏| 356/390 [04:27<00:09,  3.41it/s] 92%|█████████▏| 357/390 [04:27<00:09,  3.41it/s] 92%|█████████▏| 358/390 [04:28<00:09,  3.41it/s] 92%|█████████▏| 359/390 [04:28<00:09,  3.41it/s] 92%|█████████▏| 360/390 [04:28<00:08,  3.42it/s] 93%|█████████▎| 361/390 [04:28<00:08,  3.42it/s] 93%|█████████▎| 362/390 [04:29<00:08,  3.42it/s] 93%|█████████▎| 363/390 [04:29<00:07,  3.42it/s] 93%|█████████▎| 364/390 [04:29<00:07,  3.42it/s] 94%|█████████▎| 365/390 [04:30<00:07,  3.42it/s] 94%|█████████▍| 366/390 [04:30<00:07,  3.26it/s] 94%|█████████▍| 367/390 [04:30<00:06,  3.31it/s] 94%|█████████▍| 368/390 [04:30<00:06,  3.34it/s] 95%|█████████▍| 369/390 [04:31<00:06,  3.36it/s] 95%|█████████▍| 370/390 [04:31<00:05,  3.38it/s] 95%|█████████▌| 371/390 [04:31<00:05,  3.39it/s] 95%|█████████▌| 372/390 [04:32<00:05,  3.40it/s] 96%|█████████▌| 373/390 [04:32<00:04,  3.41it/s] 96%|█████████▌| 374/390 [04:32<00:04,  3.42it/s] 96%|█████████▌| 375/390 [04:33<00:04,  3.44it/s] 96%|█████████▋| 376/390 [04:33<00:04,  3.45it/s] 97%|█████████▋| 377/390 [04:33<00:03,  3.31it/s] 97%|█████████▋| 378/390 [04:33<00:03,  3.36it/s] 97%|█████████▋| 379/390 [04:34<00:03,  3.39it/s] 97%|█████████▋| 380/390 [04:34<00:02,  3.41it/s] 98%|█████████▊| 381/390 [04:34<00:02,  3.43it/s] 98%|█████████▊| 382/390 [04:35<00:02,  3.44it/s] 98%|█████████▊| 383/390 [04:35<00:02,  3.45it/s] 98%|█████████▊| 384/390 [04:35<00:01,  3.45it/s] 99%|█████████▊| 385/390 [04:35<00:01,  3.46it/s] 99%|█████████▉| 386/390 [04:36<00:01,  3.46it/s] 99%|█████████▉| 387/390 [04:36<00:00,  3.46it/s] 99%|█████████▉| 388/390 [04:36<00:00,  3.31it/s]100%|█████████▉| 389/390 [04:37<00:00,  3.36it/s]100%|██████████| 390/390 [04:37<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 14:35:23,872 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:35:23,872 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 14:35:23,872 >>   Batch size = 8
{'eval_loss': 0.9198050498962402, 'eval_runtime': 24.423, 'eval_samples_per_second': 354.502, 'eval_steps_per_second': 44.343, 'epoch': 3.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.78it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.15it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.19it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.33it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 45.93it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.37it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.32it/s][A
  4%|▍         | 42/1083 [00:00<00:22, 45.29it/s][A
  4%|▍         | 47/1083 [00:01<00:22, 45.20it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.38it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.35it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.33it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.23it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.03it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.97it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.99it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 45.06it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 45.03it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.20it/s][A
  9%|▉         | 102/1083 [00:02<00:22, 43.15it/s][A
 10%|▉         | 107/1083 [00:02<00:22, 43.89it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.29it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.42it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.54it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.72it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.77it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 45.01it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.87it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 44.84it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 44.99it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.09it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.07it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.04it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 45.06it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 45.04it/s][A
 17%|█▋        | 182/1083 [00:04<00:19, 45.13it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 45.01it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.11it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.12it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.18it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.20it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.16it/s][A
 20%|██        | 217/1083 [00:04<00:19, 45.10it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.97it/s][A
 21%|██        | 227/1083 [00:05<00:19, 45.03it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 45.02it/s][A
 22%|██▏       | 237/1083 [00:05<00:19, 43.17it/s][A
 22%|██▏       | 242/1083 [00:05<00:19, 43.79it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 44.26it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.61it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.87it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.92it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.93it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.97it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.80it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.87it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.95it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.07it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.24it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.15it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 45.18it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 45.01it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.98it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.92it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.91it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.04it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.25it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.13it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 45.21it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 45.06it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.93it/s][A
 34%|███▍      | 372/1083 [00:08<00:16, 42.11it/s][A
 35%|███▍      | 377/1083 [00:08<00:16, 43.14it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 43.86it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.23it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 44.64it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 44.74it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 44.90it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.88it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 44.59it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.59it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.83it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.02it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.08it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.17it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.22it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.26it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 45.05it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.83it/s][A
 43%|████▎     | 462/1083 [00:10<00:15, 40.04it/s][A
 43%|████▎     | 467/1083 [00:10<00:14, 41.67it/s][A
 44%|████▎     | 472/1083 [00:10<00:14, 42.77it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 43.44it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 44.03it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 44.46it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 44.68it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.88it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.76it/s][A
 47%|████▋     | 507/1083 [00:11<00:14, 38.77it/s][A
 47%|████▋     | 512/1083 [00:11<00:14, 40.56it/s][A
 48%|████▊     | 517/1083 [00:11<00:13, 41.85it/s][A
 48%|████▊     | 522/1083 [00:11<00:13, 42.91it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 43.69it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.21it/s][A
 50%|████▉     | 537/1083 [00:12<00:12, 44.50it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.75it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 44.51it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 44.56it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.66it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.78it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.00it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.13it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.24it/s][A
 54%|█████▎    | 582/1083 [00:13<00:11, 45.22it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 45.06it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.87it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.75it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 44.84it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 44.91it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.03it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.07it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 45.20it/s][A
 58%|█████▊    | 627/1083 [00:14<00:10, 45.26it/s][A
 58%|█████▊    | 632/1083 [00:14<00:09, 45.14it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.89it/s][A
 59%|█████▉    | 642/1083 [00:14<00:11, 39.86it/s][A
 60%|█████▉    | 647/1083 [00:14<00:10, 41.42it/s][A
 60%|██████    | 652/1083 [00:14<00:10, 42.59it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 43.29it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 43.90it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.34it/s][A
 62%|██████▏   | 672/1083 [00:15<00:09, 44.67it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.84it/s][A
 63%|██████▎   | 682/1083 [00:15<00:09, 44.43it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.46it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.82it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 44.94it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.06it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.10it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 45.20it/s][A
 66%|██████▌   | 717/1083 [00:16<00:08, 45.22it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 45.04it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.79it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.76it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.98it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.04it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.10it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 45.13it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.23it/s][A
 70%|███████   | 762/1083 [00:17<00:07, 45.15it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.97it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.89it/s][A
 72%|███████▏  | 777/1083 [00:17<00:07, 39.69it/s][A
 72%|███████▏  | 782/1083 [00:17<00:07, 41.26it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 42.30it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 43.21it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 43.87it/s][A
 74%|███████▍  | 802/1083 [00:18<00:06, 44.33it/s][A
 75%|███████▍  | 807/1083 [00:18<00:06, 44.60it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.76it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.49it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.61it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.79it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.84it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.96it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.09it/s][A
 78%|███████▊  | 847/1083 [00:19<00:05, 45.15it/s][A
 79%|███████▊  | 852/1083 [00:19<00:05, 45.18it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 45.06it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.81it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.81it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.84it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.02it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.07it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.26it/s][A
 82%|████████▏ | 892/1083 [00:20<00:04, 45.21it/s][A
 83%|████████▎ | 897/1083 [00:20<00:04, 45.15it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 45.05it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.88it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.74it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 44.88it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.02it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.17it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.14it/s][A
 87%|████████▋ | 937/1083 [00:21<00:03, 45.20it/s][A
 87%|████████▋ | 942/1083 [00:21<00:03, 45.17it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 45.03it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.92it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 44.84it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 44.82it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.95it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 45.12it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 45.20it/s][A
 91%|█████████ | 982/1083 [00:22<00:02, 45.27it/s][A
 91%|█████████ | 987/1083 [00:22<00:02, 45.13it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 45.08it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.98it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.98it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 44.95it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 44.91it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.95it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 45.01it/s][A
 95%|█████████▍| 1027/1083 [00:23<00:01, 45.20it/s][A
 95%|█████████▌| 1032/1083 [00:23<00:01, 45.18it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 45.05it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.90it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 42.75it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 43.45it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.00it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.28it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.57it/s][A
 99%|█████████▉| 1072/1083 [00:24<00:00, 44.82it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 44.92it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 44.89it/s][A                                                 
                                                   [A100%|██████████| 390/390 [05:01<00:00,  3.39it/s]
100%|██████████| 1083/1083 [00:24<00:00, 44.89it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-28 14:35:48,486 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 14:35:48,734 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:35:52,432 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:35:52,630 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:35:52,697 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:36:02,156 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:36:02,185 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78 (score: 0.8896969556808472).
                                                 100%|██████████| 390/390 [05:29<00:00,  3.39it/s]100%|██████████| 390/390 [05:29<00:00,  1.18it/s]
[INFO|trainer.py:1894] 2023-08-28 14:36:16,004 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 14:36:16,210 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:36:19,876 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:36:20,097 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:36:20,181 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:36:20,787 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:20,788 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:20,788 >>   train_loss               =      0.553
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:20,788 >>   train_runtime            = 0:05:29.50
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:20,788 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:20,788 >>   train_samples_per_second =     75.872
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:20,788 >>   train_steps_per_second   =      1.184
{'eval_loss': 0.9200587868690491, 'eval_runtime': 24.2849, 'eval_samples_per_second': 356.517, 'eval_steps_per_second': 44.596, 'epoch': 4.99}
{'train_runtime': 329.5027, 'train_samples_per_second': 75.872, 'train_steps_per_second': 1.184, 'train_loss': 0.5529728424854767, 'epoch': 4.99}
08/28/2023 14:36:21 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:36:21,122 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:36:21,122 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 14:36:21,122 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.64it/s]  1%|          | 12/1083 [00:00<00:21, 49.27it/s]  2%|▏         | 17/1083 [00:00<00:22, 47.72it/s]  2%|▏         | 22/1083 [00:00<00:22, 46.94it/s]  2%|▏         | 27/1083 [00:00<00:22, 46.43it/s]  3%|▎         | 32/1083 [00:00<00:22, 46.17it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.95it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.78it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.34it/s]  5%|▍         | 52/1083 [00:01<00:22, 45.14it/s]  5%|▌         | 57/1083 [00:01<00:22, 45.24it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.25it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.41it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.44it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.49it/s]  8%|▊         | 82/1083 [00:01<00:22, 45.45it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.35it/s]  8%|▊         | 92/1083 [00:02<00:22, 44.21it/s]  9%|▉         | 97/1083 [00:02<00:22, 44.38it/s]  9%|▉         | 102/1083 [00:02<00:21, 44.70it/s] 10%|▉         | 107/1083 [00:02<00:21, 44.75it/s] 10%|█         | 112/1083 [00:02<00:21, 45.01it/s] 11%|█         | 117/1083 [00:02<00:21, 45.11it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.23it/s] 12%|█▏        | 127/1083 [00:02<00:21, 45.27it/s] 12%|█▏        | 132/1083 [00:02<00:21, 45.14it/s] 13%|█▎        | 137/1083 [00:03<00:20, 45.06it/s] 13%|█▎        | 142/1083 [00:03<00:20, 45.00it/s] 14%|█▎        | 147/1083 [00:03<00:20, 45.01it/s] 14%|█▍        | 152/1083 [00:03<00:20, 45.05it/s] 14%|█▍        | 157/1083 [00:03<00:20, 45.14it/s] 15%|█▍        | 162/1083 [00:03<00:20, 45.20it/s] 15%|█▌        | 167/1083 [00:03<00:20, 45.41it/s] 16%|█▌        | 172/1083 [00:03<00:20, 45.23it/s] 16%|█▋        | 177/1083 [00:03<00:20, 45.23it/s] 17%|█▋        | 182/1083 [00:04<00:19, 45.07it/s] 17%|█▋        | 187/1083 [00:04<00:19, 45.06it/s] 18%|█▊        | 192/1083 [00:04<00:19, 44.98it/s] 18%|█▊        | 197/1083 [00:04<00:19, 45.07it/s] 19%|█▊        | 202/1083 [00:04<00:19, 45.05it/s] 19%|█▉        | 207/1083 [00:04<00:19, 45.22it/s] 20%|█▉        | 212/1083 [00:04<00:19, 45.32it/s] 20%|██        | 217/1083 [00:04<00:19, 45.31it/s] 20%|██        | 222/1083 [00:04<00:19, 45.18it/s] 21%|██        | 227/1083 [00:05<00:20, 42.49it/s] 21%|██▏       | 232/1083 [00:05<00:19, 43.18it/s] 22%|██▏       | 237/1083 [00:05<00:19, 44.01it/s] 22%|██▏       | 242/1083 [00:05<00:18, 44.46it/s] 23%|██▎       | 247/1083 [00:05<00:18, 44.70it/s] 23%|██▎       | 252/1083 [00:05<00:18, 44.93it/s] 24%|██▎       | 257/1083 [00:05<00:18, 45.11it/s] 24%|██▍       | 262/1083 [00:05<00:18, 45.23it/s] 25%|██▍       | 267/1083 [00:05<00:18, 44.88it/s] 25%|██▌       | 272/1083 [00:06<00:18, 44.85it/s] 26%|██▌       | 277/1083 [00:06<00:17, 44.93it/s] 26%|██▌       | 282/1083 [00:06<00:17, 45.16it/s] 27%|██▋       | 287/1083 [00:06<00:17, 45.24it/s] 27%|██▋       | 292/1083 [00:06<00:17, 45.29it/s] 27%|██▋       | 297/1083 [00:06<00:17, 45.29it/s] 28%|██▊       | 302/1083 [00:06<00:17, 45.42it/s] 28%|██▊       | 307/1083 [00:06<00:17, 45.32it/s] 29%|██▉       | 312/1083 [00:06<00:17, 45.02it/s] 29%|██▉       | 317/1083 [00:07<00:17, 44.97it/s] 30%|██▉       | 322/1083 [00:07<00:16, 44.94it/s] 30%|███       | 327/1083 [00:07<00:16, 45.14it/s] 31%|███       | 332/1083 [00:07<00:16, 45.13it/s] 31%|███       | 337/1083 [00:07<00:16, 45.25it/s] 32%|███▏      | 342/1083 [00:07<00:16, 45.26it/s] 32%|███▏      | 347/1083 [00:07<00:16, 45.29it/s] 33%|███▎      | 352/1083 [00:07<00:16, 45.17it/s] 33%|███▎      | 357/1083 [00:07<00:16, 44.89it/s] 33%|███▎      | 362/1083 [00:08<00:17, 42.10it/s] 34%|███▍      | 367/1083 [00:08<00:16, 43.07it/s] 34%|███▍      | 372/1083 [00:08<00:16, 43.82it/s] 35%|███▍      | 377/1083 [00:08<00:15, 44.32it/s] 35%|███▌      | 382/1083 [00:08<00:15, 44.65it/s] 36%|███▌      | 387/1083 [00:08<00:15, 44.86it/s] 36%|███▌      | 392/1083 [00:08<00:15, 44.91it/s] 37%|███▋      | 397/1083 [00:08<00:15, 44.91it/s] 37%|███▋      | 402/1083 [00:08<00:15, 44.60it/s] 38%|███▊      | 407/1083 [00:09<00:15, 44.60it/s] 38%|███▊      | 412/1083 [00:09<00:14, 44.78it/s] 39%|███▊      | 417/1083 [00:09<00:14, 44.95it/s] 39%|███▉      | 422/1083 [00:09<00:14, 45.16it/s] 39%|███▉      | 427/1083 [00:09<00:14, 45.24it/s] 40%|███▉      | 432/1083 [00:09<00:14, 45.30it/s] 40%|████      | 437/1083 [00:09<00:14, 45.25it/s] 41%|████      | 442/1083 [00:09<00:14, 45.06it/s] 41%|████▏     | 447/1083 [00:09<00:14, 44.84it/s] 42%|████▏     | 452/1083 [00:10<00:14, 44.83it/s] 42%|████▏     | 457/1083 [00:10<00:13, 44.80it/s] 43%|████▎     | 462/1083 [00:10<00:13, 45.05it/s] 43%|████▎     | 467/1083 [00:10<00:13, 45.11it/s] 44%|████▎     | 472/1083 [00:10<00:13, 45.26it/s] 44%|████▍     | 477/1083 [00:10<00:13, 45.33it/s] 45%|████▍     | 482/1083 [00:10<00:13, 45.19it/s] 45%|████▍     | 487/1083 [00:10<00:13, 45.07it/s] 45%|████▌     | 492/1083 [00:10<00:13, 44.86it/s] 46%|████▌     | 497/1083 [00:11<00:14, 40.92it/s] 46%|████▋     | 502/1083 [00:11<00:13, 42.20it/s] 47%|████▋     | 507/1083 [00:11<00:13, 43.16it/s] 47%|████▋     | 512/1083 [00:11<00:13, 43.84it/s] 48%|████▊     | 517/1083 [00:11<00:12, 44.29it/s] 48%|████▊     | 522/1083 [00:11<00:12, 44.62it/s] 49%|████▊     | 527/1083 [00:11<00:12, 44.85it/s] 49%|████▉     | 532/1083 [00:11<00:12, 45.09it/s] 50%|████▉     | 537/1083 [00:11<00:12, 44.68it/s] 50%|█████     | 542/1083 [00:12<00:12, 44.71it/s] 51%|█████     | 547/1083 [00:12<00:11, 44.76it/s] 51%|█████     | 552/1083 [00:12<00:11, 44.90it/s] 51%|█████▏    | 557/1083 [00:12<00:11, 45.12it/s] 52%|█████▏    | 562/1083 [00:12<00:11, 45.20it/s] 52%|█████▏    | 567/1083 [00:12<00:11, 45.18it/s] 53%|█████▎    | 572/1083 [00:12<00:11, 45.18it/s] 53%|█████▎    | 577/1083 [00:12<00:11, 45.10it/s] 54%|█████▎    | 582/1083 [00:12<00:11, 44.85it/s] 54%|█████▍    | 587/1083 [00:13<00:11, 41.89it/s] 55%|█████▍    | 592/1083 [00:13<00:11, 42.92it/s] 55%|█████▌    | 597/1083 [00:13<00:11, 43.58it/s] 56%|█████▌    | 602/1083 [00:13<00:10, 44.20it/s] 56%|█████▌    | 607/1083 [00:13<00:10, 44.55it/s] 57%|█████▋    | 612/1083 [00:13<00:10, 44.88it/s] 57%|█████▋    | 617/1083 [00:13<00:10, 45.02it/s] 57%|█████▋    | 622/1083 [00:13<00:10, 44.99it/s] 58%|█████▊    | 627/1083 [00:13<00:10, 44.79it/s] 58%|█████▊    | 632/1083 [00:14<00:11, 40.22it/s] 59%|█████▉    | 637/1083 [00:14<00:10, 41.73it/s] 59%|█████▉    | 642/1083 [00:14<00:10, 42.74it/s] 60%|█████▉    | 647/1083 [00:14<00:10, 43.56it/s] 60%|██████    | 652/1083 [00:14<00:09, 44.18it/s] 61%|██████    | 657/1083 [00:14<00:09, 44.64it/s] 61%|██████    | 662/1083 [00:14<00:09, 44.90it/s] 62%|██████▏   | 667/1083 [00:14<00:09, 44.97it/s] 62%|██████▏   | 672/1083 [00:15<00:09, 44.56it/s] 63%|██████▎   | 677/1083 [00:15<00:09, 44.39it/s] 63%|██████▎   | 682/1083 [00:15<00:09, 44.49it/s] 63%|██████▎   | 687/1083 [00:15<00:08, 44.65it/s] 64%|██████▍   | 692/1083 [00:15<00:08, 44.89it/s] 64%|██████▍   | 697/1083 [00:15<00:08, 45.11it/s] 65%|██████▍   | 702/1083 [00:15<00:08, 45.27it/s] 65%|██████▌   | 707/1083 [00:15<00:08, 45.36it/s] 66%|██████▌   | 712/1083 [00:15<00:08, 45.26it/s] 66%|██████▌   | 717/1083 [00:16<00:08, 44.94it/s] 67%|██████▋   | 722/1083 [00:16<00:08, 44.64it/s] 67%|██████▋   | 727/1083 [00:16<00:07, 44.58it/s] 68%|██████▊   | 732/1083 [00:16<00:07, 44.77it/s] 68%|██████▊   | 737/1083 [00:16<00:07, 44.95it/s] 69%|██████▊   | 742/1083 [00:16<00:07, 45.03it/s] 69%|██████▉   | 747/1083 [00:16<00:07, 45.08it/s] 69%|██████▉   | 752/1083 [00:16<00:07, 45.22it/s] 70%|██████▉   | 757/1083 [00:16<00:07, 45.27it/s] 70%|███████   | 762/1083 [00:17<00:07, 45.17it/s] 71%|███████   | 767/1083 [00:17<00:07, 41.57it/s] 71%|███████▏  | 772/1083 [00:17<00:07, 42.65it/s] 72%|███████▏  | 777/1083 [00:17<00:07, 43.48it/s] 72%|███████▏  | 782/1083 [00:17<00:06, 43.99it/s] 73%|███████▎  | 787/1083 [00:17<00:06, 44.34it/s] 73%|███████▎  | 792/1083 [00:17<00:06, 44.58it/s] 74%|███████▎  | 797/1083 [00:17<00:06, 44.84it/s] 74%|███████▍  | 802/1083 [00:17<00:06, 44.78it/s] 75%|███████▍  | 807/1083 [00:18<00:06, 44.42it/s] 75%|███████▍  | 812/1083 [00:18<00:06, 44.48it/s] 75%|███████▌  | 817/1083 [00:18<00:05, 44.69it/s] 76%|███████▌  | 822/1083 [00:18<00:05, 44.84it/s] 76%|███████▋  | 827/1083 [00:18<00:05, 45.00it/s] 77%|███████▋  | 832/1083 [00:18<00:05, 45.02it/s] 77%|███████▋  | 837/1083 [00:18<00:05, 45.19it/s] 78%|███████▊  | 842/1083 [00:18<00:05, 45.18it/s] 78%|███████▊  | 847/1083 [00:18<00:05, 45.01it/s] 79%|███████▊  | 852/1083 [00:19<00:05, 44.36it/s] 79%|███████▉  | 857/1083 [00:19<00:05, 44.73it/s] 80%|███████▉  | 862/1083 [00:19<00:04, 44.80it/s] 80%|████████  | 867/1083 [00:19<00:04, 44.92it/s] 81%|████████  | 872/1083 [00:19<00:04, 45.04it/s] 81%|████████  | 877/1083 [00:19<00:04, 45.17it/s] 81%|████████▏ | 882/1083 [00:19<00:04, 45.23it/s] 82%|████████▏ | 887/1083 [00:19<00:04, 45.19it/s] 82%|████████▏ | 892/1083 [00:19<00:04, 44.99it/s] 83%|████████▎ | 897/1083 [00:20<00:04, 44.63it/s] 83%|████████▎ | 902/1083 [00:20<00:04, 37.08it/s] 84%|████████▎ | 907/1083 [00:20<00:04, 39.28it/s] 84%|████████▍ | 912/1083 [00:20<00:04, 40.98it/s] 85%|████████▍ | 917/1083 [00:20<00:03, 42.26it/s] 85%|████████▌ | 922/1083 [00:20<00:03, 43.21it/s] 86%|████████▌ | 927/1083 [00:20<00:03, 43.89it/s] 86%|████████▌ | 932/1083 [00:20<00:03, 44.44it/s] 87%|████████▋ | 937/1083 [00:20<00:03, 44.59it/s] 87%|████████▋ | 942/1083 [00:21<00:03, 44.50it/s] 87%|████████▋ | 947/1083 [00:21<00:03, 44.37it/s] 88%|████████▊ | 952/1083 [00:21<00:02, 44.49it/s] 88%|████████▊ | 957/1083 [00:21<00:02, 44.69it/s] 89%|████████▉ | 962/1083 [00:21<00:02, 44.86it/s] 89%|████████▉ | 967/1083 [00:21<00:02, 45.07it/s] 90%|████████▉ | 972/1083 [00:21<00:02, 45.29it/s] 90%|█████████ | 977/1083 [00:21<00:02, 45.29it/s] 91%|█████████ | 982/1083 [00:21<00:02, 45.41it/s] 91%|█████████ | 987/1083 [00:22<00:02, 45.33it/s] 92%|█████████▏| 992/1083 [00:22<00:02, 45.07it/s] 92%|█████████▏| 997/1083 [00:22<00:01, 45.07it/s] 93%|█████████▎| 1002/1083 [00:22<00:01, 45.06it/s] 93%|█████████▎| 1007/1083 [00:22<00:01, 45.05it/s] 93%|█████████▎| 1012/1083 [00:22<00:01, 45.18it/s] 94%|█████████▍| 1017/1083 [00:22<00:01, 45.28it/s] 94%|█████████▍| 1022/1083 [00:22<00:01, 45.35it/s] 95%|█████████▍| 1027/1083 [00:22<00:01, 45.44it/s] 95%|█████████▌| 1032/1083 [00:23<00:01, 45.22it/s] 96%|█████████▌| 1037/1083 [00:23<00:01, 41.64it/s] 96%|█████████▌| 1042/1083 [00:23<00:00, 42.82it/s] 97%|█████████▋| 1047/1083 [00:23<00:00, 43.62it/s] 97%|█████████▋| 1052/1083 [00:23<00:00, 44.09it/s] 98%|█████████▊| 1057/1083 [00:23<00:00, 44.47it/s] 98%|█████████▊| 1062/1083 [00:23<00:00, 44.80it/s] 99%|█████████▊| 1067/1083 [00:23<00:00, 45.01it/s] 99%|█████████▉| 1072/1083 [00:24<00:00, 45.05it/s] 99%|█████████▉| 1077/1083 [00:24<00:00, 44.75it/s]100%|█████████▉| 1082/1083 [00:24<00:00, 44.73it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.63it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:36:45,404 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   eval_loss               =     0.8897
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   eval_runtime            = 0:00:24.28
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   eval_samples_per_second =    356.565
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   eval_steps_per_second   =     44.602
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:36:45,404 >>   perplexity              =     2.4344
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:58,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:58,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:58,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:58,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:58,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:36:58,994 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:36:58,996 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:59,619 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:37:00,776 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:37:00,776 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:37:03,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:37:03,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:37:03,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:37:03,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:37:03,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:37:04,704 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:37:04,705 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:37:05,315 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:37:05,567 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:37:05,567 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.44it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:12,  1.47it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.61it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.59it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.57it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.51it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.58it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:33,  1.51it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:35,  1.59it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:39,  1.53it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:40,  1.65it/s]Extractor Predicting: 65it [00:41,  1.62it/s]Extractor Predicting: 66it [00:42,  1.63it/s]Extractor Predicting: 67it [00:42,  1.60it/s]Extractor Predicting: 68it [00:43,  1.63it/s]Extractor Predicting: 69it [00:43,  1.62it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:45,  1.58it/s]Extractor Predicting: 73it [00:46,  1.62it/s]Extractor Predicting: 74it [00:47,  1.63it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:48,  1.60it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:49,  1.59it/s]Extractor Predicting: 79it [00:50,  1.60it/s]Extractor Predicting: 80it [00:50,  1.60it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.61it/s]Extractor Predicting: 83it [00:52,  1.60it/s]Extractor Predicting: 84it [00:53,  1.62it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:54,  1.54it/s]Extractor Predicting: 87it [00:55,  1.52it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:56,  1.57it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:57,  1.59it/s]Extractor Predicting: 92it [00:58,  1.59it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.61it/s]Extractor Predicting: 96it [01:01,  1.58it/s]Extractor Predicting: 97it [01:01,  1.58it/s]Extractor Predicting: 98it [01:02,  1.57it/s]Extractor Predicting: 99it [01:02,  1.59it/s]Extractor Predicting: 100it [01:03,  1.58it/s]Extractor Predicting: 101it [01:04,  1.58it/s]Extractor Predicting: 102it [01:04,  1.61it/s]Extractor Predicting: 103it [01:05,  1.43it/s]Extractor Predicting: 104it [01:06,  1.48it/s]Extractor Predicting: 105it [01:06,  1.51it/s]Extractor Predicting: 106it [01:07,  1.52it/s]Extractor Predicting: 107it [01:08,  1.53it/s]Extractor Predicting: 108it [01:08,  1.55it/s]Extractor Predicting: 109it [01:09,  1.57it/s]Extractor Predicting: 110it [01:10,  1.57it/s]Extractor Predicting: 111it [01:10,  1.55it/s]Extractor Predicting: 112it [01:11,  1.57it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:12,  1.60it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:13,  1.54it/s]Extractor Predicting: 117it [01:14,  1.58it/s]Extractor Predicting: 118it [01:15,  1.59it/s]Extractor Predicting: 119it [01:15,  1.58it/s]Extractor Predicting: 120it [01:16,  1.58it/s]Extractor Predicting: 121it [01:17,  1.56it/s]Extractor Predicting: 122it [01:17,  1.67it/s]Extractor Predicting: 123it [01:18,  1.66it/s]Extractor Predicting: 124it [01:18,  1.61it/s]Extractor Predicting: 125it [01:19,  1.57it/s]Extractor Predicting: 126it [01:20,  1.55it/s]Extractor Predicting: 127it [01:20,  1.58it/s]Extractor Predicting: 128it [01:21,  1.59it/s]Extractor Predicting: 129it [01:22,  1.57it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.63it/s]Extractor Predicting: 132it [01:23,  1.61it/s]Extractor Predicting: 133it [01:24,  1.57it/s]Extractor Predicting: 134it [01:25,  1.57it/s]Extractor Predicting: 135it [01:25,  1.58it/s]Extractor Predicting: 136it [01:26,  1.57it/s]Extractor Predicting: 137it [01:27,  1.57it/s]Extractor Predicting: 138it [01:27,  1.59it/s]Extractor Predicting: 139it [01:28,  1.52it/s]Extractor Predicting: 140it [01:29,  1.55it/s]Extractor Predicting: 141it [01:29,  1.58it/s]Extractor Predicting: 142it [01:30,  1.59it/s]Extractor Predicting: 143it [01:30,  1.58it/s]Extractor Predicting: 144it [01:31,  1.56it/s]Extractor Predicting: 145it [01:32,  1.55it/s]Extractor Predicting: 146it [01:32,  1.55it/s]Extractor Predicting: 147it [01:33,  1.58it/s]Extractor Predicting: 148it [01:34,  1.61it/s]Extractor Predicting: 149it [01:34,  1.62it/s]Extractor Predicting: 150it [01:35,  1.62it/s]Extractor Predicting: 151it [01:35,  1.63it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:37,  1.52it/s]Extractor Predicting: 154it [01:38,  1.46it/s]Extractor Predicting: 155it [01:38,  1.44it/s]Extractor Predicting: 156it [01:39,  1.44it/s]Extractor Predicting: 157it [01:40,  1.43it/s]Extractor Predicting: 158it [01:40,  1.43it/s]Extractor Predicting: 159it [01:41,  1.43it/s]Extractor Predicting: 160it [01:42,  1.43it/s]Extractor Predicting: 161it [01:42,  1.43it/s]Extractor Predicting: 162it [01:43,  1.43it/s]Extractor Predicting: 163it [01:44,  1.42it/s]Extractor Predicting: 164it [01:45,  1.42it/s]Extractor Predicting: 165it [01:45,  1.43it/s]Extractor Predicting: 166it [01:46,  1.42it/s]Extractor Predicting: 167it [01:47,  1.41it/s]Extractor Predicting: 168it [01:47,  1.44it/s]Extractor Predicting: 169it [01:48,  1.49it/s]Extractor Predicting: 170it [01:49,  1.54it/s]Extractor Predicting: 171it [01:49,  1.53it/s]Extractor Predicting: 172it [01:50,  1.52it/s]Extractor Predicting: 173it [01:51,  1.52it/s]Extractor Predicting: 174it [01:51,  1.52it/s]Extractor Predicting: 175it [01:52,  1.50it/s]Extractor Predicting: 176it [01:53,  1.47it/s]Extractor Predicting: 177it [01:53,  1.47it/s]Extractor Predicting: 178it [01:54,  1.47it/s]Extractor Predicting: 179it [01:55,  1.49it/s]Extractor Predicting: 180it [01:55,  1.48it/s]Extractor Predicting: 181it [01:56,  1.48it/s]Extractor Predicting: 182it [01:57,  1.49it/s]Extractor Predicting: 183it [01:57,  1.54it/s]Extractor Predicting: 184it [01:58,  1.56it/s]Extractor Predicting: 185it [01:58,  1.60it/s]Extractor Predicting: 186it [01:59,  1.55it/s]Extractor Predicting: 187it [02:00,  1.40it/s]Extractor Predicting: 188it [02:01,  1.45it/s]Extractor Predicting: 189it [02:01,  1.49it/s]Extractor Predicting: 190it [02:02,  1.50it/s]Extractor Predicting: 191it [02:03,  1.48it/s]Extractor Predicting: 192it [02:03,  1.51it/s]Extractor Predicting: 193it [02:04,  1.54it/s]Extractor Predicting: 194it [02:05,  1.56it/s]Extractor Predicting: 195it [02:05,  1.54it/s]Extractor Predicting: 196it [02:06,  1.52it/s]Extractor Predicting: 197it [02:07,  1.54it/s]Extractor Predicting: 198it [02:07,  1.54it/s]Extractor Predicting: 199it [02:08,  1.59it/s]Extractor Predicting: 200it [02:08,  1.58it/s]Extractor Predicting: 201it [02:09,  1.56it/s]Extractor Predicting: 202it [02:10,  1.55it/s]Extractor Predicting: 203it [02:10,  1.54it/s]Extractor Predicting: 204it [02:11,  1.56it/s]Extractor Predicting: 205it [02:12,  1.55it/s]Extractor Predicting: 206it [02:12,  1.55it/s]Extractor Predicting: 207it [02:13,  1.54it/s]Extractor Predicting: 208it [02:14,  1.57it/s]Extractor Predicting: 209it [02:14,  1.59it/s]Extractor Predicting: 210it [02:15,  1.59it/s]Extractor Predicting: 211it [02:15,  1.59it/s]Extractor Predicting: 212it [02:16,  1.59it/s]Extractor Predicting: 213it [02:17,  1.58it/s]Extractor Predicting: 214it [02:17,  1.61it/s]Extractor Predicting: 215it [02:18,  1.63it/s]Extractor Predicting: 216it [02:19,  1.63it/s]Extractor Predicting: 217it [02:19,  1.62it/s]Extractor Predicting: 218it [02:20,  1.61it/s]Extractor Predicting: 219it [02:20,  1.61it/s]Extractor Predicting: 220it [02:21,  1.59it/s]Extractor Predicting: 221it [02:22,  1.63it/s]Extractor Predicting: 222it [02:22,  1.63it/s]Extractor Predicting: 223it [02:23,  1.59it/s]Extractor Predicting: 224it [02:24,  1.57it/s]Extractor Predicting: 225it [02:24,  1.52it/s]Extractor Predicting: 226it [02:25,  1.55it/s]Extractor Predicting: 227it [02:25,  1.56it/s]Extractor Predicting: 228it [02:26,  1.55it/s]Extractor Predicting: 229it [02:27,  1.59it/s]Extractor Predicting: 230it [02:27,  1.58it/s]Extractor Predicting: 231it [02:28,  1.56it/s]Extractor Predicting: 232it [02:29,  1.59it/s]Extractor Predicting: 233it [02:29,  1.59it/s]Extractor Predicting: 234it [02:30,  1.57it/s]Extractor Predicting: 235it [02:31,  1.56it/s]Extractor Predicting: 236it [02:31,  1.54it/s]Extractor Predicting: 237it [02:32,  1.56it/s]Extractor Predicting: 238it [02:33,  1.54it/s]Extractor Predicting: 239it [02:33,  1.58it/s]Extractor Predicting: 240it [02:34,  1.57it/s]Extractor Predicting: 241it [02:34,  1.59it/s]Extractor Predicting: 242it [02:35,  1.61it/s]Extractor Predicting: 243it [02:36,  1.51it/s]Extractor Predicting: 244it [02:36,  1.54it/s]Extractor Predicting: 245it [02:37,  1.56it/s]Extractor Predicting: 246it [02:38,  1.57it/s]Extractor Predicting: 247it [02:38,  1.58it/s]Extractor Predicting: 248it [02:39,  1.55it/s]Extractor Predicting: 249it [02:40,  1.56it/s]Extractor Predicting: 250it [02:40,  1.55it/s]Extractor Predicting: 251it [02:41,  1.56it/s]Extractor Predicting: 252it [02:41,  1.57it/s]Extractor Predicting: 253it [02:42,  1.56it/s]Extractor Predicting: 254it [02:43,  1.58it/s]Extractor Predicting: 255it [02:43,  1.57it/s]Extractor Predicting: 256it [02:44,  1.59it/s]Extractor Predicting: 257it [02:45,  1.60it/s]Extractor Predicting: 258it [02:45,  1.62it/s]Extractor Predicting: 259it [02:46,  1.60it/s]Extractor Predicting: 260it [02:47,  1.57it/s]Extractor Predicting: 261it [02:47,  1.59it/s]Extractor Predicting: 262it [02:48,  1.58it/s]Extractor Predicting: 263it [02:48,  1.55it/s]Extractor Predicting: 264it [02:49,  1.54it/s]Extractor Predicting: 265it [02:50,  1.54it/s]Extractor Predicting: 266it [02:50,  1.52it/s]Extractor Predicting: 267it [02:51,  1.51it/s]Extractor Predicting: 268it [02:52,  1.52it/s]Extractor Predicting: 269it [02:52,  1.60it/s]Extractor Predicting: 269it [02:52,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:13,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:13,610 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:13,610 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:13,610 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:13,610 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:40:14,428 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:40:14,429 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:40:15,070 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:40:16,190 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:40:16,190 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:19,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:19,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:19,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:19,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:19,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:40:20,118 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:40:20,119 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:40:20,771 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:40:21,036 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:40:21,036 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.35472779369627505,
  "recall": 0.07149457149457149,
  "score": 0.11900413342305102,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:02,  1.52it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:11,  1.56it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:26,  1.59it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.57it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.53it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:33,  1.59it/s]Extractor Predicting: 52it [00:34,  1.59it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.63it/s]Extractor Predicting: 55it [00:35,  1.62it/s]Extractor Predicting: 56it [00:36,  1.65it/s]Extractor Predicting: 57it [00:37,  1.63it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:38,  1.63it/s]Extractor Predicting: 61it [00:39,  1.63it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:40,  1.51it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:44,  1.58it/s]Extractor Predicting: 70it [00:45,  1.60it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:46,  1.46it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:48,  1.53it/s]Extractor Predicting: 76it [00:49,  1.57it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.58it/s]Extractor Predicting: 80it [00:51,  1.57it/s]Extractor Predicting: 81it [00:52,  1.63it/s]Extractor Predicting: 82it [00:53,  1.62it/s]Extractor Predicting: 83it [00:53,  1.63it/s]Extractor Predicting: 84it [00:54,  1.67it/s]Extractor Predicting: 85it [00:54,  1.68it/s]Extractor Predicting: 86it [00:55,  1.65it/s]Extractor Predicting: 87it [00:56,  1.63it/s]Extractor Predicting: 88it [00:56,  1.64it/s]Extractor Predicting: 89it [00:57,  1.61it/s]Extractor Predicting: 90it [00:58,  1.65it/s]Extractor Predicting: 91it [00:58,  1.65it/s]Extractor Predicting: 92it [00:59,  1.63it/s]Extractor Predicting: 93it [00:59,  1.60it/s]Extractor Predicting: 94it [01:00,  1.60it/s]Extractor Predicting: 95it [01:01,  1.68it/s]Extractor Predicting: 96it [01:01,  1.64it/s]Extractor Predicting: 97it [01:02,  1.64it/s]Extractor Predicting: 98it [01:02,  1.61it/s]Extractor Predicting: 99it [01:03,  1.61it/s]Extractor Predicting: 100it [01:04,  1.62it/s]Extractor Predicting: 101it [01:04,  1.63it/s]Extractor Predicting: 102it [01:05,  1.66it/s]Extractor Predicting: 103it [01:05,  1.65it/s]Extractor Predicting: 104it [01:06,  1.66it/s]Extractor Predicting: 105it [01:07,  1.66it/s]Extractor Predicting: 106it [01:07,  1.71it/s]Extractor Predicting: 107it [01:08,  1.68it/s]Extractor Predicting: 108it [01:08,  1.74it/s]Extractor Predicting: 109it [01:09,  1.76it/s]Extractor Predicting: 110it [01:10,  1.73it/s]Extractor Predicting: 111it [01:10,  1.75it/s]Extractor Predicting: 112it [01:11,  1.73it/s]Extractor Predicting: 113it [01:11,  1.69it/s]Extractor Predicting: 114it [01:12,  1.62it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:13,  1.56it/s]Extractor Predicting: 117it [01:14,  1.52it/s]Extractor Predicting: 118it [01:15,  1.51it/s]Extractor Predicting: 119it [01:15,  1.54it/s]Extractor Predicting: 120it [01:16,  1.55it/s]Extractor Predicting: 121it [01:17,  1.56it/s]Extractor Predicting: 122it [01:17,  1.57it/s]Extractor Predicting: 123it [01:18,  1.52it/s]Extractor Predicting: 124it [01:19,  1.55it/s]Extractor Predicting: 125it [01:19,  1.57it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.50it/s]Extractor Predicting: 128it [01:21,  1.48it/s]Extractor Predicting: 129it [01:22,  1.53it/s]Extractor Predicting: 130it [01:22,  1.56it/s]Extractor Predicting: 131it [01:23,  1.54it/s]Extractor Predicting: 132it [01:24,  1.53it/s]Extractor Predicting: 133it [01:24,  1.49it/s]Extractor Predicting: 134it [01:25,  1.52it/s]Extractor Predicting: 135it [01:26,  1.52it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:27,  1.55it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:28,  1.59it/s]Extractor Predicting: 140it [01:29,  1.58it/s]Extractor Predicting: 141it [01:30,  1.56it/s]Extractor Predicting: 142it [01:30,  1.58it/s]Extractor Predicting: 143it [01:30,  1.90it/s]Extractor Predicting: 143it [01:30,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:04,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:04,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:04,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:04,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:04,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:42:05,983 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:42:05,984 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:06,692 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:07,810 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:07,810 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,069 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:42:12,011 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:42:12,012 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:12,637 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:12,895 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:12,895 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2469733656174334,
  "recall": 0.05977146205684149,
  "score": 0.09624911535739561,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.51it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:21,  1.56it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.57it/s]Extractor Predicting: 38it [00:24,  1.62it/s]Extractor Predicting: 39it [00:25,  1.68it/s]Extractor Predicting: 40it [00:25,  1.77it/s]Extractor Predicting: 41it [00:26,  1.86it/s]Extractor Predicting: 42it [00:26,  1.89it/s]Extractor Predicting: 43it [00:27,  1.90it/s]Extractor Predicting: 44it [00:27,  1.90it/s]Extractor Predicting: 45it [00:28,  1.94it/s]Extractor Predicting: 46it [00:28,  1.91it/s]Extractor Predicting: 47it [00:29,  1.89it/s]Extractor Predicting: 48it [00:30,  1.89it/s]Extractor Predicting: 49it [00:30,  1.89it/s]Extractor Predicting: 50it [00:31,  1.88it/s]Extractor Predicting: 51it [00:31,  1.85it/s]Extractor Predicting: 52it [00:32,  1.87it/s]Extractor Predicting: 53it [00:32,  1.89it/s]Extractor Predicting: 54it [00:33,  1.94it/s]Extractor Predicting: 55it [00:33,  1.96it/s]Extractor Predicting: 56it [00:34,  1.91it/s]Extractor Predicting: 57it [00:34,  1.92it/s]Extractor Predicting: 58it [00:35,  1.92it/s]Extractor Predicting: 59it [00:35,  1.94it/s]Extractor Predicting: 60it [00:36,  1.89it/s]Extractor Predicting: 61it [00:36,  1.85it/s]Extractor Predicting: 62it [00:37,  1.86it/s]Extractor Predicting: 63it [00:37,  1.88it/s]Extractor Predicting: 64it [00:38,  1.89it/s]Extractor Predicting: 65it [00:38,  1.89it/s]Extractor Predicting: 66it [00:39,  1.88it/s]Extractor Predicting: 67it [00:40,  1.80it/s]Extractor Predicting: 68it [00:40,  1.71it/s]Extractor Predicting: 69it [00:41,  1.65it/s]Extractor Predicting: 70it [00:42,  1.59it/s]Extractor Predicting: 71it [00:42,  1.55it/s]Extractor Predicting: 72it [00:43,  1.56it/s]Extractor Predicting: 72it [00:43,  1.66it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:43:00,255 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:43:00,256 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:43:00,325 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:43:00,326 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:43:00,369 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:43:15,217 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:43:15,243 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:43:15,370 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:43:15,370 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:43:15,453 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:43:15,522 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:43:15,523 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:43:15,523 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:43:15,523 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:43:15,523 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:43:15,523 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7415730337078652,
  "recall": 0.16467065868263472,
  "score": 0.2694977541853818,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:43:15,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:16,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:17,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:17,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:18,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:18,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:19,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:19,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:20,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:21,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:21,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:22,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:22,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:23,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:24,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:24,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:25,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:25,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:26,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:27,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:27,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:51, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-28 14:43:28,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:28,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:29,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:30,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:30,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:31,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:31,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:32,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:33,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:34,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:34,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:35,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:36,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:36,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:37,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:38,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:38,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:39,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:40,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:40,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:25<01:41, 12.75s/it][WARNING|generation_utils.py:914] 2023-08-28 14:43:41,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:41,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:42,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:43,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:43,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:44,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:44,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:45,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:45,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:46,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:46,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:47,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:47,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:48,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:49,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:49,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:50,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:50,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:51,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:51,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:52,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:37<01:25, 12.26s/it][WARNING|generation_utils.py:914] 2023-08-28 14:43:52,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:53,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:54,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:54,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:55,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:56,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:56,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:57,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:57,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:58,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:59,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:43:59,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:00,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:00,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:01,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:02,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:02,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:03,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:03,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:04,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:05,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:05,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:06,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:51<01:17, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:06,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:07,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:07,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:08,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:09,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:09,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:10,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:11,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:11,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:12,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:12,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:13,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:14,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:14,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:15,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:15,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:16,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:17,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:17,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:02<01:01, 12.32s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:18,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:19,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:19,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:20,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:20,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:21,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:21,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:22,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:23,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:23,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:24,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:24,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:25,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:25,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:26,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:27,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:27,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:28,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:28,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:13<00:47, 11.93s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:29,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:29,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:30,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:31,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:31,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:32,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:32,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:33,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:33,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:34,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:34,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:35,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:36,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:36,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:37,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:37,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:38,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:38,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:39,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:40,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:40,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:25<00:35, 11.94s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:41,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:41,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:42,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:42,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:43,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:43,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:44,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:44,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:45,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:45,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:46,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:47,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:47,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:48,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:48,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:49,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:49,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:50,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:50,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:51,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:35<00:22, 11.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:51,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:52,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:53,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:53,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:54,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:54,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:55,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:56,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:56,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:57,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:58,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:58,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:59,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:59,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:00,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:01,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:01,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:02,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:02,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:03,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:04,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:04,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:49<00:12, 12.12s/it][WARNING|generation_utils.py:914] 2023-08-28 14:45:05,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:05,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:06,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:07,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:08,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:08,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:09,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:10,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:10,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:11,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:12,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:12,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:13,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:14,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:15,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:15,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:16,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:17,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:17,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:18,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:19,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:04<00:00, 12.89s/it]Generating: 100%|██████████| 10/10 [02:04<00:00, 12.40s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:27,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:27,226 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:27,226 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:27,226 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:27,226 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:45:27,983 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:45:27,984 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:45:28,579 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:45:29,749 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:45:29,749 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:32,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:32,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:32,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:32,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:45:32,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:45:33,703 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:45:33,704 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:45:34,318 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:45:34,543 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:45:34,543 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 538, 'raw': 544}
{'target': 600, 'success': 570, 'raw': 576}
{'target': 600, 'success': 602, 'raw': 608}
{'prompt': 'Relation : sport .', 'success_rate': 0.9901315789473685, 'errors': {'', "('Jorge Sánchez', 'sport', '', 'The Swiss won the individual event with Jorge Sánchez , and also won the mens team event for the team event at the 1996 Summer Olympics .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 9080
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9180, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.72it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:03,  1.69it/s]Extractor Estimating: 6it [00:03,  1.68it/s]Extractor Estimating: 7it [00:04,  1.72it/s]Extractor Estimating: 8it [00:04,  1.74it/s]Extractor Estimating: 9it [00:05,  1.70it/s]Extractor Estimating: 10it [00:05,  1.70it/s]Extractor Estimating: 11it [00:06,  1.77it/s]Extractor Estimating: 12it [00:07,  1.77it/s]Extractor Estimating: 13it [00:07,  1.73it/s]Extractor Estimating: 14it [00:08,  1.70it/s]Extractor Estimating: 15it [00:08,  1.69it/s]Extractor Estimating: 16it [00:09,  1.68it/s]Extractor Estimating: 17it [00:09,  1.72it/s]Extractor Estimating: 18it [00:10,  1.68it/s]Extractor Estimating: 19it [00:11,  1.70it/s]Extractor Estimating: 20it [00:11,  1.70it/s]Extractor Estimating: 21it [00:12,  1.77it/s]Extractor Estimating: 22it [00:12,  1.70it/s]Extractor Estimating: 23it [00:13,  1.68it/s]Extractor Estimating: 24it [00:14,  1.65it/s]Extractor Estimating: 25it [00:14,  1.61it/s]Extractor Estimating: 26it [00:15,  1.44it/s]Extractor Estimating: 27it [00:16,  1.49it/s]Extractor Estimating: 28it [00:16,  1.50it/s]Extractor Estimating: 29it [00:17,  1.54it/s]Extractor Estimating: 30it [00:18,  1.57it/s]Extractor Estimating: 31it [00:18,  1.63it/s]Extractor Estimating: 32it [00:19,  1.62it/s]Extractor Estimating: 33it [00:19,  1.63it/s]Extractor Estimating: 34it [00:20,  1.65it/s]Extractor Estimating: 35it [00:21,  1.67it/s]Extractor Estimating: 36it [00:21,  1.71it/s]Extractor Estimating: 37it [00:22,  1.71it/s]Extractor Estimating: 38it [00:22,  1.68it/s]Extractor Estimating: 39it [00:23,  1.65it/s]Extractor Estimating: 40it [00:24,  1.64it/s]Extractor Estimating: 41it [00:24,  1.64it/s]Extractor Estimating: 42it [00:25,  1.63it/s]Extractor Estimating: 43it [00:25,  1.66it/s]Extractor Estimating: 44it [00:26,  1.67it/s]Extractor Estimating: 45it [00:27,  1.70it/s]Extractor Estimating: 46it [00:27,  1.68it/s]Extractor Estimating: 47it [00:28,  1.72it/s]Extractor Estimating: 48it [00:28,  1.70it/s]Extractor Estimating: 49it [00:29,  1.67it/s]Extractor Estimating: 50it [00:30,  1.71it/s]Extractor Estimating: 51it [00:30,  1.74it/s]Extractor Estimating: 52it [00:31,  1.83it/s]Extractor Estimating: 53it [00:31,  1.83it/s]Extractor Estimating: 54it [00:32,  1.87it/s]Extractor Estimating: 55it [00:32,  1.90it/s]Extractor Estimating: 56it [00:33,  1.90it/s]Extractor Estimating: 57it [00:33,  1.89it/s]Extractor Estimating: 58it [00:34,  1.87it/s]Extractor Estimating: 59it [00:34,  1.90it/s]Extractor Estimating: 60it [00:35,  1.97it/s]Extractor Estimating: 61it [00:35,  1.99it/s]Extractor Estimating: 62it [00:36,  1.99it/s]Extractor Estimating: 63it [00:36,  2.07it/s]Extractor Estimating: 64it [00:37,  2.05it/s]Extractor Estimating: 65it [00:37,  1.94it/s]Extractor Estimating: 66it [00:38,  1.94it/s]Extractor Estimating: 67it [00:38,  1.95it/s]Extractor Estimating: 68it [00:39,  1.93it/s]Extractor Estimating: 69it [00:39,  1.95it/s]Extractor Estimating: 70it [00:40,  1.92it/s]Extractor Estimating: 71it [00:40,  1.95it/s]Extractor Estimating: 72it [00:41,  2.02it/s]Extractor Estimating: 73it [00:41,  1.93it/s]Extractor Estimating: 74it [00:42,  1.97it/s]Extractor Estimating: 75it [00:42,  1.84it/s]Extractor Estimating: 76it [00:43,  1.82it/s]Extractor Estimating: 77it [00:44,  1.83it/s]Extractor Estimating: 78it [00:44,  1.78it/s]Extractor Estimating: 79it [00:45,  1.69it/s]Extractor Estimating: 80it [00:45,  1.65it/s]Extractor Estimating: 81it [00:46,  1.62it/s]Extractor Estimating: 82it [00:47,  1.61it/s]Extractor Estimating: 83it [00:47,  1.61it/s]Extractor Estimating: 84it [00:48,  1.63it/s]Extractor Estimating: 85it [00:49,  1.66it/s]Extractor Estimating: 86it [00:49,  1.72it/s]Extractor Estimating: 87it [00:50,  1.68it/s]Extractor Estimating: 88it [00:50,  1.65it/s]Extractor Estimating: 89it [00:51,  1.62it/s]Extractor Estimating: 90it [00:52,  1.65it/s]Extractor Estimating: 91it [00:52,  1.66it/s]Extractor Estimating: 92it [00:53,  1.65it/s]Extractor Estimating: 93it [00:53,  1.66it/s]Extractor Estimating: 94it [00:54,  1.64it/s]Extractor Estimating: 95it [00:55,  1.59it/s]Extractor Estimating: 96it [00:55,  1.61it/s]Extractor Estimating: 97it [00:56,  1.60it/s]Extractor Estimating: 98it [00:56,  1.64it/s]Extractor Estimating: 99it [00:57,  1.70it/s]Extractor Estimating: 100it [00:58,  1.66it/s]Extractor Estimating: 101it [00:58,  1.76it/s]Extractor Estimating: 102it [00:59,  1.84it/s]Extractor Estimating: 103it [00:59,  1.81it/s]Extractor Estimating: 104it [01:00,  1.87it/s]Extractor Estimating: 105it [01:00,  1.92it/s]Extractor Estimating: 106it [01:01,  1.95it/s]Extractor Estimating: 107it [01:01,  1.86it/s]Extractor Estimating: 108it [01:02,  1.92it/s]Extractor Estimating: 109it [01:02,  1.88it/s]Extractor Estimating: 110it [01:03,  1.96it/s]Extractor Estimating: 111it [01:03,  1.97it/s]Extractor Estimating: 112it [01:04,  2.03it/s]Extractor Estimating: 113it [01:04,  1.84it/s]Extractor Estimating: 114it [01:05,  1.90it/s]Extractor Estimating: 115it [01:05,  1.92it/s]Extractor Estimating: 116it [01:06,  1.94it/s]Extractor Estimating: 117it [01:06,  1.94it/s]Extractor Estimating: 118it [01:07,  1.93it/s]Extractor Estimating: 119it [01:07,  1.98it/s]Extractor Estimating: 120it [01:08,  2.03it/s]Extractor Estimating: 121it [01:08,  1.96it/s]Extractor Estimating: 122it [01:09,  1.98it/s]Extractor Estimating: 123it [01:09,  2.00it/s]Extractor Estimating: 124it [01:10,  2.01it/s]Extractor Estimating: 125it [01:10,  1.99it/s]Extractor Estimating: 126it [01:11,  1.91it/s]Extractor Estimating: 127it [01:12,  1.85it/s]Extractor Estimating: 128it [01:12,  1.81it/s]Extractor Estimating: 129it [01:13,  1.80it/s]Extractor Estimating: 130it [01:13,  1.75it/s]Extractor Estimating: 131it [01:14,  1.75it/s]Extractor Estimating: 132it [01:14,  1.74it/s]Extractor Estimating: 133it [01:15,  1.80it/s]Extractor Estimating: 134it [01:16,  1.76it/s]Extractor Estimating: 135it [01:16,  1.73it/s]Extractor Estimating: 136it [01:17,  1.68it/s]Extractor Estimating: 137it [01:17,  1.71it/s]Extractor Estimating: 138it [01:18,  1.80it/s]Extractor Estimating: 139it [01:18,  1.80it/s]Extractor Estimating: 140it [01:19,  1.72it/s]Extractor Estimating: 141it [01:20,  1.71it/s]Extractor Estimating: 142it [01:20,  1.75it/s]Extractor Estimating: 143it [01:21,  1.76it/s]Extractor Estimating: 144it [01:21,  1.70it/s]Extractor Estimating: 145it [01:22,  1.74it/s]Extractor Estimating: 146it [01:22,  1.73it/s]Extractor Estimating: 147it [01:23,  1.79it/s]Extractor Estimating: 148it [01:24,  1.74it/s]Extractor Estimating: 149it [01:24,  1.73it/s]Extractor Estimating: 150it [01:25,  1.72it/s]Extractor Estimating: 151it [01:25,  1.71it/s]Extractor Estimating: 152it [01:26,  1.70it/s]Extractor Estimating: 153it [01:27,  1.60it/s]Extractor Estimating: 154it [01:27,  1.60it/s]Extractor Estimating: 155it [01:28,  1.58it/s]Extractor Estimating: 156it [01:29,  1.55it/s]Extractor Estimating: 157it [01:29,  1.56it/s]Extractor Estimating: 158it [01:30,  1.54it/s]Extractor Estimating: 159it [01:31,  1.57it/s]Extractor Estimating: 160it [01:31,  1.55it/s]Extractor Estimating: 161it [01:32,  1.57it/s]Extractor Estimating: 162it [01:32,  1.61it/s]Extractor Estimating: 163it [01:33,  1.60it/s]Extractor Estimating: 164it [01:34,  1.58it/s]Extractor Estimating: 165it [01:34,  1.57it/s]Extractor Estimating: 166it [01:35,  1.61it/s]Extractor Estimating: 167it [01:36,  1.62it/s]Extractor Estimating: 168it [01:36,  1.57it/s]Extractor Estimating: 169it [01:37,  1.56it/s]Extractor Estimating: 170it [01:37,  1.56it/s]Extractor Estimating: 171it [01:38,  1.57it/s]Extractor Estimating: 172it [01:39,  1.56it/s]Extractor Estimating: 173it [01:39,  1.58it/s]Extractor Estimating: 174it [01:40,  1.59it/s]Extractor Estimating: 175it [01:41,  1.61it/s]Extractor Estimating: 176it [01:41,  1.66it/s]Extractor Estimating: 177it [01:42,  1.66it/s]Extractor Estimating: 178it [01:42,  1.76it/s]Extractor Estimating: 179it [01:43,  1.81it/s]Extractor Estimating: 180it [01:43,  1.86it/s]Extractor Estimating: 181it [01:44,  1.84it/s]Extractor Estimating: 182it [01:44,  1.88it/s]Extractor Estimating: 183it [01:45,  1.89it/s]Extractor Estimating: 184it [01:45,  1.85it/s]Extractor Estimating: 185it [01:46,  1.91it/s]Extractor Estimating: 186it [01:46,  1.95it/s]Extractor Estimating: 187it [01:47,  1.89it/s]Extractor Estimating: 188it [01:47,  1.94it/s]Extractor Estimating: 189it [01:48,  1.90it/s]Extractor Estimating: 190it [01:48,  1.96it/s]Extractor Estimating: 191it [01:49,  1.97it/s]Extractor Estimating: 192it [01:49,  1.97it/s]Extractor Estimating: 193it [01:50,  1.92it/s]Extractor Estimating: 194it [01:51,  1.95it/s]Extractor Estimating: 195it [01:51,  1.92it/s]Extractor Estimating: 196it [01:52,  1.98it/s]Extractor Estimating: 197it [01:52,  1.90it/s]Extractor Estimating: 198it [01:53,  1.88it/s]Extractor Estimating: 199it [01:53,  1.91it/s]Extractor Estimating: 200it [01:54,  1.86it/s]Extractor Estimating: 201it [01:54,  1.78it/s]Extractor Estimating: 202it [01:55,  1.75it/s]Extractor Estimating: 203it [01:56,  1.73it/s]Extractor Estimating: 204it [01:56,  1.73it/s]Extractor Estimating: 205it [01:57,  1.57it/s]Extractor Estimating: 206it [01:57,  1.60it/s]Extractor Estimating: 207it [01:58,  1.61it/s]Extractor Estimating: 208it [01:59,  1.66it/s]Extractor Estimating: 209it [01:59,  1.64it/s]Extractor Estimating: 210it [02:00,  1.67it/s]Extractor Estimating: 211it [02:00,  1.69it/s]Extractor Estimating: 212it [02:01,  1.70it/s]Extractor Estimating: 213it [02:02,  1.70it/s]Extractor Estimating: 214it [02:02,  1.67it/s]Extractor Estimating: 215it [02:03,  1.66it/s]Extractor Estimating: 216it [02:03,  1.66it/s]Extractor Estimating: 217it [02:04,  1.71it/s]Extractor Estimating: 218it [02:05,  1.66it/s]Extractor Estimating: 219it [02:05,  1.69it/s]Extractor Estimating: 220it [02:06,  1.71it/s]Extractor Estimating: 221it [02:06,  1.69it/s]Extractor Estimating: 222it [02:07,  1.67it/s]Extractor Estimating: 223it [02:08,  1.63it/s]Extractor Estimating: 224it [02:08,  1.63it/s]Extractor Estimating: 225it [02:09,  1.60it/s]Extractor Estimating: 226it [02:10,  1.58it/s]Extractor Estimating: 227it [02:10,  1.62it/s]Extractor Estimating: 228it [02:11,  1.62it/s]Extractor Estimating: 229it [02:11,  1.65it/s]Extractor Estimating: 230it [02:12,  1.66it/s]Extractor Estimating: 231it [02:12,  1.68it/s]Extractor Estimating: 232it [02:13,  1.68it/s]Extractor Estimating: 233it [02:14,  1.69it/s]Extractor Estimating: 234it [02:14,  1.67it/s]Extractor Estimating: 235it [02:15,  1.64it/s]Extractor Estimating: 236it [02:16,  1.66it/s]Extractor Estimating: 237it [02:16,  1.65it/s]Extractor Estimating: 238it [02:17,  1.65it/s]Extractor Estimating: 239it [02:17,  1.65it/s]Extractor Estimating: 240it [02:18,  1.62it/s]Extractor Estimating: 241it [02:19,  1.62it/s]Extractor Estimating: 242it [02:19,  1.62it/s]Extractor Estimating: 243it [02:20,  1.69it/s]Extractor Estimating: 244it [02:20,  1.70it/s]Extractor Estimating: 245it [02:21,  1.72it/s]Extractor Estimating: 246it [02:21,  1.75it/s]Extractor Estimating: 247it [02:22,  1.78it/s]Extractor Estimating: 248it [02:23,  1.76it/s]Extractor Estimating: 249it [02:23,  1.65it/s]Extractor Estimating: 249it [02:23,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:19,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:19,265 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:19,265 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:19,265 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:19,265 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:48:20,004 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:48:20,005 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:48:20,735 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:48:21,850 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:48:21,850 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:23,365 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:23,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:23,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:23,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:48:23,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:48:23,893 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:48:23,894 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:48:24,195 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:48:24,423 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:48:24,423 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:24:59,072 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:24:59,204 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4975 mean pseudo reward: 0.9395603788515154
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 21769
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21869, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21869, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.934, loss:480.9884
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.940, loss:430.2760
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 0.933, loss:408.4692
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 0.932, loss:414.0211
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.933, loss:386.1552
>> valid entity prec:0.5324, rec:0.3824, f1:0.4451
>> valid relation prec:0.1640, rec:0.0305, f1:0.0515
>> valid relation with NER prec:0.1640, rec:0.0305, f1:0.0515
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.299, loss:389.2076
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.945, loss:356.3908
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.934, loss:395.3395
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.931, loss:381.0362
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.939, loss:396.5935
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4982, rec:0.3833, f1:0.4333
>> valid relation prec:0.1933, rec:0.0388, f1:0.0647
>> valid relation with NER prec:0.1933, rec:0.0388, f1:0.0647
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 3.298, loss:378.6019
g_step 1200, step 160, avg_time 0.940, loss:386.0059
g_step 1300, step 52, avg_time 0.935, loss:354.6024
g_step 1400, step 152, avg_time 0.940, loss:364.2316
g_step 1500, step 44, avg_time 0.936, loss:350.4970
>> valid entity prec:0.4794, rec:0.4274, f1:0.4519
>> valid relation prec:0.1517, rec:0.0295, f1:0.0494
>> valid relation with NER prec:0.1517, rec:0.0295, f1:0.0494
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 144, avg_time 3.340, loss:339.8052
g_step 1700, step 36, avg_time 0.940, loss:339.4292
g_step 1800, step 136, avg_time 0.934, loss:317.2277
g_step 1900, step 28, avg_time 0.930, loss:342.5726
g_step 2000, step 128, avg_time 0.946, loss:314.7454
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4837, rec:0.4188, f1:0.4489
>> valid relation prec:0.1363, rec:0.0296, f1:0.0486
>> valid relation with NER prec:0.1363, rec:0.0296, f1:0.0486
g_step 2100, step 20, avg_time 3.293, loss:311.0176
g_step 2200, step 120, avg_time 0.934, loss:297.8459
g_step 2300, step 12, avg_time 0.931, loss:307.4007
g_step 2400, step 112, avg_time 0.936, loss:282.5433
g_step 2500, step 4, avg_time 0.934, loss:279.9132
>> valid entity prec:0.4768, rec:0.4238, f1:0.4488
>> valid relation prec:0.1464, rec:0.0287, f1:0.0480
>> valid relation with NER prec:0.1464, rec:0.0287, f1:0.0480
g_step 2600, step 104, avg_time 3.301, loss:272.8988
g_step 2700, step 204, avg_time 0.939, loss:287.9423
g_step 2800, step 96, avg_time 0.933, loss:243.1732
g_step 2900, step 196, avg_time 0.944, loss:280.3733
g_step 3000, step 88, avg_time 0.935, loss:238.5544
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5053, rec:0.3824, f1:0.4354
>> valid relation prec:0.1118, rec:0.0223, f1:0.0372
>> valid relation with NER prec:0.1118, rec:0.0223, f1:0.0372
g_step 3100, step 188, avg_time 3.295, loss:261.9910
g_step 3200, step 80, avg_time 0.938, loss:247.2394
g_step 3300, step 180, avg_time 0.937, loss:243.8379
g_step 3400, step 72, avg_time 0.930, loss:231.4370
g_step 3500, step 172, avg_time 0.943, loss:242.9703
>> valid entity prec:0.4706, rec:0.3945, f1:0.4292
>> valid relation prec:0.1338, rec:0.0304, f1:0.0496
>> valid relation with NER prec:0.1338, rec:0.0304, f1:0.0496
g_step 3600, step 64, avg_time 3.284, loss:212.9221
g_step 3700, step 164, avg_time 0.940, loss:241.5459
g_step 3800, step 56, avg_time 0.938, loss:221.0476
g_step 3900, step 156, avg_time 0.944, loss:221.9365
g_step 4000, step 48, avg_time 0.930, loss:202.3017
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4831, rec:0.3894, f1:0.4312
>> valid relation prec:0.1151, rec:0.0295, f1:0.0469
>> valid relation with NER prec:0.1151, rec:0.0295, f1:0.0469
g_step 4100, step 148, avg_time 3.294, loss:215.1637
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:24:59 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:24:59 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-24-59_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:25:00 - WARNING - datasets.builder -   Using custom data configuration default-7f63ceb84ccb1b8b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7f63ceb84ccb1b8b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:25:01,253 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:25:01,254 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:25:01,255 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:25:01,256 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:25:01,294 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:25:01,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:25:01,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:25:01,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:25:01,306 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:25:01,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:25:01,306 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:25:01,541 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:25:04,782 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:25:04,796 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7f63ceb84ccb1b8b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.14ba/s] 40%|████      | 2/5 [00:00<00:00,  4.01ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.43ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.62ba/s]100%|██████████| 5/5 [00:01<00:00,  4.72ba/s]100%|██████████| 5/5 [00:01<00:00,  4.45ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:03,  2.43ba/s] 22%|██▏       | 2/9 [00:00<00:02,  3.34ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.75ba/s] 44%|████▍     | 4/9 [00:01<00:01,  4.02ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.19ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.32ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.38ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.41ba/s]100%|██████████| 9/9 [00:02<00:00,  4.95ba/s]100%|██████████| 9/9 [00:02<00:00,  4.26ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.66ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.24ba/s]100%|██████████| 5/5 [00:00<00:00,  8.83ba/s]100%|██████████| 5/5 [00:00<00:00,  7.88ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  4.26ba/s] 33%|███▎      | 3/9 [00:00<00:00,  7.87ba/s] 56%|█████▌    | 5/9 [00:00<00:00,  9.23ba/s] 78%|███████▊  | 7/9 [00:00<00:00,  7.65ba/s]100%|██████████| 9/9 [00:01<00:00,  9.07ba/s]100%|██████████| 9/9 [00:01<00:00,  8.41ba/s]
[INFO|trainer.py:414] 2023-08-28 16:25:11,802 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:25:11,843 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:25:11,844 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 16:25:11,844 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:25:11,844 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:25:11,844 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:25:11,844 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:25:11,844 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.35it/s]  1%|          | 2/390 [00:00<01:53,  3.42it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:51,  3.45it/s]  1%|▏         | 5/390 [00:01<01:52,  3.43it/s]  2%|▏         | 6/390 [00:01<01:52,  3.43it/s]  2%|▏         | 7/390 [00:02<01:51,  3.42it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.42it/s]  3%|▎         | 10/390 [00:02<01:51,  3.42it/s]  3%|▎         | 11/390 [00:03<01:50,  3.42it/s]  3%|▎         | 12/390 [00:03<01:51,  3.38it/s]  3%|▎         | 13/390 [00:03<01:51,  3.39it/s]  4%|▎         | 14/390 [00:04<01:50,  3.40it/s]  4%|▍         | 15/390 [00:04<01:50,  3.40it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:04<01:49,  3.41it/s]  5%|▍         | 18/390 [00:05<01:49,  3.41it/s]  5%|▍         | 19/390 [00:05<01:48,  3.41it/s]  5%|▌         | 20/390 [00:05<01:48,  3.41it/s]  5%|▌         | 21/390 [00:06<01:48,  3.41it/s]  6%|▌         | 22/390 [00:06<01:47,  3.41it/s]  6%|▌         | 23/390 [00:06<01:47,  3.42it/s]  6%|▌         | 24/390 [00:07<01:47,  3.41it/s]  6%|▋         | 25/390 [00:07<01:47,  3.41it/s]  7%|▋         | 26/390 [00:07<01:46,  3.41it/s]  7%|▋         | 27/390 [00:07<01:46,  3.41it/s]  7%|▋         | 28/390 [00:08<01:46,  3.41it/s]  7%|▋         | 29/390 [00:08<01:46,  3.40it/s]  8%|▊         | 30/390 [00:08<01:45,  3.40it/s]  8%|▊         | 31/390 [00:09<01:45,  3.40it/s]  8%|▊         | 32/390 [00:09<01:45,  3.40it/s]  8%|▊         | 33/390 [00:09<01:44,  3.41it/s]  9%|▊         | 34/390 [00:09<01:44,  3.41it/s]  9%|▉         | 35/390 [00:10<01:44,  3.41it/s]  9%|▉         | 36/390 [00:10<01:43,  3.41it/s]  9%|▉         | 37/390 [00:10<01:43,  3.41it/s] 10%|▉         | 38/390 [00:11<01:43,  3.41it/s] 10%|█         | 39/390 [00:11<01:42,  3.41it/s] 10%|█         | 40/390 [00:11<01:42,  3.41it/s] 11%|█         | 41/390 [00:12<01:42,  3.41it/s] 11%|█         | 42/390 [00:12<01:42,  3.41it/s] 11%|█         | 43/390 [00:12<01:41,  3.41it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.41it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.41it/s] 12%|█▏        | 46/390 [00:13<01:41,  3.37it/s] 12%|█▏        | 47/390 [00:13<01:41,  3.39it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.39it/s] 13%|█▎        | 49/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 50/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 51/390 [00:14<01:39,  3.40it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.40it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.41it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.40it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.40it/s] 14%|█▍        | 56/390 [00:16<01:38,  3.41it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.40it/s] 15%|█▍        | 58/390 [00:17<01:37,  3.40it/s] 15%|█▌        | 59/390 [00:17<01:37,  3.41it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.41it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.40it/s] 16%|█▌        | 62/390 [00:18<01:36,  3.40it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.41it/s] 16%|█▋        | 64/390 [00:18<01:36,  3.39it/s] 17%|█▋        | 65/390 [00:19<01:35,  3.39it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.40it/s] 17%|█▋        | 67/390 [00:19<01:35,  3.40it/s] 17%|█▋        | 68/390 [00:19<01:34,  3.40it/s] 18%|█▊        | 69/390 [00:20<01:34,  3.40it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.40it/s] 18%|█▊        | 71/390 [00:20<01:33,  3.40it/s] 18%|█▊        | 72/390 [00:21<01:33,  3.40it/s] 19%|█▊        | 73/390 [00:21<01:33,  3.40it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.40it/s] 19%|█▉        | 75/390 [00:22<01:32,  3.40it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.41it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.41it/s] 20%|██        | 78/390 [00:22<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 16:25:34,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:25:34,790 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 16:25:34,790 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.35it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.35it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.66it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.64it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.08it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.54it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.08it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.89it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 45.03it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.27it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.39it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.45it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.39it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.29it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 44.98it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.91it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.87it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.88it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 45.02it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.15it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.30it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.35it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.26it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.96it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.85it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.87it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.88it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.94it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.09it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.29it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.33it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.27it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.04it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.94it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.88it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.87it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 44.98it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.08it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.27it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.27it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.29it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.05it/s][A
 20%|██        | 217/1083 [00:04<00:19, 44.94it/s][A
 20%|██        | 222/1083 [00:04<00:19, 44.79it/s][A
 21%|██        | 227/1083 [00:05<00:19, 44.93it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 45.01it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 45.14it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.16it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.20it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.27it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.27it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.76it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.88it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.94it/s][A
 26%|██▌       | 277/1083 [00:06<00:17, 44.91it/s][A
 26%|██▌       | 282/1083 [00:06<00:17, 44.98it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 45.08it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 45.17it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 45.16it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.12it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 45.04it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.97it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 45.00it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.98it/s][A
 30%|███       | 327/1083 [00:07<00:16, 45.03it/s][A
 31%|███       | 332/1083 [00:07<00:16, 45.10it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.22it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.20it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.18it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.14it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 45.07it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.93it/s][A
 34%|███▍      | 367/1083 [00:08<00:16, 44.58it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.92it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 45.09it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.13it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.26it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.19it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.16it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.08it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 44.96it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 44.83it/s][A
 39%|███▊      | 417/1083 [00:09<00:14, 44.92it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 45.04it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 45.13it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 45.24it/s][A
 40%|████      | 437/1083 [00:09<00:14, 45.27it/s][A
 41%|████      | 442/1083 [00:09<00:14, 45.13it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.13it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.91it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.88it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.98it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.98it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.12it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.22it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.31it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 45.18it/s][A
 45%|████▌     | 492/1083 [00:10<00:14, 42.04it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 42.97it/s][A
 46%|████▋     | 502/1083 [00:11<00:13, 43.63it/s][A
 47%|████▋     | 507/1083 [00:11<00:13, 44.09it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.45it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.70it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.93it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 44.98it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 44.67it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 44.61it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.76it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 44.91it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.04it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 45.06it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 45.18it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.28it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 45.19it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 44.97it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.73it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.92it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 45.00it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 45.09it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.00it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.18it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.26it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.22it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 44.98it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.88it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.95it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.94it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 45.09it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.15it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.23it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.30it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 45.18it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.95it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.88it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.86it/s][A
 63%|██████▎   | 682/1083 [00:15<00:08, 44.88it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 45.04it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 45.15it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.32it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 45.29it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.23it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 44.97it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 44.89it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 42.26it/s][A
 67%|██████▋   | 727/1083 [00:16<00:08, 43.19it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 43.91it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 44.31it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 44.70it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 44.83it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 44.87it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 44.83it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 44.46it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 44.66it/s][A
 71%|███████▏  | 772/1083 [00:17<00:06, 44.87it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 45.11it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 45.07it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 45.24it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 45.27it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 45.20it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 44.88it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 44.72it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 44.77it/s][A
 75%|███████▌  | 817/1083 [00:18<00:05, 44.84it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 45.02it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 45.21it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 45.32it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 45.29it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 45.26it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 45.00it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.76it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.73it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.75it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.94it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 44.93it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.28it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.37it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 45.18it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 45.08it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.79it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.68it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.82it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 44.94it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.04it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.19it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.26it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 45.23it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 45.08it/s][A
 87%|████████▋ | 942/1083 [00:20<00:03, 44.87it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.76it/s][A
 88%|████████▊ | 952/1083 [00:21<00:03, 42.37it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 43.23it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 43.86it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 44.27it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.63it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.84it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 45.04it/s][A
 91%|█████████ | 987/1083 [00:21<00:02, 45.08it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 44.72it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 44.67it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 44.90it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 45.00it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.09it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 42.79it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 43.81it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.34it/s][A
 95%|█████████▌| 1032/1083 [00:22<00:01, 44.46it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 44.41it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 44.44it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.67it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 44.82it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 44.90it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 44.78it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 44.94it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 45.02it/s][A
 99%|█████████▉| 1077/1083 [00:23<00:00, 45.12it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 45.04it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:24<00:00, 45.04it/s][A 20%|██        | 78/390 [00:47<01:31,  3.40it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 16:25:59,291 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 16:25:59,535 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:26:03,654 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:26:03,888 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:26:03,997 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:03<1:04:41, 12.48s/it] 21%|██        | 80/390 [01:04<45:41,  8.84s/it]   21%|██        | 81/390 [01:04<32:19,  6.28s/it] 21%|██        | 82/390 [01:04<23:00,  4.48s/it] 21%|██▏       | 83/390 [01:05<16:30,  3.23s/it] 22%|██▏       | 84/390 [01:05<11:57,  2.35s/it] 22%|██▏       | 85/390 [01:05<08:47,  1.73s/it] 22%|██▏       | 86/390 [01:05<06:34,  1.30s/it] 22%|██▏       | 87/390 [01:06<05:02,  1.00it/s] 23%|██▎       | 88/390 [01:06<03:57,  1.27it/s] 23%|██▎       | 89/390 [01:06<03:12,  1.57it/s] 23%|██▎       | 90/390 [01:07<02:40,  1.87it/s] 23%|██▎       | 91/390 [01:07<02:24,  2.07it/s] 24%|██▎       | 92/390 [01:07<02:07,  2.35it/s] 24%|██▍       | 93/390 [01:08<01:54,  2.59it/s] 24%|██▍       | 94/390 [01:08<01:46,  2.79it/s] 24%|██▍       | 95/390 [01:08<01:40,  2.95it/s] 25%|██▍       | 96/390 [01:08<01:35,  3.07it/s] 25%|██▍       | 97/390 [01:09<01:32,  3.17it/s] 25%|██▌       | 98/390 [01:09<01:30,  3.24it/s] 25%|██▌       | 99/390 [01:09<01:28,  3.29it/s] 26%|██▌       | 100/390 [01:10<01:27,  3.32it/s] 26%|██▌       | 101/390 [01:10<01:29,  3.23it/s] 26%|██▌       | 102/390 [01:10<01:27,  3.28it/s] 26%|██▋       | 103/390 [01:11<01:26,  3.32it/s] 27%|██▋       | 104/390 [01:11<01:25,  3.35it/s] 27%|██▋       | 105/390 [01:11<01:24,  3.37it/s] 27%|██▋       | 106/390 [01:11<01:24,  3.38it/s] 27%|██▋       | 107/390 [01:12<01:23,  3.39it/s] 28%|██▊       | 108/390 [01:12<01:23,  3.39it/s] 28%|██▊       | 109/390 [01:12<01:22,  3.40it/s] 28%|██▊       | 110/390 [01:13<01:22,  3.40it/s] 28%|██▊       | 111/390 [01:13<01:22,  3.40it/s] 29%|██▊       | 112/390 [01:13<01:25,  3.27it/s] 29%|██▉       | 113/390 [01:13<01:23,  3.32it/s] 29%|██▉       | 114/390 [01:14<01:22,  3.36it/s] 29%|██▉       | 115/390 [01:14<01:21,  3.39it/s] 30%|██▉       | 116/390 [01:14<01:20,  3.41it/s] 30%|███       | 117/390 [01:15<01:19,  3.43it/s] 30%|███       | 118/390 [01:15<01:19,  3.43it/s] 31%|███       | 119/390 [01:15<01:18,  3.44it/s] 31%|███       | 120/390 [01:16<01:18,  3.45it/s] 31%|███       | 121/390 [01:16<01:17,  3.45it/s] 31%|███▏      | 122/390 [01:16<01:17,  3.45it/s] 32%|███▏      | 123/390 [01:16<01:20,  3.31it/s] 32%|███▏      | 124/390 [01:17<01:19,  3.35it/s] 32%|███▏      | 125/390 [01:17<01:18,  3.38it/s] 32%|███▏      | 126/390 [01:17<01:17,  3.40it/s] 33%|███▎      | 127/390 [01:18<01:16,  3.42it/s] 33%|███▎      | 128/390 [01:18<01:16,  3.43it/s] 33%|███▎      | 129/390 [01:18<01:15,  3.44it/s] 33%|███▎      | 130/390 [01:18<01:15,  3.44it/s] 34%|███▎      | 131/390 [01:19<01:15,  3.45it/s] 34%|███▍      | 132/390 [01:19<01:14,  3.45it/s] 34%|███▍      | 133/390 [01:19<01:14,  3.45it/s] 34%|███▍      | 134/390 [01:20<01:17,  3.31it/s] 35%|███▍      | 135/390 [01:20<01:16,  3.35it/s] 35%|███▍      | 136/390 [01:20<01:15,  3.38it/s] 35%|███▌      | 137/390 [01:21<01:14,  3.40it/s] 35%|███▌      | 138/390 [01:21<01:13,  3.42it/s] 36%|███▌      | 139/390 [01:21<01:13,  3.43it/s] 36%|███▌      | 140/390 [01:21<01:12,  3.44it/s] 36%|███▌      | 141/390 [01:22<01:12,  3.44it/s] 36%|███▋      | 142/390 [01:22<01:11,  3.45it/s] 37%|███▋      | 143/390 [01:22<01:11,  3.45it/s] 37%|███▋      | 144/390 [01:23<01:11,  3.45it/s] 37%|███▋      | 145/390 [01:23<01:14,  3.30it/s] 37%|███▋      | 146/390 [01:23<01:12,  3.34it/s] 38%|███▊      | 147/390 [01:23<01:12,  3.37it/s] 38%|███▊      | 148/390 [01:24<01:11,  3.40it/s] 38%|███▊      | 149/390 [01:24<01:10,  3.41it/s] 38%|███▊      | 150/390 [01:24<01:10,  3.43it/s] 39%|███▊      | 151/390 [01:25<01:09,  3.43it/s] 39%|███▉      | 152/390 [01:25<01:09,  3.44it/s] 39%|███▉      | 153/390 [01:25<01:08,  3.44it/s] 39%|███▉      | 154/390 [01:25<01:08,  3.45it/s] 40%|███▉      | 155/390 [01:26<01:08,  3.45it/s] 40%|████      | 156/390 [01:26<01:11,  3.26it/s][INFO|trainer.py:2140] 2023-08-28 16:26:38,514 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:26:38,514 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 16:26:38,514 >>   Batch size = 8
{'eval_loss': 0.9077656865119934, 'eval_runtime': 24.1118, 'eval_samples_per_second': 359.078, 'eval_steps_per_second': 44.916, 'epoch': 0.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.53it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.53it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.70it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.95it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.09it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.52it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.28it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.92it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.11it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.30it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.44it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.55it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.47it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.20it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.08it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 44.90it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 44.85it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.95it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 45.03it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.20it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.37it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.44it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.30it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.14it/s][A
 12%|█▏        | 128/1083 [00:02<00:22, 42.12it/s][A
 12%|█▏        | 133/1083 [00:02<00:22, 43.16it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 43.74it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 44.34it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 44.72it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.95it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.15it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.14it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.69it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.67it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.87it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.97it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 45.13it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 45.27it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.39it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.28it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.08it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 44.83it/s][A
 20%|██        | 218/1083 [00:04<00:19, 44.75it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.88it/s][A
 21%|██        | 228/1083 [00:05<00:18, 45.05it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 45.19it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 45.31it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 45.33it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.25it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.02it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.89it/s][A
 24%|██▍       | 263/1083 [00:05<00:19, 42.22it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 43.10it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 43.87it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 44.38it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 44.70it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.88it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.04it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.12it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 44.50it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 44.80it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 44.91it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.11it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.28it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.26it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.29it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.32it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.15it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 44.96it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 44.81it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 44.77it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.93it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 45.08it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 45.19it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 45.19it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.21it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.25it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.11it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 43.69it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 44.03it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.30it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 44.70it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 44.87it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 45.08it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 45.08it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 45.10it/s][A
 40%|████      | 438/1083 [00:09<00:14, 44.93it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.83it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.89it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.96it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.16it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.23it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.24it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.24it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.22it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.01it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.90it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 44.87it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.85it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 45.03it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 45.17it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 45.26it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.19it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.21it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 45.01it/s][A
 49%|████▉     | 533/1083 [00:11<00:13, 39.91it/s][A
 50%|████▉     | 538/1083 [00:12<00:13, 41.45it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 42.51it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 43.31it/s][A
 51%|█████     | 553/1083 [00:12<00:12, 43.80it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 44.28it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 44.62it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.79it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 44.58it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 44.55it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.76it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.96it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 45.14it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 45.23it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 45.23it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.28it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.09it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.04it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 44.92it/s][A
 58%|█████▊    | 628/1083 [00:13<00:10, 45.01it/s][A
 58%|█████▊    | 633/1083 [00:14<00:09, 45.10it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 45.21it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 45.20it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 45.18it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.12it/s][A
 61%|██████    | 658/1083 [00:14<00:09, 45.10it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 45.00it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 41.75it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 42.88it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 43.60it/s][A
 63%|██████▎   | 683/1083 [00:15<00:09, 44.22it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.59it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.71it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.81it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.68it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.49it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 44.47it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.79it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.99it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.11it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.24it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 45.25it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.30it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.12it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.00it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 44.95it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 45.01it/s][A
 71%|███████   | 768/1083 [00:17<00:06, 45.02it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 45.17it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.11it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.23it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.16it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.03it/s][A
 74%|███████▎  | 798/1083 [00:17<00:06, 44.69it/s][A
 74%|███████▍  | 803/1083 [00:17<00:06, 41.94it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 42.96it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 43.59it/s][A
 76%|███████▌  | 818/1083 [00:18<00:05, 44.19it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.50it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.82it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 45.00it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.99it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.69it/s][A
 78%|███████▊  | 848/1083 [00:18<00:05, 42.98it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 43.73it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.19it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 44.50it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 44.82it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.97it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.12it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 45.08it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.85it/s][A
 82%|████████▏ | 893/1083 [00:19<00:04, 44.82it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.86it/s][A
 83%|████████▎ | 903/1083 [00:20<00:04, 44.91it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 45.02it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 45.17it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 45.29it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.16it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 45.06it/s][A
 86%|████████▌ | 933/1083 [00:20<00:03, 44.89it/s][A
 87%|████████▋ | 938/1083 [00:20<00:03, 40.10it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 41.57it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 42.67it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 43.46it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 43.90it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 44.40it/s][A
 89%|████████▉ | 968/1083 [00:21<00:02, 44.65it/s][A
 90%|████████▉ | 973/1083 [00:21<00:02, 44.83it/s][A
 90%|█████████ | 978/1083 [00:21<00:02, 44.61it/s][A
 91%|█████████ | 983/1083 [00:21<00:02, 44.61it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.74it/s][A
 92%|█████████▏| 993/1083 [00:22<00:01, 45.02it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 45.15it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 45.12it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 45.16it/s][A
 94%|█████████▎| 1013/1083 [00:22<00:01, 45.11it/s][A
 94%|█████████▍| 1018/1083 [00:22<00:01, 45.09it/s][A
 94%|█████████▍| 1023/1083 [00:22<00:01, 44.85it/s][A
 95%|█████████▍| 1028/1083 [00:22<00:01, 44.84it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 44.85it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:01, 44.92it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:00, 45.09it/s][A
 97%|█████████▋| 1048/1083 [00:23<00:00, 45.16it/s][A
 97%|█████████▋| 1053/1083 [00:23<00:00, 45.29it/s][A
 98%|█████████▊| 1058/1083 [00:23<00:00, 45.18it/s][A
 98%|█████████▊| 1063/1083 [00:23<00:00, 45.11it/s][A
 99%|█████████▊| 1068/1083 [00:23<00:00, 44.98it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 42.15it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 43.13it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 43.76it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 43.76it/s][A 40%|████      | 156/390 [01:50<01:11,  3.26it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 16:27:03,160 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 16:27:03,532 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:27:07,342 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:27:07,530 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:27:07,624 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [02:08<49:16, 12.69s/it] 41%|████      | 158/390 [02:08<34:45,  8.99s/it] 41%|████      | 159/390 [02:08<24:33,  6.38s/it] 41%|████      | 160/390 [02:09<17:27,  4.55s/it] 41%|████▏     | 161/390 [02:09<12:30,  3.28s/it] 42%|████▏     | 162/390 [02:09<09:02,  2.38s/it] 42%|████▏     | 163/390 [02:10<06:38,  1.75s/it] 42%|████▏     | 164/390 [02:10<04:57,  1.32s/it] 42%|████▏     | 165/390 [02:10<03:47,  1.01s/it] 43%|████▎     | 166/390 [02:10<02:57,  1.26it/s] 43%|████▎     | 167/390 [02:11<02:23,  1.55it/s] 43%|████▎     | 168/390 [02:11<01:59,  1.86it/s] 43%|████▎     | 169/390 [02:11<01:47,  2.05it/s] 44%|████▎     | 170/390 [02:12<01:34,  2.33it/s] 44%|████▍     | 171/390 [02:12<01:25,  2.57it/s] 44%|████▍     | 172/390 [02:12<01:18,  2.78it/s] 44%|████▍     | 173/390 [02:13<01:13,  2.94it/s] 45%|████▍     | 174/390 [02:13<01:10,  3.07it/s] 45%|████▍     | 175/390 [02:13<01:07,  3.17it/s] 45%|████▌     | 176/390 [02:13<01:06,  3.24it/s] 45%|████▌     | 177/390 [02:14<01:04,  3.29it/s] 46%|████▌     | 178/390 [02:14<01:03,  3.33it/s] 46%|████▌     | 179/390 [02:14<01:05,  3.22it/s] 46%|████▌     | 180/390 [02:15<01:04,  3.27it/s] 46%|████▋     | 181/390 [02:15<01:03,  3.31it/s] 47%|████▋     | 182/390 [02:15<01:02,  3.34it/s] 47%|████▋     | 183/390 [02:16<01:01,  3.36it/s] 47%|████▋     | 184/390 [02:16<01:01,  3.38it/s] 47%|████▋     | 185/390 [02:16<01:00,  3.38it/s] 48%|████▊     | 186/390 [02:16<01:00,  3.39it/s] 48%|████▊     | 187/390 [02:17<00:59,  3.40it/s] 48%|████▊     | 188/390 [02:17<00:59,  3.40it/s] 48%|████▊     | 189/390 [02:17<00:59,  3.40it/s] 49%|████▊     | 190/390 [02:18<01:01,  3.23it/s] 49%|████▉     | 191/390 [02:18<01:00,  3.28it/s] 49%|████▉     | 192/390 [02:18<00:59,  3.32it/s] 49%|████▉     | 193/390 [02:18<00:59,  3.34it/s] 50%|████▉     | 194/390 [02:19<00:58,  3.36it/s] 50%|█████     | 195/390 [02:19<00:57,  3.37it/s] 50%|█████     | 196/390 [02:19<00:57,  3.38it/s] 51%|█████     | 197/390 [02:20<00:56,  3.39it/s] 51%|█████     | 198/390 [02:20<00:56,  3.39it/s] 51%|█████     | 199/390 [02:20<00:56,  3.40it/s] 51%|█████▏    | 200/390 [02:21<00:55,  3.40it/s] 52%|█████▏    | 201/390 [02:21<00:55,  3.40it/s] 52%|█████▏    | 202/390 [02:21<00:58,  3.23it/s] 52%|█████▏    | 203/390 [02:21<00:56,  3.28it/s] 52%|█████▏    | 204/390 [02:22<00:55,  3.33it/s] 53%|█████▎    | 205/390 [02:22<00:55,  3.36it/s] 53%|█████▎    | 206/390 [02:22<00:54,  3.39it/s] 53%|█████▎    | 207/390 [02:23<00:53,  3.41it/s] 53%|█████▎    | 208/390 [02:23<00:53,  3.42it/s] 54%|█████▎    | 209/390 [02:23<00:52,  3.43it/s] 54%|█████▍    | 210/390 [02:24<00:52,  3.44it/s] 54%|█████▍    | 211/390 [02:24<00:51,  3.45it/s] 54%|█████▍    | 212/390 [02:24<00:51,  3.45it/s] 55%|█████▍    | 213/390 [02:24<00:53,  3.28it/s] 55%|█████▍    | 214/390 [02:25<00:52,  3.33it/s] 55%|█████▌    | 215/390 [02:25<00:51,  3.37it/s] 55%|█████▌    | 216/390 [02:25<00:51,  3.39it/s] 56%|█████▌    | 217/390 [02:26<00:50,  3.41it/s] 56%|█████▌    | 218/390 [02:26<00:50,  3.42it/s] 56%|█████▌    | 219/390 [02:26<00:49,  3.43it/s] 56%|█████▋    | 220/390 [02:26<00:49,  3.44it/s] 57%|█████▋    | 221/390 [02:27<00:49,  3.44it/s] 57%|█████▋    | 222/390 [02:27<00:48,  3.45it/s] 57%|█████▋    | 223/390 [02:27<00:48,  3.45it/s] 57%|█████▋    | 224/390 [02:28<00:50,  3.32it/s] 58%|█████▊    | 225/390 [02:28<00:49,  3.35it/s] 58%|█████▊    | 226/390 [02:28<00:48,  3.39it/s] 58%|█████▊    | 227/390 [02:29<00:47,  3.40it/s] 58%|█████▊    | 228/390 [02:29<00:47,  3.42it/s] 59%|█████▊    | 229/390 [02:29<00:46,  3.43it/s] 59%|█████▉    | 230/390 [02:29<00:46,  3.44it/s] 59%|█████▉    | 231/390 [02:30<00:46,  3.44it/s] 59%|█████▉    | 232/390 [02:30<00:45,  3.44it/s] 60%|█████▉    | 233/390 [02:30<00:45,  3.45it/s] 60%|██████    | 234/390 [02:31<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:27:42,978 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:27:42,978 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 16:27:42,978 >>   Batch size = 8
{'eval_loss': 0.9291279911994934, 'eval_runtime': 24.2391, 'eval_samples_per_second': 357.192, 'eval_steps_per_second': 44.68, 'epoch': 1.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.61it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.56it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.60it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.84it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.46it/s][A
  3%|▎         | 33/1083 [00:00<00:22, 46.18it/s][A
  4%|▎         | 38/1083 [00:00<00:22, 45.93it/s][A
  4%|▍         | 43/1083 [00:00<00:22, 45.22it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.02it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.05it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.23it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.27it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.29it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.39it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.46it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 45.32it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 45.00it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.78it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 44.91it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.00it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.20it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.26it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.35it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.38it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 45.27it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 45.01it/s][A
 13%|█▎        | 138/1083 [00:03<00:22, 42.10it/s][A
 13%|█▎        | 143/1083 [00:03<00:21, 43.09it/s][A
 14%|█▎        | 148/1083 [00:03<00:21, 43.85it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 44.32it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 44.62it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 44.96it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 45.01it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.94it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.59it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.58it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.69it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.89it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.00it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.10it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.31it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.36it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.23it/s][A
 21%|██        | 223/1083 [00:04<00:19, 44.85it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.74it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.89it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.89it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 45.01it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.11it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.33it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.34it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.21it/s][A
 25%|██▍       | 268/1083 [00:05<00:18, 44.92it/s][A
 25%|██▌       | 273/1083 [00:06<00:19, 42.52it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 43.42it/s][A
 26%|██▌       | 283/1083 [00:06<00:18, 43.96it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.35it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.68it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.99it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.09it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.12it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 44.69it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.73it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 44.88it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.08it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.12it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.15it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.28it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.21it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 45.11it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 44.83it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.79it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 44.87it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.99it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 45.15it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.20it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.32it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.29it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 45.18it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 44.96it/s][A
 38%|███▊      | 408/1083 [00:09<00:16, 41.13it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 42.41it/s][A
 39%|███▊      | 418/1083 [00:09<00:15, 43.32it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.01it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.48it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.74it/s][A
 40%|████      | 438/1083 [00:09<00:14, 44.87it/s][A
 41%|████      | 443/1083 [00:09<00:14, 44.77it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 44.53it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.44it/s][A
 42%|████▏     | 458/1083 [00:10<00:14, 44.61it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.85it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.11it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.29it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.38it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.31it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 44.91it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 44.84it/s][A
 46%|████▌     | 498/1083 [00:11<00:13, 44.58it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.65it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.69it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 45.01it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.19it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.35it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 45.45it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 45.28it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.89it/s][A
 50%|█████     | 543/1083 [00:12<00:13, 39.26it/s][A
 51%|█████     | 548/1083 [00:12<00:13, 40.97it/s][A
 51%|█████     | 553/1083 [00:12<00:12, 42.33it/s][A
 52%|█████▏    | 558/1083 [00:12<00:12, 43.24it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 43.98it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 44.44it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 44.82it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 44.96it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.58it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.30it/s][A
 55%|█████▍    | 593/1083 [00:13<00:11, 44.37it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.56it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.84it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 45.14it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 45.15it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 45.29it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 45.27it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 45.02it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.69it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.68it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.69it/s][A
 60%|█████▉    | 648/1083 [00:14<00:11, 36.29it/s][A
 60%|██████    | 653/1083 [00:14<00:11, 38.66it/s][A
 61%|██████    | 658/1083 [00:14<00:10, 40.48it/s][A
 61%|██████    | 663/1083 [00:14<00:10, 41.72it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 42.86it/s][A
 62%|██████▏   | 673/1083 [00:15<00:10, 40.56it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 41.91it/s][A
 63%|██████▎   | 683/1083 [00:15<00:09, 42.83it/s][A
 64%|██████▎   | 688/1083 [00:15<00:09, 43.36it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 43.93it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.32it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.70it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.82it/s][A
 66%|██████▌   | 713/1083 [00:16<00:08, 44.69it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 44.80it/s][A
 67%|██████▋   | 723/1083 [00:16<00:08, 44.88it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 44.88it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 44.87it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.92it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 45.07it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 45.14it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 45.13it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 45.00it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 44.93it/s][A
 71%|███████   | 768/1083 [00:17<00:07, 44.96it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 44.92it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 44.97it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 45.04it/s][A
 73%|███████▎  | 788/1083 [00:17<00:06, 45.12it/s][A
 73%|███████▎  | 793/1083 [00:17<00:06, 45.13it/s][A
 74%|███████▎  | 798/1083 [00:17<00:07, 39.51it/s][A
 74%|███████▍  | 803/1083 [00:18<00:06, 41.19it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 42.41it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 43.29it/s][A
 76%|███████▌  | 818/1083 [00:18<00:06, 44.04it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 44.49it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 44.81it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 44.90it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.54it/s][A
 78%|███████▊  | 843/1083 [00:18<00:05, 44.37it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.50it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 44.65it/s][A
 79%|███████▉  | 858/1083 [00:19<00:05, 44.92it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.02it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 45.23it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 45.35it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 45.29it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.93it/s][A
 82%|████████▏ | 888/1083 [00:19<00:04, 44.69it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 44.71it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 44.81it/s][A
 83%|████████▎ | 903/1083 [00:20<00:03, 45.01it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 45.15it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 45.13it/s][A
 85%|████████▍ | 918/1083 [00:20<00:03, 45.25it/s][A
 85%|████████▌ | 923/1083 [00:20<00:03, 45.22it/s][A
 86%|████████▌ | 928/1083 [00:20<00:03, 44.96it/s][A
 86%|████████▌ | 933/1083 [00:21<00:03, 40.10it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 41.62it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 42.63it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 43.48it/s][A
 88%|████████▊ | 953/1083 [00:21<00:02, 43.99it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 44.44it/s][A
 89%|████████▉ | 963/1083 [00:21<00:03, 36.29it/s][A
 89%|████████▉ | 969/1083 [00:21<00:02, 40.21it/s][A
 90%|████████▉ | 974/1083 [00:21<00:02, 41.55it/s][A
 90%|█████████ | 979/1083 [00:22<00:02, 42.66it/s][A
 91%|█████████ | 984/1083 [00:22<00:02, 43.45it/s][A
 91%|█████████▏| 989/1083 [00:22<00:02, 44.00it/s][A
 92%|█████████▏| 994/1083 [00:22<00:02, 44.38it/s][A
 92%|█████████▏| 999/1083 [00:22<00:01, 44.66it/s][A
 93%|█████████▎| 1004/1083 [00:22<00:01, 44.72it/s][A
 93%|█████████▎| 1009/1083 [00:22<00:01, 44.39it/s][A
 94%|█████████▎| 1014/1083 [00:22<00:01, 44.47it/s][A
 94%|█████████▍| 1019/1083 [00:22<00:01, 44.69it/s][A
 95%|█████████▍| 1024/1083 [00:23<00:01, 44.99it/s][A
 95%|█████████▌| 1029/1083 [00:23<00:01, 45.11it/s][A
 95%|█████████▌| 1034/1083 [00:23<00:01, 45.13it/s][A
 96%|█████████▌| 1039/1083 [00:23<00:00, 45.14it/s][A
 96%|█████████▋| 1044/1083 [00:23<00:00, 45.18it/s][A
 97%|█████████▋| 1049/1083 [00:23<00:00, 44.97it/s][A
 97%|█████████▋| 1054/1083 [00:23<00:00, 44.76it/s][A
 98%|█████████▊| 1059/1083 [00:23<00:00, 44.69it/s][A
 98%|█████████▊| 1064/1083 [00:24<00:00, 40.94it/s][A
 99%|█████████▊| 1069/1083 [00:24<00:00, 42.21it/s][A
 99%|█████████▉| 1074/1083 [00:24<00:00, 43.10it/s][A
100%|█████████▉| 1079/1083 [00:24<00:00, 43.78it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 43.78it/s][A 60%|██████    | 234/390 [02:55<00:45,  3.45it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 16:28:07,897 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:28:08,231 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:28:12,781 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:28:13,080 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:28:13,211 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [03:13<33:02, 12.79s/it] 61%|██████    | 236/390 [03:13<23:13,  9.05s/it] 61%|██████    | 237/390 [03:13<16:22,  6.42s/it] 61%|██████    | 238/390 [03:13<11:36,  4.58s/it] 61%|██████▏   | 239/390 [03:14<08:17,  3.30s/it] 62%|██████▏   | 240/390 [03:14<05:59,  2.40s/it] 62%|██████▏   | 241/390 [03:14<04:22,  1.76s/it] 62%|██████▏   | 242/390 [03:15<03:15,  1.32s/it] 62%|██████▏   | 243/390 [03:15<02:29,  1.01s/it] 63%|██████▎   | 244/390 [03:15<01:56,  1.25it/s] 63%|██████▎   | 245/390 [03:15<01:33,  1.55it/s] 63%|██████▎   | 246/390 [03:16<01:17,  1.85it/s] 63%|██████▎   | 247/390 [03:16<01:09,  2.07it/s] 64%|██████▎   | 248/390 [03:16<01:00,  2.34it/s] 64%|██████▍   | 249/390 [03:17<00:54,  2.59it/s] 64%|██████▍   | 250/390 [03:17<00:50,  2.79it/s] 64%|██████▍   | 251/390 [03:17<00:47,  2.95it/s] 65%|██████▍   | 252/390 [03:18<00:44,  3.07it/s] 65%|██████▍   | 253/390 [03:18<00:43,  3.16it/s] 65%|██████▌   | 254/390 [03:18<00:42,  3.24it/s] 65%|██████▌   | 255/390 [03:18<00:41,  3.29it/s] 66%|██████▌   | 256/390 [03:19<00:40,  3.32it/s] 66%|██████▌   | 257/390 [03:19<00:39,  3.35it/s] 66%|██████▌   | 258/390 [03:19<00:40,  3.24it/s] 66%|██████▋   | 259/390 [03:20<00:39,  3.28it/s] 67%|██████▋   | 260/390 [03:20<00:39,  3.32it/s] 67%|██████▋   | 261/390 [03:20<00:38,  3.35it/s] 67%|██████▋   | 262/390 [03:21<00:38,  3.37it/s] 67%|██████▋   | 263/390 [03:21<00:37,  3.38it/s] 68%|██████▊   | 264/390 [03:21<00:37,  3.39it/s] 68%|██████▊   | 265/390 [03:21<00:36,  3.39it/s] 68%|██████▊   | 266/390 [03:22<00:36,  3.41it/s] 68%|██████▊   | 267/390 [03:22<00:35,  3.42it/s] 69%|██████▊   | 268/390 [03:22<00:35,  3.44it/s] 69%|██████▉   | 269/390 [03:23<00:36,  3.31it/s] 69%|██████▉   | 270/390 [03:23<00:35,  3.36it/s] 69%|██████▉   | 271/390 [03:23<00:35,  3.38it/s] 70%|██████▉   | 272/390 [03:23<00:34,  3.41it/s] 70%|███████   | 273/390 [03:24<00:34,  3.42it/s] 70%|███████   | 274/390 [03:24<00:33,  3.43it/s] 71%|███████   | 275/390 [03:24<00:33,  3.44it/s] 71%|███████   | 276/390 [03:25<00:33,  3.45it/s] 71%|███████   | 277/390 [03:25<00:32,  3.45it/s] 71%|███████▏  | 278/390 [03:25<00:32,  3.45it/s] 72%|███████▏  | 279/390 [03:26<00:32,  3.45it/s] 72%|███████▏  | 280/390 [03:26<00:34,  3.18it/s] 72%|███████▏  | 281/390 [03:26<00:33,  3.26it/s] 72%|███████▏  | 282/390 [03:26<00:32,  3.32it/s] 73%|███████▎  | 283/390 [03:27<00:31,  3.36it/s] 73%|███████▎  | 284/390 [03:27<00:31,  3.39it/s] 73%|███████▎  | 285/390 [03:27<00:30,  3.41it/s] 73%|███████▎  | 286/390 [03:28<00:30,  3.42it/s] 74%|███████▎  | 287/390 [03:28<00:30,  3.43it/s] 74%|███████▍  | 288/390 [03:28<00:29,  3.44it/s] 74%|███████▍  | 289/390 [03:28<00:29,  3.44it/s] 74%|███████▍  | 290/390 [03:29<00:29,  3.44it/s] 75%|███████▍  | 291/390 [03:29<00:29,  3.33it/s] 75%|███████▍  | 292/390 [03:29<00:29,  3.37it/s] 75%|███████▌  | 293/390 [03:30<00:28,  3.39it/s] 75%|███████▌  | 294/390 [03:30<00:28,  3.41it/s] 76%|███████▌  | 295/390 [03:30<00:27,  3.42it/s] 76%|███████▌  | 296/390 [03:31<00:27,  3.43it/s] 76%|███████▌  | 297/390 [03:31<00:27,  3.44it/s] 76%|███████▋  | 298/390 [03:31<00:26,  3.44it/s] 77%|███████▋  | 299/390 [03:31<00:26,  3.44it/s] 77%|███████▋  | 300/390 [03:32<00:26,  3.45it/s] 77%|███████▋  | 301/390 [03:32<00:25,  3.45it/s] 77%|███████▋  | 302/390 [03:32<00:26,  3.32it/s] 78%|███████▊  | 303/390 [03:33<00:25,  3.36it/s] 78%|███████▊  | 304/390 [03:33<00:25,  3.39it/s] 78%|███████▊  | 305/390 [03:33<00:24,  3.41it/s] 78%|███████▊  | 306/390 [03:33<00:24,  3.42it/s] 79%|███████▊  | 307/390 [03:34<00:24,  3.43it/s] 79%|███████▉  | 308/390 [03:34<00:23,  3.44it/s] 79%|███████▉  | 309/390 [03:34<00:23,  3.44it/s] 79%|███████▉  | 310/390 [03:35<00:23,  3.44it/s] 80%|███████▉  | 311/390 [03:35<00:22,  3.45it/s] 80%|████████  | 312/390 [03:35<00:22,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 16:28:47,647 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:28:47,647 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 16:28:47,647 >>   Batch size = 8
{'eval_loss': 0.9346216917037964, 'eval_runtime': 24.4457, 'eval_samples_per_second': 354.173, 'eval_steps_per_second': 44.302, 'epoch': 2.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.60it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.28it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.77it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.80it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.31it/s][A
  3%|▎         | 32/1083 [00:00<00:22, 45.99it/s][A
  3%|▎         | 37/1083 [00:00<00:22, 45.85it/s][A
  4%|▍         | 42/1083 [00:00<00:22, 45.32it/s][A
  4%|▍         | 47/1083 [00:01<00:23, 45.03it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.09it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.10it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.21it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.23it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.24it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.31it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 45.26it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.96it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.83it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.83it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.06it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.10it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.23it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.25it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.25it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 45.17it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 45.00it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.85it/s][A
 13%|█▎        | 142/1083 [00:03<00:22, 42.37it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 43.26it/s][A
 14%|█▍        | 152/1083 [00:03<00:21, 43.93it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.39it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.74it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 45.02it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 45.09it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 45.10it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.71it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.62it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.90it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.04it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.23it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.18it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.30it/s][A
 20%|██        | 217/1083 [00:04<00:19, 45.27it/s][A
 20%|██        | 222/1083 [00:04<00:19, 45.18it/s][A
 21%|██        | 227/1083 [00:05<00:19, 45.03it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 44.86it/s][A
 22%|██▏       | 237/1083 [00:05<00:18, 44.99it/s][A
 22%|██▏       | 242/1083 [00:05<00:18, 45.05it/s][A
 23%|██▎       | 247/1083 [00:05<00:18, 45.23it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 45.28it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 45.26it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 45.26it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 45.25it/s][A
 25%|██▌       | 272/1083 [00:06<00:17, 45.14it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.01it/s][A
 26%|██▌       | 282/1083 [00:06<00:18, 44.47it/s][A
 27%|██▋       | 287/1083 [00:06<00:17, 44.66it/s][A
 27%|██▋       | 292/1083 [00:06<00:17, 44.92it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.90it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 45.08it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.98it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 45.05it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.79it/s][A
 30%|██▉       | 322/1083 [00:07<00:16, 44.83it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.91it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.94it/s][A
 31%|███       | 337/1083 [00:07<00:16, 45.11it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 45.23it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.32it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.26it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 45.17it/s][A
 33%|███▎      | 362/1083 [00:08<00:16, 44.96it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.99it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.90it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.94it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 45.08it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 45.14it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.28it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.20it/s][A
 37%|███▋      | 402/1083 [00:08<00:15, 45.08it/s][A
 38%|███▊      | 407/1083 [00:09<00:15, 45.00it/s][A
 38%|███▊      | 412/1083 [00:09<00:15, 42.73it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 43.47it/s][A
 39%|███▉      | 422/1083 [00:09<00:14, 44.07it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 44.51it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.67it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.89it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.36it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 44.61it/s][A
 42%|████▏     | 452/1083 [00:10<00:14, 44.44it/s][A
 42%|████▏     | 457/1083 [00:10<00:14, 44.49it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.71it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.96it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 45.20it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.22it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.14it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 45.05it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 45.09it/s][A
 46%|████▌     | 497/1083 [00:11<00:13, 44.88it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 44.89it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 45.06it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 45.25it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 45.21it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.15it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 45.13it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 45.09it/s][A
 50%|█████     | 542/1083 [00:12<00:12, 44.86it/s][A
 51%|█████     | 547/1083 [00:12<00:12, 43.53it/s][A
 51%|█████     | 552/1083 [00:12<00:12, 43.84it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.54it/s][A
 52%|█████▏    | 562/1083 [00:12<00:11, 44.76it/s][A
 52%|█████▏    | 567/1083 [00:12<00:11, 45.01it/s][A
 53%|█████▎    | 572/1083 [00:12<00:11, 44.98it/s][A
 53%|█████▎    | 577/1083 [00:12<00:11, 45.03it/s][A
 54%|█████▎    | 582/1083 [00:12<00:11, 44.92it/s][A
 54%|█████▍    | 587/1083 [00:13<00:11, 44.63it/s][A
 55%|█████▍    | 592/1083 [00:13<00:10, 44.69it/s][A
 55%|█████▌    | 597/1083 [00:13<00:10, 44.84it/s][A
 56%|█████▌    | 602/1083 [00:13<00:10, 45.02it/s][A
 56%|█████▌    | 607/1083 [00:13<00:10, 45.16it/s][A
 57%|█████▋    | 612/1083 [00:13<00:10, 45.29it/s][A
 57%|█████▋    | 617/1083 [00:13<00:10, 45.19it/s][A
 57%|█████▋    | 622/1083 [00:13<00:10, 45.09it/s][A
 58%|█████▊    | 627/1083 [00:13<00:10, 44.96it/s][A
 58%|█████▊    | 632/1083 [00:14<00:10, 44.75it/s][A
 59%|█████▉    | 637/1083 [00:14<00:09, 44.74it/s][A
 59%|█████▉    | 642/1083 [00:14<00:09, 44.88it/s][A
 60%|█████▉    | 647/1083 [00:14<00:09, 45.09it/s][A
 60%|██████    | 652/1083 [00:14<00:09, 45.14it/s][A
 61%|██████    | 657/1083 [00:14<00:09, 45.30it/s][A
 61%|██████    | 662/1083 [00:14<00:09, 43.79it/s][A
 62%|██████▏   | 667/1083 [00:14<00:09, 44.25it/s][A
 62%|██████▏   | 672/1083 [00:14<00:09, 44.40it/s][A
 63%|██████▎   | 677/1083 [00:15<00:09, 44.33it/s][A
 63%|██████▎   | 682/1083 [00:15<00:09, 44.41it/s][A
 63%|██████▎   | 687/1083 [00:15<00:08, 44.74it/s][A
 64%|██████▍   | 692/1083 [00:15<00:08, 44.89it/s][A
 64%|██████▍   | 697/1083 [00:15<00:08, 45.09it/s][A
 65%|██████▍   | 702/1083 [00:15<00:08, 44.89it/s][A
 65%|██████▌   | 707/1083 [00:15<00:08, 45.07it/s][A
 66%|██████▌   | 712/1083 [00:15<00:08, 45.09it/s][A
 66%|██████▌   | 717/1083 [00:15<00:08, 45.01it/s][A
 67%|██████▋   | 722/1083 [00:16<00:08, 44.88it/s][A
 67%|██████▋   | 727/1083 [00:16<00:07, 44.76it/s][A
 68%|██████▊   | 732/1083 [00:16<00:07, 44.95it/s][A
 68%|██████▊   | 737/1083 [00:16<00:07, 45.10it/s][A
 69%|██████▊   | 742/1083 [00:16<00:07, 45.20it/s][A
 69%|██████▉   | 747/1083 [00:16<00:07, 45.03it/s][A
 69%|██████▉   | 752/1083 [00:16<00:07, 45.04it/s][A
 70%|██████▉   | 757/1083 [00:16<00:07, 45.07it/s][A
 70%|███████   | 762/1083 [00:16<00:07, 45.06it/s][A
 71%|███████   | 767/1083 [00:17<00:07, 42.29it/s][A
 71%|███████▏  | 772/1083 [00:17<00:07, 43.39it/s][A
 72%|███████▏  | 777/1083 [00:17<00:06, 43.94it/s][A
 72%|███████▏  | 782/1083 [00:17<00:06, 44.41it/s][A
 73%|███████▎  | 787/1083 [00:17<00:06, 44.65it/s][A
 73%|███████▎  | 792/1083 [00:17<00:06, 44.88it/s][A
 74%|███████▎  | 797/1083 [00:17<00:06, 42.07it/s][A
 74%|███████▍  | 802/1083 [00:17<00:06, 43.03it/s][A
 75%|███████▍  | 807/1083 [00:17<00:06, 43.52it/s][A
 75%|███████▍  | 812/1083 [00:18<00:06, 43.95it/s][A
 75%|███████▌  | 817/1083 [00:18<00:06, 44.31it/s][A
 76%|███████▌  | 822/1083 [00:18<00:05, 44.68it/s][A
 76%|███████▋  | 827/1083 [00:18<00:05, 44.90it/s][A
 77%|███████▋  | 832/1083 [00:18<00:05, 44.94it/s][A
 77%|███████▋  | 837/1083 [00:18<00:05, 44.70it/s][A
 78%|███████▊  | 842/1083 [00:18<00:05, 44.82it/s][A
 78%|███████▊  | 847/1083 [00:18<00:05, 44.89it/s][A
 79%|███████▊  | 852/1083 [00:18<00:05, 44.84it/s][A
 79%|███████▉  | 857/1083 [00:19<00:05, 44.92it/s][A
 80%|███████▉  | 862/1083 [00:19<00:04, 44.95it/s][A
 80%|████████  | 867/1083 [00:19<00:04, 44.99it/s][A
 81%|████████  | 872/1083 [00:19<00:04, 45.04it/s][A
 81%|████████  | 877/1083 [00:19<00:04, 45.09it/s][A
 81%|████████▏ | 882/1083 [00:19<00:04, 45.01it/s][A
 82%|████████▏ | 887/1083 [00:19<00:04, 44.93it/s][A
 82%|████████▏ | 892/1083 [00:19<00:04, 44.92it/s][A
 83%|████████▎ | 897/1083 [00:19<00:04, 44.97it/s][A
 83%|████████▎ | 902/1083 [00:20<00:04, 44.94it/s][A
 84%|████████▎ | 907/1083 [00:20<00:03, 44.95it/s][A
 84%|████████▍ | 912/1083 [00:20<00:03, 45.08it/s][A
 85%|████████▍ | 917/1083 [00:20<00:03, 45.08it/s][A
 85%|████████▌ | 922/1083 [00:20<00:03, 45.07it/s][A
 86%|████████▌ | 927/1083 [00:20<00:03, 45.00it/s][A
 86%|████████▌ | 932/1083 [00:20<00:03, 44.01it/s][A
 87%|████████▋ | 937/1083 [00:20<00:03, 44.41it/s][A
 87%|████████▋ | 942/1083 [00:20<00:03, 44.62it/s][A
 87%|████████▋ | 947/1083 [00:21<00:03, 44.84it/s][A
 88%|████████▊ | 952/1083 [00:21<00:02, 44.87it/s][A
 88%|████████▊ | 957/1083 [00:21<00:02, 45.05it/s][A
 89%|████████▉ | 962/1083 [00:21<00:02, 45.14it/s][A
 89%|████████▉ | 967/1083 [00:21<00:02, 45.00it/s][A
 90%|████████▉ | 972/1083 [00:21<00:02, 44.86it/s][A
 90%|█████████ | 977/1083 [00:21<00:02, 44.73it/s][A
 91%|█████████ | 982/1083 [00:21<00:02, 44.88it/s][A
 91%|█████████ | 987/1083 [00:21<00:02, 45.01it/s][A
 92%|█████████▏| 992/1083 [00:22<00:02, 45.13it/s][A
 92%|█████████▏| 997/1083 [00:22<00:01, 45.12it/s][A
 93%|█████████▎| 1002/1083 [00:22<00:01, 45.20it/s][A
 93%|█████████▎| 1007/1083 [00:22<00:01, 45.22it/s][A
 93%|█████████▎| 1012/1083 [00:22<00:01, 45.09it/s][A
 94%|█████████▍| 1017/1083 [00:22<00:01, 44.92it/s][A
 94%|█████████▍| 1022/1083 [00:22<00:01, 44.89it/s][A
 95%|█████████▍| 1027/1083 [00:22<00:01, 44.85it/s][A
 95%|█████████▌| 1032/1083 [00:22<00:01, 44.92it/s][A
 96%|█████████▌| 1037/1083 [00:23<00:01, 45.09it/s][A
 96%|█████████▌| 1042/1083 [00:23<00:00, 45.06it/s][A
 97%|█████████▋| 1047/1083 [00:23<00:00, 44.94it/s][A
 97%|█████████▋| 1052/1083 [00:23<00:00, 45.22it/s][A
 98%|█████████▊| 1057/1083 [00:23<00:00, 45.10it/s][A
 98%|█████████▊| 1062/1083 [00:23<00:00, 45.04it/s][A
 99%|█████████▊| 1067/1083 [00:23<00:00, 40.03it/s][A
 99%|█████████▉| 1072/1083 [00:23<00:00, 41.58it/s][A
 99%|█████████▉| 1077/1083 [00:24<00:00, 42.69it/s][A
100%|█████████▉| 1082/1083 [00:24<00:00, 43.52it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 43.52it/s][A 80%|████████  | 312/390 [03:59<00:22,  3.45it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 16:29:12,390 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 16:29:12,867 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:29:18,152 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:29:18,527 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:29:18,736 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [04:17<16:19, 12.72s/it] 81%|████████  | 314/390 [04:17<11:24,  9.01s/it] 81%|████████  | 315/390 [04:18<07:59,  6.39s/it] 81%|████████  | 316/390 [04:18<05:37,  4.56s/it] 81%|████████▏ | 317/390 [04:18<03:59,  3.28s/it] 82%|████████▏ | 318/390 [04:18<02:51,  2.39s/it] 82%|████████▏ | 319/390 [04:19<02:04,  1.76s/it] 82%|████████▏ | 320/390 [04:19<01:32,  1.32s/it] 82%|████████▏ | 321/390 [04:19<01:09,  1.01s/it] 83%|████████▎ | 322/390 [04:20<00:54,  1.26it/s] 83%|████████▎ | 323/390 [04:20<00:43,  1.55it/s] 83%|████████▎ | 324/390 [04:20<00:35,  1.85it/s] 83%|████████▎ | 325/390 [04:21<00:31,  2.08it/s] 84%|████████▎ | 326/390 [04:21<00:27,  2.36it/s] 84%|████████▍ | 327/390 [04:21<00:24,  2.60it/s] 84%|████████▍ | 328/390 [04:21<00:22,  2.80it/s] 84%|████████▍ | 329/390 [04:22<00:20,  2.96it/s] 85%|████████▍ | 330/390 [04:22<00:19,  3.08it/s] 85%|████████▍ | 331/390 [04:22<00:18,  3.17it/s] 85%|████████▌ | 332/390 [04:23<00:17,  3.24it/s] 85%|████████▌ | 333/390 [04:23<00:17,  3.29it/s] 86%|████████▌ | 334/390 [04:23<00:16,  3.32it/s] 86%|████████▌ | 335/390 [04:24<00:16,  3.35it/s] 86%|████████▌ | 336/390 [04:24<00:16,  3.37it/s] 86%|████████▋ | 337/390 [04:24<00:16,  3.27it/s] 87%|████████▋ | 338/390 [04:24<00:15,  3.31it/s] 87%|████████▋ | 339/390 [04:25<00:15,  3.34it/s] 87%|████████▋ | 340/390 [04:25<00:14,  3.36it/s] 87%|████████▋ | 341/390 [04:25<00:14,  3.38it/s] 88%|████████▊ | 342/390 [04:26<00:14,  3.38it/s] 88%|████████▊ | 343/390 [04:26<00:13,  3.39it/s] 88%|████████▊ | 344/390 [04:26<00:13,  3.40it/s] 88%|████████▊ | 345/390 [04:26<00:13,  3.40it/s] 89%|████████▊ | 346/390 [04:27<00:12,  3.40it/s] 89%|████████▉ | 347/390 [04:27<00:12,  3.41it/s] 89%|████████▉ | 348/390 [04:27<00:12,  3.30it/s] 89%|████████▉ | 349/390 [04:28<00:12,  3.33it/s] 90%|████████▉ | 350/390 [04:28<00:11,  3.35it/s] 90%|█████████ | 351/390 [04:28<00:11,  3.37it/s] 90%|█████████ | 352/390 [04:29<00:11,  3.38it/s] 91%|█████████ | 353/390 [04:29<00:10,  3.39it/s] 91%|█████████ | 354/390 [04:29<00:10,  3.40it/s] 91%|█████████ | 355/390 [04:29<00:10,  3.40it/s] 91%|█████████▏| 356/390 [04:30<00:09,  3.40it/s] 92%|█████████▏| 357/390 [04:30<00:09,  3.40it/s] 92%|█████████▏| 358/390 [04:30<00:09,  3.40it/s] 92%|█████████▏| 359/390 [04:31<00:09,  3.32it/s] 92%|█████████▏| 360/390 [04:31<00:08,  3.34it/s] 93%|█████████▎| 361/390 [04:31<00:08,  3.36it/s] 93%|█████████▎| 362/390 [04:32<00:08,  3.38it/s] 93%|█████████▎| 363/390 [04:32<00:07,  3.38it/s] 93%|█████████▎| 364/390 [04:32<00:07,  3.39it/s] 94%|█████████▎| 365/390 [04:32<00:07,  3.39it/s] 94%|█████████▍| 366/390 [04:33<00:07,  3.40it/s] 94%|█████████▍| 367/390 [04:33<00:06,  3.40it/s] 94%|█████████▍| 368/390 [04:33<00:06,  3.40it/s] 95%|█████████▍| 369/390 [04:34<00:06,  3.40it/s] 95%|█████████▍| 370/390 [04:34<00:06,  3.26it/s] 95%|█████████▌| 371/390 [04:34<00:05,  3.30it/s] 95%|█████████▌| 372/390 [04:34<00:05,  3.34it/s] 96%|█████████▌| 373/390 [04:35<00:05,  3.35it/s] 96%|█████████▌| 374/390 [04:35<00:04,  3.37it/s] 96%|█████████▌| 375/390 [04:35<00:04,  3.38it/s] 96%|█████████▋| 376/390 [04:36<00:04,  3.39it/s] 97%|█████████▋| 377/390 [04:36<00:03,  3.39it/s] 97%|█████████▋| 378/390 [04:36<00:03,  3.39it/s] 97%|█████████▋| 379/390 [04:37<00:03,  3.39it/s] 97%|█████████▋| 380/390 [04:37<00:02,  3.39it/s] 98%|█████████▊| 381/390 [04:37<00:02,  3.29it/s] 98%|█████████▊| 382/390 [04:37<00:02,  3.32it/s] 98%|█████████▊| 383/390 [04:38<00:02,  3.34it/s] 98%|█████████▊| 384/390 [04:38<00:01,  3.36it/s] 99%|█████████▊| 385/390 [04:38<00:01,  3.38it/s] 99%|█████████▉| 386/390 [04:39<00:01,  3.38it/s] 99%|█████████▉| 387/390 [04:39<00:00,  3.39it/s] 99%|█████████▉| 388/390 [04:39<00:00,  3.39it/s]100%|█████████▉| 389/390 [04:40<00:00,  3.39it/s]100%|██████████| 390/390 [04:40<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 16:29:52,172 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:29:52,172 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 16:29:52,172 >>   Batch size = 8
{'eval_loss': 0.9378021955490112, 'eval_runtime': 24.186, 'eval_samples_per_second': 357.975, 'eval_steps_per_second': 44.778, 'epoch': 3.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.19it/s][A
  1%|          | 12/1083 [00:00<00:22, 47.27it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 46.59it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.19it/s][A
  2%|▏         | 27/1083 [00:00<00:23, 45.85it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.48it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.24it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 44.89it/s][A
  4%|▍         | 47/1083 [00:01<00:22, 45.05it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.10it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.27it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.37it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.42it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.27it/s][A
  7%|▋         | 77/1083 [00:01<00:22, 45.18it/s][A
  8%|▊         | 82/1083 [00:01<00:22, 44.94it/s][A
  8%|▊         | 87/1083 [00:01<00:22, 44.83it/s][A
  8%|▊         | 92/1083 [00:02<00:22, 44.75it/s][A
  9%|▉         | 97/1083 [00:02<00:21, 44.98it/s][A
  9%|▉         | 102/1083 [00:02<00:21, 45.15it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 45.33it/s][A
 10%|█         | 112/1083 [00:02<00:21, 45.37it/s][A
 11%|█         | 117/1083 [00:02<00:21, 45.23it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 45.17it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.97it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.81it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 44.79it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 44.98it/s][A
 14%|█▎        | 147/1083 [00:03<00:21, 42.91it/s][A
 14%|█▍        | 152/1083 [00:03<00:21, 43.67it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 44.19it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 44.55it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.87it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.85it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.87it/s][A
 17%|█▋        | 182/1083 [00:04<00:20, 44.86it/s][A
 17%|█▋        | 187/1083 [00:04<00:20, 44.71it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 44.78it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.00it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.15it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.19it/s][A
 20%|█▉        | 212/1083 [00:04<00:19, 45.24it/s][A
 20%|██        | 217/1083 [00:04<00:19, 45.25it/s][A
 20%|██        | 222/1083 [00:04<00:19, 45.21it/s][A
 21%|██        | 227/1083 [00:05<00:18, 45.07it/s][A
 21%|██▏       | 232/1083 [00:05<00:18, 45.05it/s][A
 22%|██▏       | 237/1083 [00:05<00:20, 41.55it/s][A
 22%|██▏       | 242/1083 [00:05<00:19, 42.91it/s][A
 23%|██▎       | 247/1083 [00:05<00:19, 43.73it/s][A
 23%|██▎       | 252/1083 [00:05<00:18, 44.29it/s][A
 24%|██▎       | 257/1083 [00:05<00:18, 44.67it/s][A
 24%|██▍       | 262/1083 [00:05<00:18, 44.89it/s][A
 25%|██▍       | 267/1083 [00:05<00:18, 44.91it/s][A
 25%|██▌       | 272/1083 [00:06<00:18, 44.87it/s][A
 26%|██▌       | 277/1083 [00:06<00:18, 44.73it/s][A
 26%|██▌       | 282/1083 [00:06<00:19, 41.59it/s][A
 27%|██▋       | 287/1083 [00:06<00:18, 42.76it/s][A
 27%|██▋       | 292/1083 [00:06<00:18, 43.57it/s][A
 27%|██▋       | 297/1083 [00:06<00:17, 44.17it/s][A
 28%|██▊       | 302/1083 [00:06<00:17, 44.63it/s][A
 28%|██▊       | 307/1083 [00:06<00:17, 44.71it/s][A
 29%|██▉       | 312/1083 [00:06<00:17, 44.98it/s][A
 29%|██▉       | 317/1083 [00:07<00:17, 44.87it/s][A
 30%|██▉       | 322/1083 [00:07<00:17, 44.56it/s][A
 30%|███       | 327/1083 [00:07<00:16, 44.53it/s][A
 31%|███       | 332/1083 [00:07<00:16, 44.68it/s][A
 31%|███       | 337/1083 [00:07<00:16, 44.91it/s][A
 32%|███▏      | 342/1083 [00:07<00:16, 44.99it/s][A
 32%|███▏      | 347/1083 [00:07<00:16, 45.24it/s][A
 33%|███▎      | 352/1083 [00:07<00:16, 45.37it/s][A
 33%|███▎      | 357/1083 [00:07<00:16, 45.32it/s][A
 33%|███▎      | 362/1083 [00:08<00:15, 45.14it/s][A
 34%|███▍      | 367/1083 [00:08<00:15, 44.89it/s][A
 34%|███▍      | 372/1083 [00:08<00:15, 44.74it/s][A
 35%|███▍      | 377/1083 [00:08<00:15, 44.70it/s][A
 35%|███▌      | 382/1083 [00:08<00:15, 44.84it/s][A
 36%|███▌      | 387/1083 [00:08<00:15, 44.99it/s][A
 36%|███▌      | 392/1083 [00:08<00:15, 45.25it/s][A
 37%|███▋      | 397/1083 [00:08<00:15, 45.35it/s][A
 37%|███▋      | 402/1083 [00:08<00:14, 45.42it/s][A
 38%|███▊      | 407/1083 [00:09<00:14, 45.23it/s][A
 38%|███▊      | 412/1083 [00:09<00:14, 45.03it/s][A
 39%|███▊      | 417/1083 [00:09<00:15, 42.62it/s][A
 39%|███▉      | 422/1083 [00:09<00:15, 43.38it/s][A
 39%|███▉      | 427/1083 [00:09<00:14, 43.93it/s][A
 40%|███▉      | 432/1083 [00:09<00:14, 44.39it/s][A
 40%|████      | 437/1083 [00:09<00:14, 44.60it/s][A
 41%|████      | 442/1083 [00:09<00:14, 44.85it/s][A
 41%|████▏     | 447/1083 [00:09<00:14, 45.05it/s][A
 42%|████▏     | 452/1083 [00:10<00:13, 45.17it/s][A
 42%|████▏     | 457/1083 [00:10<00:13, 44.91it/s][A
 43%|████▎     | 462/1083 [00:10<00:13, 44.81it/s][A
 43%|████▎     | 467/1083 [00:10<00:13, 44.87it/s][A
 44%|████▎     | 472/1083 [00:10<00:13, 44.98it/s][A
 44%|████▍     | 477/1083 [00:10<00:13, 45.04it/s][A
 45%|████▍     | 482/1083 [00:10<00:13, 45.12it/s][A
 45%|████▍     | 487/1083 [00:10<00:13, 45.26it/s][A
 45%|████▌     | 492/1083 [00:10<00:13, 45.26it/s][A
 46%|████▌     | 497/1083 [00:11<00:12, 45.28it/s][A
 46%|████▋     | 502/1083 [00:11<00:12, 45.12it/s][A
 47%|████▋     | 507/1083 [00:11<00:12, 45.05it/s][A
 47%|████▋     | 512/1083 [00:11<00:12, 44.99it/s][A
 48%|████▊     | 517/1083 [00:11<00:12, 44.96it/s][A
 48%|████▊     | 522/1083 [00:11<00:12, 44.98it/s][A
 49%|████▊     | 527/1083 [00:11<00:12, 45.03it/s][A
 49%|████▉     | 532/1083 [00:11<00:12, 45.13it/s][A
 50%|████▉     | 537/1083 [00:11<00:12, 45.20it/s][A
 50%|█████     | 542/1083 [00:12<00:11, 45.13it/s][A
 51%|█████     | 547/1083 [00:12<00:11, 45.08it/s][A
 51%|█████     | 552/1083 [00:12<00:11, 45.02it/s][A
 51%|█████▏    | 557/1083 [00:12<00:11, 44.95it/s][A
 52%|█████▏    | 562/1083 [00:12<00:13, 39.99it/s][A
 52%|█████▏    | 568/1083 [00:12<00:11, 42.96it/s][A
 53%|█████▎    | 573/1083 [00:12<00:11, 43.74it/s][A
 53%|█████▎    | 578/1083 [00:12<00:11, 44.22it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 44.64it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 44.85it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 44.90it/s][A
 55%|█████▌    | 598/1083 [00:13<00:10, 44.91it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 44.71it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.58it/s][A
 57%|█████▋    | 613/1083 [00:13<00:10, 44.69it/s][A
 57%|█████▋    | 618/1083 [00:13<00:10, 44.83it/s][A
 58%|█████▊    | 623/1083 [00:13<00:10, 44.92it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 45.03it/s][A
 58%|█████▊    | 633/1083 [00:14<00:09, 45.21it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 45.30it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 45.21it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.95it/s][A
 60%|██████    | 653/1083 [00:14<00:10, 40.03it/s][A
 61%|██████    | 658/1083 [00:14<00:10, 41.50it/s][A
 61%|██████    | 663/1083 [00:14<00:09, 42.64it/s][A
 62%|██████▏   | 668/1083 [00:14<00:09, 43.35it/s][A
 62%|██████▏   | 673/1083 [00:15<00:09, 43.94it/s][A
 63%|██████▎   | 678/1083 [00:15<00:09, 44.41it/s][A
 63%|██████▎   | 683/1083 [00:15<00:08, 44.71it/s][A
 64%|██████▎   | 688/1083 [00:15<00:08, 44.92it/s][A
 64%|██████▍   | 693/1083 [00:15<00:08, 44.59it/s][A
 64%|██████▍   | 698/1083 [00:15<00:08, 44.66it/s][A
 65%|██████▍   | 703/1083 [00:15<00:08, 44.74it/s][A
 65%|██████▌   | 708/1083 [00:15<00:08, 44.86it/s][A
 66%|██████▌   | 713/1083 [00:15<00:08, 45.09it/s][A
 66%|██████▋   | 718/1083 [00:16<00:08, 45.14it/s][A
 67%|██████▋   | 723/1083 [00:16<00:07, 45.18it/s][A
 67%|██████▋   | 728/1083 [00:16<00:07, 45.15it/s][A
 68%|██████▊   | 733/1083 [00:16<00:07, 45.00it/s][A
 68%|██████▊   | 738/1083 [00:16<00:07, 44.74it/s][A
 69%|██████▊   | 743/1083 [00:16<00:07, 44.84it/s][A
 69%|██████▉   | 748/1083 [00:16<00:07, 44.85it/s][A
 70%|██████▉   | 753/1083 [00:16<00:07, 44.99it/s][A
 70%|██████▉   | 758/1083 [00:16<00:07, 45.06it/s][A
 70%|███████   | 763/1083 [00:17<00:07, 45.16it/s][A
 71%|███████   | 768/1083 [00:17<00:06, 45.20it/s][A
 71%|███████▏  | 773/1083 [00:17<00:06, 45.22it/s][A
 72%|███████▏  | 778/1083 [00:17<00:06, 45.06it/s][A
 72%|███████▏  | 783/1083 [00:17<00:06, 44.90it/s][A
 73%|███████▎  | 788/1083 [00:17<00:09, 31.39it/s][A
 73%|███████▎  | 793/1083 [00:17<00:08, 34.66it/s][A
 74%|███████▎  | 798/1083 [00:18<00:07, 37.40it/s][A
 74%|███████▍  | 803/1083 [00:18<00:07, 39.52it/s][A
 75%|███████▍  | 808/1083 [00:18<00:06, 41.18it/s][A
 75%|███████▌  | 813/1083 [00:18<00:06, 42.37it/s][A
 76%|███████▌  | 818/1083 [00:18<00:06, 43.35it/s][A
 76%|███████▌  | 823/1083 [00:18<00:05, 43.87it/s][A
 76%|███████▋  | 828/1083 [00:18<00:05, 43.82it/s][A
 77%|███████▋  | 833/1083 [00:18<00:05, 43.84it/s][A
 77%|███████▋  | 838/1083 [00:18<00:05, 44.13it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 44.54it/s][A
 78%|███████▊  | 848/1083 [00:19<00:05, 44.82it/s][A
 79%|███████▉  | 853/1083 [00:19<00:05, 45.02it/s][A
 79%|███████▉  | 858/1083 [00:19<00:04, 45.17it/s][A
 80%|███████▉  | 863/1083 [00:19<00:04, 45.26it/s][A
 80%|████████  | 868/1083 [00:19<00:04, 45.15it/s][A
 81%|████████  | 873/1083 [00:19<00:04, 44.84it/s][A
 81%|████████  | 878/1083 [00:19<00:04, 44.66it/s][A
 82%|████████▏ | 883/1083 [00:19<00:04, 44.65it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 44.90it/s][A
 82%|████████▏ | 893/1083 [00:20<00:04, 45.07it/s][A
 83%|████████▎ | 898/1083 [00:20<00:04, 45.11it/s][A
 83%|████████▎ | 903/1083 [00:20<00:03, 45.17it/s][A
 84%|████████▍ | 908/1083 [00:20<00:03, 45.31it/s][A
 84%|████████▍ | 913/1083 [00:20<00:03, 45.20it/s][A
 85%|████████▍ | 918/1083 [00:20<00:06, 25.64it/s][A
 85%|████████▌ | 923/1083 [00:21<00:05, 29.50it/s][A
 86%|████████▌ | 928/1083 [00:21<00:04, 33.00it/s][A
 86%|████████▌ | 933/1083 [00:21<00:04, 35.99it/s][A
 87%|████████▋ | 938/1083 [00:21<00:03, 38.36it/s][A
 87%|████████▋ | 943/1083 [00:21<00:03, 40.32it/s][A
 88%|████████▊ | 948/1083 [00:21<00:03, 41.77it/s][A
 88%|████████▊ | 953/1083 [00:21<00:03, 42.79it/s][A
 88%|████████▊ | 958/1083 [00:21<00:02, 43.11it/s][A
 89%|████████▉ | 963/1083 [00:21<00:02, 43.34it/s][A
 89%|████████▉ | 968/1083 [00:22<00:02, 43.73it/s][A
 90%|████████▉ | 973/1083 [00:22<00:02, 44.18it/s][A
 90%|█████████ | 978/1083 [00:22<00:02, 44.45it/s][A
 91%|█████████ | 983/1083 [00:22<00:02, 44.71it/s][A
 91%|█████████ | 988/1083 [00:22<00:02, 44.91it/s][A
 92%|█████████▏| 993/1083 [00:22<00:01, 45.21it/s][A
 92%|█████████▏| 998/1083 [00:22<00:01, 45.28it/s][A
 93%|█████████▎| 1003/1083 [00:22<00:01, 44.94it/s][A
 93%|█████████▎| 1008/1083 [00:22<00:01, 44.79it/s][A
 94%|█████████▎| 1013/1083 [00:23<00:01, 44.73it/s][A
 94%|█████████▍| 1018/1083 [00:23<00:01, 44.85it/s][A
 94%|█████████▍| 1023/1083 [00:23<00:01, 44.94it/s][A
 95%|█████████▍| 1028/1083 [00:23<00:01, 45.02it/s][A
 95%|█████████▌| 1033/1083 [00:23<00:01, 45.20it/s][A
 96%|█████████▌| 1038/1083 [00:23<00:00, 45.36it/s][A
 96%|█████████▋| 1043/1083 [00:23<00:01, 25.76it/s][A
 97%|█████████▋| 1048/1083 [00:24<00:01, 29.66it/s][A
 97%|█████████▋| 1053/1083 [00:24<00:00, 33.17it/s][A
 98%|█████████▊| 1058/1083 [00:24<00:00, 36.14it/s][A
 98%|█████████▊| 1063/1083 [00:24<00:00, 38.50it/s][A
 99%|█████████▊| 1068/1083 [00:24<00:00, 40.42it/s][A
 99%|█████████▉| 1073/1083 [00:24<00:00, 41.84it/s][A
100%|█████████▉| 1078/1083 [00:24<00:00, 42.84it/s][A
100%|██████████| 1083/1083 [00:24<00:00, 43.12it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:24<00:00, 43.12it/s][A100%|██████████| 390/390 [05:05<00:00,  3.40it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 16:30:18,683 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 16:30:19,422 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:30:36,338 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:30:37,097 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:30:37,324 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:31:07,882 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:31:07,883 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78 (score: 0.9077656865119934).
                                                 100%|██████████| 390/390 [06:35<00:00,  3.40it/s]100%|██████████| 390/390 [06:35<00:00,  1.01s/it]
[INFO|trainer.py:1894] 2023-08-28 16:31:48,363 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 16:31:48,957 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:31:58,226 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:31:58,970 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:31:59,223 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:31:59,879 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:31:59,879 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:31:59,879 >>   train_loss               =     0.4717
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:31:59,879 >>   train_runtime            = 0:06:35.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:31:59,879 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:31:59,880 >>   train_samples_per_second =      63.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:31:59,880 >>   train_steps_per_second   =      0.987
{'eval_loss': 0.9451776742935181, 'eval_runtime': 24.8889, 'eval_samples_per_second': 347.865, 'eval_steps_per_second': 43.513, 'epoch': 4.99}
{'train_runtime': 395.2422, 'train_samples_per_second': 63.24, 'train_steps_per_second': 0.987, 'train_loss': 0.47174420479016427, 'epoch': 4.99}
08/28/2023 16:32:01 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:32:01,138 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:32:01,138 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 16:32:01,138 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 55.83it/s]  1%|          | 12/1083 [00:00<00:25, 42.40it/s]  2%|▏         | 17/1083 [00:00<00:24, 43.95it/s]  2%|▏         | 22/1083 [00:00<00:23, 44.60it/s]  2%|▏         | 27/1083 [00:00<00:23, 45.10it/s]  3%|▎         | 32/1083 [00:00<00:23, 45.35it/s]  3%|▎         | 37/1083 [00:00<00:22, 45.62it/s]  4%|▍         | 42/1083 [00:00<00:22, 45.49it/s]  4%|▍         | 47/1083 [00:01<00:22, 45.33it/s]  5%|▍         | 52/1083 [00:01<00:22, 45.04it/s]  5%|▌         | 57/1083 [00:01<00:22, 45.11it/s]  6%|▌         | 62/1083 [00:01<00:22, 45.15it/s]  6%|▌         | 67/1083 [00:01<00:22, 45.42it/s]  7%|▋         | 72/1083 [00:01<00:22, 45.54it/s]  7%|▋         | 77/1083 [00:01<00:22, 45.69it/s]  8%|▊         | 82/1083 [00:01<00:21, 45.79it/s]  8%|▊         | 87/1083 [00:01<00:21, 45.60it/s]  8%|▊         | 92/1083 [00:02<00:21, 45.40it/s]  9%|▉         | 97/1083 [00:02<00:21, 45.25it/s]  9%|▉         | 102/1083 [00:02<00:21, 44.87it/s] 10%|▉         | 107/1083 [00:02<00:21, 45.21it/s] 10%|█         | 112/1083 [00:02<00:21, 45.24it/s] 11%|█         | 117/1083 [00:02<00:21, 45.38it/s] 11%|█▏        | 122/1083 [00:02<00:21, 45.51it/s] 12%|█▏        | 127/1083 [00:02<00:20, 45.70it/s] 12%|█▏        | 132/1083 [00:02<00:20, 45.71it/s] 13%|█▎        | 137/1083 [00:03<00:20, 45.60it/s] 13%|█▎        | 142/1083 [00:03<00:20, 45.34it/s] 14%|█▎        | 147/1083 [00:03<00:24, 38.27it/s] 14%|█▍        | 152/1083 [00:03<00:23, 40.36it/s] 14%|█▍        | 157/1083 [00:03<00:22, 41.79it/s] 15%|█▍        | 162/1083 [00:03<00:21, 42.95it/s] 15%|█▌        | 167/1083 [00:04<00:35, 25.49it/s] 16%|█▌        | 172/1083 [00:04<00:30, 29.77it/s] 16%|█▋        | 177/1083 [00:04<00:27, 33.28it/s] 17%|█▋        | 182/1083 [00:04<00:24, 36.23it/s] 17%|█▋        | 187/1083 [00:04<00:23, 38.64it/s] 18%|█▊        | 192/1083 [00:04<00:21, 40.52it/s] 18%|█▊        | 197/1083 [00:04<00:21, 41.85it/s] 19%|█▊        | 202/1083 [00:04<00:20, 42.94it/s] 19%|█▉        | 207/1083 [00:04<00:20, 43.53it/s] 20%|█▉        | 212/1083 [00:05<00:23, 37.37it/s] 20%|██        | 218/1083 [00:05<00:21, 41.02it/s] 21%|██        | 223/1083 [00:05<00:20, 42.31it/s] 21%|██        | 228/1083 [00:05<00:19, 43.18it/s] 22%|██▏       | 233/1083 [00:05<00:22, 37.31it/s] 22%|██▏       | 238/1083 [00:05<00:21, 39.46it/s] 22%|██▏       | 243/1083 [00:05<00:20, 41.17it/s] 23%|██▎       | 248/1083 [00:05<00:19, 42.52it/s] 23%|██▎       | 253/1083 [00:06<00:19, 43.38it/s] 24%|██▍       | 258/1083 [00:06<00:18, 44.09it/s] 24%|██▍       | 263/1083 [00:06<00:18, 44.62it/s] 25%|██▍       | 268/1083 [00:06<00:18, 44.88it/s] 25%|██▌       | 273/1083 [00:06<00:18, 44.62it/s] 26%|██▌       | 278/1083 [00:06<00:18, 44.53it/s] 26%|██▌       | 283/1083 [00:06<00:17, 44.59it/s] 27%|██▋       | 288/1083 [00:06<00:17, 44.94it/s] 27%|██▋       | 293/1083 [00:06<00:17, 45.18it/s] 28%|██▊       | 298/1083 [00:07<00:17, 45.29it/s] 28%|██▊       | 303/1083 [00:07<00:17, 45.48it/s] 28%|██▊       | 308/1083 [00:07<00:16, 45.61it/s] 29%|██▉       | 313/1083 [00:07<00:16, 45.47it/s] 29%|██▉       | 318/1083 [00:07<00:16, 45.20it/s] 30%|██▉       | 323/1083 [00:07<00:16, 44.93it/s] 30%|███       | 328/1083 [00:07<00:16, 44.88it/s] 31%|███       | 333/1083 [00:07<00:16, 44.97it/s] 31%|███       | 338/1083 [00:07<00:16, 45.17it/s] 32%|███▏      | 343/1083 [00:08<00:16, 45.15it/s] 32%|███▏      | 348/1083 [00:08<00:16, 45.31it/s] 33%|███▎      | 353/1083 [00:08<00:16, 45.16it/s] 33%|███▎      | 358/1083 [00:08<00:15, 45.37it/s] 34%|███▎      | 363/1083 [00:08<00:15, 45.27it/s] 34%|███▍      | 368/1083 [00:08<00:17, 40.63it/s] 34%|███▍      | 373/1083 [00:08<00:16, 42.06it/s] 35%|███▍      | 378/1083 [00:08<00:16, 43.09it/s] 35%|███▌      | 383/1083 [00:08<00:15, 43.80it/s] 36%|███▌      | 388/1083 [00:09<00:15, 44.32it/s] 36%|███▋      | 393/1083 [00:09<00:15, 44.69it/s] 37%|███▋      | 398/1083 [00:09<00:15, 45.01it/s] 37%|███▋      | 403/1083 [00:09<00:15, 45.15it/s] 38%|███▊      | 408/1083 [00:09<00:15, 44.79it/s] 38%|███▊      | 413/1083 [00:09<00:14, 44.82it/s] 39%|███▊      | 418/1083 [00:09<00:14, 44.92it/s] 39%|███▉      | 423/1083 [00:09<00:14, 45.11it/s] 40%|███▉      | 428/1083 [00:09<00:14, 45.27it/s] 40%|███▉      | 433/1083 [00:10<00:14, 45.36it/s] 40%|████      | 438/1083 [00:10<00:14, 45.40it/s] 41%|████      | 443/1083 [00:10<00:14, 45.39it/s] 41%|████▏     | 448/1083 [00:10<00:14, 45.19it/s] 42%|████▏     | 453/1083 [00:10<00:13, 45.02it/s] 42%|████▏     | 458/1083 [00:10<00:13, 44.91it/s] 43%|████▎     | 463/1083 [00:10<00:13, 44.97it/s] 43%|████▎     | 468/1083 [00:10<00:13, 45.04it/s] 44%|████▎     | 473/1083 [00:10<00:13, 45.28it/s] 44%|████▍     | 478/1083 [00:11<00:13, 45.34it/s] 45%|████▍     | 483/1083 [00:11<00:13, 45.47it/s] 45%|████▌     | 488/1083 [00:11<00:13, 45.49it/s] 46%|████▌     | 493/1083 [00:11<00:13, 45.27it/s] 46%|████▌     | 498/1083 [00:11<00:12, 45.10it/s] 46%|████▋     | 503/1083 [00:11<00:13, 42.03it/s] 47%|████▋     | 508/1083 [00:11<00:13, 43.08it/s] 47%|████▋     | 513/1083 [00:11<00:12, 43.85it/s] 48%|████▊     | 518/1083 [00:11<00:12, 44.28it/s] 48%|████▊     | 523/1083 [00:12<00:12, 44.67it/s] 49%|████▉     | 528/1083 [00:12<00:12, 44.97it/s] 49%|████▉     | 533/1083 [00:12<00:12, 45.08it/s] 50%|████▉     | 538/1083 [00:12<00:12, 45.13it/s] 50%|█████     | 543/1083 [00:12<00:12, 44.79it/s] 51%|█████     | 548/1083 [00:12<00:11, 44.60it/s] 51%|█████     | 553/1083 [00:12<00:11, 44.88it/s] 52%|█████▏    | 558/1083 [00:12<00:11, 45.01it/s] 52%|█████▏    | 563/1083 [00:12<00:11, 45.19it/s] 52%|█████▏    | 568/1083 [00:13<00:11, 45.32it/s] 53%|█████▎    | 573/1083 [00:13<00:11, 45.38it/s] 53%|█████▎    | 578/1083 [00:13<00:11, 45.46it/s] 54%|█████▍    | 583/1083 [00:13<00:11, 45.39it/s] 54%|█████▍    | 588/1083 [00:13<00:10, 45.14it/s] 55%|█████▍    | 593/1083 [00:13<00:10, 45.03it/s] 55%|█████▌    | 598/1083 [00:13<00:10, 45.00it/s] 56%|█████▌    | 603/1083 [00:13<00:10, 45.12it/s] 56%|█████▌    | 608/1083 [00:13<00:10, 45.23it/s] 57%|█████▋    | 613/1083 [00:14<00:10, 45.32it/s] 57%|█████▋    | 618/1083 [00:14<00:10, 45.41it/s] 58%|█████▊    | 623/1083 [00:14<00:10, 45.48it/s] 58%|█████▊    | 628/1083 [00:14<00:10, 45.33it/s] 58%|█████▊    | 633/1083 [00:14<00:09, 45.26it/s] 59%|█████▉    | 638/1083 [00:14<00:15, 28.18it/s] 59%|█████▉    | 643/1083 [00:14<00:13, 31.84it/s] 60%|█████▉    | 648/1083 [00:15<00:12, 35.04it/s] 60%|██████    | 653/1083 [00:15<00:11, 37.67it/s] 61%|██████    | 658/1083 [00:15<00:10, 39.75it/s] 61%|██████    | 663/1083 [00:15<00:10, 41.35it/s] 62%|██████▏   | 668/1083 [00:15<00:09, 42.60it/s] 62%|██████▏   | 673/1083 [00:15<00:09, 43.30it/s] 63%|██████▎   | 678/1083 [00:15<00:09, 43.57it/s] 63%|██████▎   | 683/1083 [00:15<00:09, 43.66it/s] 64%|██████▎   | 688/1083 [00:15<00:08, 44.00it/s] 64%|██████▍   | 693/1083 [00:16<00:08, 44.52it/s] 64%|██████▍   | 698/1083 [00:16<00:08, 44.84it/s] 65%|██████▍   | 703/1083 [00:16<00:08, 45.08it/s] 65%|██████▌   | 708/1083 [00:16<00:08, 45.25it/s] 66%|██████▌   | 713/1083 [00:16<00:08, 45.32it/s] 66%|██████▋   | 718/1083 [00:16<00:08, 45.21it/s] 67%|██████▋   | 723/1083 [00:16<00:08, 44.99it/s] 67%|██████▋   | 728/1083 [00:16<00:07, 44.73it/s] 68%|██████▊   | 733/1083 [00:16<00:07, 44.76it/s] 68%|██████▊   | 738/1083 [00:17<00:07, 44.87it/s] 69%|██████▊   | 743/1083 [00:17<00:07, 45.09it/s] 69%|██████▉   | 748/1083 [00:17<00:07, 45.13it/s] 70%|██████▉   | 753/1083 [00:17<00:07, 45.38it/s] 70%|██████▉   | 758/1083 [00:17<00:07, 45.48it/s] 70%|███████   | 763/1083 [00:17<00:07, 45.30it/s] 71%|███████   | 768/1083 [00:17<00:09, 33.07it/s] 71%|███████▏  | 773/1083 [00:17<00:08, 36.10it/s] 72%|███████▏  | 778/1083 [00:18<00:07, 38.48it/s] 72%|███████▏  | 783/1083 [00:18<00:07, 40.41it/s] 73%|███████▎  | 788/1083 [00:18<00:07, 41.84it/s] 73%|███████▎  | 793/1083 [00:18<00:06, 42.96it/s] 74%|███████▎  | 798/1083 [00:18<00:06, 43.69it/s] 74%|███████▍  | 803/1083 [00:18<00:06, 44.07it/s] 75%|███████▍  | 808/1083 [00:18<00:06, 43.98it/s] 75%|███████▌  | 813/1083 [00:18<00:06, 44.02it/s] 76%|███████▌  | 818/1083 [00:18<00:05, 44.34it/s] 76%|███████▌  | 823/1083 [00:19<00:05, 44.54it/s] 76%|███████▋  | 828/1083 [00:19<00:05, 44.83it/s] 77%|███████▋  | 833/1083 [00:19<00:05, 45.08it/s] 77%|███████▋  | 838/1083 [00:19<00:05, 45.24it/s] 78%|███████▊  | 843/1083 [00:19<00:05, 45.33it/s] 78%|███████▊  | 848/1083 [00:19<00:05, 45.31it/s] 79%|███████▉  | 853/1083 [00:19<00:05, 45.10it/s] 79%|███████▉  | 858/1083 [00:19<00:04, 45.00it/s] 80%|███████▉  | 863/1083 [00:19<00:04, 44.87it/s] 80%|████████  | 868/1083 [00:20<00:04, 44.98it/s] 81%|████████  | 873/1083 [00:20<00:04, 45.04it/s] 81%|████████  | 878/1083 [00:20<00:04, 45.14it/s] 82%|████████▏ | 883/1083 [00:20<00:04, 45.21it/s] 82%|████████▏ | 888/1083 [00:20<00:04, 45.27it/s] 82%|████████▏ | 893/1083 [00:20<00:04, 45.33it/s] 83%|████████▎ | 898/1083 [00:20<00:05, 34.42it/s] 83%|████████▎ | 903/1083 [00:20<00:04, 37.15it/s] 84%|████████▍ | 908/1083 [00:21<00:04, 39.30it/s] 84%|████████▍ | 913/1083 [00:21<00:04, 41.03it/s] 85%|████████▍ | 918/1083 [00:21<00:03, 42.24it/s] 85%|████████▌ | 923/1083 [00:21<00:03, 43.19it/s] 86%|████████▌ | 928/1083 [00:21<00:03, 43.90it/s] 86%|████████▌ | 933/1083 [00:21<00:03, 44.31it/s] 87%|████████▋ | 938/1083 [00:21<00:03, 44.23it/s] 87%|████████▋ | 943/1083 [00:21<00:03, 44.26it/s] 88%|████████▊ | 948/1083 [00:21<00:03, 44.42it/s] 88%|████████▊ | 953/1083 [00:22<00:02, 44.71it/s] 88%|████████▊ | 958/1083 [00:22<00:02, 44.95it/s] 89%|████████▉ | 963/1083 [00:22<00:02, 45.16it/s] 89%|████████▉ | 968/1083 [00:22<00:02, 45.24it/s] 90%|████████▉ | 973/1083 [00:22<00:02, 45.28it/s] 90%|█████████ | 978/1083 [00:22<00:02, 45.09it/s] 91%|█████████ | 983/1083 [00:22<00:02, 44.81it/s] 91%|█████████ | 988/1083 [00:22<00:02, 44.62it/s] 92%|█████████▏| 993/1083 [00:22<00:02, 44.73it/s] 92%|█████████▏| 998/1083 [00:23<00:01, 44.80it/s] 93%|█████████▎| 1003/1083 [00:23<00:01, 44.94it/s] 93%|█████████▎| 1008/1083 [00:23<00:01, 45.15it/s] 94%|█████████▎| 1013/1083 [00:23<00:01, 45.26it/s] 94%|█████████▍| 1018/1083 [00:23<00:01, 45.35it/s] 94%|█████████▍| 1023/1083 [00:23<00:01, 45.21it/s] 95%|█████████▍| 1028/1083 [00:23<00:01, 44.85it/s] 95%|█████████▌| 1033/1083 [00:23<00:01, 42.05it/s] 96%|█████████▌| 1038/1083 [00:23<00:01, 43.00it/s] 96%|█████████▋| 1043/1083 [00:24<00:00, 43.70it/s] 97%|█████████▋| 1048/1083 [00:24<00:00, 44.20it/s] 97%|█████████▋| 1053/1083 [00:24<00:00, 44.54it/s] 98%|█████████▊| 1058/1083 [00:24<00:00, 44.78it/s] 98%|█████████▊| 1063/1083 [00:24<00:00, 44.97it/s] 99%|█████████▊| 1068/1083 [00:24<00:00, 45.01it/s] 99%|█████████▉| 1073/1083 [00:24<00:00, 44.68it/s]100%|█████████▉| 1078/1083 [00:24<00:00, 44.68it/s]100%|██████████| 1083/1083 [00:24<00:00, 44.74it/s]100%|██████████| 1083/1083 [00:24<00:00, 43.42it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:32:26,099 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   eval_loss               =     0.9078
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   eval_runtime            = 0:00:24.96
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   eval_samples_per_second =    346.862
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   eval_steps_per_second   =     43.388
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:32:26,099 >>   perplexity              =     2.4788
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:50,125 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:50,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:50,170 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:50,170 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:50,170 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:32:51,075 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:32:51,076 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:32:51,800 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:32:53,164 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:32:53,212 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:57,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:57,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:57,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:57,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:32:57,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:32:58,746 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:32:58,748 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:32:59,454 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:32:59,738 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:32:59,738 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.33it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.40it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:18,  1.57it/s]Extractor Predicting: 28it [00:18,  1.58it/s]Extractor Predicting: 29it [00:19,  1.56it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:21,  1.58it/s]Extractor Predicting: 33it [00:21,  1.59it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.23it/s]Extractor Predicting: 37it [00:24,  1.32it/s]Extractor Predicting: 38it [00:25,  1.33it/s]Extractor Predicting: 39it [00:26,  1.40it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:27,  1.42it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:29,  1.38it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:33,  1.45it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.54it/s]Extractor Predicting: 52it [00:35,  1.53it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.52it/s]Extractor Predicting: 55it [00:37,  1.57it/s]Extractor Predicting: 56it [00:37,  1.58it/s]Extractor Predicting: 57it [00:38,  1.58it/s]Extractor Predicting: 58it [00:39,  1.49it/s]Extractor Predicting: 59it [00:39,  1.48it/s]Extractor Predicting: 60it [00:40,  1.45it/s]Extractor Predicting: 61it [00:41,  1.50it/s]Extractor Predicting: 62it [00:41,  1.53it/s]Extractor Predicting: 63it [00:42,  1.56it/s]Extractor Predicting: 64it [00:42,  1.60it/s]Extractor Predicting: 65it [00:43,  1.59it/s]Extractor Predicting: 66it [00:44,  1.60it/s]Extractor Predicting: 67it [00:44,  1.57it/s]Extractor Predicting: 68it [00:45,  1.58it/s]Extractor Predicting: 69it [00:46,  1.60it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:47,  1.54it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:48,  1.51it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:50,  1.57it/s]Extractor Predicting: 77it [00:51,  1.55it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:52,  1.56it/s]Extractor Predicting: 80it [00:53,  1.56it/s]Extractor Predicting: 81it [00:53,  1.60it/s]Extractor Predicting: 82it [00:54,  1.60it/s]Extractor Predicting: 83it [00:55,  1.57it/s]Extractor Predicting: 84it [00:55,  1.59it/s]Extractor Predicting: 85it [00:56,  1.56it/s]Extractor Predicting: 86it [00:56,  1.54it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:58,  1.53it/s]Extractor Predicting: 89it [00:58,  1.56it/s]Extractor Predicting: 90it [00:59,  1.47it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.57it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.56it/s]Extractor Predicting: 98it [01:04,  1.56it/s]Extractor Predicting: 99it [01:05,  1.57it/s]Extractor Predicting: 100it [01:06,  1.24it/s]Extractor Predicting: 101it [01:07,  1.34it/s]Extractor Predicting: 102it [01:07,  1.42it/s]Extractor Predicting: 103it [01:08,  1.47it/s]Extractor Predicting: 104it [01:09,  1.20it/s]Extractor Predicting: 105it [01:10,  1.29it/s]Extractor Predicting: 106it [01:10,  1.38it/s]Extractor Predicting: 107it [01:11,  1.42it/s]Extractor Predicting: 108it [01:12,  1.41it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.50it/s]Extractor Predicting: 111it [01:14,  1.52it/s]Extractor Predicting: 112it [01:14,  1.55it/s]Extractor Predicting: 113it [01:15,  1.43it/s]Extractor Predicting: 114it [01:16,  1.48it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:18,  1.55it/s]Extractor Predicting: 118it [01:18,  1.50it/s]Extractor Predicting: 119it [01:19,  1.54it/s]Extractor Predicting: 120it [01:20,  1.55it/s]Extractor Predicting: 121it [01:20,  1.54it/s]Extractor Predicting: 122it [01:21,  1.66it/s]Extractor Predicting: 123it [01:21,  1.60it/s]Extractor Predicting: 124it [01:22,  1.58it/s]Extractor Predicting: 125it [01:23,  1.55it/s]Extractor Predicting: 126it [01:23,  1.54it/s]Extractor Predicting: 127it [01:24,  1.57it/s]Extractor Predicting: 128it [01:25,  1.50it/s]Extractor Predicting: 129it [01:26,  1.37it/s]Extractor Predicting: 130it [01:26,  1.42it/s]Extractor Predicting: 131it [01:27,  1.50it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:28,  1.50it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:29,  1.55it/s]Extractor Predicting: 136it [01:30,  1.47it/s]Extractor Predicting: 137it [01:31,  1.50it/s]Extractor Predicting: 138it [01:31,  1.52it/s]Extractor Predicting: 139it [01:32,  1.53it/s]Extractor Predicting: 140it [01:33,  1.55it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:34,  1.52it/s]Extractor Predicting: 143it [01:35,  1.53it/s]Extractor Predicting: 144it [01:35,  1.56it/s]Extractor Predicting: 145it [01:36,  1.54it/s]Extractor Predicting: 146it [01:37,  1.44it/s]Extractor Predicting: 147it [01:37,  1.50it/s]Extractor Predicting: 148it [01:38,  1.54it/s]Extractor Predicting: 149it [01:39,  1.59it/s]Extractor Predicting: 150it [01:39,  1.59it/s]Extractor Predicting: 151it [01:40,  1.53it/s]Extractor Predicting: 152it [01:41,  1.50it/s]Extractor Predicting: 153it [01:41,  1.47it/s]Extractor Predicting: 154it [01:42,  1.46it/s]Extractor Predicting: 155it [01:43,  1.44it/s]Extractor Predicting: 156it [01:44,  1.22it/s]Extractor Predicting: 157it [01:45,  1.27it/s]Extractor Predicting: 158it [01:45,  1.31it/s]Extractor Predicting: 159it [01:46,  1.35it/s]Extractor Predicting: 160it [01:47,  1.32it/s]Extractor Predicting: 161it [01:47,  1.35it/s]Extractor Predicting: 162it [01:48,  1.37it/s]Extractor Predicting: 163it [01:49,  1.38it/s]Extractor Predicting: 164it [01:50,  1.39it/s]Extractor Predicting: 165it [01:50,  1.30it/s]Extractor Predicting: 166it [01:51,  1.34it/s]Extractor Predicting: 167it [01:52,  1.36it/s]Extractor Predicting: 168it [01:53,  1.40it/s]Extractor Predicting: 169it [01:53,  1.45it/s]Extractor Predicting: 170it [01:54,  1.41it/s]Extractor Predicting: 171it [01:55,  1.45it/s]Extractor Predicting: 172it [01:55,  1.46it/s]Extractor Predicting: 173it [01:56,  1.47it/s]Extractor Predicting: 174it [01:57,  1.48it/s]Extractor Predicting: 175it [01:57,  1.45it/s]Extractor Predicting: 176it [01:58,  1.46it/s]Extractor Predicting: 177it [01:59,  1.47it/s]Extractor Predicting: 178it [01:59,  1.46it/s]Extractor Predicting: 179it [02:00,  1.49it/s]Extractor Predicting: 180it [02:01,  1.34it/s]Extractor Predicting: 181it [02:02,  1.40it/s]Extractor Predicting: 182it [02:02,  1.44it/s]Extractor Predicting: 183it [02:03,  1.50it/s]Extractor Predicting: 184it [02:03,  1.53it/s]Extractor Predicting: 185it [02:04,  1.52it/s]Extractor Predicting: 186it [02:05,  1.51it/s]Extractor Predicting: 187it [02:05,  1.54it/s]Extractor Predicting: 188it [02:06,  1.55it/s]Extractor Predicting: 189it [02:07,  1.56it/s]Extractor Predicting: 190it [02:07,  1.50it/s]Extractor Predicting: 191it [02:08,  1.50it/s]Extractor Predicting: 192it [02:09,  1.53it/s]Extractor Predicting: 193it [02:09,  1.55it/s]Extractor Predicting: 194it [02:10,  1.57it/s]Extractor Predicting: 195it [02:11,  1.36it/s]Extractor Predicting: 196it [02:12,  1.40it/s]Extractor Predicting: 197it [02:12,  1.46it/s]Extractor Predicting: 198it [02:13,  1.49it/s]Extractor Predicting: 199it [02:13,  1.55it/s]Extractor Predicting: 200it [02:14,  1.47it/s]Extractor Predicting: 201it [02:15,  1.51it/s]Extractor Predicting: 202it [02:15,  1.51it/s]Extractor Predicting: 203it [02:16,  1.51it/s]Extractor Predicting: 204it [02:17,  1.54it/s]Extractor Predicting: 205it [02:18,  1.25it/s]Extractor Predicting: 206it [02:18,  1.33it/s]Extractor Predicting: 207it [02:19,  1.39it/s]Extractor Predicting: 208it [02:20,  1.46it/s]Extractor Predicting: 209it [02:21,  1.32it/s]Extractor Predicting: 210it [02:21,  1.39it/s]Extractor Predicting: 211it [02:22,  1.44it/s]Extractor Predicting: 212it [02:23,  1.47it/s]Extractor Predicting: 213it [02:23,  1.51it/s]Extractor Predicting: 214it [02:24,  1.52it/s]Extractor Predicting: 215it [02:24,  1.56it/s]Extractor Predicting: 216it [02:25,  1.57it/s]Extractor Predicting: 217it [02:26,  1.57it/s]Extractor Predicting: 218it [02:26,  1.58it/s]Extractor Predicting: 219it [02:27,  1.56it/s]Extractor Predicting: 220it [02:28,  1.55it/s]Extractor Predicting: 221it [02:28,  1.45it/s]Extractor Predicting: 222it [02:29,  1.50it/s]Extractor Predicting: 223it [02:30,  1.52it/s]Extractor Predicting: 224it [02:30,  1.52it/s]Extractor Predicting: 225it [02:31,  1.48it/s]Extractor Predicting: 226it [02:32,  1.47it/s]Extractor Predicting: 227it [02:32,  1.50it/s]Extractor Predicting: 228it [02:33,  1.52it/s]Extractor Predicting: 229it [02:34,  1.57it/s]Extractor Predicting: 230it [02:34,  1.56it/s]Extractor Predicting: 231it [02:35,  1.50it/s]Extractor Predicting: 232it [02:36,  1.55it/s]Extractor Predicting: 233it [02:36,  1.59it/s]Extractor Predicting: 234it [02:37,  1.56it/s]Extractor Predicting: 235it [02:37,  1.55it/s]Extractor Predicting: 236it [02:38,  1.43it/s]Extractor Predicting: 237it [02:39,  1.47it/s]Extractor Predicting: 238it [02:40,  1.52it/s]Extractor Predicting: 239it [02:40,  1.55it/s]Extractor Predicting: 240it [02:41,  1.55it/s]Extractor Predicting: 241it [02:41,  1.53it/s]Extractor Predicting: 242it [02:42,  1.57it/s]Extractor Predicting: 243it [02:43,  1.54it/s]Extractor Predicting: 244it [02:43,  1.56it/s]Extractor Predicting: 245it [02:44,  1.57it/s]Extractor Predicting: 246it [02:45,  1.51it/s]Extractor Predicting: 247it [02:45,  1.53it/s]Extractor Predicting: 248it [02:46,  1.57it/s]Extractor Predicting: 249it [02:47,  1.57it/s]Extractor Predicting: 250it [02:47,  1.56it/s]Extractor Predicting: 251it [02:48,  1.46it/s]Extractor Predicting: 252it [02:49,  1.50it/s]Extractor Predicting: 253it [02:49,  1.53it/s]Extractor Predicting: 254it [02:50,  1.55it/s]Extractor Predicting: 255it [02:51,  1.54it/s]Extractor Predicting: 256it [02:51,  1.52it/s]Extractor Predicting: 257it [02:52,  1.55it/s]Extractor Predicting: 258it [02:52,  1.59it/s]Extractor Predicting: 259it [02:53,  1.57it/s]Extractor Predicting: 260it [02:54,  1.55it/s]Extractor Predicting: 261it [02:54,  1.50it/s]Extractor Predicting: 262it [02:55,  1.53it/s]Extractor Predicting: 263it [02:56,  1.51it/s]Extractor Predicting: 264it [02:56,  1.52it/s]Extractor Predicting: 265it [02:57,  1.53it/s]Extractor Predicting: 266it [02:58,  1.46it/s]Extractor Predicting: 267it [02:58,  1.50it/s]Extractor Predicting: 268it [02:59,  1.30it/s]Extractor Predicting: 269it [03:00,  1.42it/s]Extractor Predicting: 269it [03:00,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:36,212 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:36,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:36,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:36,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:36,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:36:37,965 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:36:37,966 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:36:38,903 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:36:40,192 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:36:40,263 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:45,121 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:45,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:45,124 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:45,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:45,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:36:46,803 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:36:46,834 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:36:47,555 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:36:47,802 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:36:47,803 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3202662721893491,
  "recall": 0.05001155001155001,
  "score": 0.08651348651348652,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.23it/s]Extractor Predicting: 22it [00:15,  1.31it/s]Extractor Predicting: 23it [00:15,  1.32it/s]Extractor Predicting: 24it [00:16,  1.38it/s]Extractor Predicting: 25it [00:17,  1.23it/s]Extractor Predicting: 26it [00:18,  1.28it/s]Extractor Predicting: 27it [00:18,  1.35it/s]Extractor Predicting: 28it [00:19,  1.38it/s]Extractor Predicting: 29it [00:20,  1.42it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:21,  1.47it/s]Extractor Predicting: 32it [00:22,  1.44it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:23,  1.49it/s]Extractor Predicting: 35it [00:24,  1.47it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:25,  1.44it/s]Extractor Predicting: 38it [00:26,  1.40it/s]Extractor Predicting: 39it [00:27,  1.47it/s]Extractor Predicting: 40it [00:27,  1.51it/s]Extractor Predicting: 41it [00:28,  1.53it/s]Extractor Predicting: 42it [00:29,  1.36it/s]Extractor Predicting: 43it [00:29,  1.44it/s]Extractor Predicting: 44it [00:30,  1.47it/s]Extractor Predicting: 45it [00:31,  1.50it/s]Extractor Predicting: 46it [00:31,  1.50it/s]Extractor Predicting: 47it [00:32,  1.41it/s]Extractor Predicting: 48it [00:33,  1.45it/s]Extractor Predicting: 49it [00:33,  1.48it/s]Extractor Predicting: 50it [00:34,  1.54it/s]Extractor Predicting: 51it [00:35,  1.55it/s]Extractor Predicting: 52it [00:36,  1.22it/s]Extractor Predicting: 53it [00:36,  1.32it/s]Extractor Predicting: 54it [00:37,  1.40it/s]Extractor Predicting: 55it [00:38,  1.46it/s]Extractor Predicting: 56it [00:38,  1.51it/s]Extractor Predicting: 57it [00:39,  1.54it/s]Extractor Predicting: 58it [00:40,  1.56it/s]Extractor Predicting: 59it [00:40,  1.57it/s]Extractor Predicting: 60it [00:41,  1.60it/s]Extractor Predicting: 61it [00:41,  1.55it/s]Extractor Predicting: 62it [00:42,  1.56it/s]Extractor Predicting: 63it [00:43,  1.55it/s]Extractor Predicting: 64it [00:43,  1.52it/s]Extractor Predicting: 65it [00:44,  1.55it/s]Extractor Predicting: 66it [00:45,  1.51it/s]Extractor Predicting: 67it [00:45,  1.53it/s]Extractor Predicting: 68it [00:46,  1.53it/s]Extractor Predicting: 69it [00:47,  1.57it/s]Extractor Predicting: 70it [00:47,  1.59it/s]Extractor Predicting: 71it [00:48,  1.58it/s]Extractor Predicting: 72it [00:49,  1.54it/s]Extractor Predicting: 73it [00:49,  1.56it/s]Extractor Predicting: 74it [00:50,  1.57it/s]Extractor Predicting: 75it [00:50,  1.58it/s]Extractor Predicting: 76it [00:51,  1.60it/s]Extractor Predicting: 77it [00:52,  1.51it/s]Extractor Predicting: 78it [00:53,  1.46it/s]Extractor Predicting: 79it [00:53,  1.52it/s]Extractor Predicting: 80it [00:54,  1.53it/s]Extractor Predicting: 81it [00:54,  1.60it/s]Extractor Predicting: 82it [00:55,  1.62it/s]Extractor Predicting: 83it [00:56,  1.53it/s]Extractor Predicting: 84it [00:56,  1.59it/s]Extractor Predicting: 85it [00:57,  1.62it/s]Extractor Predicting: 86it [00:57,  1.61it/s]Extractor Predicting: 87it [00:58,  1.61it/s]Extractor Predicting: 88it [00:59,  1.55it/s]Extractor Predicting: 89it [00:59,  1.54it/s]Extractor Predicting: 90it [01:00,  1.60it/s]Extractor Predicting: 91it [01:01,  1.61it/s]Extractor Predicting: 92it [01:01,  1.60it/s]Extractor Predicting: 93it [01:02,  1.53it/s]Extractor Predicting: 94it [01:03,  1.55it/s]Extractor Predicting: 95it [01:03,  1.63it/s]Extractor Predicting: 96it [01:04,  1.62it/s]Extractor Predicting: 97it [01:04,  1.61it/s]Extractor Predicting: 98it [01:05,  1.54it/s]Extractor Predicting: 99it [01:06,  1.55it/s]Extractor Predicting: 100it [01:06,  1.58it/s]Extractor Predicting: 101it [01:07,  1.61it/s]Extractor Predicting: 102it [01:08,  1.64it/s]Extractor Predicting: 103it [01:08,  1.60it/s]Extractor Predicting: 104it [01:09,  1.62it/s]Extractor Predicting: 105it [01:09,  1.64it/s]Extractor Predicting: 106it [01:10,  1.68it/s]Extractor Predicting: 107it [01:11,  1.68it/s]Extractor Predicting: 108it [01:11,  1.67it/s]Extractor Predicting: 109it [01:12,  1.71it/s]Extractor Predicting: 110it [01:12,  1.70it/s]Extractor Predicting: 111it [01:13,  1.72it/s]Extractor Predicting: 112it [01:13,  1.71it/s]Extractor Predicting: 113it [01:14,  1.69it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:16,  1.52it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:17,  1.49it/s]Extractor Predicting: 118it [01:18,  1.51it/s]Extractor Predicting: 119it [01:19,  1.10it/s]Extractor Predicting: 120it [01:20,  1.21it/s]Extractor Predicting: 121it [01:20,  1.23it/s]Extractor Predicting: 122it [01:21,  1.32it/s]Extractor Predicting: 123it [01:22,  1.36it/s]Extractor Predicting: 124it [01:23,  1.31it/s]Extractor Predicting: 125it [01:24,  1.10it/s]Extractor Predicting: 126it [01:25,  1.19it/s]Extractor Predicting: 127it [01:25,  1.25it/s]Extractor Predicting: 128it [01:26,  1.32it/s]Extractor Predicting: 129it [01:27,  1.28it/s]Extractor Predicting: 130it [01:27,  1.37it/s]Extractor Predicting: 131it [01:28,  1.40it/s]Extractor Predicting: 132it [01:29,  1.42it/s]Extractor Predicting: 133it [01:29,  1.45it/s]Extractor Predicting: 134it [01:30,  1.24it/s]Extractor Predicting: 135it [01:31,  1.31it/s]Extractor Predicting: 136it [01:32,  1.39it/s]Extractor Predicting: 137it [01:32,  1.42it/s]Extractor Predicting: 138it [01:33,  1.42it/s]Extractor Predicting: 139it [01:34,  1.48it/s]Extractor Predicting: 140it [01:34,  1.50it/s]Extractor Predicting: 141it [01:35,  1.50it/s]Extractor Predicting: 142it [01:36,  1.53it/s]Extractor Predicting: 143it [01:36,  1.76it/s]Extractor Predicting: 143it [01:36,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:50,795 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:50,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:50,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:50,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:50,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:38:52,317 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:38:52,318 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:38:52,903 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:38:54,030 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:38:54,030 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:56,119 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:56,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:56,179 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:56,179 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:38:56,179 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:38:57,089 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:38:57,090 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:38:57,635 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:38:57,996 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:38:57,996 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.24390243902439024,
  "recall": 0.05566949897450923,
  "score": 0.09064885496183206,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.30it/s]Extractor Predicting: 7it [00:04,  1.35it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:07,  1.34it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.27it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.39it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:14,  1.10it/s]Extractor Predicting: 21it [00:15,  1.20it/s]Extractor Predicting: 22it [00:16,  1.28it/s]Extractor Predicting: 23it [00:16,  1.36it/s]Extractor Predicting: 24it [00:17,  1.35it/s]Extractor Predicting: 25it [00:18,  1.39it/s]Extractor Predicting: 26it [00:18,  1.45it/s]Extractor Predicting: 27it [00:19,  1.48it/s]Extractor Predicting: 28it [00:20,  1.50it/s]Extractor Predicting: 29it [00:21,  1.37it/s]Extractor Predicting: 30it [00:21,  1.42it/s]Extractor Predicting: 31it [00:22,  1.47it/s]Extractor Predicting: 32it [00:23,  1.48it/s]Extractor Predicting: 33it [00:23,  1.51it/s]Extractor Predicting: 34it [00:24,  1.41it/s]Extractor Predicting: 35it [00:25,  1.45it/s]Extractor Predicting: 36it [00:26,  1.31it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:27,  1.50it/s]Extractor Predicting: 39it [00:27,  1.59it/s]Extractor Predicting: 40it [00:28,  1.68it/s]Extractor Predicting: 41it [00:28,  1.56it/s]Extractor Predicting: 42it [00:29,  1.66it/s]Extractor Predicting: 43it [00:30,  1.72it/s]Extractor Predicting: 44it [00:30,  1.78it/s]Extractor Predicting: 45it [00:31,  1.85it/s]Extractor Predicting: 46it [00:31,  1.84it/s]Extractor Predicting: 47it [00:32,  1.73it/s]Extractor Predicting: 48it [00:32,  1.77it/s]Extractor Predicting: 49it [00:33,  1.80it/s]Extractor Predicting: 50it [00:33,  1.85it/s]Extractor Predicting: 51it [00:34,  1.83it/s]Extractor Predicting: 52it [00:34,  1.85it/s]Extractor Predicting: 53it [00:35,  1.81it/s]Extractor Predicting: 54it [00:35,  1.87it/s]Extractor Predicting: 55it [00:36,  1.90it/s]Extractor Predicting: 56it [00:37,  1.91it/s]Extractor Predicting: 57it [00:37,  1.92it/s]Extractor Predicting: 58it [00:38,  1.91it/s]Extractor Predicting: 59it [00:38,  1.70it/s]Extractor Predicting: 60it [00:39,  1.71it/s]Extractor Predicting: 61it [00:39,  1.73it/s]Extractor Predicting: 62it [00:40,  1.82it/s]Extractor Predicting: 63it [00:40,  1.85it/s]Extractor Predicting: 64it [00:41,  1.87it/s]Extractor Predicting: 65it [00:42,  1.81it/s]Extractor Predicting: 66it [00:42,  1.87it/s]Extractor Predicting: 67it [00:43,  1.79it/s]Extractor Predicting: 68it [00:43,  1.70it/s]Extractor Predicting: 69it [00:44,  1.64it/s]Extractor Predicting: 70it [00:45,  1.32it/s]Extractor Predicting: 71it [00:46,  1.37it/s]Extractor Predicting: 72it [00:46,  1.42it/s]Extractor Predicting: 72it [00:46,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:39:50,784 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:39:50,889 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:39:51,072 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:39:51,073 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:39:51,101 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:40:46,196 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:40:46,244 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:40:46,710 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:40:46,710 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:40:46,953 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:47,263 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:47,263 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:47,263 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:47,263 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:47,263 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:47,263 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7490589711417817,
  "recall": 0.14895209580838323,
  "score": 0.2484911550468262,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:40:49,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:49,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:50,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:50,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:51,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:51,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:52,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:53,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:53,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:54,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:54,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:55,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:56,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:56,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:57,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:57,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:58,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:58,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:40:59,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:00,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:11<01:43, 11.50s/it][WARNING|generation_utils.py:914] 2023-08-28 16:41:00,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:01,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:02,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:02,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:03,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:03,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:04,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:05,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:05,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:06,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:07,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:07,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:08,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:08,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:09,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:10,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:10,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:11,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:12,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:12,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:13,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:24<01:40, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-28 16:41:14,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:14,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:15,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:15,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:16,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:16,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:17,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:17,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:18,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:18,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:19,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:19,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:20,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:21,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:21,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:22,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:22,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:23,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:23,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:24,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:35<01:21, 11.59s/it][WARNING|generation_utils.py:914] 2023-08-28 16:41:24,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:25,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:25,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:26,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:27,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:28,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:29,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:29,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:30,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:31,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:32,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:32,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:33,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:33,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:34,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:35,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:35,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:36,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:37,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:37,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:49<01:15, 12.58s/it][WARNING|generation_utils.py:914] 2023-08-28 16:41:38,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:39,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:39,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:40,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:40,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:41,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:41,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:42,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:43,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:43,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:44,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:44,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:45,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:45,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:46,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:47,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:47,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:48,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:49,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:00<01:00, 12.06s/it][WARNING|generation_utils.py:914] 2023-08-28 16:41:49,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:50,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:50,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:51,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:52,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:52,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:53,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:54,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:54,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:55,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:56,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:56,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:57,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:57,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:58,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:59,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:41:59,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:00,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:01,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:12<00:48, 12.06s/it][WARNING|generation_utils.py:914] 2023-08-28 16:42:01,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:02,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:03,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:03,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:04,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:05,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:05,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:06,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:06,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:07,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:08,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:08,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:09,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:10,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:10,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:11,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:12,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:13,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:14,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:14,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:26<00:37, 12.67s/it][WARNING|generation_utils.py:914] 2023-08-28 16:42:15,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:16,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:16,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:17,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:18,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:18,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:19,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:20,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:20,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:21,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:21,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:22,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:22,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:23,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:23,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:24,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:24,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:25,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:25,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:26,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:37<00:24, 12.24s/it][WARNING|generation_utils.py:914] 2023-08-28 16:42:27,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:27,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:28,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:28,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:29,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:30,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:31,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:31,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:32,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:32,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:33,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:34,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:34,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:35,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:35,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:36,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:37,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:37,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:38,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:38,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:39,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:50<00:12, 12.50s/it][WARNING|generation_utils.py:914] 2023-08-28 16:42:40,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:40,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:41,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:42,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:43,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:43,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:44,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:45,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:45,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:46,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:47,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:48,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:48,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:49,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:50,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:51,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:51,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:52,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:53,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:54,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:42:54,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:06<00:00, 13.36s/it]Generating: 100%|██████████| 10/10 [02:06<00:00, 12.61s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:07,411 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:07,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:07,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:07,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:07,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:43:09,107 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:43:09,108 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:43:09,639 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:43:11,089 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:43:11,257 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:15,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:15,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:15,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:15,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:15,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:43:17,127 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:43:17,128 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:43:18,056 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:43:18,454 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:43:18,454 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 446, 'raw': 448}
{'target': 600, 'success': 477, 'raw': 480}
{'target': 600, 'success': 509, 'raw': 512}
{'target': 600, 'success': 541, 'raw': 544}
{'target': 600, 'success': 573, 'raw': 576}
{'target': 600, 'success': 605, 'raw': 608}
{'prompt': 'Relation : sport .', 'success_rate': 0.9950657894736842, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : cast member .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : league .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 599, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.984375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 7810
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7910, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.66it/s]Extractor Estimating: 4it [00:02,  1.73it/s]Extractor Estimating: 5it [00:03,  1.23it/s]Extractor Estimating: 6it [00:04,  1.36it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:07,  1.66it/s]Extractor Estimating: 12it [00:07,  1.68it/s]Extractor Estimating: 13it [00:08,  1.68it/s]Extractor Estimating: 14it [00:08,  1.70it/s]Extractor Estimating: 15it [00:09,  1.74it/s]Extractor Estimating: 16it [00:09,  1.76it/s]Extractor Estimating: 17it [00:10,  1.69it/s]Extractor Estimating: 18it [00:11,  1.69it/s]Extractor Estimating: 19it [00:11,  1.49it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:14,  1.31it/s]Extractor Estimating: 23it [00:14,  1.44it/s]Extractor Estimating: 24it [00:15,  1.52it/s]Extractor Estimating: 25it [00:15,  1.55it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:17,  1.42it/s]Extractor Estimating: 28it [00:17,  1.52it/s]Extractor Estimating: 29it [00:18,  1.54it/s]Extractor Estimating: 30it [00:19,  1.17it/s]Extractor Estimating: 31it [00:20,  1.30it/s]Extractor Estimating: 32it [00:21,  1.40it/s]Extractor Estimating: 33it [00:21,  1.47it/s]Extractor Estimating: 34it [00:22,  1.41it/s]Extractor Estimating: 35it [00:23,  1.44it/s]Extractor Estimating: 36it [00:23,  1.51it/s]Extractor Estimating: 37it [00:24,  1.55it/s]Extractor Estimating: 38it [00:24,  1.55it/s]Extractor Estimating: 39it [00:25,  1.62it/s]Extractor Estimating: 40it [00:26,  1.64it/s]Extractor Estimating: 41it [00:26,  1.64it/s]Extractor Estimating: 42it [00:27,  1.67it/s]Extractor Estimating: 43it [00:28,  1.45it/s]Extractor Estimating: 44it [00:28,  1.53it/s]Extractor Estimating: 45it [00:29,  1.61it/s]Extractor Estimating: 46it [00:29,  1.65it/s]Extractor Estimating: 47it [00:30,  1.62it/s]Extractor Estimating: 48it [00:31,  1.69it/s]Extractor Estimating: 49it [00:31,  1.72it/s]Extractor Estimating: 50it [00:32,  1.76it/s]Extractor Estimating: 51it [00:32,  1.85it/s]Extractor Estimating: 52it [00:33,  1.86it/s]Extractor Estimating: 53it [00:33,  1.93it/s]Extractor Estimating: 54it [00:34,  1.98it/s]Extractor Estimating: 55it [00:34,  2.00it/s]Extractor Estimating: 56it [00:35,  2.09it/s]Extractor Estimating: 57it [00:35,  2.00it/s]Extractor Estimating: 58it [00:36,  2.06it/s]Extractor Estimating: 59it [00:36,  2.08it/s]Extractor Estimating: 60it [00:36,  2.09it/s]Extractor Estimating: 61it [00:37,  2.05it/s]Extractor Estimating: 62it [00:37,  2.02it/s]Extractor Estimating: 63it [00:38,  1.78it/s]Extractor Estimating: 64it [00:39,  1.80it/s]Extractor Estimating: 65it [00:39,  1.93it/s]Extractor Estimating: 66it [00:40,  1.90it/s]Extractor Estimating: 67it [00:40,  1.98it/s]Extractor Estimating: 68it [00:41,  2.00it/s]Extractor Estimating: 69it [00:41,  1.98it/s]Extractor Estimating: 70it [00:42,  2.02it/s]Extractor Estimating: 71it [00:42,  2.05it/s]Extractor Estimating: 72it [00:43,  2.06it/s]Extractor Estimating: 73it [00:43,  1.91it/s]Extractor Estimating: 74it [00:44,  1.96it/s]Extractor Estimating: 75it [00:44,  2.01it/s]Extractor Estimating: 76it [00:45,  1.88it/s]Extractor Estimating: 77it [00:45,  1.81it/s]Extractor Estimating: 78it [00:46,  1.80it/s]Extractor Estimating: 79it [00:47,  1.76it/s]Extractor Estimating: 80it [00:47,  1.75it/s]Extractor Estimating: 81it [00:48,  1.76it/s]Extractor Estimating: 82it [00:48,  1.62it/s]Extractor Estimating: 83it [00:49,  1.66it/s]Extractor Estimating: 84it [00:50,  1.70it/s]Extractor Estimating: 85it [00:50,  1.49it/s]Extractor Estimating: 86it [00:51,  1.56it/s]Extractor Estimating: 87it [00:52,  1.63it/s]Extractor Estimating: 88it [00:52,  1.69it/s]Extractor Estimating: 89it [00:53,  1.75it/s]Extractor Estimating: 90it [00:53,  1.71it/s]Extractor Estimating: 91it [00:54,  1.72it/s]Extractor Estimating: 92it [00:54,  1.74it/s]Extractor Estimating: 93it [00:55,  1.80it/s]Extractor Estimating: 94it [00:56,  1.36it/s]Extractor Estimating: 95it [00:57,  1.48it/s]Extractor Estimating: 96it [00:57,  1.53it/s]Extractor Estimating: 97it [00:58,  1.60it/s]Extractor Estimating: 98it [00:58,  1.63it/s]Extractor Estimating: 99it [00:59,  1.39it/s]Extractor Estimating: 100it [01:00,  1.52it/s]Extractor Estimating: 101it [01:00,  1.65it/s]Extractor Estimating: 102it [01:01,  1.80it/s]Extractor Estimating: 103it [01:02,  1.45it/s]Extractor Estimating: 104it [01:02,  1.59it/s]Extractor Estimating: 105it [01:03,  1.72it/s]Extractor Estimating: 106it [01:03,  1.83it/s]Extractor Estimating: 107it [01:04,  1.72it/s]Extractor Estimating: 108it [01:05,  1.29it/s]Extractor Estimating: 109it [01:05,  1.48it/s]Extractor Estimating: 110it [01:06,  1.62it/s]Extractor Estimating: 111it [01:06,  1.73it/s]Extractor Estimating: 112it [01:07,  1.43it/s]Extractor Estimating: 113it [01:08,  1.59it/s]Extractor Estimating: 114it [01:08,  1.74it/s]Extractor Estimating: 115it [01:09,  1.88it/s]Extractor Estimating: 116it [01:09,  1.95it/s]Extractor Estimating: 117it [01:10,  1.88it/s]Extractor Estimating: 118it [01:10,  1.98it/s]Extractor Estimating: 119it [01:11,  2.03it/s]Extractor Estimating: 120it [01:11,  2.08it/s]Extractor Estimating: 121it [01:12,  2.08it/s]Extractor Estimating: 122it [01:12,  2.03it/s]Extractor Estimating: 123it [01:13,  1.46it/s]Extractor Estimating: 124it [01:14,  1.63it/s]Extractor Estimating: 125it [01:14,  1.74it/s]Extractor Estimating: 126it [01:15,  1.74it/s]Extractor Estimating: 127it [01:16,  1.45it/s]Extractor Estimating: 128it [01:16,  1.50it/s]Extractor Estimating: 129it [01:17,  1.56it/s]Extractor Estimating: 130it [01:17,  1.62it/s]Extractor Estimating: 131it [01:18,  1.65it/s]Extractor Estimating: 132it [01:19,  1.55it/s]Extractor Estimating: 133it [01:19,  1.63it/s]Extractor Estimating: 134it [01:20,  1.73it/s]Extractor Estimating: 135it [01:20,  1.78it/s]Extractor Estimating: 136it [01:21,  1.77it/s]Extractor Estimating: 137it [01:22,  1.77it/s]Extractor Estimating: 138it [01:22,  1.55it/s]Extractor Estimating: 139it [01:23,  1.61it/s]Extractor Estimating: 140it [01:23,  1.64it/s]Extractor Estimating: 141it [01:24,  1.68it/s]Extractor Estimating: 142it [01:25,  1.75it/s]Extractor Estimating: 143it [01:25,  1.54it/s]Extractor Estimating: 144it [01:26,  1.59it/s]Extractor Estimating: 145it [01:27,  1.67it/s]Extractor Estimating: 146it [01:27,  1.74it/s]Extractor Estimating: 147it [01:28,  1.77it/s]Extractor Estimating: 148it [01:28,  1.74it/s]Extractor Estimating: 149it [01:29,  1.79it/s]Extractor Estimating: 150it [01:29,  1.74it/s]Extractor Estimating: 151it [01:30,  1.46it/s]Extractor Estimating: 152it [01:31,  1.48it/s]Extractor Estimating: 153it [01:32,  1.44it/s]Extractor Estimating: 154it [01:32,  1.46it/s]Extractor Estimating: 155it [01:33,  1.52it/s]Extractor Estimating: 156it [01:34,  1.45it/s]Extractor Estimating: 157it [01:34,  1.49it/s]Extractor Estimating: 158it [01:35,  1.52it/s]Extractor Estimating: 159it [01:36,  1.52it/s]Extractor Estimating: 160it [01:36,  1.51it/s]Extractor Estimating: 161it [01:37,  1.43it/s]Extractor Estimating: 162it [01:38,  1.49it/s]Extractor Estimating: 163it [01:38,  1.53it/s]Extractor Estimating: 164it [01:39,  1.55it/s]Extractor Estimating: 165it [01:40,  1.53it/s]Extractor Estimating: 166it [01:40,  1.56it/s]Extractor Estimating: 167it [01:41,  1.53it/s]Extractor Estimating: 168it [01:41,  1.57it/s]Extractor Estimating: 169it [01:43,  1.18it/s]Extractor Estimating: 170it [01:43,  1.29it/s]Extractor Estimating: 171it [01:44,  1.35it/s]Extractor Estimating: 172it [01:45,  1.40it/s]Extractor Estimating: 173it [01:45,  1.44it/s]Extractor Estimating: 174it [01:46,  1.42it/s]Extractor Estimating: 175it [01:47,  1.43it/s]Extractor Estimating: 176it [01:47,  1.48it/s]Extractor Estimating: 177it [01:48,  1.58it/s]Extractor Estimating: 178it [01:48,  1.66it/s]Extractor Estimating: 179it [01:49,  1.73it/s]Extractor Estimating: 180it [01:50,  1.73it/s]Extractor Estimating: 181it [01:50,  1.60it/s]Extractor Estimating: 182it [01:51,  1.67it/s]Extractor Estimating: 183it [01:51,  1.71it/s]Extractor Estimating: 184it [01:52,  1.79it/s]Extractor Estimating: 185it [01:53,  1.35it/s]Extractor Estimating: 186it [01:54,  1.49it/s]Extractor Estimating: 187it [01:54,  1.59it/s]Extractor Estimating: 188it [01:55,  1.68it/s]Extractor Estimating: 189it [01:55,  1.76it/s]Extractor Estimating: 190it [01:56,  1.81it/s]Extractor Estimating: 191it [01:56,  1.80it/s]Extractor Estimating: 192it [01:57,  1.83it/s]Extractor Estimating: 193it [01:57,  1.85it/s]Extractor Estimating: 194it [01:58,  1.85it/s]Extractor Estimating: 195it [01:58,  1.88it/s]Extractor Estimating: 196it [01:59,  1.80it/s]Extractor Estimating: 197it [01:59,  1.83it/s]Extractor Estimating: 198it [02:00,  1.71it/s]Extractor Estimating: 199it [02:01,  1.72it/s]Extractor Estimating: 200it [02:01,  1.74it/s]Extractor Estimating: 201it [02:02,  1.77it/s]Extractor Estimating: 202it [02:02,  1.71it/s]Extractor Estimating: 203it [02:03,  1.56it/s]Extractor Estimating: 204it [02:04,  1.57it/s]Extractor Estimating: 205it [02:04,  1.62it/s]Extractor Estimating: 206it [02:05,  1.63it/s]Extractor Estimating: 207it [02:06,  1.67it/s]Extractor Estimating: 208it [02:06,  1.68it/s]Extractor Estimating: 209it [02:07,  1.71it/s]Extractor Estimating: 210it [02:07,  1.70it/s]Extractor Estimating: 211it [02:08,  1.77it/s]Extractor Estimating: 212it [02:09,  1.62it/s]Extractor Estimating: 213it [02:09,  1.65it/s]Extractor Estimating: 214it [02:10,  1.64it/s]Extractor Estimating: 215it [02:10,  1.61it/s]Extractor Estimating: 216it [02:11,  1.68it/s]Extractor Estimating: 217it [02:11,  1.70it/s]Extractor Estimating: 218it [02:12,  1.75it/s]Extractor Estimating: 219it [02:13,  1.77it/s]Extractor Estimating: 220it [02:13,  1.74it/s]Extractor Estimating: 221it [02:14,  1.54it/s]Extractor Estimating: 222it [02:15,  1.56it/s]Extractor Estimating: 223it [02:15,  1.62it/s]Extractor Estimating: 224it [02:16,  1.21it/s]Extractor Estimating: 225it [02:17,  1.32it/s]Extractor Estimating: 226it [02:18,  1.44it/s]Extractor Estimating: 227it [02:18,  1.49it/s]Extractor Estimating: 228it [02:19,  1.45it/s]Extractor Estimating: 229it [02:20,  1.55it/s]Extractor Estimating: 230it [02:20,  1.65it/s]Extractor Estimating: 231it [02:21,  1.68it/s]Extractor Estimating: 232it [02:21,  1.64it/s]Extractor Estimating: 233it [02:22,  1.65it/s]Extractor Estimating: 234it [02:22,  1.67it/s]Extractor Estimating: 235it [02:23,  1.67it/s]Extractor Estimating: 236it [02:24,  1.67it/s]Extractor Estimating: 237it [02:24,  1.53it/s]Extractor Estimating: 238it [02:25,  1.60it/s]Extractor Estimating: 239it [02:26,  1.62it/s]Extractor Estimating: 240it [02:26,  1.67it/s]Extractor Estimating: 241it [02:27,  1.71it/s]Extractor Estimating: 242it [02:27,  1.78it/s]Extractor Estimating: 243it [02:28,  1.74it/s]Extractor Estimating: 244it [02:28,  1.77it/s]Extractor Estimating: 245it [02:29,  1.76it/s]Extractor Estimating: 246it [02:30,  1.72it/s]Extractor Estimating: 247it [02:30,  2.11it/s]Extractor Estimating: 247it [02:30,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:26,355 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:26,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:26,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:26,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:26,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:46:28,288 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:46:28,289 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:46:29,254 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:46:30,367 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:46:30,367 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:33,456 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:33,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:33,589 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:33,589 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:46:33,589 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:46:34,949 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:46:34,950 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:46:35,484 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:46:35,741 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:46:35,741 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:22:11,106 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:22:11,153 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4908 mean pseudo reward: 0.9433148978739786
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl'}
train vocab size: 19680
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19780, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19780, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.988, loss:443.8971
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.948, loss:401.4430
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 95, avg_time 0.948, loss:364.1625
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 195, avg_time 0.941, loss:364.1330
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 90, avg_time 0.946, loss:340.4693
>> valid entity prec:0.5380, rec:0.4670, f1:0.5000
>> valid relation prec:0.2777, rec:0.0498, f1:0.0845
>> valid relation with NER prec:0.2777, rec:0.0498, f1:0.0845
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 190, avg_time 3.320, loss:354.4527
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 85, avg_time 0.945, loss:317.9872
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 185, avg_time 0.948, loss:344.0642
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 80, avg_time 0.935, loss:309.5724
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 180, avg_time 0.942, loss:344.8494
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5661, rec:0.4177, f1:0.4807
>> valid relation prec:0.2590, rec:0.0441, f1:0.0753
>> valid relation with NER prec:0.2590, rec:0.0441, f1:0.0753
g_step 1100, step 75, avg_time 3.314, loss:323.2571
g_step 1200, step 175, avg_time 0.952, loss:316.8556
g_step 1300, step 70, avg_time 0.941, loss:291.5636
g_step 1400, step 170, avg_time 0.945, loss:311.6478
g_step 1500, step 65, avg_time 0.935, loss:292.0952
>> valid entity prec:0.5348, rec:0.4106, f1:0.4646
>> valid relation prec:0.1817, rec:0.0414, f1:0.0674
>> valid relation with NER prec:0.1817, rec:0.0414, f1:0.0674
g_step 1600, step 165, avg_time 3.306, loss:290.6001
g_step 1700, step 60, avg_time 0.957, loss:268.2533
g_step 1800, step 160, avg_time 0.947, loss:271.3805
g_step 1900, step 55, avg_time 0.937, loss:274.5479
g_step 2000, step 155, avg_time 0.941, loss:264.1221
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5352, rec:0.4260, f1:0.4744
>> valid relation prec:0.2239, rec:0.0679, f1:0.1042
>> valid relation with NER prec:0.2239, rec:0.0679, f1:0.1042
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 50, avg_time 3.281, loss:256.0101
g_step 2200, step 150, avg_time 0.944, loss:242.5033
g_step 2300, step 45, avg_time 0.934, loss:250.7072
g_step 2400, step 145, avg_time 0.930, loss:238.4576
g_step 2500, step 40, avg_time 0.923, loss:232.6163
>> valid entity prec:0.5372, rec:0.3941, f1:0.4547
>> valid relation prec:0.1948, rec:0.0552, f1:0.0860
>> valid relation with NER prec:0.1948, rec:0.0552, f1:0.0860
g_step 2600, step 140, avg_time 3.269, loss:224.2223
g_step 2700, step 35, avg_time 0.925, loss:227.1122
g_step 2800, step 135, avg_time 0.948, loss:221.2186
g_step 2900, step 30, avg_time 0.929, loss:226.6262
g_step 3000, step 130, avg_time 0.931, loss:210.4370
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5060, rec:0.4652, f1:0.4847
>> valid relation prec:0.1602, rec:0.0567, f1:0.0837
>> valid relation with NER prec:0.1602, rec:0.0567, f1:0.0837
g_step 3100, step 25, avg_time 3.219, loss:217.9002
g_step 3200, step 125, avg_time 0.904, loss:211.5222
g_step 3300, step 20, avg_time 0.909, loss:209.3947
g_step 3400, step 120, avg_time 0.911, loss:197.2963
g_step 3500, step 15, avg_time 0.924, loss:203.8298
>> valid entity prec:0.5323, rec:0.4091, f1:0.4626
>> valid relation prec:0.1888, rec:0.0604, f1:0.0915
>> valid relation with NER prec:0.1888, rec:0.0604, f1:0.0915
g_step 3600, step 115, avg_time 3.214, loss:198.2521
g_step 3700, step 10, avg_time 0.908, loss:196.6303
g_step 3800, step 110, avg_time 1.044, loss:191.1014
g_step 3900, step 5, avg_time 0.908, loss:188.2554
g_step 4000, step 105, avg_time 0.918, loss:181.0937
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5109, rec:0.3778, f1:0.4344
>> valid relation prec:0.1503, rec:0.0483, f1:0.0731
>> valid relation with NER prec:0.1503, rec:0.0483, f1:0.0731
g_step 4100, step 205, avg_time 3.254, loss:189.8051
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:22:11 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:22:11 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-22-11_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:22:12 - WARNING - datasets.builder -   Using custom data configuration default-4ab376c6af471b0b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4ab376c6af471b0b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:22:18,405 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:22:18,436 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:22:18,437 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:22:18,438 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:22:18,643 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:18,774 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:18,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:18,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:18,775 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:18,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:22:18,775 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:22:20,731 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:22:24,120 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:22:24,198 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4ab376c6af471b0b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.27ba/s] 40%|████      | 2/5 [00:00<00:00,  3.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.01ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.42ba/s]100%|██████████| 5/5 [00:01<00:00,  4.73ba/s]100%|██████████| 5/5 [00:01<00:00,  4.15ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:03,  2.60ba/s] 22%|██▏       | 2/9 [00:00<00:02,  3.49ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.91ba/s] 44%|████▍     | 4/9 [00:01<00:01,  4.15ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.34ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.46ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  3.77ba/s] 89%|████████▉ | 8/9 [00:02<00:00,  4.03ba/s]100%|██████████| 9/9 [00:02<00:00,  4.65ba/s]100%|██████████| 9/9 [00:02<00:00,  4.15ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.56ba/s] 40%|████      | 2/5 [00:00<00:00,  5.58ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.99ba/s]100%|██████████| 5/5 [00:00<00:00,  8.90ba/s]100%|██████████| 5/5 [00:00<00:00,  7.50ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:03,  2.03ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.70ba/s] 44%|████▍     | 4/9 [00:00<00:00,  6.41ba/s] 67%|██████▋   | 6/9 [00:00<00:00,  8.05ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  9.13ba/s]100%|██████████| 9/9 [00:01<00:00,  7.53ba/s]
[INFO|trainer.py:414] 2023-08-28 18:22:38,332 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:22:38,404 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:22:38,404 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 18:22:38,404 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:22:38,404 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:22:38,404 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:22:38,404 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:22:38,404 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:02<14:44,  2.27s/it]  1%|          | 2/390 [00:02<08:25,  1.30s/it]  1%|          | 3/390 [00:03<06:22,  1.01it/s]  1%|          | 4/390 [00:04<05:39,  1.14it/s]  1%|▏         | 5/390 [00:04<04:17,  1.50it/s]  2%|▏         | 6/390 [00:05<04:30,  1.42it/s]  2%|▏         | 7/390 [00:05<03:49,  1.67it/s]  2%|▏         | 8/390 [00:05<03:11,  2.00it/s]  2%|▏         | 9/390 [00:07<04:25,  1.43it/s]  3%|▎         | 10/390 [00:07<04:04,  1.55it/s]  3%|▎         | 11/390 [00:07<03:23,  1.86it/s]  3%|▎         | 12/390 [00:08<03:08,  2.01it/s]  3%|▎         | 13/390 [00:08<02:44,  2.30it/s]  4%|▎         | 14/390 [00:08<02:27,  2.55it/s]  4%|▍         | 15/390 [00:09<02:15,  2.76it/s]  4%|▍         | 16/390 [00:09<02:46,  2.24it/s]  4%|▍         | 17/390 [00:10<02:29,  2.50it/s]  5%|▍         | 18/390 [00:10<02:16,  2.72it/s]  5%|▍         | 19/390 [00:10<02:07,  2.90it/s]  5%|▌         | 20/390 [00:11<02:01,  3.04it/s]  5%|▌         | 21/390 [00:11<01:57,  3.15it/s]  6%|▌         | 22/390 [00:11<01:54,  3.23it/s]  6%|▌         | 23/390 [00:11<01:51,  3.28it/s]  6%|▌         | 24/390 [00:12<01:50,  3.32it/s]  6%|▋         | 25/390 [00:12<01:48,  3.35it/s]  7%|▋         | 26/390 [00:13<02:33,  2.38it/s]  7%|▋         | 27/390 [00:13<02:18,  2.62it/s]  7%|▋         | 28/390 [00:13<02:08,  2.81it/s]  7%|▋         | 29/390 [00:14<02:01,  2.97it/s]  8%|▊         | 30/390 [00:14<01:56,  3.10it/s]  8%|▊         | 31/390 [00:14<01:52,  3.19it/s]  8%|▊         | 32/390 [00:14<01:50,  3.25it/s]  8%|▊         | 33/390 [00:15<01:48,  3.30it/s]  9%|▊         | 34/390 [00:15<01:46,  3.34it/s]  9%|▉         | 35/390 [00:15<01:57,  3.02it/s]  9%|▉         | 36/390 [00:16<01:53,  3.13it/s]  9%|▉         | 37/390 [00:16<01:49,  3.21it/s] 10%|▉         | 38/390 [00:16<01:47,  3.27it/s] 10%|█         | 39/390 [00:17<01:45,  3.32it/s] 10%|█         | 40/390 [00:17<01:44,  3.35it/s] 11%|█         | 41/390 [00:17<01:43,  3.37it/s] 11%|█         | 42/390 [00:17<01:42,  3.38it/s] 11%|█         | 43/390 [00:18<01:42,  3.39it/s] 11%|█▏        | 44/390 [00:18<01:41,  3.40it/s] 12%|█▏        | 45/390 [00:19<02:03,  2.80it/s] 12%|█▏        | 46/390 [00:19<01:56,  2.96it/s] 12%|█▏        | 47/390 [00:19<01:51,  3.09it/s] 12%|█▏        | 48/390 [00:19<01:47,  3.18it/s] 13%|█▎        | 49/390 [00:20<01:44,  3.25it/s] 13%|█▎        | 50/390 [00:20<01:43,  3.30it/s] 13%|█▎        | 51/390 [00:20<01:41,  3.33it/s] 13%|█▎        | 52/390 [00:21<01:40,  3.36it/s] 14%|█▎        | 53/390 [00:21<01:39,  3.37it/s] 14%|█▍        | 54/390 [00:21<01:39,  3.39it/s] 14%|█▍        | 55/390 [00:22<01:53,  2.96it/s] 14%|█▍        | 56/390 [00:22<01:48,  3.08it/s] 15%|█▍        | 57/390 [00:22<01:44,  3.18it/s] 15%|█▍        | 58/390 [00:22<01:42,  3.25it/s] 15%|█▌        | 59/390 [00:23<01:40,  3.29it/s] 15%|█▌        | 60/390 [00:23<01:39,  3.33it/s] 16%|█▌        | 61/390 [00:23<01:38,  3.36it/s] 16%|█▌        | 62/390 [00:24<01:37,  3.37it/s] 16%|█▌        | 63/390 [00:24<01:36,  3.39it/s] 16%|█▋        | 64/390 [00:24<01:36,  3.39it/s] 17%|█▋        | 65/390 [00:25<01:49,  2.96it/s] 17%|█▋        | 66/390 [00:25<01:45,  3.08it/s] 17%|█▋        | 67/390 [00:25<01:41,  3.18it/s] 17%|█▋        | 68/390 [00:26<01:39,  3.24it/s] 18%|█▊        | 69/390 [00:26<01:37,  3.29it/s] 18%|█▊        | 70/390 [00:26<01:36,  3.33it/s] 18%|█▊        | 71/390 [00:26<01:35,  3.36it/s] 18%|█▊        | 72/390 [00:27<01:34,  3.38it/s] 19%|█▊        | 73/390 [00:27<01:33,  3.39it/s] 19%|█▉        | 74/390 [00:27<01:33,  3.39it/s] 19%|█▉        | 75/390 [00:28<01:53,  2.78it/s] 19%|█▉        | 76/390 [00:28<01:46,  2.95it/s] 20%|█▉        | 77/390 [00:28<01:42,  3.07it/s] 20%|██        | 78/390 [00:29<01:38,  3.17it/s][INFO|trainer.py:2140] 2023-08-28 18:23:07,661 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:23:07,661 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 18:23:07,661 >>   Batch size = 8

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.26it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.34it/s][A
  2%|▏         | 17/1083 [00:00<00:22, 47.58it/s][A
  2%|▏         | 22/1083 [00:00<00:22, 46.53it/s][A
  2%|▏         | 27/1083 [00:00<00:22, 46.00it/s][A
  3%|▎         | 32/1083 [00:00<00:23, 45.59it/s][A
  3%|▎         | 37/1083 [00:00<00:23, 45.45it/s][A
  4%|▍         | 42/1083 [00:00<00:23, 45.24it/s][A
  4%|▍         | 47/1083 [00:01<00:22, 45.19it/s][A
  5%|▍         | 52/1083 [00:01<00:22, 45.26it/s][A
  5%|▌         | 57/1083 [00:01<00:22, 45.36it/s][A
  6%|▌         | 62/1083 [00:01<00:22, 45.44it/s][A
  6%|▌         | 67/1083 [00:01<00:22, 45.46it/s][A
  7%|▋         | 72/1083 [00:01<00:22, 45.23it/s][A
  7%|▋         | 77/1083 [00:01<00:27, 36.94it/s][A
  8%|▊         | 82/1083 [00:01<00:25, 39.22it/s][A
  8%|▊         | 87/1083 [00:01<00:24, 41.05it/s][A
  8%|▊         | 92/1083 [00:02<00:23, 42.34it/s][A
  9%|▉         | 97/1083 [00:02<00:22, 43.32it/s][A
  9%|▉         | 102/1083 [00:02<00:22, 44.03it/s][A
 10%|▉         | 107/1083 [00:02<00:21, 44.56it/s][A
 10%|█         | 112/1083 [00:02<00:21, 44.79it/s][A
 11%|█         | 117/1083 [00:02<00:21, 44.59it/s][A
 11%|█▏        | 122/1083 [00:02<00:21, 44.48it/s][A
 12%|█▏        | 127/1083 [00:02<00:21, 44.54it/s][A
 12%|█▏        | 132/1083 [00:02<00:21, 44.77it/s][A
 13%|█▎        | 137/1083 [00:03<00:21, 45.03it/s][A
 13%|█▎        | 142/1083 [00:03<00:20, 45.27it/s][A
 14%|█▎        | 147/1083 [00:03<00:20, 45.25it/s][A
 14%|█▍        | 152/1083 [00:03<00:20, 45.47it/s][A
 14%|█▍        | 157/1083 [00:03<00:20, 45.35it/s][A
 15%|█▍        | 162/1083 [00:03<00:20, 45.20it/s][A
 15%|█▌        | 167/1083 [00:03<00:20, 44.92it/s][A
 16%|█▌        | 172/1083 [00:03<00:20, 44.85it/s][A
 16%|█▋        | 177/1083 [00:03<00:20, 44.93it/s][A
 17%|█▋        | 182/1083 [00:04<00:19, 45.09it/s][A
 17%|█▋        | 187/1083 [00:04<00:19, 45.34it/s][A
 18%|█▊        | 192/1083 [00:04<00:19, 45.40it/s][A
 18%|█▊        | 197/1083 [00:04<00:19, 45.52it/s][A
 19%|█▊        | 202/1083 [00:04<00:19, 45.44it/s][A
 19%|█▉        | 207/1083 [00:04<00:19, 45.23it/s][A
 20%|█▉        | 212/1083 [00:04<00:22, 38.03it/s][A
 20%|█▉        | 216/1083 [00:05<01:06, 13.00it/s][A
 20%|██        | 221/1083 [00:05<00:51, 16.86it/s][A
 21%|██        | 226/1083 [00:05<00:40, 20.96it/s][A
 21%|██        | 230/1083 [00:06<00:38, 21.99it/s][A
 22%|██▏       | 234/1083 [00:06<00:38, 22.12it/s][A
 22%|██▏       | 239/1083 [00:06<00:31, 26.61it/s][A
 23%|██▎       | 244/1083 [00:06<00:27, 30.71it/s][A
 23%|██▎       | 249/1083 [00:06<00:24, 34.23it/s][A
 23%|██▎       | 254/1083 [00:06<00:22, 37.13it/s][A
 24%|██▍       | 259/1083 [00:06<00:20, 39.43it/s][A
 24%|██▍       | 264/1083 [00:06<00:19, 41.11it/s][A
 25%|██▍       | 269/1083 [00:07<00:19, 42.53it/s][A
 25%|██▌       | 274/1083 [00:07<00:18, 43.12it/s][A
 26%|██▌       | 279/1083 [00:07<00:18, 43.29it/s][A
 26%|██▌       | 284/1083 [00:07<00:18, 43.66it/s][A
 27%|██▋       | 289/1083 [00:07<00:18, 44.07it/s][A
 27%|██▋       | 294/1083 [00:07<00:24, 31.64it/s][A
 28%|██▊       | 299/1083 [00:07<00:22, 34.86it/s][A
 28%|██▊       | 304/1083 [00:07<00:20, 37.57it/s][A
 29%|██▊       | 309/1083 [00:08<00:19, 39.63it/s][A
 29%|██▉       | 314/1083 [00:08<00:18, 41.23it/s][A
 29%|██▉       | 319/1083 [00:08<00:18, 42.40it/s][A
 30%|██▉       | 324/1083 [00:08<00:17, 43.29it/s][A
 30%|███       | 329/1083 [00:08<00:17, 43.94it/s][A
 31%|███       | 334/1083 [00:08<00:17, 43.89it/s][A
 31%|███▏      | 339/1083 [00:08<00:16, 44.22it/s][A
 32%|███▏      | 344/1083 [00:08<00:16, 44.66it/s][A
 32%|███▏      | 349/1083 [00:08<00:16, 44.90it/s][A
 33%|███▎      | 354/1083 [00:09<00:16, 45.13it/s][A
 33%|███▎      | 359/1083 [00:09<00:16, 45.18it/s][A
 34%|███▎      | 364/1083 [00:09<00:15, 45.30it/s][A
 34%|███▍      | 369/1083 [00:09<00:15, 45.32it/s][A
 35%|███▍      | 374/1083 [00:09<00:15, 45.25it/s][A
 35%|███▍      | 379/1083 [00:09<00:15, 44.99it/s][A
 35%|███▌      | 384/1083 [00:09<00:15, 44.88it/s][A
 36%|███▌      | 389/1083 [00:09<00:15, 44.90it/s][A
 36%|███▋      | 394/1083 [00:09<00:15, 45.01it/s][A
 37%|███▋      | 399/1083 [00:10<00:15, 45.16it/s][A
 37%|███▋      | 404/1083 [00:10<00:14, 45.36it/s][A
 38%|███▊      | 409/1083 [00:10<00:14, 45.48it/s][A
 38%|███▊      | 414/1083 [00:10<00:14, 45.32it/s][A
 39%|███▊      | 419/1083 [00:10<00:14, 45.30it/s][A
 39%|███▉      | 424/1083 [00:10<00:16, 41.05it/s][A
 40%|███▉      | 429/1083 [00:10<00:15, 42.41it/s][A
 40%|████      | 434/1083 [00:10<00:15, 43.26it/s][A
 41%|████      | 439/1083 [00:10<00:14, 43.89it/s][A
 41%|████      | 444/1083 [00:11<00:14, 44.27it/s][A
 41%|████▏     | 449/1083 [00:11<00:14, 44.57it/s][A
 42%|████▏     | 454/1083 [00:11<00:14, 44.85it/s][A
 42%|████▏     | 459/1083 [00:11<00:13, 44.85it/s][A
 43%|████▎     | 464/1083 [00:11<00:13, 44.60it/s][A
 43%|████▎     | 469/1083 [00:11<00:13, 44.62it/s][A
 44%|████▍     | 474/1083 [00:11<00:13, 44.69it/s][A
 44%|████▍     | 479/1083 [00:11<00:13, 44.90it/s][A
 45%|████▍     | 484/1083 [00:11<00:13, 45.03it/s][A
 45%|████▌     | 489/1083 [00:12<00:13, 45.16it/s][A
 46%|████▌     | 494/1083 [00:12<00:12, 45.33it/s][A
 46%|████▌     | 499/1083 [00:12<00:12, 45.37it/s][A
 47%|████▋     | 504/1083 [00:12<00:12, 45.27it/s][A
 47%|████▋     | 509/1083 [00:12<00:12, 45.05it/s][A
 47%|████▋     | 514/1083 [00:12<00:12, 44.95it/s][A
 48%|████▊     | 519/1083 [00:12<00:12, 44.91it/s][A
 48%|████▊     | 524/1083 [00:12<00:12, 45.05it/s][A
 49%|████▉     | 529/1083 [00:12<00:12, 45.11it/s][A
 49%|████▉     | 534/1083 [00:13<00:12, 45.37it/s][A
 50%|████▉     | 539/1083 [00:13<00:11, 45.38it/s][A
 50%|█████     | 544/1083 [00:13<00:11, 45.44it/s][A
 51%|█████     | 549/1083 [00:13<00:11, 45.31it/s][A
 51%|█████     | 554/1083 [00:13<00:11, 45.12it/s][A
 52%|█████▏    | 559/1083 [00:13<00:17, 30.81it/s][A
 52%|█████▏    | 564/1083 [00:13<00:15, 34.19it/s][A
 53%|█████▎    | 569/1083 [00:14<00:13, 36.98it/s][A
 53%|█████▎    | 574/1083 [00:14<00:12, 39.26it/s][A
 53%|█████▎    | 579/1083 [00:14<00:12, 41.01it/s][A
 54%|█████▍    | 584/1083 [00:14<00:11, 42.37it/s][A
 54%|█████▍    | 589/1083 [00:14<00:11, 43.39it/s][A
 55%|█████▍    | 594/1083 [00:14<00:11, 43.93it/s][A
 55%|█████▌    | 599/1083 [00:14<00:10, 44.01it/s][A
 56%|█████▌    | 604/1083 [00:14<00:10, 44.02it/s][A
 56%|█████▌    | 609/1083 [00:14<00:10, 44.07it/s][A
 57%|█████▋    | 614/1083 [00:15<00:10, 44.45it/s][A
 57%|█████▋    | 619/1083 [00:15<00:10, 44.65it/s][A
 58%|█████▊    | 624/1083 [00:15<00:10, 44.98it/s][A
 58%|█████▊    | 629/1083 [00:15<00:10, 45.18it/s][A
 59%|█████▊    | 634/1083 [00:15<00:09, 45.31it/s][A
 59%|█████▉    | 639/1083 [00:15<00:09, 45.35it/s][A
 59%|█████▉    | 644/1083 [00:15<00:09, 45.23it/s][A
 60%|█████▉    | 649/1083 [00:15<00:09, 45.02it/s][A
 60%|██████    | 654/1083 [00:15<00:09, 44.82it/s][A
 61%|██████    | 659/1083 [00:16<00:09, 44.82it/s][A
 61%|██████▏   | 664/1083 [00:16<00:09, 45.00it/s][A
 62%|██████▏   | 669/1083 [00:16<00:09, 45.19it/s][A
 62%|██████▏   | 674/1083 [00:16<00:09, 45.30it/s][A
 63%|██████▎   | 679/1083 [00:16<00:08, 45.40it/s][A
 63%|██████▎   | 684/1083 [00:16<00:08, 45.35it/s][A
 64%|██████▎   | 689/1083 [00:17<00:17, 22.40it/s][A
 64%|██████▍   | 694/1083 [00:17<00:14, 26.47it/s][A
 65%|██████▍   | 699/1083 [00:17<00:12, 30.31it/s][A
 65%|██████▌   | 704/1083 [00:17<00:11, 33.74it/s][A
 65%|██████▌   | 709/1083 [00:17<00:10, 36.60it/s][A
 66%|██████▌   | 714/1083 [00:17<00:09, 38.94it/s][A
 66%|██████▋   | 719/1083 [00:17<00:08, 40.80it/s][A
 67%|██████▋   | 724/1083 [00:17<00:08, 42.03it/s][A
 67%|██████▋   | 729/1083 [00:17<00:08, 42.60it/s][A
 68%|██████▊   | 734/1083 [00:18<00:08, 43.01it/s][A
 68%|██████▊   | 739/1083 [00:18<00:07, 43.50it/s][A
 69%|██████▊   | 744/1083 [00:18<00:07, 43.99it/s][A
 69%|██████▉   | 749/1083 [00:18<00:07, 44.49it/s][A
 70%|██████▉   | 754/1083 [00:18<00:07, 44.77it/s][A
 70%|███████   | 759/1083 [00:18<00:07, 45.06it/s][A
 71%|███████   | 764/1083 [00:18<00:07, 45.32it/s][A
 71%|███████   | 769/1083 [00:18<00:06, 45.33it/s][A
 71%|███████▏  | 774/1083 [00:18<00:06, 45.07it/s][A
 72%|███████▏  | 779/1083 [00:19<00:06, 44.75it/s][A
 72%|███████▏  | 784/1083 [00:19<00:06, 44.74it/s][A
 73%|███████▎  | 789/1083 [00:19<00:06, 44.84it/s][A
 73%|███████▎  | 794/1083 [00:19<00:06, 45.05it/s][A
 74%|███████▍  | 799/1083 [00:19<00:06, 45.21it/s][A
 74%|███████▍  | 804/1083 [00:19<00:06, 45.31it/s][A
 75%|███████▍  | 809/1083 [00:19<00:08, 32.64it/s][A
 75%|███████▌  | 814/1083 [00:19<00:07, 35.77it/s][A
 76%|███████▌  | 819/1083 [00:20<00:06, 38.22it/s][A
 76%|███████▌  | 824/1083 [00:20<00:06, 40.17it/s][A
 77%|███████▋  | 829/1083 [00:20<00:06, 41.78it/s][A
 77%|███████▋  | 834/1083 [00:20<00:05, 42.85it/s][A
 77%|███████▋  | 839/1083 [00:20<00:05, 43.76it/s][A
 78%|███████▊  | 844/1083 [00:20<00:05, 44.16it/s][A
 78%|███████▊  | 849/1083 [00:20<00:05, 44.06it/s][A
 79%|███████▉  | 854/1083 [00:20<00:05, 44.02it/s][A
 79%|███████▉  | 859/1083 [00:20<00:05, 44.15it/s][A
 80%|███████▉  | 864/1083 [00:21<00:04, 44.45it/s][A
 80%|████████  | 869/1083 [00:21<00:04, 44.86it/s][A
 81%|████████  | 874/1083 [00:21<00:06, 31.15it/s][A
 81%|████████  | 879/1083 [00:21<00:05, 34.51it/s][A
 82%|████████▏ | 884/1083 [00:21<00:05, 37.26it/s][A
 82%|████████▏ | 889/1083 [00:21<00:04, 39.46it/s][A
 83%|████████▎ | 894/1083 [00:21<00:04, 41.10it/s][A
 83%|████████▎ | 899/1083 [00:21<00:04, 42.35it/s][A
 83%|████████▎ | 904/1083 [00:22<00:04, 43.35it/s][A
 84%|████████▍ | 909/1083 [00:22<00:03, 44.04it/s][A
 84%|████████▍ | 914/1083 [00:22<00:03, 44.08it/s][A
 85%|████████▍ | 919/1083 [00:22<00:03, 43.95it/s][A
 85%|████████▌ | 924/1083 [00:22<00:03, 44.09it/s][A
 86%|████████▌ | 929/1083 [00:23<00:03, 44.38it/s][A
 86%|████████▌ | 934/1083 [00:23<00:07, 19.28it/s][A
 87%|████████▋ | 939/1083 [00:23<00:06, 23.34it/s][A
 87%|████████▋ | 944/1083 [00:23<00:05, 27.37it/s][A
 88%|████████▊ | 949/1083 [00:23<00:04, 31.12it/s][A
 88%|████████▊ | 954/1083 [00:23<00:03, 34.37it/s][A
 89%|████████▊ | 959/1083 [00:23<00:03, 37.11it/s][A
 89%|████████▉ | 964/1083 [00:23<00:03, 39.35it/s][A
 89%|████████▉ | 969/1083 [00:24<00:02, 41.01it/s][A
 90%|████████▉ | 974/1083 [00:24<00:02, 41.84it/s][A
 90%|█████████ | 979/1083 [00:24<00:02, 42.44it/s][A
 91%|█████████ | 984/1083 [00:24<00:02, 43.16it/s][A
 91%|█████████▏| 989/1083 [00:24<00:02, 43.71it/s][A
 92%|█████████▏| 994/1083 [00:24<00:02, 44.26it/s][A
 92%|█████████▏| 999/1083 [00:24<00:01, 44.64it/s][A
 93%|█████████▎| 1004/1083 [00:24<00:01, 44.99it/s][A
 93%|█████████▎| 1009/1083 [00:24<00:01, 45.23it/s][A
 94%|█████████▎| 1014/1083 [00:25<00:01, 45.25it/s][A
 94%|█████████▍| 1019/1083 [00:25<00:01, 44.49it/s][A
 95%|█████████▍| 1024/1083 [00:25<00:01, 44.62it/s][A
 95%|█████████▌| 1029/1083 [00:25<00:01, 44.67it/s][A
 95%|█████████▌| 1034/1083 [00:25<00:01, 44.78it/s][A
 96%|█████████▌| 1039/1083 [00:25<00:00, 44.95it/s][A
 96%|█████████▋| 1044/1083 [00:25<00:00, 45.12it/s][A
 97%|█████████▋| 1049/1083 [00:25<00:01, 32.43it/s][A
 97%|█████████▋| 1054/1083 [00:26<00:00, 35.51it/s][A
 98%|█████████▊| 1059/1083 [00:26<00:00, 38.09it/s][A
 98%|█████████▊| 1064/1083 [00:26<00:00, 40.14it/s][A
 99%|█████████▊| 1069/1083 [00:26<00:00, 41.69it/s][A
 99%|█████████▉| 1074/1083 [00:26<00:00, 42.75it/s][A
100%|█████████▉| 1079/1083 [00:26<00:00, 43.63it/s][A
                                                   [A                                                
100%|██████████| 1083/1083 [00:26<00:00, 43.63it/s][A 20%|██        | 78/390 [00:55<01:38,  3.17it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:23:35,640 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 18:23:36,334 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:23:55,047 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:23:56,092 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:23:56,308 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [01:51<2:08:22, 24.77s/it] 21%|██        | 80/390 [01:51<1:30:53, 17.59s/it] 21%|██        | 81/390 [01:52<1:03:52, 12.40s/it] 21%|██        | 82/390 [01:52<45:00,  8.77s/it]   21%|██▏       | 83/390 [01:52<31:51,  6.23s/it] 22%|██▏       | 84/390 [01:53<22:40,  4.44s/it] 22%|██▏       | 85/390 [01:53<16:15,  3.20s/it] 22%|██▏       | 86/390 [01:53<11:47,  2.33s/it] 22%|██▏       | 87/390 [01:53<08:39,  1.72s/it] 23%|██▎       | 88/390 [01:54<06:29,  1.29s/it] 23%|██▎       | 89/390 [01:54<05:03,  1.01s/it] 23%|██▎       | 90/390 [01:54<03:58,  1.26it/s] 23%|██▎       | 91/390 [01:55<03:12,  1.56it/s] 24%|██▎       | 92/390 [01:55<02:40,  1.86it/s] 24%|██▍       | 93/390 [01:55<02:17,  2.16it/s] 24%|██▍       | 94/390 [01:56<02:01,  2.43it/s] 24%|██▍       | 95/390 [01:56<01:50,  2.66it/s] 25%|██▍       | 96/390 [01:56<01:43,  2.85it/s] 25%|██▍       | 97/390 [01:56<01:37,  3.00it/s] 25%|██▌       | 98/390 [01:57<01:33,  3.11it/s] 25%|██▌       | 99/390 [01:57<01:30,  3.20it/s] 26%|██▌       | 100/390 [01:58<01:49,  2.64it/s] 26%|██▌       | 101/390 [01:58<01:41,  2.84it/s] 26%|██▌       | 102/390 [01:58<01:36,  2.99it/s] 26%|██▋       | 103/390 [01:58<01:32,  3.11it/s] 27%|██▋       | 104/390 [01:59<01:29,  3.20it/s] 27%|██▋       | 105/390 [01:59<01:27,  3.26it/s] 27%|██▋       | 106/390 [01:59<01:25,  3.31it/s] 27%|██▋       | 107/390 [02:00<01:24,  3.35it/s] 28%|██▊       | 108/390 [02:00<01:23,  3.37it/s] 28%|██▊       | 109/390 [02:00<01:22,  3.39it/s] 28%|██▊       | 110/390 [02:00<01:26,  3.23it/s] 28%|██▊       | 111/390 [02:01<01:24,  3.29it/s] 29%|██▊       | 112/390 [02:01<01:23,  3.32it/s] 29%|██▉       | 113/390 [02:01<01:22,  3.36it/s] 29%|██▉       | 114/390 [02:02<01:21,  3.38it/s] 29%|██▉       | 115/390 [02:02<01:21,  3.39it/s] 30%|██▉       | 116/390 [02:02<01:20,  3.41it/s] 30%|███       | 117/390 [02:03<01:20,  3.41it/s] 30%|███       | 118/390 [02:03<01:19,  3.42it/s] 31%|███       | 119/390 [02:03<01:19,  3.42it/s] 31%|███       | 120/390 [02:03<01:18,  3.42it/s] 31%|███       | 121/390 [02:04<01:23,  3.22it/s] 31%|███▏      | 122/390 [02:04<01:21,  3.28it/s] 32%|███▏      | 123/390 [02:04<01:20,  3.32it/s] 32%|███▏      | 124/390 [02:05<01:19,  3.35it/s] 32%|███▏      | 125/390 [02:05<01:18,  3.38it/s] 32%|███▏      | 126/390 [02:05<01:17,  3.39it/s] 33%|███▎      | 127/390 [02:06<01:17,  3.40it/s] 33%|███▎      | 128/390 [02:06<01:16,  3.41it/s] 33%|███▎      | 129/390 [02:06<01:16,  3.41it/s] 33%|███▎      | 130/390 [02:06<01:16,  3.42it/s] 34%|███▎      | 131/390 [02:07<01:15,  3.42it/s] 34%|███▍      | 132/390 [02:07<01:21,  3.15it/s] 34%|███▍      | 133/390 [02:07<01:19,  3.23it/s] 34%|███▍      | 134/390 [02:08<01:17,  3.28it/s] 35%|███▍      | 135/390 [02:08<01:16,  3.33it/s] 35%|███▍      | 136/390 [02:08<01:15,  3.36it/s] 35%|███▌      | 137/390 [02:09<01:14,  3.38it/s] 35%|███▌      | 138/390 [02:09<01:40,  2.51it/s] 36%|███▌      | 139/390 [02:09<01:31,  2.73it/s] 36%|███▌      | 140/390 [02:10<01:26,  2.90it/s] 36%|███▌      | 141/390 [02:10<01:21,  3.04it/s] 36%|███▋      | 142/390 [02:10<01:18,  3.15it/s] 37%|███▋      | 143/390 [02:11<01:16,  3.23it/s] 37%|███▋      | 144/390 [02:11<01:14,  3.28it/s] 37%|███▋      | 145/390 [02:11<01:13,  3.32it/s] 37%|███▋      | 146/390 [02:11<01:12,  3.35it/s] 38%|███▊      | 147/390 [02:12<01:39,  2.44it/s] 38%|███▊      | 148/390 [02:12<01:30,  2.67it/s] 38%|███▊      | 149/390 [02:13<01:24,  2.86it/s] 38%|███▊      | 150/390 [02:13<01:19,  3.01it/s] 39%|███▊      | 151/390 [02:13<01:16,  3.12it/s] 39%|███▉      | 152/390 [02:14<01:14,  3.20it/s] 39%|███▉      | 153/390 [02:14<01:12,  3.26it/s] 39%|███▉      | 154/390 [02:14<01:11,  3.31it/s] 40%|███▉      | 155/390 [02:14<01:10,  3.34it/s] 40%|████      | 156/390 [02:15<01:31,  2.57it/s][INFO|trainer.py:2140] 2023-08-28 18:24:54,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:24:54,048 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 18:24:54,048 >>   Batch size = 8
{'eval_loss': 0.9555922150611877, 'eval_runtime': 26.7117, 'eval_samples_per_second': 324.127, 'eval_steps_per_second': 40.544, 'epoch': 0.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:19, 56.41it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.63it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.82it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 47.16it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.43it/s][A
  3%|▎         | 33/1083 [00:00<00:22, 45.72it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.43it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 45.07it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.23it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.40it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.49it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.61it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.64it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.63it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.43it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 45.28it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 45.14it/s][A
  9%|▊         | 93/1083 [00:02<00:21, 45.12it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 45.26it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.36it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.47it/s][A
 10%|█         | 113/1083 [00:02<00:41, 23.43it/s][A
 11%|█         | 118/1083 [00:02<00:35, 27.44it/s][A
 11%|█▏        | 123/1083 [00:03<00:30, 31.19it/s][A
 12%|█▏        | 128/1083 [00:03<00:27, 34.52it/s][A
 12%|█▏        | 133/1083 [00:03<00:25, 37.27it/s][A
 13%|█▎        | 138/1083 [00:03<00:23, 39.48it/s][A
 13%|█▎        | 143/1083 [00:03<00:22, 41.17it/s][A
 14%|█▎        | 148/1083 [00:03<00:22, 42.40it/s][A
 14%|█▍        | 153/1083 [00:03<00:21, 42.90it/s][A
 15%|█▍        | 158/1083 [00:03<00:21, 43.25it/s][A
 15%|█▌        | 163/1083 [00:03<00:21, 43.76it/s][A
 16%|█▌        | 168/1083 [00:04<00:20, 44.20it/s][A
 16%|█▌        | 173/1083 [00:04<00:20, 44.48it/s][A
 16%|█▋        | 178/1083 [00:04<00:20, 44.94it/s][A
 17%|█▋        | 183/1083 [00:04<00:19, 45.20it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 45.40it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 45.40it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.26it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.06it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 44.89it/s][A
 20%|█▉        | 213/1083 [00:05<00:19, 44.95it/s][A
 20%|██        | 218/1083 [00:05<00:19, 45.05it/s][A
 21%|██        | 223/1083 [00:05<00:19, 45.21it/s][A
 21%|██        | 228/1083 [00:05<00:18, 45.35it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 45.43it/s][A
 22%|██▏       | 238/1083 [00:05<00:25, 32.63it/s][A
 22%|██▏       | 243/1083 [00:05<00:23, 35.78it/s][A
 23%|██▎       | 248/1083 [00:05<00:21, 38.31it/s][A
 23%|██▎       | 253/1083 [00:06<00:20, 40.24it/s][A
 24%|██▍       | 258/1083 [00:06<00:19, 41.76it/s][A
 24%|██▍       | 263/1083 [00:06<00:19, 42.95it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 43.75it/s][A
 25%|██▌       | 273/1083 [00:06<00:18, 44.32it/s][A
 26%|██▌       | 278/1083 [00:06<00:18, 44.22it/s][A
 26%|██▌       | 283/1083 [00:06<00:18, 44.19it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 44.39it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.64it/s][A
 28%|██▊       | 298/1083 [00:07<00:17, 44.81it/s][A
 28%|██▊       | 303/1083 [00:07<00:17, 45.00it/s][A
 28%|██▊       | 308/1083 [00:07<00:17, 45.25it/s][A
 29%|██▉       | 313/1083 [00:07<00:16, 45.43it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.53it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.36it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.01it/s][A
 31%|███       | 333/1083 [00:07<00:16, 44.94it/s][A
 31%|███       | 338/1083 [00:07<00:16, 44.92it/s][A
 32%|███▏      | 343/1083 [00:08<00:16, 45.01it/s][A
 32%|███▏      | 348/1083 [00:08<00:16, 45.19it/s][A
 33%|███▎      | 353/1083 [00:08<00:16, 45.21it/s][A
 33%|███▎      | 358/1083 [00:08<00:15, 45.39it/s][A
 34%|███▎      | 363/1083 [00:08<00:15, 45.53it/s][A
 34%|███▍      | 368/1083 [00:08<00:22, 31.18it/s][A
 34%|███▍      | 373/1083 [00:08<00:20, 34.45it/s][A
 35%|███▍      | 378/1083 [00:08<00:18, 37.26it/s][A
 35%|███▌      | 383/1083 [00:09<00:17, 39.48it/s][A
 36%|███▌      | 388/1083 [00:09<00:16, 41.12it/s][A
 36%|███▋      | 393/1083 [00:09<00:16, 42.46it/s][A
 37%|███▋      | 398/1083 [00:09<00:15, 43.40it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.03it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.14it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 44.13it/s][A
 39%|███▊      | 418/1083 [00:09<00:15, 44.21it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.54it/s][A
 40%|███▉      | 428/1083 [00:10<00:14, 44.80it/s][A
 40%|███▉      | 433/1083 [00:10<00:14, 45.07it/s][A
 40%|████      | 438/1083 [00:10<00:14, 45.27it/s][A
 41%|████      | 443/1083 [00:10<00:14, 45.41it/s][A
 41%|████▏     | 448/1083 [00:10<00:13, 45.45it/s][A
 42%|████▏     | 453/1083 [00:10<00:13, 45.27it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.03it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.92it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 44.96it/s][A
 44%|████▎     | 473/1083 [00:11<00:13, 44.95it/s][A
 44%|████▍     | 478/1083 [00:11<00:13, 45.18it/s][A
 45%|████▍     | 483/1083 [00:11<00:13, 45.40it/s][A
 45%|████▌     | 488/1083 [00:11<00:13, 45.51it/s][A
 46%|████▌     | 493/1083 [00:11<00:12, 45.53it/s][A
 46%|████▌     | 498/1083 [00:11<00:18, 32.41it/s][A
 46%|████▋     | 503/1083 [00:11<00:16, 35.56it/s][A
 47%|████▋     | 508/1083 [00:11<00:15, 38.10it/s][A
 47%|████▋     | 513/1083 [00:12<00:14, 40.11it/s][A
 48%|████▊     | 518/1083 [00:12<00:13, 41.62it/s][A
 48%|████▊     | 523/1083 [00:12<00:13, 42.84it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 43.69it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 44.14it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.13it/s][A
 50%|█████     | 543/1083 [00:12<00:12, 44.13it/s][A
 51%|█████     | 548/1083 [00:12<00:12, 44.31it/s][A
 51%|█████     | 553/1083 [00:13<00:21, 24.61it/s][A
 52%|█████▏    | 558/1083 [00:13<00:18, 28.67it/s][A
 52%|█████▏    | 563/1083 [00:13<00:16, 32.29it/s][A
 52%|█████▏    | 568/1083 [00:13<00:14, 35.45it/s][A
 53%|█████▎    | 573/1083 [00:13<00:13, 38.01it/s][A
 53%|█████▎    | 578/1083 [00:13<00:12, 39.90it/s][A
 54%|█████▍    | 583/1083 [00:13<00:12, 41.36it/s][A
 54%|█████▍    | 588/1083 [00:14<00:11, 42.52it/s][A
 55%|█████▍    | 593/1083 [00:14<00:11, 43.22it/s][A
 55%|█████▌    | 598/1083 [00:14<00:11, 43.43it/s][A
 56%|█████▌    | 603/1083 [00:14<00:11, 43.59it/s][A
 56%|█████▌    | 608/1083 [00:14<00:10, 43.93it/s][A
 57%|█████▋    | 613/1083 [00:15<00:10, 44.27it/s][A
 57%|█████▋    | 618/1083 [00:15<00:23, 19.60it/s][A
 58%|█████▊    | 623/1083 [00:15<00:19, 23.65it/s][A
 58%|█████▊    | 628/1083 [00:15<00:16, 27.69it/s][A
 58%|█████▊    | 633/1083 [00:15<00:14, 31.43it/s][A
 59%|█████▉    | 638/1083 [00:15<00:12, 34.69it/s][A
 59%|█████▉    | 643/1083 [00:15<00:11, 37.39it/s][A
 60%|█████▉    | 648/1083 [00:15<00:10, 39.59it/s][A
 60%|██████    | 653/1083 [00:15<00:10, 41.18it/s][A
 61%|██████    | 658/1083 [00:16<00:10, 42.03it/s][A
 61%|██████    | 663/1083 [00:16<00:09, 42.54it/s][A
 62%|██████▏   | 668/1083 [00:16<00:09, 43.17it/s][A
 62%|██████▏   | 673/1083 [00:16<00:09, 43.78it/s][A
 63%|██████▎   | 678/1083 [00:16<00:09, 44.29it/s][A
 63%|██████▎   | 683/1083 [00:16<00:08, 44.63it/s][A
 64%|██████▎   | 688/1083 [00:16<00:08, 44.94it/s][A
 64%|██████▍   | 693/1083 [00:16<00:08, 45.09it/s][A
 64%|██████▍   | 698/1083 [00:16<00:08, 45.28it/s][A
 65%|██████▍   | 703/1083 [00:17<00:08, 45.12it/s][A
 65%|██████▌   | 708/1083 [00:17<00:08, 45.08it/s][A
 66%|██████▌   | 713/1083 [00:17<00:08, 44.99it/s][A
 66%|██████▋   | 718/1083 [00:17<00:08, 44.89it/s][A
 67%|██████▋   | 723/1083 [00:17<00:07, 45.01it/s][A
 67%|██████▋   | 728/1083 [00:17<00:07, 45.17it/s][A
 68%|██████▊   | 733/1083 [00:17<00:11, 30.16it/s][A
 68%|██████▊   | 738/1083 [00:18<00:10, 33.60it/s][A
 69%|██████▊   | 743/1083 [00:18<00:09, 36.53it/s][A
 69%|██████▉   | 748/1083 [00:18<00:08, 38.91it/s][A
 70%|██████▉   | 753/1083 [00:18<00:08, 40.75it/s][A
 70%|██████▉   | 758/1083 [00:18<00:07, 42.17it/s][A
 70%|███████   | 763/1083 [00:18<00:07, 43.10it/s][A
 71%|███████   | 768/1083 [00:18<00:07, 43.77it/s][A
 71%|███████▏  | 773/1083 [00:18<00:09, 32.36it/s][A
 72%|███████▏  | 777/1083 [00:19<00:11, 27.23it/s][A
 72%|███████▏  | 781/1083 [00:20<00:27, 11.01it/s][A
 72%|███████▏  | 784/1083 [00:20<00:25, 11.55it/s][A
 73%|███████▎  | 789/1083 [00:20<00:18, 15.69it/s][A
 73%|███████▎  | 792/1083 [00:20<00:18, 15.89it/s][A
 73%|███████▎  | 795/1083 [00:20<00:16, 17.32it/s][A
 74%|███████▎  | 798/1083 [00:20<00:16, 17.60it/s][A
 74%|███████▍  | 801/1083 [00:21<00:14, 19.31it/s][A
 74%|███████▍  | 806/1083 [00:21<00:10, 25.27it/s][A
 75%|███████▍  | 811/1083 [00:21<00:09, 30.09it/s][A
 75%|███████▌  | 816/1083 [00:21<00:07, 34.09it/s][A
 76%|███████▌  | 821/1083 [00:21<00:07, 37.21it/s][A
 76%|███████▋  | 826/1083 [00:21<00:06, 39.58it/s][A
 77%|███████▋  | 831/1083 [00:21<00:06, 41.29it/s][A
 77%|███████▋  | 836/1083 [00:21<00:05, 42.31it/s][A
 78%|███████▊  | 841/1083 [00:21<00:05, 42.99it/s][A
 78%|███████▊  | 846/1083 [00:21<00:05, 43.34it/s][A
 79%|███████▊  | 851/1083 [00:22<00:05, 43.71it/s][A
 79%|███████▉  | 856/1083 [00:22<00:05, 44.16it/s][A
 80%|███████▉  | 861/1083 [00:22<00:04, 44.49it/s][A
 80%|███████▉  | 866/1083 [00:22<00:04, 44.91it/s][A
 80%|████████  | 871/1083 [00:22<00:04, 45.10it/s][A
 81%|████████  | 876/1083 [00:22<00:04, 45.24it/s][A
 81%|████████▏ | 881/1083 [00:22<00:04, 45.36it/s][A
 82%|████████▏ | 886/1083 [00:22<00:04, 45.30it/s][A
 82%|████████▏ | 891/1083 [00:22<00:04, 45.11it/s][A
 83%|████████▎ | 896/1083 [00:23<00:04, 45.01it/s][A
 83%|████████▎ | 901/1083 [00:23<00:04, 45.01it/s][A
 84%|████████▎ | 906/1083 [00:23<00:03, 45.15it/s][A
 84%|████████▍ | 911/1083 [00:23<00:03, 45.27it/s][A
 85%|████████▍ | 916/1083 [00:23<00:03, 45.38it/s][A
 85%|████████▌ | 921/1083 [00:23<00:03, 45.43it/s][A
 86%|████████▌ | 926/1083 [00:24<00:03, 45.35it/s][A
 86%|████████▌ | 931/1083 [00:24<00:06, 22.37it/s][A
 86%|████████▋ | 936/1083 [00:24<00:05, 26.43it/s][A
 87%|████████▋ | 941/1083 [00:24<00:04, 30.29it/s][A
 87%|████████▋ | 946/1083 [00:24<00:04, 33.68it/s][A
 88%|████████▊ | 951/1083 [00:24<00:03, 36.52it/s][A
 88%|████████▊ | 956/1083 [00:24<00:03, 38.81it/s][A
 89%|████████▊ | 961/1083 [00:24<00:02, 40.75it/s][A
 89%|████████▉ | 966/1083 [00:25<00:02, 41.96it/s][A
 90%|████████▉ | 971/1083 [00:25<00:02, 42.58it/s][A
 90%|█████████ | 976/1083 [00:25<00:02, 42.99it/s][A
 91%|█████████ | 981/1083 [00:25<00:02, 43.55it/s][A
 91%|█████████ | 986/1083 [00:25<00:02, 44.13it/s][A
 92%|█████████▏| 991/1083 [00:25<00:02, 44.54it/s][A
 92%|█████████▏| 996/1083 [00:25<00:01, 44.77it/s][A
 92%|█████████▏| 1001/1083 [00:25<00:01, 45.06it/s][A
 93%|█████████▎| 1006/1083 [00:25<00:01, 45.31it/s][A
 93%|█████████▎| 1011/1083 [00:26<00:01, 45.28it/s][A
 94%|█████████▍| 1016/1083 [00:26<00:01, 45.11it/s][A
 94%|█████████▍| 1021/1083 [00:26<00:01, 44.90it/s][A
 95%|█████████▍| 1026/1083 [00:26<00:01, 44.79it/s][A
 95%|█████████▌| 1031/1083 [00:26<00:01, 44.87it/s][A
 96%|█████████▌| 1036/1083 [00:26<00:01, 45.06it/s][A
 96%|█████████▌| 1041/1083 [00:26<00:00, 45.20it/s][A
 97%|█████████▋| 1046/1083 [00:26<00:00, 45.32it/s][A
 97%|█████████▋| 1051/1083 [00:27<00:00, 45.46it/s][A
 98%|█████████▊| 1056/1083 [00:27<00:00, 33.72it/s][A
 98%|█████████▊| 1061/1083 [00:27<00:00, 36.63it/s][A
 98%|█████████▊| 1066/1083 [00:27<00:00, 38.93it/s][A
 99%|█████████▉| 1071/1083 [00:27<00:00, 40.80it/s][A
 99%|█████████▉| 1076/1083 [00:27<00:00, 42.21it/s][A
100%|█████████▉| 1081/1083 [00:27<00:00, 43.19it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:27<00:00, 43.19it/s][A 40%|████      | 156/390 [02:43<01:31,  2.57it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:25:22,665 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 18:25:23,613 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:25:32,592 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:25:33,006 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:25:33,148 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [03:29<1:27:30, 22.53s/it] 41%|████      | 158/390 [03:30<1:01:36, 15.93s/it] 41%|████      | 159/390 [03:30<43:16, 11.24s/it]   41%|████      | 160/390 [03:30<30:29,  7.96s/it] 41%|████▏     | 161/390 [03:31<21:35,  5.66s/it] 42%|████▏     | 162/390 [03:31<15:22,  4.05s/it] 42%|████▏     | 163/390 [03:31<11:02,  2.92s/it] 42%|████▏     | 164/390 [03:32<08:01,  2.13s/it] 42%|████▏     | 165/390 [03:32<05:55,  1.58s/it] 43%|████▎     | 166/390 [03:32<04:27,  1.19s/it] 43%|████▎     | 167/390 [03:32<03:25,  1.08it/s] 43%|████▎     | 168/390 [03:33<02:57,  1.25it/s] 43%|████▎     | 169/390 [03:33<02:23,  1.54it/s] 44%|████▎     | 170/390 [03:34<01:59,  1.85it/s] 44%|████▍     | 171/390 [03:34<01:42,  2.14it/s] 44%|████▍     | 172/390 [03:35<01:57,  1.86it/s] 44%|████▍     | 173/390 [03:35<01:43,  2.09it/s] 45%|████▍     | 174/390 [03:35<01:31,  2.37it/s] 45%|████▍     | 175/390 [03:36<01:37,  2.21it/s] 45%|████▌     | 176/390 [03:37<02:03,  1.74it/s] 45%|████▌     | 177/390 [03:37<01:44,  2.04it/s] 46%|████▌     | 178/390 [03:37<01:31,  2.32it/s] 46%|████▌     | 179/390 [03:37<01:22,  2.57it/s] 46%|████▌     | 180/390 [03:38<01:15,  2.78it/s] 46%|████▋     | 181/390 [03:38<01:11,  2.94it/s] 47%|████▋     | 182/390 [03:38<01:07,  3.07it/s] 47%|████▋     | 183/390 [03:39<01:05,  3.17it/s] 47%|████▋     | 184/390 [03:39<01:03,  3.24it/s] 47%|████▋     | 185/390 [03:39<01:18,  2.62it/s] 48%|████▊     | 186/390 [03:40<01:12,  2.82it/s] 48%|████▊     | 187/390 [03:40<01:08,  2.97it/s] 48%|████▊     | 188/390 [03:40<01:05,  3.10it/s] 48%|████▊     | 189/390 [03:41<01:03,  3.18it/s] 49%|████▊     | 190/390 [03:41<01:01,  3.25it/s] 49%|████▉     | 191/390 [03:41<01:00,  3.30it/s] 49%|████▉     | 192/390 [03:42<00:59,  3.34it/s] 49%|████▉     | 193/390 [03:42<00:58,  3.36it/s] 50%|████▉     | 194/390 [03:42<00:57,  3.38it/s] 50%|█████     | 195/390 [03:43<01:16,  2.56it/s] 50%|█████     | 196/390 [03:43<01:10,  2.77it/s] 51%|█████     | 197/390 [03:43<01:05,  2.94it/s] 51%|█████     | 198/390 [03:44<01:02,  3.07it/s] 51%|█████     | 199/390 [03:44<01:00,  3.17it/s] 51%|█████▏    | 200/390 [03:44<00:58,  3.24it/s] 52%|█████▏    | 201/390 [03:44<00:57,  3.29it/s] 52%|█████▏    | 202/390 [03:45<00:56,  3.33it/s] 52%|█████▏    | 203/390 [03:45<00:55,  3.36it/s] 52%|█████▏    | 204/390 [03:45<00:55,  3.37it/s] 53%|█████▎    | 205/390 [03:46<00:58,  3.19it/s] 53%|█████▎    | 206/390 [03:46<01:02,  2.95it/s] 53%|█████▎    | 207/390 [03:46<00:59,  3.08it/s] 53%|█████▎    | 208/390 [03:47<00:57,  3.17it/s] 54%|█████▎    | 209/390 [03:47<00:55,  3.24it/s] 54%|█████▍    | 210/390 [03:47<00:54,  3.29it/s] 54%|█████▍    | 211/390 [03:48<00:53,  3.33it/s] 54%|█████▍    | 212/390 [03:48<00:52,  3.36it/s] 55%|█████▍    | 213/390 [03:48<00:52,  3.38it/s] 55%|█████▍    | 214/390 [03:48<00:51,  3.39it/s] 55%|█████▌    | 215/390 [03:49<00:51,  3.40it/s] 55%|█████▌    | 216/390 [03:49<01:14,  2.32it/s] 56%|█████▌    | 217/390 [03:50<01:07,  2.57it/s] 56%|█████▌    | 218/390 [03:50<01:01,  2.78it/s] 56%|█████▌    | 219/390 [03:50<00:58,  2.95it/s] 56%|█████▋    | 220/390 [03:51<00:55,  3.07it/s] 57%|█████▋    | 221/390 [03:51<00:55,  3.02it/s] 57%|█████▋    | 222/390 [03:51<00:53,  3.13it/s] 57%|█████▋    | 223/390 [03:52<00:51,  3.21it/s] 57%|█████▋    | 224/390 [03:52<00:50,  3.27it/s] 58%|█████▊    | 225/390 [03:52<00:51,  3.18it/s] 58%|█████▊    | 226/390 [03:52<00:50,  3.25it/s] 58%|█████▊    | 227/390 [03:53<00:49,  3.30it/s] 58%|█████▊    | 228/390 [03:53<00:48,  3.33it/s] 59%|█████▊    | 229/390 [03:53<00:47,  3.36it/s] 59%|█████▉    | 230/390 [03:54<00:47,  3.38it/s] 59%|█████▉    | 231/390 [03:54<00:46,  3.39it/s] 59%|█████▉    | 232/390 [03:54<00:46,  3.40it/s] 60%|█████▉    | 233/390 [03:55<00:46,  3.41it/s] 60%|██████    | 234/390 [03:55<00:45,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 18:26:33,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:26:33,798 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 18:26:33,798 >>   Batch size = 8
{'eval_loss': 0.968624472618103, 'eval_runtime': 27.7367, 'eval_samples_per_second': 312.15, 'eval_steps_per_second': 39.046, 'epoch': 1.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 57.56it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.61it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.73it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.85it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.21it/s][A
  3%|▎         | 33/1083 [00:00<00:22, 45.93it/s][A
  4%|▎         | 38/1083 [00:00<00:22, 45.72it/s][A
  4%|▍         | 43/1083 [00:00<00:22, 45.28it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.09it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.11it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.27it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.54it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.57it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.53it/s][A
  7%|▋         | 78/1083 [00:01<00:22, 45.40it/s][A
  8%|▊         | 83/1083 [00:01<00:22, 45.38it/s][A
  8%|▊         | 88/1083 [00:01<00:22, 45.15it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.88it/s][A
  9%|▉         | 98/1083 [00:02<00:21, 45.00it/s][A
 10%|▉         | 103/1083 [00:02<00:21, 45.08it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 45.08it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.21it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.41it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.53it/s][A
 12%|█▏        | 128/1083 [00:02<00:20, 45.51it/s][A
 12%|█▏        | 133/1083 [00:02<00:20, 45.29it/s][A
 13%|█▎        | 138/1083 [00:03<00:20, 45.19it/s][A
 13%|█▎        | 143/1083 [00:03<00:22, 41.20it/s][A
 14%|█▎        | 148/1083 [00:03<00:21, 42.53it/s][A
 14%|█▍        | 153/1083 [00:03<00:21, 43.44it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 44.15it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 44.59it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.89it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.99it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.96it/s][A
 17%|█▋        | 183/1083 [00:04<00:20, 44.70it/s][A
 17%|█▋        | 188/1083 [00:04<00:20, 44.64it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 44.72it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 44.99it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.16it/s][A
 19%|█▉        | 208/1083 [00:04<00:19, 45.37it/s][A
 20%|█▉        | 213/1083 [00:04<00:19, 45.47it/s][A
 20%|██        | 218/1083 [00:04<00:19, 45.48it/s][A
 21%|██        | 223/1083 [00:04<00:19, 45.24it/s][A
 21%|██        | 228/1083 [00:05<00:18, 45.05it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.85it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.80it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 44.91it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 45.10it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 45.26it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 45.27it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.42it/s][A
 25%|██▍       | 268/1083 [00:05<00:17, 45.43it/s][A
 25%|██▌       | 273/1083 [00:06<00:17, 45.35it/s][A
 26%|██▌       | 278/1083 [00:06<00:19, 41.29it/s][A
 26%|██▌       | 283/1083 [00:06<00:18, 42.54it/s][A
 27%|██▋       | 288/1083 [00:06<00:18, 43.51it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 44.13it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 44.56it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 44.89it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.04it/s][A
 29%|██▉       | 313/1083 [00:06<00:17, 44.93it/s][A
 29%|██▉       | 318/1083 [00:07<00:17, 44.72it/s][A
 30%|██▉       | 323/1083 [00:07<00:17, 44.63it/s][A
 30%|███       | 328/1083 [00:07<00:16, 44.83it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.05it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.25it/s][A
 32%|███▏      | 343/1083 [00:07<00:16, 45.27it/s][A
 32%|███▏      | 348/1083 [00:07<00:16, 45.47it/s][A
 33%|███▎      | 353/1083 [00:07<00:16, 45.42it/s][A
 33%|███▎      | 358/1083 [00:07<00:16, 45.29it/s][A
 34%|███▎      | 363/1083 [00:08<00:16, 44.99it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 44.88it/s][A
 34%|███▍      | 373/1083 [00:08<00:15, 44.81it/s][A
 35%|███▍      | 378/1083 [00:08<00:15, 45.13it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 45.15it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 45.35it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 45.42it/s][A
 37%|███▋      | 398/1083 [00:08<00:15, 45.38it/s][A
 37%|███▋      | 403/1083 [00:08<00:15, 45.16it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.91it/s][A
 38%|███▊      | 413/1083 [00:09<00:15, 42.70it/s][A
 39%|███▊      | 418/1083 [00:09<00:15, 43.46it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 44.01it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 44.33it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.72it/s][A
 40%|████      | 438/1083 [00:09<00:14, 45.05it/s][A
 41%|████      | 443/1083 [00:09<00:14, 45.15it/s][A
 41%|████▏     | 448/1083 [00:09<00:14, 45.06it/s][A
 42%|████▏     | 453/1083 [00:10<00:14, 44.81it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 44.80it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 44.89it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.03it/s][A
 44%|████▎     | 473/1083 [00:10<00:13, 45.18it/s][A
 44%|████▍     | 478/1083 [00:10<00:13, 45.37it/s][A
 45%|████▍     | 483/1083 [00:10<00:13, 45.47it/s][A
 45%|████▌     | 488/1083 [00:10<00:13, 45.45it/s][A
 46%|████▌     | 493/1083 [00:10<00:13, 45.17it/s][A
 46%|████▌     | 498/1083 [00:11<00:12, 45.04it/s][A
 46%|████▋     | 503/1083 [00:11<00:12, 44.95it/s][A
 47%|████▋     | 508/1083 [00:11<00:12, 44.93it/s][A
 47%|████▋     | 513/1083 [00:11<00:12, 45.13it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 45.17it/s][A
 48%|████▊     | 523/1083 [00:11<00:12, 45.31it/s][A
 49%|████▉     | 528/1083 [00:11<00:12, 45.50it/s][A
 49%|████▉     | 533/1083 [00:11<00:12, 45.52it/s][A
 50%|████▉     | 538/1083 [00:11<00:12, 45.39it/s][A
 50%|█████     | 543/1083 [00:12<00:11, 45.16it/s][A
 51%|█████     | 548/1083 [00:12<00:26, 20.50it/s][A
 51%|█████     | 553/1083 [00:12<00:21, 24.58it/s][A
 52%|█████▏    | 558/1083 [00:12<00:18, 28.51it/s][A
 52%|█████▏    | 563/1083 [00:12<00:16, 32.18it/s][A
 52%|█████▏    | 568/1083 [00:13<00:14, 35.31it/s][A
 53%|█████▎    | 573/1083 [00:13<00:13, 37.90it/s][A
 53%|█████▎    | 578/1083 [00:13<00:12, 40.02it/s][A
 54%|█████▍    | 583/1083 [00:13<00:12, 41.41it/s][A
 54%|█████▍    | 588/1083 [00:13<00:11, 42.18it/s][A
 55%|█████▍    | 593/1083 [00:13<00:11, 42.72it/s][A
 55%|█████▌    | 598/1083 [00:13<00:11, 43.29it/s][A
 56%|█████▌    | 603/1083 [00:13<00:10, 43.98it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 44.45it/s][A
 57%|█████▋    | 613/1083 [00:14<00:10, 44.75it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 45.05it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 45.27it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 45.23it/s][A
 58%|█████▊    | 633/1083 [00:14<00:10, 44.98it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.73it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.70it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.89it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.02it/s][A
 61%|██████    | 658/1083 [00:15<00:09, 45.16it/s][A
 61%|██████    | 663/1083 [00:15<00:09, 45.44it/s][A
 62%|██████▏   | 668/1083 [00:16<00:27, 15.11it/s][A
 62%|██████▏   | 673/1083 [00:16<00:21, 18.93it/s][A
 63%|██████▎   | 678/1083 [00:16<00:17, 22.97it/s][A
 63%|██████▎   | 683/1083 [00:16<00:14, 26.98it/s][A
 64%|██████▎   | 688/1083 [00:16<00:12, 30.76it/s][A
 64%|██████▍   | 693/1083 [00:16<00:11, 34.14it/s][A
 64%|██████▍   | 698/1083 [00:16<00:10, 36.97it/s][A
 65%|██████▍   | 703/1083 [00:16<00:09, 39.16it/s][A
 65%|██████▌   | 708/1083 [00:16<00:09, 40.46it/s][A
 66%|██████▌   | 713/1083 [00:17<00:08, 41.33it/s][A
 66%|██████▋   | 718/1083 [00:17<00:08, 42.42it/s][A
 67%|██████▋   | 723/1083 [00:17<00:08, 43.27it/s][A
 67%|██████▋   | 728/1083 [00:17<00:08, 43.98it/s][A
 68%|██████▊   | 733/1083 [00:17<00:07, 44.56it/s][A
 68%|██████▊   | 738/1083 [00:17<00:07, 44.92it/s][A
 69%|██████▊   | 743/1083 [00:17<00:07, 45.09it/s][A
 69%|██████▉   | 748/1083 [00:17<00:07, 45.23it/s][A
 70%|██████▉   | 753/1083 [00:17<00:07, 44.90it/s][A
 70%|██████▉   | 758/1083 [00:17<00:07, 44.62it/s][A
 70%|███████   | 763/1083 [00:18<00:07, 44.68it/s][A
 71%|███████   | 768/1083 [00:18<00:07, 44.82it/s][A
 71%|███████▏  | 773/1083 [00:18<00:08, 35.61it/s][A
 72%|███████▏  | 778/1083 [00:18<00:07, 38.19it/s][A
 72%|███████▏  | 783/1083 [00:18<00:07, 40.20it/s][A
 73%|███████▎  | 788/1083 [00:18<00:07, 41.75it/s][A
 73%|███████▎  | 793/1083 [00:18<00:06, 42.86it/s][A
 74%|███████▎  | 798/1083 [00:18<00:06, 43.61it/s][A
 74%|███████▍  | 803/1083 [00:19<00:06, 44.28it/s][A
 75%|███████▍  | 808/1083 [00:19<00:06, 44.61it/s][A
 75%|███████▌  | 813/1083 [00:19<00:06, 44.41it/s][A
 76%|███████▌  | 818/1083 [00:19<00:05, 44.28it/s][A
 76%|███████▌  | 823/1083 [00:19<00:05, 44.33it/s][A
 76%|███████▋  | 828/1083 [00:19<00:05, 44.61it/s][A
 77%|███████▋  | 833/1083 [00:19<00:05, 45.00it/s][A
 77%|███████▋  | 838/1083 [00:19<00:05, 45.27it/s][A
 78%|███████▊  | 843/1083 [00:19<00:05, 45.39it/s][A
 78%|███████▊  | 848/1083 [00:20<00:05, 45.56it/s][A
 79%|███████▉  | 853/1083 [00:20<00:05, 45.36it/s][A
 79%|███████▉  | 858/1083 [00:20<00:04, 45.14it/s][A
 80%|███████▉  | 863/1083 [00:20<00:04, 44.78it/s][A
 80%|████████  | 868/1083 [00:20<00:04, 44.72it/s][A
 81%|████████  | 873/1083 [00:20<00:04, 44.95it/s][A
 81%|████████  | 878/1083 [00:20<00:04, 45.13it/s][A
 82%|████████▏ | 883/1083 [00:20<00:04, 45.20it/s][A
 82%|████████▏ | 888/1083 [00:20<00:04, 45.43it/s][A
 82%|████████▏ | 893/1083 [00:21<00:04, 45.59it/s][A
 83%|████████▎ | 898/1083 [00:21<00:04, 45.49it/s][A
 83%|████████▎ | 903/1083 [00:21<00:03, 45.22it/s][A
 84%|████████▍ | 908/1083 [00:21<00:04, 38.48it/s][A
 84%|████████▍ | 913/1083 [00:21<00:05, 32.16it/s][A
 85%|████████▍ | 918/1083 [00:21<00:04, 35.31it/s][A
 85%|████████▌ | 923/1083 [00:21<00:04, 37.85it/s][A
 86%|████████▌ | 928/1083 [00:22<00:03, 39.84it/s][A
 86%|████████▌ | 933/1083 [00:22<00:03, 41.43it/s][A
 87%|████████▋ | 938/1083 [00:22<00:03, 42.58it/s][A
 87%|████████▋ | 943/1083 [00:22<00:03, 43.42it/s][A
 88%|████████▊ | 948/1083 [00:22<00:03, 43.94it/s][A
 88%|████████▊ | 953/1083 [00:22<00:02, 43.97it/s][A
 88%|████████▊ | 958/1083 [00:22<00:02, 44.25it/s][A
 89%|████████▉ | 963/1083 [00:22<00:02, 44.60it/s][A
 89%|████████▉ | 968/1083 [00:22<00:02, 44.80it/s][A
 90%|████████▉ | 973/1083 [00:23<00:02, 45.01it/s][A
 90%|█████████ | 978/1083 [00:23<00:02, 45.16it/s][A
 91%|█████████ | 983/1083 [00:23<00:02, 45.26it/s][A
 91%|█████████ | 988/1083 [00:23<00:02, 45.32it/s][A
 92%|█████████▏| 993/1083 [00:23<00:01, 45.17it/s][A
 92%|█████████▏| 998/1083 [00:23<00:01, 45.08it/s][A
 93%|█████████▎| 1003/1083 [00:23<00:01, 45.08it/s][A
 93%|█████████▎| 1008/1083 [00:23<00:01, 45.11it/s][A
 94%|█████████▎| 1013/1083 [00:23<00:01, 45.13it/s][A
 94%|█████████▍| 1018/1083 [00:24<00:01, 45.23it/s][A
 94%|█████████▍| 1023/1083 [00:24<00:01, 45.25it/s][A
 95%|█████████▍| 1028/1083 [00:24<00:01, 45.24it/s][A
 95%|█████████▌| 1033/1083 [00:24<00:01, 45.28it/s][A
 96%|█████████▌| 1038/1083 [00:24<00:00, 45.16it/s][A
 96%|█████████▋| 1043/1083 [00:24<00:01, 39.13it/s][A
 97%|█████████▋| 1048/1083 [00:24<00:00, 40.84it/s][A
 97%|█████████▋| 1053/1083 [00:24<00:00, 42.24it/s][A
 98%|█████████▊| 1058/1083 [00:24<00:00, 43.18it/s][A
 98%|█████████▊| 1063/1083 [00:25<00:00, 43.93it/s][A
 99%|█████████▊| 1068/1083 [00:25<00:00, 44.48it/s][A
 99%|█████████▉| 1073/1083 [00:25<00:00, 44.80it/s][A
100%|█████████▉| 1078/1083 [00:25<00:00, 44.94it/s][A
100%|██████████| 1083/1083 [00:25<00:00, 44.74it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:25<00:00, 44.74it/s][A 60%|██████    | 234/390 [04:20<00:45,  3.41it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:27:01,294 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:27:01,848 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:27:10,047 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:27:11,209 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:27:11,396 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [04:50<43:19, 16.77s/it] 61%|██████    | 236/390 [04:50<30:22, 11.84s/it] 61%|██████    | 237/390 [04:51<21:21,  8.37s/it] 61%|██████    | 238/390 [04:51<15:17,  6.03s/it] 61%|██████▏   | 239/390 [04:52<10:50,  4.31s/it] 62%|██████▏   | 240/390 [04:52<07:45,  3.11s/it] 62%|██████▏   | 241/390 [04:52<05:36,  2.26s/it] 62%|██████▏   | 242/390 [04:52<04:07,  1.67s/it] 62%|██████▏   | 243/390 [04:53<03:04,  1.26s/it] 63%|██████▎   | 244/390 [04:53<02:21,  1.03it/s] 63%|██████▎   | 245/390 [04:53<01:50,  1.31it/s] 63%|██████▎   | 246/390 [04:54<01:37,  1.48it/s] 63%|██████▎   | 247/390 [04:54<01:20,  1.79it/s] 64%|██████▎   | 248/390 [04:54<01:08,  2.09it/s] 64%|██████▍   | 249/390 [04:55<00:59,  2.36it/s] 64%|██████▍   | 250/390 [04:55<00:53,  2.61it/s] 64%|██████▍   | 251/390 [04:55<00:49,  2.81it/s] 65%|██████▍   | 252/390 [04:55<00:46,  2.97it/s] 65%|██████▍   | 253/390 [04:56<00:44,  3.09it/s] 65%|██████▌   | 254/390 [04:56<00:42,  3.18it/s] 65%|██████▌   | 255/390 [04:56<00:41,  3.25it/s] 66%|██████▌   | 256/390 [04:57<00:42,  3.19it/s] 66%|██████▌   | 257/390 [04:57<00:40,  3.26it/s] 66%|██████▌   | 258/390 [04:57<00:39,  3.31it/s] 66%|██████▋   | 259/390 [04:58<00:39,  3.34it/s] 67%|██████▋   | 260/390 [04:58<00:38,  3.37it/s] 67%|██████▋   | 261/390 [04:58<00:38,  3.38it/s] 67%|██████▋   | 262/390 [04:58<00:37,  3.40it/s] 67%|██████▋   | 263/390 [04:59<00:37,  3.40it/s] 68%|██████▊   | 264/390 [04:59<00:37,  3.41it/s] 68%|██████▊   | 265/390 [04:59<00:36,  3.41it/s] 68%|██████▊   | 266/390 [05:00<00:36,  3.41it/s] 68%|██████▊   | 267/390 [05:00<00:38,  3.21it/s] 69%|██████▊   | 268/390 [05:00<00:37,  3.27it/s] 69%|██████▉   | 269/390 [05:01<00:36,  3.31it/s] 69%|██████▉   | 270/390 [05:01<00:35,  3.34it/s] 69%|██████▉   | 271/390 [05:01<00:35,  3.37it/s] 70%|██████▉   | 272/390 [05:01<00:34,  3.39it/s] 70%|███████   | 273/390 [05:02<00:34,  3.40it/s] 70%|███████   | 274/390 [05:02<00:34,  3.41it/s] 71%|███████   | 275/390 [05:02<00:33,  3.41it/s] 71%|███████   | 276/390 [05:03<00:33,  3.41it/s] 71%|███████   | 277/390 [05:03<00:33,  3.41it/s] 71%|███████▏  | 278/390 [05:04<01:07,  1.66it/s] 72%|███████▏  | 279/390 [05:05<00:56,  1.97it/s] 72%|███████▏  | 280/390 [05:05<00:48,  2.26it/s] 72%|███████▏  | 281/390 [05:05<00:43,  2.52it/s] 72%|███████▏  | 282/390 [05:05<00:39,  2.75it/s] 73%|███████▎  | 283/390 [05:06<00:36,  2.93it/s] 73%|███████▎  | 284/390 [05:06<00:34,  3.08it/s] 73%|███████▎  | 285/390 [05:07<00:59,  1.77it/s] 73%|███████▎  | 286/390 [05:07<00:50,  2.08it/s] 74%|███████▎  | 287/390 [05:08<00:43,  2.36it/s] 74%|███████▍  | 288/390 [05:08<00:39,  2.61it/s] 74%|███████▍  | 289/390 [05:08<00:35,  2.82it/s] 74%|███████▍  | 290/390 [05:09<00:33,  2.99it/s] 75%|███████▍  | 291/390 [05:09<00:31,  3.12it/s] 75%|███████▍  | 292/390 [05:09<00:30,  3.22it/s] 75%|███████▌  | 293/390 [05:10<00:44,  2.19it/s] 75%|███████▌  | 294/390 [05:10<00:38,  2.46it/s] 76%|███████▌  | 295/390 [05:10<00:35,  2.70it/s] 76%|███████▌  | 296/390 [05:11<00:32,  2.89it/s] 76%|███████▌  | 297/390 [05:11<00:30,  3.05it/s] 76%|███████▋  | 298/390 [05:11<00:29,  3.16it/s] 77%|███████▋  | 299/390 [05:12<00:28,  3.25it/s] 77%|███████▋  | 300/390 [05:12<00:27,  3.31it/s] 77%|███████▋  | 301/390 [05:12<00:26,  3.35it/s] 77%|███████▋  | 302/390 [05:13<00:36,  2.44it/s] 78%|███████▊  | 303/390 [05:13<00:32,  2.68it/s] 78%|███████▊  | 304/390 [05:13<00:29,  2.88it/s] 78%|███████▊  | 305/390 [05:14<00:28,  3.03it/s] 78%|███████▊  | 306/390 [05:14<00:26,  3.15it/s] 79%|███████▊  | 307/390 [05:14<00:25,  3.24it/s] 79%|███████▉  | 308/390 [05:15<00:24,  3.31it/s] 79%|███████▉  | 309/390 [05:15<00:24,  3.35it/s] 79%|███████▉  | 310/390 [05:15<00:23,  3.39it/s] 80%|███████▉  | 311/390 [05:15<00:23,  3.41it/s] 80%|████████  | 312/390 [05:16<00:33,  2.35it/s][INFO|trainer.py:2140] 2023-08-28 18:27:55,122 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:27:55,122 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 18:27:55,122 >>   Batch size = 8
{'eval_loss': 0.9787517189979553, 'eval_runtime': 25.5481, 'eval_samples_per_second': 338.89, 'eval_steps_per_second': 42.391, 'epoch': 2.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.89it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.75it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.81it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 47.06it/s][A
  3%|▎         | 28/1083 [00:00<00:22, 46.35it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.61it/s][A
  4%|▎         | 38/1083 [00:00<00:23, 45.20it/s][A
  4%|▍         | 43/1083 [00:00<00:23, 44.97it/s][A
  4%|▍         | 48/1083 [00:01<00:22, 45.04it/s][A
  5%|▍         | 53/1083 [00:01<00:22, 45.17it/s][A
  5%|▌         | 58/1083 [00:01<00:22, 45.31it/s][A
  6%|▌         | 63/1083 [00:01<00:22, 45.49it/s][A
  6%|▋         | 68/1083 [00:01<00:22, 45.53it/s][A
  7%|▋         | 73/1083 [00:01<00:22, 45.56it/s][A
  7%|▋         | 78/1083 [00:01<00:27, 36.80it/s][A
  8%|▊         | 83/1083 [00:01<00:25, 39.09it/s][A
  8%|▊         | 88/1083 [00:02<00:24, 40.87it/s][A
  9%|▊         | 93/1083 [00:02<00:23, 42.24it/s][A
  9%|▉         | 98/1083 [00:02<00:22, 43.22it/s][A
 10%|▉         | 103/1083 [00:02<00:22, 43.98it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 44.51it/s][A
 10%|█         | 113/1083 [00:02<00:21, 44.73it/s][A
 11%|█         | 118/1083 [00:02<00:21, 44.62it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 44.40it/s][A
 12%|█▏        | 128/1083 [00:02<00:21, 44.46it/s][A
 12%|█▏        | 133/1083 [00:02<00:21, 44.68it/s][A
 13%|█▎        | 138/1083 [00:03<00:20, 45.04it/s][A
 13%|█▎        | 143/1083 [00:03<00:20, 45.21it/s][A
 14%|█▎        | 148/1083 [00:03<00:20, 45.42it/s][A
 14%|█▍        | 153/1083 [00:03<00:20, 45.45it/s][A
 15%|█▍        | 158/1083 [00:03<00:20, 45.53it/s][A
 15%|█▌        | 163/1083 [00:03<00:20, 45.31it/s][A
 16%|█▌        | 168/1083 [00:03<00:20, 44.97it/s][A
 16%|█▌        | 173/1083 [00:03<00:20, 44.79it/s][A
 16%|█▋        | 178/1083 [00:03<00:20, 44.92it/s][A
 17%|█▋        | 183/1083 [00:04<00:19, 45.07it/s][A
 17%|█▋        | 188/1083 [00:04<00:19, 45.25it/s][A
 18%|█▊        | 193/1083 [00:04<00:19, 45.42it/s][A
 18%|█▊        | 198/1083 [00:04<00:19, 45.56it/s][A
 19%|█▊        | 203/1083 [00:04<00:19, 45.62it/s][A
 19%|█▉        | 208/1083 [00:04<00:21, 41.40it/s][A
 20%|█▉        | 213/1083 [00:04<00:20, 42.38it/s][A
 20%|██        | 218/1083 [00:04<00:19, 43.25it/s][A
 21%|██        | 223/1083 [00:05<00:19, 43.80it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.33it/s][A
 22%|██▏       | 233/1083 [00:05<00:19, 44.69it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 44.93it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 45.16it/s][A
 23%|██▎       | 248/1083 [00:05<00:18, 44.95it/s][A
 23%|██▎       | 253/1083 [00:05<00:18, 44.92it/s][A
 24%|██▍       | 258/1083 [00:05<00:18, 44.92it/s][A
 24%|██▍       | 263/1083 [00:05<00:18, 45.02it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 45.06it/s][A
 25%|██▌       | 273/1083 [00:06<00:17, 45.15it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 45.27it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 45.37it/s][A
 27%|██▋       | 288/1083 [00:06<00:17, 45.44it/s][A
 27%|██▋       | 293/1083 [00:06<00:17, 45.27it/s][A
 28%|██▊       | 298/1083 [00:06<00:17, 45.01it/s][A
 28%|██▊       | 303/1083 [00:06<00:17, 45.07it/s][A
 28%|██▊       | 308/1083 [00:06<00:17, 45.04it/s][A
 29%|██▉       | 313/1083 [00:07<00:17, 45.14it/s][A
 29%|██▉       | 318/1083 [00:07<00:16, 45.20it/s][A
 30%|██▉       | 323/1083 [00:07<00:16, 45.26it/s][A
 30%|███       | 328/1083 [00:07<00:16, 45.35it/s][A
 31%|███       | 333/1083 [00:07<00:16, 45.38it/s][A
 31%|███       | 338/1083 [00:07<00:16, 45.26it/s][A
 32%|███▏      | 343/1083 [00:07<00:24, 30.81it/s][A
 32%|███▏      | 348/1083 [00:07<00:21, 34.19it/s][A
 33%|███▎      | 353/1083 [00:08<00:19, 36.93it/s][A
 33%|███▎      | 358/1083 [00:08<00:18, 39.22it/s][A
 34%|███▎      | 363/1083 [00:08<00:17, 41.00it/s][A
 34%|███▍      | 368/1083 [00:08<00:16, 42.35it/s][A
 34%|███▍      | 373/1083 [00:08<00:16, 43.33it/s][A
 35%|███▍      | 378/1083 [00:08<00:16, 43.92it/s][A
 35%|███▌      | 383/1083 [00:08<00:15, 43.93it/s][A
 36%|███▌      | 388/1083 [00:08<00:15, 43.94it/s][A
 36%|███▋      | 393/1083 [00:08<00:15, 44.20it/s][A
 37%|███▋      | 398/1083 [00:09<00:15, 44.52it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 44.85it/s][A
 38%|███▊      | 408/1083 [00:09<00:15, 44.97it/s][A
 38%|███▊      | 413/1083 [00:09<00:14, 45.27it/s][A
 39%|███▊      | 418/1083 [00:09<00:14, 45.41it/s][A
 39%|███▉      | 423/1083 [00:09<00:14, 45.44it/s][A
 40%|███▉      | 428/1083 [00:09<00:14, 45.23it/s][A
 40%|███▉      | 433/1083 [00:09<00:14, 44.95it/s][A
 40%|████      | 438/1083 [00:09<00:14, 44.87it/s][A
 41%|████      | 443/1083 [00:10<00:14, 44.83it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 45.07it/s][A
 42%|████▏     | 453/1083 [00:10<00:13, 45.25it/s][A
 42%|████▏     | 458/1083 [00:10<00:13, 45.42it/s][A
 43%|████▎     | 463/1083 [00:10<00:13, 45.47it/s][A
 43%|████▎     | 468/1083 [00:10<00:13, 45.51it/s][A
 44%|████▎     | 473/1083 [00:10<00:22, 27.50it/s][A
 44%|████▍     | 478/1083 [00:11<00:19, 31.26it/s][A
 45%|████▍     | 483/1083 [00:11<00:17, 34.57it/s][A
 45%|████▌     | 488/1083 [00:11<00:15, 37.31it/s][A
 46%|████▌     | 493/1083 [00:11<00:14, 39.44it/s][A
 46%|████▌     | 498/1083 [00:11<00:14, 41.15it/s][A
 46%|████▋     | 503/1083 [00:11<00:13, 42.41it/s][A
 47%|████▋     | 508/1083 [00:11<00:13, 43.19it/s][A
 47%|████▋     | 513/1083 [00:11<00:13, 43.44it/s][A
 48%|████▊     | 518/1083 [00:11<00:12, 43.67it/s][A
 48%|████▊     | 523/1083 [00:12<00:12, 43.95it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 44.37it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 44.66it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 44.97it/s][A
 50%|█████     | 543/1083 [00:12<00:11, 45.16it/s][A
 51%|█████     | 548/1083 [00:12<00:11, 45.28it/s][A
 51%|█████     | 553/1083 [00:12<00:11, 45.19it/s][A
 52%|█████▏    | 558/1083 [00:12<00:11, 45.25it/s][A
 52%|█████▏    | 563/1083 [00:12<00:11, 45.01it/s][A
 52%|█████▏    | 568/1083 [00:13<00:11, 44.99it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 45.11it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 45.19it/s][A
 54%|█████▍    | 583/1083 [00:13<00:11, 45.25it/s][A
 54%|█████▍    | 588/1083 [00:13<00:10, 45.36it/s][A
 55%|█████▍    | 593/1083 [00:13<00:10, 45.34it/s][A
 55%|█████▌    | 598/1083 [00:13<00:11, 41.44it/s][A
 56%|█████▌    | 603/1083 [00:13<00:11, 42.58it/s][A
 56%|█████▌    | 608/1083 [00:13<00:10, 43.32it/s][A
 57%|█████▋    | 613/1083 [00:14<00:10, 43.93it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 44.39it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 44.83it/s][A
 58%|█████▊    | 628/1083 [00:14<00:10, 45.05it/s][A
 58%|█████▊    | 633/1083 [00:14<00:09, 45.08it/s][A
 59%|█████▉    | 638/1083 [00:14<00:09, 44.80it/s][A
 59%|█████▉    | 643/1083 [00:14<00:09, 44.77it/s][A
 60%|█████▉    | 648/1083 [00:14<00:09, 44.87it/s][A
 60%|██████    | 653/1083 [00:14<00:09, 45.09it/s][A
 61%|██████    | 658/1083 [00:15<00:18, 22.92it/s][A
 61%|██████    | 663/1083 [00:15<00:15, 26.99it/s][A
 62%|██████▏   | 668/1083 [00:15<00:13, 30.77it/s][A
 62%|██████▏   | 673/1083 [00:15<00:12, 34.12it/s][A
 63%|██████▎   | 678/1083 [00:15<00:10, 36.89it/s][A
 63%|██████▎   | 683/1083 [00:15<00:10, 39.20it/s][A
 64%|██████▎   | 688/1083 [00:16<00:09, 40.92it/s][A
 64%|██████▍   | 693/1083 [00:16<00:09, 42.28it/s][A
 64%|██████▍   | 698/1083 [00:16<00:08, 42.87it/s][A
 65%|██████▍   | 703/1083 [00:16<00:08, 43.15it/s][A
 65%|██████▌   | 708/1083 [00:16<00:08, 43.55it/s][A
 66%|██████▌   | 713/1083 [00:17<00:08, 44.10it/s][A
 66%|██████▋   | 718/1083 [00:17<00:22, 15.93it/s][A
 67%|██████▋   | 723/1083 [00:17<00:18, 19.81it/s][A
 67%|██████▋   | 728/1083 [00:17<00:14, 23.88it/s][A
 68%|██████▊   | 733/1083 [00:17<00:12, 27.87it/s][A
 68%|██████▊   | 738/1083 [00:17<00:10, 31.55it/s][A
 69%|██████▊   | 743/1083 [00:17<00:09, 34.74it/s][A
 69%|██████▉   | 748/1083 [00:18<00:08, 37.49it/s][A
 70%|██████▉   | 753/1083 [00:18<00:08, 39.51it/s][A
 70%|██████▉   | 758/1083 [00:18<00:07, 40.76it/s][A
 70%|███████   | 763/1083 [00:18<00:07, 41.58it/s][A
 71%|███████   | 768/1083 [00:18<00:07, 42.51it/s][A
 71%|███████▏  | 773/1083 [00:18<00:07, 43.40it/s][A
 72%|███████▏  | 778/1083 [00:18<00:06, 43.99it/s][A
 72%|███████▏  | 783/1083 [00:18<00:06, 44.45it/s][A
 73%|███████▎  | 788/1083 [00:18<00:06, 44.67it/s][A
 73%|███████▎  | 793/1083 [00:19<00:06, 44.92it/s][A
 74%|███████▎  | 798/1083 [00:19<00:06, 45.10it/s][A
 74%|███████▍  | 803/1083 [00:19<00:06, 45.08it/s][A
 75%|███████▍  | 808/1083 [00:19<00:06, 45.01it/s][A
 75%|███████▌  | 813/1083 [00:19<00:05, 45.02it/s][A
 76%|███████▌  | 818/1083 [00:19<00:05, 45.04it/s][A
 76%|███████▌  | 823/1083 [00:20<00:05, 45.14it/s][A
 76%|███████▋  | 828/1083 [00:20<00:14, 17.20it/s][A
 77%|███████▋  | 833/1083 [00:20<00:11, 21.18it/s][A
 77%|███████▋  | 838/1083 [00:20<00:09, 25.23it/s][A
 78%|███████▊  | 843/1083 [00:20<00:08, 29.12it/s][A
 78%|███████▊  | 848/1083 [00:20<00:07, 32.70it/s][A
 79%|███████▉  | 853/1083 [00:21<00:06, 35.78it/s][A
 79%|███████▉  | 858/1083 [00:21<00:05, 38.29it/s][A
 80%|███████▉  | 863/1083 [00:21<00:05, 40.15it/s][A
 80%|████████  | 868/1083 [00:21<00:05, 41.23it/s][A
 81%|████████  | 873/1083 [00:21<00:05, 41.94it/s][A
 81%|████████  | 878/1083 [00:21<00:04, 42.80it/s][A
 82%|████████▏ | 883/1083 [00:21<00:04, 43.54it/s][A
 82%|████████▏ | 888/1083 [00:21<00:04, 44.12it/s][A
 82%|████████▏ | 893/1083 [00:21<00:04, 44.48it/s][A
 83%|████████▎ | 898/1083 [00:22<00:04, 44.74it/s][A
 83%|████████▎ | 903/1083 [00:22<00:03, 45.03it/s][A
 84%|████████▍ | 908/1083 [00:22<00:03, 45.21it/s][A
 84%|████████▍ | 913/1083 [00:22<00:03, 45.13it/s][A
 85%|████████▍ | 918/1083 [00:22<00:03, 44.93it/s][A
 85%|████████▌ | 923/1083 [00:22<00:03, 44.93it/s][A
 86%|████████▌ | 928/1083 [00:22<00:03, 44.95it/s][A
 86%|████████▌ | 933/1083 [00:23<00:03, 45.07it/s][A
 87%|████████▋ | 938/1083 [00:23<00:05, 26.85it/s][A
 87%|████████▋ | 943/1083 [00:23<00:04, 30.65it/s][A
 88%|████████▊ | 948/1083 [00:23<00:03, 34.05it/s][A
 88%|████████▊ | 953/1083 [00:23<00:03, 36.85it/s][A
 88%|████████▊ | 958/1083 [00:23<00:03, 39.14it/s][A
 89%|████████▉ | 963/1083 [00:23<00:02, 40.83it/s][A
 89%|████████▉ | 968/1083 [00:23<00:02, 42.20it/s][A
 90%|████████▉ | 973/1083 [00:23<00:02, 43.02it/s][A
 90%|█████████ | 978/1083 [00:24<00:02, 43.26it/s][A
 91%|█████████ | 983/1083 [00:24<00:02, 43.53it/s][A
 91%|█████████ | 988/1083 [00:24<00:02, 43.74it/s][A
 92%|█████████▏| 993/1083 [00:24<00:02, 44.38it/s][A
 92%|█████████▏| 998/1083 [00:24<00:01, 44.72it/s][A
 93%|█████████▎| 1003/1083 [00:24<00:01, 44.89it/s][A
 93%|█████████▎| 1008/1083 [00:24<00:01, 45.10it/s][A
 94%|█████████▎| 1013/1083 [00:24<00:01, 45.14it/s][A
 94%|█████████▍| 1018/1083 [00:24<00:01, 45.20it/s][A
 94%|█████████▍| 1023/1083 [00:25<00:01, 45.16it/s][A
 95%|█████████▍| 1028/1083 [00:25<00:01, 45.00it/s][A
 95%|█████████▌| 1033/1083 [00:25<00:01, 44.92it/s][A
 96%|█████████▌| 1038/1083 [00:25<00:00, 45.04it/s][A
 96%|█████████▋| 1043/1083 [00:25<00:00, 45.21it/s][A
 97%|█████████▋| 1048/1083 [00:25<00:00, 45.23it/s][A
 97%|█████████▋| 1053/1083 [00:25<00:00, 45.30it/s][A
 98%|█████████▊| 1058/1083 [00:25<00:00, 45.22it/s][A
 98%|█████████▊| 1063/1083 [00:25<00:00, 36.92it/s][A
 99%|█████████▊| 1068/1083 [00:26<00:00, 39.21it/s][A
 99%|█████████▉| 1073/1083 [00:26<00:00, 40.99it/s][A
100%|█████████▉| 1078/1083 [00:26<00:00, 42.21it/s][A
100%|██████████| 1083/1083 [00:26<00:00, 43.27it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:26<00:00, 43.27it/s][A 80%|████████  | 312/390 [05:43<00:33,  2.35it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:28:22,256 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 18:28:22,547 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:28:30,667 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:28:30,979 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:28:31,277 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [06:05<19:07, 14.90s/it] 81%|████████  | 314/390 [06:05<13:20, 10.53s/it] 81%|████████  | 315/390 [06:05<09:19,  7.46s/it] 81%|████████  | 316/390 [06:06<06:32,  5.31s/it] 81%|████████▏ | 317/390 [06:06<04:37,  3.80s/it] 82%|████████▏ | 318/390 [06:06<03:17,  2.75s/it] 82%|████████▏ | 319/390 [06:07<02:22,  2.01s/it] 82%|████████▏ | 320/390 [06:07<01:44,  1.50s/it] 82%|████████▏ | 321/390 [06:07<01:18,  1.13s/it] 83%|████████▎ | 322/390 [06:08<00:59,  1.13it/s] 83%|████████▎ | 323/390 [06:08<00:47,  1.42it/s] 83%|████████▎ | 324/390 [06:08<00:38,  1.72it/s] 83%|████████▎ | 325/390 [06:08<00:33,  1.91it/s] 84%|████████▎ | 326/390 [06:09<00:29,  2.20it/s] 84%|████████▍ | 327/390 [06:09<00:25,  2.47it/s] 84%|████████▍ | 328/390 [06:09<00:23,  2.69it/s] 84%|████████▍ | 329/390 [06:10<00:21,  2.88it/s] 85%|████████▍ | 330/390 [06:10<00:19,  3.02it/s] 85%|████████▍ | 331/390 [06:10<00:18,  3.13it/s] 85%|████████▌ | 332/390 [06:11<00:18,  3.21it/s] 85%|████████▌ | 333/390 [06:11<00:17,  3.27it/s] 86%|████████▌ | 334/390 [06:11<00:16,  3.31it/s] 86%|████████▌ | 335/390 [06:12<00:18,  2.90it/s] 86%|████████▌ | 336/390 [06:12<00:17,  3.04it/s] 86%|████████▋ | 337/390 [06:12<00:16,  3.14it/s] 87%|████████▋ | 338/390 [06:12<00:16,  3.21it/s] 87%|████████▋ | 339/390 [06:13<00:15,  3.27it/s] 87%|████████▋ | 340/390 [06:13<00:15,  3.32it/s] 87%|████████▋ | 341/390 [06:13<00:14,  3.34it/s] 88%|████████▊ | 342/390 [06:14<00:14,  3.36it/s] 88%|████████▊ | 343/390 [06:14<00:13,  3.38it/s] 88%|████████▊ | 344/390 [06:14<00:13,  3.39it/s] 88%|████████▊ | 345/390 [06:15<00:14,  3.21it/s] 89%|████████▊ | 346/390 [06:15<00:13,  3.27it/s] 89%|████████▉ | 347/390 [06:15<00:12,  3.31it/s] 89%|████████▉ | 348/390 [06:15<00:12,  3.35it/s] 89%|████████▉ | 349/390 [06:16<00:12,  3.37it/s] 90%|████████▉ | 350/390 [06:16<00:11,  3.39it/s] 90%|█████████ | 351/390 [06:16<00:11,  3.40it/s] 90%|█████████ | 352/390 [06:17<00:11,  3.41it/s] 91%|█████████ | 353/390 [06:17<00:10,  3.43it/s] 91%|█████████ | 354/390 [06:17<00:10,  3.44it/s] 91%|█████████ | 355/390 [06:17<00:10,  3.45it/s] 91%|█████████▏| 356/390 [06:18<00:10,  3.36it/s] 92%|█████████▏| 357/390 [06:18<00:09,  3.39it/s] 92%|█████████▏| 358/390 [06:18<00:09,  3.41it/s] 92%|█████████▏| 359/390 [06:19<00:09,  3.43it/s] 92%|█████████▏| 360/390 [06:19<00:08,  3.43it/s] 93%|█████████▎| 361/390 [06:19<00:08,  3.44it/s] 93%|█████████▎| 362/390 [06:20<00:08,  3.21it/s] 93%|█████████▎| 363/390 [06:20<00:08,  3.29it/s] 93%|█████████▎| 364/390 [06:20<00:07,  3.33it/s] 94%|█████████▎| 365/390 [06:20<00:07,  3.37it/s] 94%|█████████▍| 366/390 [06:21<00:07,  3.40it/s] 94%|█████████▍| 367/390 [06:21<00:06,  3.42it/s] 94%|█████████▍| 368/390 [06:21<00:06,  3.43it/s] 95%|█████████▍| 369/390 [06:22<00:06,  3.44it/s] 95%|█████████▍| 370/390 [06:22<00:05,  3.45it/s] 95%|█████████▌| 371/390 [06:22<00:05,  3.45it/s] 95%|█████████▌| 372/390 [06:22<00:05,  3.46it/s] 96%|█████████▌| 373/390 [06:23<00:05,  3.31it/s] 96%|█████████▌| 374/390 [06:23<00:04,  3.35it/s] 96%|█████████▌| 375/390 [06:23<00:04,  3.38it/s] 96%|█████████▋| 376/390 [06:24<00:04,  3.41it/s] 97%|█████████▋| 377/390 [06:24<00:03,  3.42it/s] 97%|█████████▋| 378/390 [06:24<00:03,  3.43it/s] 97%|█████████▋| 379/390 [06:25<00:03,  3.44it/s] 97%|█████████▋| 380/390 [06:25<00:02,  3.45it/s] 98%|█████████▊| 381/390 [06:25<00:02,  3.45it/s] 98%|█████████▊| 382/390 [06:25<00:02,  3.46it/s] 98%|█████████▊| 383/390 [06:26<00:02,  3.46it/s] 98%|█████████▊| 384/390 [06:26<00:01,  3.00it/s] 99%|█████████▊| 385/390 [06:26<00:01,  3.13it/s] 99%|█████████▉| 386/390 [06:27<00:01,  3.22it/s] 99%|█████████▉| 387/390 [06:27<00:00,  3.29it/s] 99%|█████████▉| 388/390 [06:27<00:00,  3.34it/s]100%|█████████▉| 389/390 [06:28<00:00,  3.37it/s]100%|██████████| 390/390 [06:28<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 18:29:06,772 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:29:06,772 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 18:29:06,772 >>   Batch size = 8
{'eval_loss': 0.9900107979774475, 'eval_runtime': 26.4428, 'eval_samples_per_second': 327.424, 'eval_steps_per_second': 40.956, 'epoch': 3.99}

  0%|          | 0/1083 [00:00<?, ?it/s][A
  1%|          | 6/1083 [00:00<00:18, 56.74it/s][A
  1%|          | 12/1083 [00:00<00:21, 49.48it/s][A
  2%|▏         | 18/1083 [00:00<00:22, 47.33it/s][A
  2%|▏         | 23/1083 [00:00<00:22, 46.49it/s][A
  3%|▎         | 28/1083 [00:00<00:23, 45.87it/s][A
  3%|▎         | 33/1083 [00:00<00:23, 45.50it/s][A
  4%|▎         | 38/1083 [00:01<00:23, 45.36it/s][A
  4%|▍         | 43/1083 [00:01<00:37, 27.91it/s][A
  4%|▍         | 48/1083 [00:01<00:32, 31.73it/s][A
  5%|▍         | 53/1083 [00:01<00:29, 35.04it/s][A
  5%|▌         | 58/1083 [00:01<00:27, 37.70it/s][A
  6%|▌         | 63/1083 [00:01<00:25, 39.81it/s][A
  6%|▋         | 68/1083 [00:01<00:24, 41.36it/s][A
  7%|▋         | 73/1083 [00:01<00:23, 42.63it/s][A
  7%|▋         | 78/1083 [00:01<00:23, 43.45it/s][A
  8%|▊         | 83/1083 [00:02<00:22, 43.67it/s][A
  8%|▊         | 88/1083 [00:02<00:22, 43.68it/s][A
  9%|▊         | 93/1083 [00:02<00:22, 44.00it/s][A
  9%|▉         | 98/1083 [00:02<00:22, 44.28it/s][A
 10%|▉         | 103/1083 [00:02<00:22, 44.48it/s][A
 10%|▉         | 108/1083 [00:02<00:21, 44.97it/s][A
 10%|█         | 113/1083 [00:02<00:21, 45.21it/s][A
 11%|█         | 118/1083 [00:02<00:21, 45.42it/s][A
 11%|█▏        | 123/1083 [00:02<00:21, 45.42it/s][A
 12%|█▏        | 128/1083 [00:03<00:21, 45.22it/s][A
 12%|█▏        | 133/1083 [00:03<00:21, 44.84it/s][A
 13%|█▎        | 138/1083 [00:03<00:21, 44.79it/s][A
 13%|█▎        | 143/1083 [00:03<00:27, 33.60it/s][A
 14%|█▎        | 148/1083 [00:03<00:25, 36.96it/s][A
 14%|█▍        | 153/1083 [00:03<00:23, 39.21it/s][A
 15%|█▍        | 158/1083 [00:03<00:22, 41.03it/s][A
 15%|█▌        | 163/1083 [00:04<00:21, 42.26it/s][A
 16%|█▌        | 168/1083 [00:04<00:36, 25.02it/s][A
 16%|█▌        | 173/1083 [00:04<00:31, 28.97it/s][A
 16%|█▋        | 178/1083 [00:04<00:27, 32.52it/s][A
 17%|█▋        | 183/1083 [00:04<00:25, 35.60it/s][A
 17%|█▋        | 188/1083 [00:04<00:23, 38.23it/s][A
 18%|█▊        | 193/1083 [00:04<00:22, 40.19it/s][A
 18%|█▊        | 198/1083 [00:04<00:21, 41.71it/s][A
 19%|█▊        | 203/1083 [00:05<00:20, 42.79it/s][A
 19%|█▉        | 208/1083 [00:05<00:20, 43.22it/s][A
 20%|█▉        | 213/1083 [00:05<00:20, 43.39it/s][A
 20%|██        | 218/1083 [00:05<00:19, 43.74it/s][A
 21%|██        | 223/1083 [00:05<00:19, 44.31it/s][A
 21%|██        | 228/1083 [00:05<00:19, 44.68it/s][A
 22%|██▏       | 233/1083 [00:05<00:18, 44.92it/s][A
 22%|██▏       | 238/1083 [00:05<00:18, 45.07it/s][A
 22%|██▏       | 243/1083 [00:05<00:18, 45.23it/s][A
 23%|██▎       | 248/1083 [00:06<00:18, 45.21it/s][A
 23%|██▎       | 253/1083 [00:06<00:18, 45.09it/s][A
 24%|██▍       | 258/1083 [00:06<00:18, 44.89it/s][A
 24%|██▍       | 263/1083 [00:06<00:18, 44.84it/s][A
 25%|██▍       | 268/1083 [00:06<00:18, 45.01it/s][A
 25%|██▌       | 273/1083 [00:06<00:17, 45.07it/s][A
 26%|██▌       | 278/1083 [00:06<00:17, 45.23it/s][A
 26%|██▌       | 283/1083 [00:06<00:17, 45.28it/s][A
 27%|██▋       | 288/1083 [00:07<00:17, 45.43it/s][A
 27%|██▋       | 293/1083 [00:07<00:30, 26.17it/s][A
 28%|██▊       | 298/1083 [00:07<00:26, 30.05it/s][A
 28%|██▊       | 303/1083 [00:07<00:23, 33.50it/s][A
 28%|██▊       | 308/1083 [00:07<00:21, 36.45it/s][A
 29%|██▉       | 313/1083 [00:07<00:19, 38.81it/s][A
 29%|██▉       | 318/1083 [00:07<00:18, 40.70it/s][A
 30%|██▉       | 323/1083 [00:07<00:18, 42.04it/s][A
 30%|███       | 328/1083 [00:08<00:17, 42.92it/s][A
 31%|███       | 333/1083 [00:08<00:17, 43.20it/s][A
 31%|███       | 338/1083 [00:08<00:17, 43.46it/s][A
 32%|███▏      | 343/1083 [00:08<00:16, 43.92it/s][A
 32%|███▏      | 348/1083 [00:08<00:16, 44.37it/s][A
 33%|███▎      | 353/1083 [00:08<00:16, 44.74it/s][A
 33%|███▎      | 358/1083 [00:08<00:16, 44.98it/s][A
 34%|███▎      | 363/1083 [00:08<00:15, 45.25it/s][A
 34%|███▍      | 368/1083 [00:08<00:15, 45.36it/s][A
 34%|███▍      | 373/1083 [00:09<00:15, 45.30it/s][A
 35%|███▍      | 378/1083 [00:09<00:15, 44.99it/s][A
 35%|███▌      | 383/1083 [00:09<00:15, 44.72it/s][A
 36%|███▌      | 388/1083 [00:09<00:15, 44.74it/s][A
 36%|███▋      | 393/1083 [00:09<00:15, 44.85it/s][A
 37%|███▋      | 398/1083 [00:09<00:15, 45.02it/s][A
 37%|███▋      | 403/1083 [00:09<00:15, 45.12it/s][A
 38%|███▊      | 408/1083 [00:09<00:14, 45.28it/s][A
 38%|███▊      | 413/1083 [00:10<00:14, 45.29it/s][A
 39%|███▊      | 418/1083 [00:10<00:23, 28.79it/s][A
 39%|███▉      | 423/1083 [00:10<00:20, 32.40it/s][A
 40%|███▉      | 428/1083 [00:10<00:18, 35.53it/s][A
 40%|███▉      | 433/1083 [00:10<00:17, 38.06it/s][A
 40%|████      | 438/1083 [00:10<00:16, 39.98it/s][A
 41%|████      | 443/1083 [00:10<00:15, 41.58it/s][A
 41%|████▏     | 448/1083 [00:10<00:14, 42.79it/s][A
 42%|████▏     | 453/1083 [00:11<00:14, 43.48it/s][A
 42%|████▏     | 458/1083 [00:11<00:14, 43.63it/s][A
 43%|████▎     | 463/1083 [00:11<00:14, 43.72it/s][A
 43%|████▎     | 468/1083 [00:11<00:13, 43.99it/s][A
 44%|████▎     | 473/1083 [00:11<00:13, 44.38it/s][A
 44%|████▍     | 478/1083 [00:11<00:13, 44.76it/s][A
 45%|████▍     | 483/1083 [00:11<00:13, 45.15it/s][A
 45%|████▌     | 488/1083 [00:11<00:13, 45.31it/s][A
 46%|████▌     | 493/1083 [00:11<00:12, 45.47it/s][A
 46%|████▌     | 498/1083 [00:12<00:12, 45.26it/s][A
 46%|████▋     | 503/1083 [00:12<00:12, 45.02it/s][A
 47%|████▋     | 508/1083 [00:12<00:12, 44.76it/s][A
 47%|████▋     | 513/1083 [00:12<00:12, 44.73it/s][A
 48%|████▊     | 518/1083 [00:12<00:12, 44.83it/s][A
 48%|████▊     | 523/1083 [00:12<00:12, 45.01it/s][A
 49%|████▉     | 528/1083 [00:12<00:12, 45.13it/s][A
 49%|████▉     | 533/1083 [00:12<00:12, 45.24it/s][A
 50%|████▉     | 538/1083 [00:12<00:12, 45.40it/s][A
 50%|█████     | 543/1083 [00:13<00:11, 45.39it/s][A
 51%|█████     | 548/1083 [00:13<00:16, 32.50it/s][A
 51%|█████     | 553/1083 [00:13<00:14, 35.64it/s][A
 52%|█████▏    | 558/1083 [00:13<00:13, 37.87it/s][A
 52%|█████▏    | 563/1083 [00:13<00:12, 40.09it/s][A
 52%|█████▏    | 568/1083 [00:13<00:12, 41.58it/s][A
 53%|█████▎    | 573/1083 [00:13<00:11, 42.69it/s][A
 53%|█████▎    | 578/1083 [00:13<00:11, 43.51it/s][A
 54%|█████▍    | 583/1083 [00:14<00:11, 43.99it/s][A
 54%|█████▍    | 588/1083 [00:14<00:11, 43.95it/s][A
 55%|█████▍    | 593/1083 [00:14<00:11, 44.26it/s][A
 55%|█████▌    | 598/1083 [00:14<00:10, 44.62it/s][A
 56%|█████▌    | 603/1083 [00:14<00:10, 44.89it/s][A
 56%|█████▌    | 608/1083 [00:14<00:10, 44.96it/s][A
 57%|█████▋    | 613/1083 [00:14<00:10, 45.05it/s][A
 57%|█████▋    | 618/1083 [00:14<00:10, 45.13it/s][A
 58%|█████▊    | 623/1083 [00:14<00:10, 45.21it/s][A
 58%|█████▊    | 628/1083 [00:15<00:10, 45.19it/s][A
 58%|█████▊    | 633/1083 [00:15<00:10, 44.94it/s][A
 59%|█████▉    | 638/1083 [00:15<00:09, 44.82it/s][A
 59%|█████▉    | 643/1083 [00:15<00:09, 44.97it/s][A
 60%|█████▉    | 648/1083 [00:15<00:09, 45.11it/s][A
 60%|██████    | 653/1083 [00:15<00:09, 45.27it/s][A
 61%|██████    | 658/1083 [00:15<00:09, 45.29it/s][A
 61%|██████    | 663/1083 [00:15<00:09, 45.40it/s][A
 62%|██████▏   | 668/1083 [00:15<00:09, 45.32it/s][A
 62%|██████▏   | 673/1083 [00:16<00:09, 45.28it/s][A
 63%|██████▎   | 678/1083 [00:16<00:17, 22.88it/s][A
 63%|██████▎   | 683/1083 [00:16<00:14, 26.91it/s][A
 64%|██████▎   | 688/1083 [00:16<00:12, 30.69it/s][A
 64%|██████▍   | 693/1083 [00:16<00:11, 34.04it/s][A
 64%|██████▍   | 698/1083 [00:16<00:10, 36.89it/s][A
 65%|██████▍   | 703/1083 [00:17<00:09, 39.16it/s][A
 65%|██████▌   | 708/1083 [00:17<00:09, 40.91it/s][A
 66%|██████▌   | 713/1083 [00:17<00:08, 42.02it/s][A
 66%|██████▋   | 718/1083 [00:17<00:08, 42.53it/s][A
 67%|██████▋   | 723/1083 [00:17<00:08, 42.98it/s][A
 67%|██████▋   | 728/1083 [00:17<00:08, 43.54it/s][A
 68%|██████▊   | 733/1083 [00:17<00:07, 44.01it/s][A
 68%|██████▊   | 738/1083 [00:17<00:07, 44.35it/s][A
 69%|██████▊   | 743/1083 [00:18<00:07, 44.59it/s][A
 69%|██████▉   | 748/1083 [00:18<00:07, 44.80it/s][A
 70%|██████▉   | 753/1083 [00:18<00:07, 45.17it/s][A
 70%|██████▉   | 758/1083 [00:18<00:07, 45.21it/s][A
 70%|███████   | 763/1083 [00:18<00:07, 45.12it/s][A
 71%|███████   | 768/1083 [00:18<00:07, 44.97it/s][A
 71%|███████▏  | 773/1083 [00:18<00:06, 44.87it/s][A
 72%|███████▏  | 778/1083 [00:18<00:06, 44.94it/s][A
 72%|███████▏  | 783/1083 [00:18<00:06, 45.04it/s][A
 73%|███████▎  | 788/1083 [00:18<00:06, 45.11it/s][A
 73%|███████▎  | 793/1083 [00:19<00:06, 45.18it/s][A
 74%|███████▎  | 798/1083 [00:19<00:06, 42.61it/s][A
 74%|███████▍  | 803/1083 [00:19<00:06, 43.56it/s][A
 75%|███████▍  | 808/1083 [00:19<00:06, 44.09it/s][A
 75%|███████▌  | 813/1083 [00:19<00:06, 44.33it/s][A
 76%|███████▌  | 818/1083 [00:19<00:05, 44.52it/s][A
 76%|███████▌  | 823/1083 [00:19<00:05, 44.73it/s][A
 76%|███████▋  | 828/1083 [00:19<00:05, 44.83it/s][A
 77%|███████▋  | 833/1083 [00:20<00:05, 44.91it/s][A
 77%|███████▋  | 838/1083 [00:20<00:05, 44.77it/s][A
 78%|███████▊  | 843/1083 [00:20<00:05, 43.16it/s][A
 78%|███████▊  | 848/1083 [00:20<00:05, 44.22it/s][A
 79%|███████▉  | 853/1083 [00:20<00:05, 44.66it/s][A
 79%|███████▉  | 858/1083 [00:20<00:09, 24.75it/s][A
 80%|███████▉  | 862/1083 [00:21<00:09, 23.16it/s][A
 80%|███████▉  | 866/1083 [00:21<00:08, 24.46it/s][A
 80%|████████  | 871/1083 [00:21<00:07, 28.85it/s][A
 81%|████████  | 875/1083 [00:21<00:10, 20.37it/s][A
 81%|████████▏ | 880/1083 [00:21<00:08, 24.96it/s][A
 82%|████████▏ | 885/1083 [00:21<00:06, 29.16it/s][A
 82%|████████▏ | 890/1083 [00:22<00:05, 32.86it/s][A
 83%|████████▎ | 895/1083 [00:22<00:05, 35.97it/s][A
 83%|████████▎ | 900/1083 [00:22<00:05, 33.17it/s][A
 84%|████████▎ | 905/1083 [00:22<00:04, 36.18it/s][A
 84%|████████▍ | 910/1083 [00:22<00:04, 38.61it/s][A
 84%|████████▍ | 915/1083 [00:22<00:04, 40.32it/s][A
 85%|████████▍ | 920/1083 [00:22<00:03, 41.73it/s][A
 85%|████████▌ | 925/1083 [00:22<00:03, 42.86it/s][A
 86%|████████▌ | 930/1083 [00:22<00:03, 43.62it/s][A
 86%|████████▋ | 935/1083 [00:23<00:03, 43.98it/s][A
 87%|████████▋ | 940/1083 [00:23<00:03, 44.03it/s][A
 87%|████████▋ | 945/1083 [00:23<00:05, 26.42it/s][A
 88%|████████▊ | 951/1083 [00:23<00:04, 31.49it/s][A
 88%|████████▊ | 956/1083 [00:23<00:03, 34.59it/s][A
 89%|████████▊ | 961/1083 [00:23<00:03, 37.22it/s][A
 89%|████████▉ | 966/1083 [00:23<00:02, 39.37it/s][A
 90%|████████▉ | 971/1083 [00:24<00:02, 41.03it/s][A
 90%|█████████ | 976/1083 [00:24<00:02, 42.21it/s][A
 91%|█████████ | 981/1083 [00:24<00:02, 43.11it/s][A
 91%|█████████ | 986/1083 [00:24<00:02, 43.69it/s][A
 92%|█████████▏| 991/1083 [00:24<00:02, 43.66it/s][A
 92%|█████████▏| 996/1083 [00:24<00:01, 43.88it/s][A
 92%|█████████▏| 1001/1083 [00:24<00:01, 44.28it/s][A
 93%|█████████▎| 1006/1083 [00:24<00:01, 44.49it/s][A
 93%|█████████▎| 1011/1083 [00:24<00:01, 44.75it/s][A
 94%|█████████▍| 1016/1083 [00:25<00:01, 45.08it/s][A
 94%|█████████▍| 1021/1083 [00:25<00:01, 41.68it/s][A
 95%|█████████▍| 1026/1083 [00:25<00:01, 42.84it/s][A
 95%|█████████▌| 1031/1083 [00:25<00:01, 43.59it/s][A
 96%|█████████▌| 1036/1083 [00:26<00:04, 11.51it/s][A
 96%|█████████▌| 1041/1083 [00:26<00:02, 14.87it/s][A
 97%|█████████▋| 1046/1083 [00:26<00:01, 18.65it/s][A
 97%|█████████▋| 1051/1083 [00:26<00:01, 22.66it/s][A
 98%|█████████▊| 1056/1083 [00:27<00:01, 26.68it/s][A
 98%|█████████▊| 1061/1083 [00:27<00:00, 30.45it/s][A
 98%|█████████▊| 1066/1083 [00:27<00:00, 33.87it/s][A
 99%|█████████▉| 1071/1083 [00:27<00:00, 36.70it/s][A
 99%|█████████▉| 1076/1083 [00:27<00:00, 38.69it/s][A
100%|█████████▉| 1081/1083 [00:27<00:00, 40.11it/s][A
                                                   [A                                                 
100%|██████████| 1083/1083 [00:27<00:00, 40.11it/s][A100%|██████████| 390/390 [06:56<00:00,  3.40it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-28 18:29:34,857 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 18:29:35,074 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:29:43,861 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:29:44,625 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:29:44,723 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:30:11,439 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:30:11,456 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78 (score: 0.9555922150611877).
                                                 100%|██████████| 390/390 [08:00<00:00,  3.40it/s]100%|██████████| 390/390 [08:00<00:00,  1.23s/it]
[INFO|trainer.py:1894] 2023-08-28 18:30:39,510 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 18:30:39,704 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:30:47,691 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:30:49,151 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:30:49,581 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:30:50,880 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:30:51,424 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:30:51,425 >>   train_loss               =     0.3677
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:30:51,425 >>   train_runtime            = 0:08:00.87
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:30:51,425 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:30:51,425 >>   train_samples_per_second =     51.989
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:30:51,425 >>   train_steps_per_second   =      0.811
{'eval_loss': 0.9934230446815491, 'eval_runtime': 27.6966, 'eval_samples_per_second': 312.602, 'eval_steps_per_second': 39.102, 'epoch': 4.99}
{'train_runtime': 480.8728, 'train_samples_per_second': 51.989, 'train_steps_per_second': 0.811, 'train_loss': 0.36772120549128606, 'epoch': 4.99}
08/28/2023 18:30:54 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:30:54,789 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:30:54,789 >>   Num examples = 8658
[INFO|trainer.py:2145] 2023-08-28 18:30:54,789 >>   Batch size = 8
  0%|          | 0/1083 [00:00<?, ?it/s]  1%|          | 6/1083 [00:00<00:19, 56.05it/s]  1%|          | 12/1083 [00:00<00:21, 50.01it/s]  2%|▏         | 18/1083 [00:00<00:22, 48.07it/s]  2%|▏         | 23/1083 [00:00<00:22, 47.39it/s]  3%|▎         | 28/1083 [00:00<00:22, 46.85it/s]  3%|▎         | 33/1083 [00:00<00:22, 46.64it/s]  4%|▎         | 38/1083 [00:00<00:22, 46.44it/s]  4%|▍         | 43/1083 [00:00<00:22, 46.03it/s]  4%|▍         | 48/1083 [00:01<00:22, 45.48it/s]  5%|▍         | 53/1083 [00:01<00:22, 45.29it/s]  5%|▌         | 58/1083 [00:01<00:22, 45.33it/s]  6%|▌         | 63/1083 [00:01<00:22, 45.50it/s]  6%|▋         | 68/1083 [00:01<00:22, 45.68it/s]  7%|▋         | 73/1083 [00:01<00:22, 45.83it/s]  7%|▋         | 78/1083 [00:01<00:21, 45.82it/s]  8%|▊         | 83/1083 [00:01<00:21, 45.81it/s]  8%|▊         | 88/1083 [00:01<00:21, 45.66it/s]  9%|▊         | 93/1083 [00:02<00:21, 45.43it/s]  9%|▉         | 98/1083 [00:02<00:21, 45.26it/s] 10%|▉         | 103/1083 [00:02<00:21, 45.26it/s] 10%|▉         | 108/1083 [00:02<00:21, 45.40it/s] 10%|█         | 113/1083 [00:02<00:21, 45.45it/s] 11%|█         | 118/1083 [00:02<00:21, 45.58it/s] 11%|█▏        | 123/1083 [00:02<00:21, 45.64it/s] 12%|█▏        | 128/1083 [00:02<00:20, 45.63it/s] 12%|█▏        | 133/1083 [00:02<00:20, 45.66it/s] 13%|█▎        | 138/1083 [00:03<00:20, 45.58it/s] 13%|█▎        | 143/1083 [00:03<00:58, 16.17it/s] 14%|█▎        | 148/1083 [00:03<00:46, 20.07it/s] 14%|█▍        | 153/1083 [00:03<00:38, 24.16it/s] 15%|█▍        | 158/1083 [00:04<00:32, 28.19it/s] 15%|█▌        | 163/1083 [00:04<00:28, 31.87it/s] 16%|█▌        | 168/1083 [00:04<00:26, 35.09it/s] 16%|█▌        | 173/1083 [00:04<00:24, 37.76it/s] 16%|█▋        | 178/1083 [00:04<00:22, 39.86it/s] 17%|█▋        | 183/1083 [00:04<00:21, 41.05it/s] 17%|█▋        | 188/1083 [00:04<00:21, 41.93it/s] 18%|█▊        | 193/1083 [00:04<00:20, 42.76it/s] 18%|█▊        | 198/1083 [00:04<00:20, 43.57it/s] 19%|█▊        | 203/1083 [00:05<00:19, 44.01it/s] 19%|█▉        | 208/1083 [00:05<00:19, 44.56it/s] 20%|█▉        | 213/1083 [00:05<00:19, 44.89it/s] 20%|██        | 218/1083 [00:05<00:19, 45.18it/s] 21%|██        | 223/1083 [00:05<00:18, 45.33it/s] 21%|██        | 228/1083 [00:05<00:18, 45.24it/s] 22%|██▏       | 233/1083 [00:05<00:18, 44.93it/s] 22%|██▏       | 238/1083 [00:05<00:18, 44.99it/s] 22%|██▏       | 243/1083 [00:05<00:18, 45.04it/s] 23%|██▎       | 248/1083 [00:06<00:18, 45.13it/s] 23%|██▎       | 253/1083 [00:06<00:40, 20.66it/s] 24%|██▍       | 258/1083 [00:06<00:33, 24.75it/s] 24%|██▍       | 263/1083 [00:06<00:28, 28.72it/s] 25%|██▍       | 268/1083 [00:06<00:25, 32.38it/s] 25%|██▌       | 273/1083 [00:07<00:22, 35.51it/s] 26%|██▌       | 278/1083 [00:07<00:21, 38.13it/s] 26%|██▌       | 283/1083 [00:07<00:19, 40.22it/s] 27%|██▋       | 288/1083 [00:07<00:19, 41.61it/s] 27%|██▋       | 293/1083 [00:07<00:18, 42.36it/s] 28%|██▊       | 298/1083 [00:07<00:18, 42.84it/s] 28%|██▊       | 303/1083 [00:07<00:17, 43.41it/s] 28%|██▊       | 308/1083 [00:07<00:17, 44.01it/s] 29%|██▉       | 313/1083 [00:07<00:17, 44.48it/s] 29%|██▉       | 318/1083 [00:08<00:17, 44.90it/s] 30%|██▉       | 323/1083 [00:08<00:16, 45.13it/s] 30%|███       | 328/1083 [00:08<00:16, 45.34it/s] 31%|███       | 333/1083 [00:08<00:16, 45.35it/s] 31%|███       | 338/1083 [00:08<00:16, 45.28it/s] 32%|███▏      | 343/1083 [00:08<00:16, 45.01it/s] 32%|███▏      | 348/1083 [00:08<00:16, 44.81it/s] 33%|███▎      | 353/1083 [00:08<00:16, 44.99it/s] 33%|███▎      | 358/1083 [00:08<00:16, 45.11it/s] 34%|███▎      | 363/1083 [00:09<00:15, 45.28it/s] 34%|███▍      | 368/1083 [00:09<00:15, 45.50it/s] 34%|███▍      | 373/1083 [00:09<00:15, 45.57it/s] 35%|███▍      | 378/1083 [00:09<00:22, 30.79it/s] 35%|███▌      | 383/1083 [00:09<00:20, 34.05it/s] 36%|███▌      | 388/1083 [00:09<00:18, 36.82it/s] 36%|███▋      | 393/1083 [00:09<00:17, 39.09it/s] 37%|███▋      | 398/1083 [00:10<00:16, 40.91it/s] 37%|███▋      | 403/1083 [00:10<00:16, 42.17it/s] 38%|███▊      | 408/1083 [00:10<00:15, 43.21it/s] 38%|███▊      | 413/1083 [00:10<00:15, 43.83it/s] 39%|███▊      | 418/1083 [00:10<00:15, 43.94it/s] 39%|███▉      | 423/1083 [00:10<00:14, 44.01it/s] 40%|███▉      | 428/1083 [00:10<00:14, 44.17it/s] 40%|███▉      | 433/1083 [00:10<00:14, 44.57it/s] 40%|████      | 438/1083 [00:10<00:14, 44.83it/s] 41%|████      | 443/1083 [00:11<00:14, 45.08it/s] 41%|████▏     | 448/1083 [00:11<00:14, 45.22it/s] 42%|████▏     | 453/1083 [00:11<00:13, 45.42it/s] 42%|████▏     | 458/1083 [00:11<00:13, 45.41it/s] 43%|████▎     | 463/1083 [00:11<00:13, 45.21it/s] 43%|████▎     | 468/1083 [00:11<00:13, 44.96it/s] 44%|████▎     | 473/1083 [00:11<00:13, 44.82it/s] 44%|████▍     | 478/1083 [00:11<00:13, 44.86it/s] 45%|████▍     | 483/1083 [00:11<00:13, 45.07it/s] 45%|████▌     | 488/1083 [00:11<00:13, 45.23it/s] 46%|████▌     | 493/1083 [00:12<00:12, 45.39it/s] 46%|████▌     | 498/1083 [00:12<00:12, 45.53it/s] 46%|████▋     | 503/1083 [00:12<00:12, 45.50it/s] 47%|████▋     | 508/1083 [00:12<00:15, 37.93it/s] 47%|████▋     | 513/1083 [00:12<00:14, 39.92it/s] 48%|████▊     | 518/1083 [00:12<00:13, 41.55it/s] 48%|████▊     | 523/1083 [00:12<00:13, 42.69it/s] 49%|████▉     | 528/1083 [00:12<00:12, 43.56it/s] 49%|████▉     | 533/1083 [00:13<00:12, 44.18it/s] 50%|████▉     | 538/1083 [00:13<00:12, 44.63it/s] 50%|█████     | 543/1083 [00:13<00:12, 44.84it/s] 51%|█████     | 548/1083 [00:13<00:11, 44.65it/s] 51%|█████     | 553/1083 [00:13<00:11, 44.49it/s] 52%|█████▏    | 558/1083 [00:13<00:11, 44.54it/s] 52%|█████▏    | 563/1083 [00:13<00:11, 44.69it/s] 52%|█████▏    | 568/1083 [00:13<00:11, 44.85it/s] 53%|█████▎    | 573/1083 [00:13<00:11, 45.10it/s] 53%|█████▎    | 578/1083 [00:14<00:11, 45.30it/s] 54%|█████▍    | 583/1083 [00:14<00:11, 45.45it/s] 54%|█████▍    | 588/1083 [00:14<00:10, 45.41it/s] 55%|█████▍    | 593/1083 [00:14<00:10, 45.05it/s] 55%|█████▌    | 598/1083 [00:14<00:10, 44.74it/s] 56%|█████▌    | 603/1083 [00:14<00:10, 44.76it/s] 56%|█████▌    | 608/1083 [00:14<00:10, 44.95it/s] 57%|█████▋    | 613/1083 [00:14<00:10, 44.98it/s] 57%|█████▋    | 618/1083 [00:14<00:10, 45.17it/s] 58%|█████▊    | 623/1083 [00:15<00:10, 45.20it/s] 58%|█████▊    | 628/1083 [00:15<00:10, 45.42it/s] 58%|█████▊    | 633/1083 [00:15<00:09, 45.40it/s] 59%|█████▉    | 638/1083 [00:15<00:09, 45.25it/s] 59%|█████▉    | 643/1083 [00:15<00:19, 22.08it/s] 60%|█████▉    | 648/1083 [00:15<00:16, 26.39it/s] 60%|██████    | 653/1083 [00:16<00:14, 30.24it/s] 61%|██████    | 658/1083 [00:16<00:12, 33.62it/s] 61%|██████    | 663/1083 [00:16<00:11, 36.51it/s] 62%|██████▏   | 668/1083 [00:16<00:10, 38.89it/s] 62%|██████▏   | 673/1083 [00:16<00:10, 40.64it/s] 63%|██████▎   | 678/1083 [00:16<00:09, 41.98it/s] 63%|██████▎   | 683/1083 [00:16<00:09, 42.86it/s] 64%|██████▎   | 688/1083 [00:16<00:09, 43.17it/s] 64%|██████▍   | 693/1083 [00:16<00:08, 43.45it/s] 64%|██████▍   | 698/1083 [00:17<00:08, 43.91it/s] 65%|██████▍   | 703/1083 [00:17<00:08, 44.36it/s] 65%|██████▌   | 708/1083 [00:17<00:08, 44.81it/s] 66%|██████▌   | 713/1083 [00:17<00:08, 45.03it/s] 66%|██████▋   | 718/1083 [00:17<00:08, 45.30it/s] 67%|██████▋   | 723/1083 [00:17<00:07, 45.32it/s] 67%|██████▋   | 728/1083 [00:17<00:07, 45.20it/s] 68%|██████▊   | 733/1083 [00:17<00:07, 44.92it/s] 68%|██████▊   | 738/1083 [00:17<00:07, 44.78it/s] 69%|██████▊   | 743/1083 [00:18<00:07, 44.69it/s] 69%|██████▉   | 748/1083 [00:18<00:07, 44.88it/s] 70%|██████▉   | 753/1083 [00:18<00:07, 45.04it/s] 70%|██████▉   | 758/1083 [00:18<00:07, 45.21it/s] 70%|███████   | 763/1083 [00:18<00:08, 39.12it/s] 71%|███████   | 768/1083 [00:18<00:07, 40.90it/s] 71%|███████▏  | 773/1083 [00:18<00:07, 42.21it/s] 72%|███████▏  | 778/1083 [00:18<00:07, 43.19it/s] 72%|███████▏  | 783/1083 [00:19<00:06, 43.88it/s] 73%|███████▎  | 788/1083 [00:19<00:06, 44.40it/s] 73%|███████▎  | 793/1083 [00:19<00:06, 44.82it/s] 74%|███████▎  | 798/1083 [00:19<00:06, 44.99it/s] 74%|███████▍  | 803/1083 [00:19<00:06, 44.74it/s] 75%|███████▍  | 808/1083 [00:19<00:06, 44.58it/s] 75%|███████▌  | 813/1083 [00:19<00:06, 44.66it/s] 76%|███████▌  | 818/1083 [00:19<00:05, 44.82it/s] 76%|███████▌  | 823/1083 [00:19<00:05, 45.01it/s] 76%|███████▋  | 828/1083 [00:20<00:05, 45.19it/s] 77%|███████▋  | 833/1083 [00:20<00:05, 45.39it/s] 77%|███████▋  | 838/1083 [00:20<00:05, 45.54it/s] 78%|███████▊  | 843/1083 [00:20<00:05, 45.59it/s] 78%|███████▊  | 848/1083 [00:20<00:05, 45.32it/s] 79%|███████▉  | 853/1083 [00:20<00:05, 45.11it/s] 79%|███████▉  | 858/1083 [00:20<00:05, 44.99it/s] 80%|███████▉  | 863/1083 [00:20<00:04, 45.07it/s] 80%|████████  | 868/1083 [00:20<00:04, 45.16it/s] 81%|████████  | 873/1083 [00:21<00:04, 45.27it/s] 81%|████████  | 878/1083 [00:21<00:04, 45.40it/s] 82%|████████▏ | 883/1083 [00:21<00:04, 45.45it/s] 82%|████████▏ | 888/1083 [00:21<00:04, 45.51it/s] 82%|████████▏ | 893/1083 [00:21<00:04, 45.35it/s] 83%|████████▎ | 898/1083 [00:21<00:05, 31.74it/s] 83%|████████▎ | 903/1083 [00:21<00:05, 34.91it/s] 84%|████████▍ | 908/1083 [00:21<00:04, 37.61it/s] 84%|████████▍ | 913/1083 [00:22<00:04, 39.60it/s] 85%|████████▍ | 918/1083 [00:22<00:03, 41.30it/s] 85%|████████▌ | 923/1083 [00:22<00:03, 42.52it/s] 86%|████████▌ | 928/1083 [00:22<00:03, 43.34it/s] 86%|████████▌ | 933/1083 [00:22<00:03, 43.96it/s] 87%|████████▋ | 938/1083 [00:22<00:03, 43.96it/s] 87%|████████▋ | 943/1083 [00:22<00:03, 44.30it/s] 88%|████████▊ | 948/1083 [00:22<00:03, 44.50it/s] 88%|████████▊ | 953/1083 [00:22<00:02, 44.86it/s] 88%|████████▊ | 958/1083 [00:23<00:02, 45.09it/s] 89%|████████▉ | 963/1083 [00:23<00:02, 45.31it/s] 89%|████████▉ | 968/1083 [00:23<00:02, 45.34it/s] 90%|████████▉ | 973/1083 [00:23<00:02, 45.48it/s] 90%|█████████ | 978/1083 [00:23<00:02, 45.35it/s] 91%|█████████ | 983/1083 [00:23<00:02, 45.11it/s] 91%|█████████ | 988/1083 [00:23<00:02, 45.00it/s] 92%|█████████▏| 993/1083 [00:23<00:02, 44.87it/s] 92%|█████████▏| 998/1083 [00:23<00:01, 44.89it/s] 93%|█████████▎| 1003/1083 [00:24<00:01, 45.21it/s] 93%|█████████▎| 1008/1083 [00:24<00:01, 45.35it/s] 94%|█████████▎| 1013/1083 [00:24<00:01, 45.47it/s] 94%|█████████▍| 1018/1083 [00:24<00:01, 45.52it/s] 94%|█████████▍| 1023/1083 [00:24<00:01, 45.33it/s] 95%|█████████▍| 1028/1083 [00:25<00:02, 19.22it/s] 95%|█████████▌| 1033/1083 [00:25<00:02, 23.27it/s] 96%|█████████▌| 1038/1083 [00:25<00:01, 27.31it/s] 96%|█████████▋| 1043/1083 [00:25<00:01, 31.09it/s] 97%|█████████▋| 1048/1083 [00:25<00:01, 34.37it/s] 97%|█████████▋| 1053/1083 [00:25<00:00, 37.07it/s] 98%|█████████▊| 1058/1083 [00:25<00:00, 39.32it/s] 98%|█████████▊| 1063/1083 [00:25<00:00, 40.95it/s] 99%|█████████▊| 1068/1083 [00:25<00:00, 41.78it/s] 99%|█████████▉| 1073/1083 [00:26<00:00, 42.48it/s]100%|█████████▉| 1078/1083 [00:26<00:00, 43.22it/s]100%|██████████| 1083/1083 [00:26<00:00, 43.90it/s]100%|██████████| 1083/1083 [00:26<00:00, 41.17it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:31:21,111 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,111 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,111 >>   eval_loss               =     0.9556
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,111 >>   eval_runtime            = 0:00:26.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,111 >>   eval_samples            =       8658
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,112 >>   eval_samples_per_second =    328.929
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,112 >>   eval_steps_per_second   =     41.145
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:31:21,112 >>   perplexity              =     2.6002
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:41,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:41,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:41,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:41,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:41,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:31:41,974 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:31:41,975 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:31:42,265 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:31:43,331 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:31:43,331 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:45,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:45,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:45,866 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:45,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:31:45,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:31:46,646 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:31:46,647 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:31:47,008 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:31:47,176 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:31:47,176 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/generator/iter5/model/checkpoint-78
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/dev.jsonl', 'labels': ['characters', 'founded by', 'located in or next to body of water', 'lowest point', 'sport'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 16815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.43it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.49it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.40it/s]Extractor Predicting: 24it [00:15,  1.45it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:26,  1.60it/s]Extractor Predicting: 41it [00:26,  1.49it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.43it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.40it/s]Extractor Predicting: 49it [00:32,  1.43it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.57it/s]Extractor Predicting: 56it [00:36,  1.58it/s]Extractor Predicting: 57it [00:37,  1.59it/s]Extractor Predicting: 58it [00:38,  1.58it/s]Extractor Predicting: 59it [00:38,  1.55it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:40,  1.54it/s]Extractor Predicting: 63it [00:41,  1.62it/s]Extractor Predicting: 64it [00:41,  1.65it/s]Extractor Predicting: 65it [00:42,  1.57it/s]Extractor Predicting: 66it [00:43,  1.59it/s]Extractor Predicting: 67it [00:43,  1.58it/s]Extractor Predicting: 68it [00:44,  1.61it/s]Extractor Predicting: 69it [00:45,  1.62it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.55it/s]Extractor Predicting: 73it [00:47,  1.60it/s]Extractor Predicting: 74it [00:48,  1.61it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:50,  1.55it/s]Extractor Predicting: 78it [00:50,  1.57it/s]Extractor Predicting: 79it [00:51,  1.58it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.58it/s]Extractor Predicting: 83it [00:54,  1.58it/s]Extractor Predicting: 84it [00:54,  1.60it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:56,  1.52it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:58,  1.46it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [00:59,  1.55it/s]Extractor Predicting: 93it [01:00,  1.39it/s]Extractor Predicting: 94it [01:01,  1.44it/s]Extractor Predicting: 95it [01:02,  1.46it/s]Extractor Predicting: 96it [01:02,  1.49it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:04,  1.54it/s]Extractor Predicting: 100it [01:05,  1.48it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:06,  1.39it/s]Extractor Predicting: 103it [01:07,  1.45it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:08,  1.50it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.45it/s]Extractor Predicting: 108it [01:10,  1.49it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.53it/s]Extractor Predicting: 111it [01:12,  1.54it/s]Extractor Predicting: 112it [01:13,  1.54it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:14,  1.57it/s]Extractor Predicting: 115it [01:15,  1.53it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:16,  1.53it/s]Extractor Predicting: 118it [01:17,  1.54it/s]Extractor Predicting: 119it [01:17,  1.56it/s]Extractor Predicting: 120it [01:18,  1.57it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:19,  1.63it/s]Extractor Predicting: 123it [01:20,  1.62it/s]Extractor Predicting: 124it [01:21,  1.60it/s]Extractor Predicting: 125it [01:21,  1.55it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:23,  1.39it/s]Extractor Predicting: 128it [01:23,  1.44it/s]Extractor Predicting: 129it [01:24,  1.48it/s]Extractor Predicting: 130it [01:25,  1.51it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.48it/s]Extractor Predicting: 133it [01:27,  1.48it/s]Extractor Predicting: 134it [01:27,  1.52it/s]Extractor Predicting: 135it [01:28,  1.54it/s]Extractor Predicting: 136it [01:29,  1.54it/s]Extractor Predicting: 137it [01:29,  1.48it/s]Extractor Predicting: 138it [01:30,  1.52it/s]Extractor Predicting: 139it [01:31,  1.53it/s]Extractor Predicting: 140it [01:31,  1.55it/s]Extractor Predicting: 141it [01:32,  1.58it/s]Extractor Predicting: 142it [01:32,  1.57it/s]Extractor Predicting: 143it [01:33,  1.56it/s]Extractor Predicting: 144it [01:34,  1.58it/s]Extractor Predicting: 145it [01:34,  1.56it/s]Extractor Predicting: 146it [01:35,  1.55it/s]Extractor Predicting: 147it [01:36,  1.57it/s]Extractor Predicting: 148it [01:36,  1.59it/s]Extractor Predicting: 149it [01:37,  1.55it/s]Extractor Predicting: 150it [01:38,  1.57it/s]Extractor Predicting: 151it [01:38,  1.60it/s]Extractor Predicting: 152it [01:39,  1.54it/s]Extractor Predicting: 153it [01:40,  1.50it/s]Extractor Predicting: 154it [01:40,  1.42it/s]Extractor Predicting: 155it [01:41,  1.41it/s]Extractor Predicting: 156it [01:42,  1.41it/s]Extractor Predicting: 157it [01:43,  1.42it/s]Extractor Predicting: 158it [01:43,  1.42it/s]Extractor Predicting: 159it [01:44,  1.37it/s]Extractor Predicting: 160it [01:45,  1.39it/s]Extractor Predicting: 161it [01:45,  1.40it/s]Extractor Predicting: 162it [01:46,  1.41it/s]Extractor Predicting: 163it [01:47,  1.40it/s]Extractor Predicting: 164it [01:48,  1.37it/s]Extractor Predicting: 165it [01:48,  1.39it/s]Extractor Predicting: 166it [01:49,  1.41it/s]Extractor Predicting: 167it [01:50,  1.41it/s]Extractor Predicting: 168it [01:50,  1.44it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:52,  1.53it/s]Extractor Predicting: 171it [01:52,  1.54it/s]Extractor Predicting: 172it [01:53,  1.52it/s]Extractor Predicting: 173it [01:54,  1.52it/s]Extractor Predicting: 174it [01:54,  1.49it/s]Extractor Predicting: 175it [01:55,  1.49it/s]Extractor Predicting: 176it [01:56,  1.49it/s]Extractor Predicting: 177it [01:56,  1.48it/s]Extractor Predicting: 178it [01:57,  1.47it/s]Extractor Predicting: 179it [01:58,  1.43it/s]Extractor Predicting: 180it [01:58,  1.44it/s]Extractor Predicting: 181it [01:59,  1.47it/s]Extractor Predicting: 182it [02:00,  1.49it/s]Extractor Predicting: 183it [02:00,  1.54it/s]Extractor Predicting: 184it [02:01,  1.48it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 186it [02:02,  1.52it/s]Extractor Predicting: 187it [02:03,  1.54it/s]Extractor Predicting: 188it [02:04,  1.55it/s]Extractor Predicting: 189it [02:04,  1.53it/s]Extractor Predicting: 190it [02:05,  1.53it/s]Extractor Predicting: 191it [02:06,  1.37it/s]Extractor Predicting: 192it [02:06,  1.43it/s]Extractor Predicting: 193it [02:07,  1.47it/s]Extractor Predicting: 194it [02:08,  1.51it/s]Extractor Predicting: 195it [02:08,  1.50it/s]Extractor Predicting: 196it [02:09,  1.50it/s]Extractor Predicting: 197it [02:10,  1.53it/s]Extractor Predicting: 198it [02:10,  1.53it/s]Extractor Predicting: 199it [02:11,  1.50it/s]Extractor Predicting: 200it [02:12,  1.52it/s]Extractor Predicting: 201it [02:12,  1.55it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:14,  1.52it/s]Extractor Predicting: 204it [02:14,  1.42it/s]Extractor Predicting: 205it [02:15,  1.45it/s]Extractor Predicting: 206it [02:16,  1.48it/s]Extractor Predicting: 207it [02:16,  1.50it/s]Extractor Predicting: 208it [02:17,  1.53it/s]Extractor Predicting: 209it [02:18,  1.38it/s]Extractor Predicting: 210it [02:18,  1.43it/s]Extractor Predicting: 211it [02:19,  1.47it/s]Extractor Predicting: 212it [02:20,  1.50it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:21,  1.54it/s]Extractor Predicting: 215it [02:22,  1.58it/s]Extractor Predicting: 216it [02:22,  1.59it/s]Extractor Predicting: 217it [02:23,  1.60it/s]Extractor Predicting: 218it [02:23,  1.60it/s]Extractor Predicting: 219it [02:24,  1.55it/s]Extractor Predicting: 220it [02:25,  1.55it/s]Extractor Predicting: 221it [02:25,  1.59it/s]Extractor Predicting: 222it [02:26,  1.60it/s]Extractor Predicting: 223it [02:27,  1.59it/s]Extractor Predicting: 224it [02:27,  1.55it/s]Extractor Predicting: 225it [02:28,  1.50it/s]Extractor Predicting: 226it [02:29,  1.54it/s]Extractor Predicting: 227it [02:29,  1.56it/s]Extractor Predicting: 228it [02:30,  1.56it/s]Extractor Predicting: 229it [02:31,  1.52it/s]Extractor Predicting: 230it [02:31,  1.53it/s]Extractor Predicting: 231it [02:32,  1.53it/s]Extractor Predicting: 232it [02:33,  1.57it/s]Extractor Predicting: 233it [02:33,  1.60it/s]Extractor Predicting: 234it [02:34,  1.56it/s]Extractor Predicting: 235it [02:34,  1.55it/s]Extractor Predicting: 236it [02:35,  1.54it/s]Extractor Predicting: 237it [02:36,  1.55it/s]Extractor Predicting: 238it [02:36,  1.57it/s]Extractor Predicting: 239it [02:37,  1.48it/s]Extractor Predicting: 240it [02:38,  1.50it/s]Extractor Predicting: 241it [02:38,  1.54it/s]Extractor Predicting: 242it [02:39,  1.57it/s]Extractor Predicting: 243it [02:40,  1.55it/s]Extractor Predicting: 244it [02:40,  1.57it/s]Extractor Predicting: 245it [02:41,  1.58it/s]Extractor Predicting: 246it [02:42,  1.55it/s]Extractor Predicting: 247it [02:42,  1.56it/s]Extractor Predicting: 248it [02:43,  1.59it/s]Extractor Predicting: 249it [02:43,  1.58it/s]Extractor Predicting: 250it [02:44,  1.56it/s]Extractor Predicting: 251it [02:45,  1.43it/s]Extractor Predicting: 252it [02:46,  1.48it/s]Extractor Predicting: 253it [02:46,  1.51it/s]Extractor Predicting: 254it [02:47,  1.54it/s]Extractor Predicting: 255it [02:47,  1.53it/s]Extractor Predicting: 256it [02:48,  1.49it/s]Extractor Predicting: 257it [02:49,  1.53it/s]Extractor Predicting: 258it [02:49,  1.57it/s]Extractor Predicting: 259it [02:50,  1.56it/s]Extractor Predicting: 260it [02:51,  1.55it/s]Extractor Predicting: 261it [02:51,  1.50it/s]Extractor Predicting: 262it [02:52,  1.53it/s]Extractor Predicting: 263it [02:53,  1.51it/s]Extractor Predicting: 264it [02:53,  1.52it/s]Extractor Predicting: 265it [02:54,  1.53it/s]Extractor Predicting: 266it [02:55,  1.50it/s]Extractor Predicting: 267it [02:55,  1.52it/s]Extractor Predicting: 268it [02:56,  1.53it/s]Extractor Predicting: 269it [02:57,  1.58it/s]Extractor Predicting: 269it [02:57,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:18,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:19,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:19,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:19,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:19,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:35:19,767 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:35:19,768 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:35:20,386 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:35:21,431 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:35:21,431 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:26,937 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:27,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:27,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:27,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:35:27,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:35:27,931 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:35:27,932 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:35:28,560 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:35:28,725 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:35:28,726 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3970775095298602,
  "recall": 0.07218757218757219,
  "score": 0.12216575449569976,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12520
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12620, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.42it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.41it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.38it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.51it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.44it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.51it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.58it/s]Extractor Predicting: 40it [00:27,  1.48it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.56it/s]Extractor Predicting: 43it [00:28,  1.59it/s]Extractor Predicting: 44it [00:29,  1.57it/s]Extractor Predicting: 45it [00:30,  1.37it/s]Extractor Predicting: 46it [00:31,  1.42it/s]Extractor Predicting: 47it [00:31,  1.44it/s]Extractor Predicting: 48it [00:32,  1.47it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.46it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.53it/s]Extractor Predicting: 53it [00:35,  1.56it/s]Extractor Predicting: 54it [00:36,  1.59it/s]Extractor Predicting: 55it [00:37,  1.36it/s]Extractor Predicting: 56it [00:37,  1.45it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.54it/s]Extractor Predicting: 59it [00:39,  1.55it/s]Extractor Predicting: 60it [00:40,  1.35it/s]Extractor Predicting: 61it [00:41,  1.42it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.48it/s]Extractor Predicting: 65it [00:44,  1.38it/s]Extractor Predicting: 66it [00:44,  1.43it/s]Extractor Predicting: 67it [00:45,  1.47it/s]Extractor Predicting: 68it [00:45,  1.50it/s]Extractor Predicting: 69it [00:46,  1.54it/s]Extractor Predicting: 70it [00:47,  1.52it/s]Extractor Predicting: 71it [00:47,  1.57it/s]Extractor Predicting: 72it [00:48,  1.41it/s]Extractor Predicting: 73it [00:49,  1.46it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:50,  1.50it/s]Extractor Predicting: 76it [00:51,  1.54it/s]Extractor Predicting: 77it [00:52,  1.46it/s]Extractor Predicting: 78it [00:52,  1.53it/s]Extractor Predicting: 79it [00:53,  1.57it/s]Extractor Predicting: 80it [00:54,  1.41it/s]Extractor Predicting: 81it [00:54,  1.51it/s]Extractor Predicting: 82it [00:55,  1.55it/s]Extractor Predicting: 83it [00:55,  1.57it/s]Extractor Predicting: 84it [00:56,  1.62it/s]Extractor Predicting: 85it [00:57,  1.55it/s]Extractor Predicting: 86it [00:57,  1.55it/s]Extractor Predicting: 87it [00:58,  1.57it/s]Extractor Predicting: 88it [00:59,  1.59it/s]Extractor Predicting: 89it [00:59,  1.56it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:01,  1.53it/s]Extractor Predicting: 92it [01:01,  1.55it/s]Extractor Predicting: 93it [01:02,  1.54it/s]Extractor Predicting: 94it [01:02,  1.55it/s]Extractor Predicting: 95it [01:03,  1.56it/s]Extractor Predicting: 96it [01:04,  1.57it/s]Extractor Predicting: 97it [01:04,  1.58it/s]Extractor Predicting: 98it [01:05,  1.57it/s]Extractor Predicting: 99it [01:06,  1.57it/s]Extractor Predicting: 100it [01:06,  1.51it/s]Extractor Predicting: 101it [01:07,  1.56it/s]Extractor Predicting: 102it [01:07,  1.61it/s]Extractor Predicting: 103it [01:08,  1.61it/s]Extractor Predicting: 104it [01:09,  1.62it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:10,  1.59it/s]Extractor Predicting: 107it [01:11,  1.62it/s]Extractor Predicting: 108it [01:11,  1.70it/s]Extractor Predicting: 109it [01:12,  1.73it/s]Extractor Predicting: 110it [01:12,  1.70it/s]Extractor Predicting: 111it [01:13,  1.48it/s]Extractor Predicting: 112it [01:14,  1.53it/s]Extractor Predicting: 113it [01:14,  1.57it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.52it/s]Extractor Predicting: 116it [01:16,  1.49it/s]Extractor Predicting: 117it [01:17,  1.47it/s]Extractor Predicting: 118it [01:18,  1.42it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:20,  1.52it/s]Extractor Predicting: 122it [01:20,  1.53it/s]Extractor Predicting: 123it [01:21,  1.42it/s]Extractor Predicting: 124it [01:22,  1.48it/s]Extractor Predicting: 125it [01:23,  1.51it/s]Extractor Predicting: 126it [01:23,  1.49it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:25,  1.42it/s]Extractor Predicting: 129it [01:25,  1.48it/s]Extractor Predicting: 130it [01:26,  1.53it/s]Extractor Predicting: 131it [01:27,  1.52it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:28,  1.47it/s]Extractor Predicting: 134it [01:29,  1.50it/s]Extractor Predicting: 135it [01:29,  1.50it/s]Extractor Predicting: 136it [01:30,  1.55it/s]Extractor Predicting: 137it [01:31,  1.54it/s]Extractor Predicting: 138it [01:31,  1.52it/s]Extractor Predicting: 139it [01:32,  1.56it/s]Extractor Predicting: 140it [01:32,  1.55it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:34,  1.56it/s]Extractor Predicting: 143it [01:34,  1.66it/s]Extractor Predicting: 143it [01:34,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:25,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:25,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:25,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:25,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:25,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:37:26,776 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:37:26,777 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:37:27,645 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:37:28,903 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:37:29,046 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:31,254 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:31,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:31,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:31,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:37:31,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:37:32,582 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:37:32,634 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:37:33,052 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:37:33,512 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:37:33,513 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2717678100263852,
  "recall": 0.06035745678288895,
  "score": 0.09877727163749701,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6005
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6105, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:11,  1.30it/s]Extractor Predicting: 17it [00:11,  1.34it/s]Extractor Predicting: 18it [00:12,  1.38it/s]Extractor Predicting: 19it [00:13,  1.42it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:16,  1.53it/s]Extractor Predicting: 25it [00:17,  1.48it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:18,  1.52it/s]Extractor Predicting: 28it [00:19,  1.54it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:20,  1.53it/s]Extractor Predicting: 31it [00:21,  1.54it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:22,  1.54it/s]Extractor Predicting: 34it [00:23,  1.53it/s]Extractor Predicting: 35it [00:24,  1.29it/s]Extractor Predicting: 36it [00:24,  1.33it/s]Extractor Predicting: 37it [00:25,  1.43it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:26,  1.60it/s]Extractor Predicting: 40it [00:26,  1.66it/s]Extractor Predicting: 41it [00:27,  1.78it/s]Extractor Predicting: 42it [00:27,  1.83it/s]Extractor Predicting: 43it [00:28,  1.86it/s]Extractor Predicting: 44it [00:29,  1.88it/s]Extractor Predicting: 45it [00:29,  1.93it/s]Extractor Predicting: 46it [00:30,  1.77it/s]Extractor Predicting: 47it [00:30,  1.79it/s]Extractor Predicting: 48it [00:31,  1.83it/s]Extractor Predicting: 49it [00:31,  1.84it/s]Extractor Predicting: 50it [00:32,  1.89it/s]Extractor Predicting: 51it [00:32,  1.86it/s]Extractor Predicting: 52it [00:33,  1.69it/s]Extractor Predicting: 53it [00:34,  1.76it/s]Extractor Predicting: 54it [00:34,  1.83it/s]Extractor Predicting: 55it [00:35,  1.88it/s]Extractor Predicting: 56it [00:35,  1.90it/s]Extractor Predicting: 57it [00:36,  1.92it/s]Extractor Predicting: 58it [00:37,  1.36it/s]Extractor Predicting: 59it [00:37,  1.50it/s]Extractor Predicting: 60it [00:38,  1.57it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.74it/s]Extractor Predicting: 63it [00:39,  1.75it/s]Extractor Predicting: 64it [00:40,  1.80it/s]Extractor Predicting: 65it [00:41,  1.83it/s]Extractor Predicting: 66it [00:41,  1.89it/s]Extractor Predicting: 67it [00:42,  1.81it/s]Extractor Predicting: 68it [00:42,  1.72it/s]Extractor Predicting: 69it [00:43,  1.64it/s]Extractor Predicting: 70it [00:44,  1.58it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:45,  1.54it/s]Extractor Predicting: 72it [00:45,  1.58it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7941176470588235,
  "recall": 0.16167664670658682,
  "score": 0.2686567164179105,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/', 'labels': ['cast member', 'league', 'located on astronomical body', 'residence', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
