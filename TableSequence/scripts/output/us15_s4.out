/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_15_seed_4', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 64049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 64149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/model', pretrained_wv='outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=64149, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.854, loss:52846.6770
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.966, loss:2546.4314
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.962, loss:2218.7883
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.967, loss:2101.7949
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 0.965, loss:2067.1751
>> valid entity prec:0.4319, rec:0.4870, f1:0.4578
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 2.393, loss:1923.3378
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 0.971, loss:1863.0637
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 0.970, loss:1710.6088
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 0.970, loss:1620.8351
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 0.975, loss:1524.6384
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4627, rec:0.6429, f1:0.5381
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.338, loss:1393.3656
g_step 1200, step 1200, avg_time 0.969, loss:1328.0582
g_step 1300, step 1300, avg_time 0.974, loss:1334.1479
g_step 1400, step 1400, avg_time 0.969, loss:1249.9666
g_step 1500, step 1500, avg_time 0.966, loss:1223.8941
>> valid entity prec:0.5071, rec:0.6350, f1:0.5639
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 2.319, loss:1182.3506
g_step 1700, step 17, avg_time 1.005, loss:1161.5563
g_step 1800, step 117, avg_time 0.970, loss:1081.5027
g_step 1900, step 217, avg_time 0.971, loss:1098.4750
g_step 2000, step 317, avg_time 0.981, loss:1119.4925
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6226, rec:0.4814, f1:0.5429
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 417, avg_time 2.273, loss:1080.9680
g_step 2200, step 517, avg_time 0.974, loss:1084.6592
g_step 2300, step 617, avg_time 0.970, loss:1057.7684
g_step 2400, step 717, avg_time 0.978, loss:1052.3063
g_step 2500, step 817, avg_time 0.976, loss:1008.3032
>> valid entity prec:0.5647, rec:0.5328, f1:0.5483
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 917, avg_time 2.275, loss:1009.3656
g_step 2700, step 1017, avg_time 0.974, loss:995.3364
g_step 2800, step 1117, avg_time 0.972, loss:982.5965
g_step 2900, step 1217, avg_time 0.973, loss:1014.3983
g_step 3000, step 1317, avg_time 0.967, loss:984.1342
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5682, rec:0.6067, f1:0.5868
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 1417, avg_time 2.340, loss:998.1018
g_step 3200, step 1517, avg_time 0.978, loss:932.9907
g_step 3300, step 1617, avg_time 0.977, loss:931.5570
g_step 3400, step 34, avg_time 0.983, loss:945.4151
g_step 3500, step 134, avg_time 0.971, loss:930.2411
>> valid entity prec:0.5288, rec:0.5116, f1:0.5200
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 234, avg_time 2.283, loss:878.7628
g_step 3700, step 334, avg_time 0.976, loss:929.0257
g_step 3800, step 434, avg_time 0.977, loss:906.0317
g_step 3900, step 534, avg_time 0.974, loss:894.6487
g_step 4000, step 634, avg_time 0.978, loss:921.3218
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5374, rec:0.5235, f1:0.5303
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 734, avg_time 2.291, loss:887.6835
g_step 4200, step 834, avg_time 0.975, loss:862.6338
g_step 4300, step 934, avg_time 0.976, loss:869.2215
g_step 4400, step 1034, avg_time 0.969, loss:894.2967
g_step 4500, step 1134, avg_time 0.983, loss:902.0380
>> valid entity prec:0.5658, rec:0.5874, f1:0.5764
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 1234, avg_time 2.289, loss:860.3503
g_step 4700, step 1334, avg_time 0.968, loss:855.7876
g_step 4800, step 1434, avg_time 0.980, loss:866.1630
g_step 4900, step 1534, avg_time 0.973, loss:861.1927
g_step 5000, step 1634, avg_time 0.980, loss:905.0230
learning rate was adjusted to 0.0008
>> valid entity prec:0.5436, rec:0.5929, f1:0.5672
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 51, avg_time 2.275, loss:818.6071
g_step 5200, step 151, avg_time 0.976, loss:859.7485
g_step 5300, step 251, avg_time 0.975, loss:797.5566
g_step 5400, step 351, avg_time 0.974, loss:796.1681
g_step 5500, step 451, avg_time 0.979, loss:813.8659
>> valid entity prec:0.5762, rec:0.5668, f1:0.5715
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 551, avg_time 2.284, loss:833.4243
g_step 5700, step 651, avg_time 0.975, loss:837.0768
g_step 5800, step 751, avg_time 0.966, loss:795.9695
g_step 5900, step 851, avg_time 0.975, loss:827.1570
g_step 6000, step 951, avg_time 0.976, loss:820.7802
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5985, rec:0.4871, f1:0.5371
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1051, avg_time 2.287, loss:808.4264
g_step 6200, step 1151, avg_time 0.973, loss:841.3793
g_step 6300, step 1251, avg_time 0.976, loss:819.0482
g_step 6400, step 1351, avg_time 0.979, loss:801.5553
g_step 6500, step 1451, avg_time 0.972, loss:804.9738
>> valid entity prec:0.5654, rec:0.5223, f1:0.5430
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 1551, avg_time 2.298, loss:827.8841
g_step 6700, step 1651, avg_time 0.971, loss:802.8761
g_step 6800, step 68, avg_time 0.968, loss:787.8717
g_step 6900, step 168, avg_time 0.980, loss:811.5966
g_step 7000, step 268, avg_time 0.974, loss:778.8316
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5509, rec:0.5811, f1:0.5656
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 368, avg_time 2.298, loss:756.6336
g_step 7200, step 468, avg_time 0.978, loss:782.6883
g_step 7300, step 568, avg_time 0.978, loss:733.5947
g_step 7400, step 668, avg_time 0.975, loss:779.5822
g_step 7500, step 768, avg_time 0.971, loss:779.5854
>> valid entity prec:0.5724, rec:0.5796, f1:0.5760
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 868, avg_time 2.286, loss:771.3090
g_step 7700, step 968, avg_time 0.978, loss:834.2653
g_step 7800, step 1068, avg_time 0.980, loss:736.6163
g_step 7900, step 1168, avg_time 0.974, loss:786.2880
g_step 8000, step 1268, avg_time 0.972, loss:795.1350
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5829, rec:0.5372, f1:0.5592
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 1368, avg_time 2.283, loss:724.8960
g_step 8200, step 1468, avg_time 0.979, loss:751.8078
g_step 8300, step 1568, avg_time 0.972, loss:761.6477
g_step 8400, step 1668, avg_time 0.973, loss:775.1193
g_step 8500, step 85, avg_time 0.973, loss:710.6377
>> valid entity prec:0.5477, rec:0.6297, f1:0.5858
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 185, avg_time 2.283, loss:755.0360
g_step 8700, step 285, avg_time 0.975, loss:738.5754
g_step 8800, step 385, avg_time 0.975, loss:750.8156
g_step 8900, step 485, avg_time 0.976, loss:707.3479
g_step 9000, step 585, avg_time 0.981, loss:758.1987
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5364, rec:0.6203, f1:0.5754
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 685, avg_time 2.288, loss:749.4325
g_step 9200, step 785, avg_time 0.985, loss:725.2441
g_step 9300, step 885, avg_time 0.973, loss:733.3845
g_step 9400, step 985, avg_time 0.971, loss:723.6454
g_step 9500, step 1085, avg_time 0.977, loss:732.9677
>> valid entity prec:0.5761, rec:0.5424, f1:0.5587
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 1185, avg_time 2.290, loss:735.7354
g_step 9700, step 1285, avg_time 0.968, loss:735.6626
g_step 9800, step 1385, avg_time 0.972, loss:728.7399
g_step 9900, step 1485, avg_time 0.977, loss:731.6136
g_step 10000, step 1585, avg_time 0.964, loss:732.7334
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5935, rec:0.4987, f1:0.5420
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.36s/it]Extractor Predicting: 2it [00:08,  3.49s/it]Extractor Predicting: 3it [00:08,  2.17s/it]Extractor Predicting: 4it [00:09,  1.55s/it]Extractor Predicting: 5it [00:10,  1.24s/it]Extractor Predicting: 6it [00:10,  1.05s/it]Extractor Predicting: 7it [00:11,  1.07it/s]Extractor Predicting: 8it [00:12,  1.16it/s]Extractor Predicting: 9it [00:12,  1.20it/s]Extractor Predicting: 10it [00:13,  1.28it/s]Extractor Predicting: 11it [00:14,  1.36it/s]Extractor Predicting: 12it [00:14,  1.39it/s]Extractor Predicting: 13it [00:15,  1.37it/s]Extractor Predicting: 14it [00:16,  1.39it/s]Extractor Predicting: 15it [00:17,  1.40it/s]Extractor Predicting: 16it [00:17,  1.42it/s]Extractor Predicting: 17it [00:18,  1.41it/s]Extractor Predicting: 18it [00:19,  1.45it/s]Extractor Predicting: 19it [00:19,  1.49it/s]Extractor Predicting: 20it [00:20,  1.47it/s]Extractor Predicting: 21it [00:21,  1.46it/s]Extractor Predicting: 22it [00:21,  1.47it/s]Extractor Predicting: 23it [00:22,  1.49it/s]Extractor Predicting: 24it [00:23,  1.48it/s]Extractor Predicting: 25it [00:23,  1.44it/s]Extractor Predicting: 26it [00:24,  1.43it/s]Extractor Predicting: 27it [00:25,  1.41it/s]Extractor Predicting: 28it [00:25,  1.43it/s]Extractor Predicting: 29it [00:26,  1.43it/s]Extractor Predicting: 30it [00:27,  1.43it/s]Extractor Predicting: 31it [00:28,  1.42it/s]Extractor Predicting: 32it [00:28,  1.44it/s]Extractor Predicting: 33it [00:29,  1.47it/s]Extractor Predicting: 34it [00:29,  1.52it/s]Extractor Predicting: 35it [00:30,  1.54it/s]Extractor Predicting: 36it [00:31,  1.56it/s]Extractor Predicting: 37it [00:31,  1.56it/s]Extractor Predicting: 38it [00:32,  1.52it/s]Extractor Predicting: 39it [00:33,  1.55it/s]Extractor Predicting: 40it [00:33,  1.54it/s]Extractor Predicting: 41it [00:34,  1.52it/s]Extractor Predicting: 42it [00:35,  1.56it/s]Extractor Predicting: 43it [00:35,  1.53it/s]Extractor Predicting: 44it [00:36,  1.53it/s]Extractor Predicting: 45it [00:37,  1.54it/s]Extractor Predicting: 46it [00:37,  1.55it/s]Extractor Predicting: 47it [00:38,  1.59it/s]Extractor Predicting: 48it [00:38,  1.59it/s]Extractor Predicting: 49it [00:39,  1.59it/s]Extractor Predicting: 50it [00:40,  1.51it/s]Extractor Predicting: 51it [00:40,  1.51it/s]Extractor Predicting: 52it [00:41,  1.51it/s]Extractor Predicting: 53it [00:42,  1.50it/s]Extractor Predicting: 54it [00:42,  1.53it/s]Extractor Predicting: 55it [00:43,  1.48it/s]Extractor Predicting: 56it [00:44,  1.51it/s]Extractor Predicting: 57it [00:44,  1.55it/s]Extractor Predicting: 58it [00:45,  1.54it/s]Extractor Predicting: 59it [00:46,  1.59it/s]Extractor Predicting: 60it [00:46,  1.52it/s]Extractor Predicting: 61it [00:47,  1.55it/s]Extractor Predicting: 62it [00:48,  1.60it/s]Extractor Predicting: 63it [00:48,  1.61it/s]Extractor Predicting: 64it [00:49,  1.65it/s]Extractor Predicting: 65it [00:49,  1.57it/s]Extractor Predicting: 66it [00:50,  1.61it/s]Extractor Predicting: 67it [00:51,  1.62it/s]Extractor Predicting: 68it [00:51,  1.63it/s]Extractor Predicting: 69it [00:52,  1.66it/s]Extractor Predicting: 70it [00:53,  1.48it/s]Extractor Predicting: 71it [00:53,  1.50it/s]Extractor Predicting: 72it [00:54,  1.56it/s]Extractor Predicting: 73it [00:55,  1.54it/s]Extractor Predicting: 74it [00:55,  1.55it/s]Extractor Predicting: 75it [00:56,  1.54it/s]Extractor Predicting: 76it [00:56,  1.57it/s]Extractor Predicting: 77it [00:57,  1.61it/s]Extractor Predicting: 78it [00:58,  1.63it/s]Extractor Predicting: 79it [00:58,  1.65it/s]Extractor Predicting: 80it [00:59,  1.57it/s]Extractor Predicting: 81it [01:00,  1.56it/s]Extractor Predicting: 82it [01:00,  1.55it/s]Extractor Predicting: 83it [01:01,  1.58it/s]Extractor Predicting: 84it [01:02,  1.55it/s]Extractor Predicting: 85it [01:02,  1.52it/s]Extractor Predicting: 86it [01:03,  1.52it/s]Extractor Predicting: 87it [01:04,  1.55it/s]Extractor Predicting: 88it [01:04,  1.51it/s]Extractor Predicting: 89it [01:05,  1.45it/s]Extractor Predicting: 90it [01:06,  1.47it/s]Extractor Predicting: 91it [01:06,  1.38it/s]Extractor Predicting: 92it [01:07,  1.41it/s]Extractor Predicting: 93it [01:08,  1.43it/s]Extractor Predicting: 94it [01:09,  1.41it/s]Extractor Predicting: 95it [01:09,  1.42it/s]Extractor Predicting: 96it [01:10,  1.46it/s]Extractor Predicting: 97it [01:11,  1.46it/s]Extractor Predicting: 98it [01:11,  1.48it/s]Extractor Predicting: 99it [01:12,  1.47it/s]Extractor Predicting: 100it [01:13,  1.50it/s]Extractor Predicting: 101it [01:13,  1.51it/s]Extractor Predicting: 102it [01:14,  1.53it/s]Extractor Predicting: 103it [01:14,  1.53it/s]Extractor Predicting: 104it [01:15,  1.51it/s]Extractor Predicting: 105it [01:16,  1.54it/s]Extractor Predicting: 106it [01:16,  1.53it/s]Extractor Predicting: 107it [01:17,  1.52it/s]Extractor Predicting: 108it [01:18,  1.53it/s]Extractor Predicting: 109it [01:18,  1.48it/s]Extractor Predicting: 110it [01:19,  1.48it/s]Extractor Predicting: 111it [01:20,  1.48it/s]Extractor Predicting: 112it [01:20,  1.52it/s]Extractor Predicting: 113it [01:21,  1.53it/s]Extractor Predicting: 114it [01:22,  1.49it/s]Extractor Predicting: 115it [01:22,  1.50it/s]Extractor Predicting: 116it [01:23,  1.53it/s]Extractor Predicting: 117it [01:24,  1.53it/s]Extractor Predicting: 118it [01:24,  1.52it/s]Extractor Predicting: 119it [01:25,  1.48it/s]Extractor Predicting: 120it [01:26,  1.49it/s]Extractor Predicting: 121it [01:26,  1.49it/s]Extractor Predicting: 122it [01:27,  1.49it/s]Extractor Predicting: 123it [01:28,  1.49it/s]Extractor Predicting: 124it [01:29,  1.42it/s]Extractor Predicting: 125it [01:29,  1.46it/s]Extractor Predicting: 126it [01:30,  1.48it/s]Extractor Predicting: 127it [01:31,  1.48it/s]Extractor Predicting: 128it [01:31,  1.47it/s]Extractor Predicting: 129it [01:32,  1.44it/s]Extractor Predicting: 130it [01:33,  1.48it/s]Extractor Predicting: 131it [01:33,  1.46it/s]Extractor Predicting: 132it [01:34,  1.50it/s]Extractor Predicting: 133it [01:35,  1.52it/s]Extractor Predicting: 134it [01:35,  1.44it/s]Extractor Predicting: 135it [01:36,  1.45it/s]Extractor Predicting: 136it [01:37,  1.47it/s]Extractor Predicting: 137it [01:37,  1.50it/s]Extractor Predicting: 138it [01:38,  1.51it/s]Extractor Predicting: 139it [01:39,  1.46it/s]Extractor Predicting: 140it [01:39,  1.50it/s]Extractor Predicting: 141it [01:40,  1.56it/s]Extractor Predicting: 141it [01:40,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.40it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:21,  1.43it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.48it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.53it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.46it/s]Extractor Predicting: 49it [00:32,  1.48it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:38,  1.37it/s]Extractor Predicting: 59it [00:39,  1.42it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:41,  1.46it/s]Extractor Predicting: 64it [00:42,  1.50it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:43,  1.49it/s]Extractor Predicting: 67it [00:44,  1.53it/s]Extractor Predicting: 68it [00:45,  1.50it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:46,  1.55it/s]Extractor Predicting: 71it [00:46,  1.54it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:48,  1.46it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:49,  1.51it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.51it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:52,  1.45it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:53,  1.49it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:56,  1.50it/s]Extractor Predicting: 86it [00:57,  1.51it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:58,  1.52it/s]Extractor Predicting: 89it [00:59,  1.47it/s]Extractor Predicting: 90it [00:59,  1.48it/s]Extractor Predicting: 91it [01:00,  1.50it/s]Extractor Predicting: 92it [01:01,  1.51it/s]Extractor Predicting: 93it [01:01,  1.50it/s]Extractor Predicting: 94it [01:02,  1.43it/s]Extractor Predicting: 95it [01:03,  1.43it/s]Extractor Predicting: 96it [01:03,  1.46it/s]Extractor Predicting: 97it [01:04,  1.47it/s]Extractor Predicting: 98it [01:05,  1.49it/s]Extractor Predicting: 99it [01:05,  1.43it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:07,  1.49it/s]Extractor Predicting: 102it [01:07,  1.49it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:09,  1.42it/s]Extractor Predicting: 105it [01:10,  1.45it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:12,  1.40it/s]Extractor Predicting: 110it [01:13,  1.42it/s]Extractor Predicting: 111it [01:14,  1.43it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.50it/s]Extractor Predicting: 114it [01:16,  1.49it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:17,  1.35it/s]Extractor Predicting: 117it [01:18,  1.43it/s]Extractor Predicting: 118it [01:19,  1.45it/s]Extractor Predicting: 119it [01:19,  1.45it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.56it/s]Extractor Predicting: 123it [01:22,  1.56it/s]Extractor Predicting: 124it [01:22,  1.57it/s]Extractor Predicting: 125it [01:23,  1.51it/s]Extractor Predicting: 126it [01:24,  1.51it/s]Extractor Predicting: 127it [01:24,  1.53it/s]Extractor Predicting: 128it [01:25,  1.56it/s]Extractor Predicting: 129it [01:26,  1.53it/s]Extractor Predicting: 130it [01:26,  1.47it/s]Extractor Predicting: 131it [01:27,  1.49it/s]Extractor Predicting: 132it [01:28,  1.51it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:29,  1.52it/s]Extractor Predicting: 135it [01:31,  1.04s/it]Extractor Predicting: 136it [01:32,  1.09it/s]Extractor Predicting: 137it [01:32,  1.19it/s]Extractor Predicting: 138it [01:33,  1.25it/s]Extractor Predicting: 139it [01:34,  1.35it/s]Extractor Predicting: 140it [01:34,  1.40it/s]Extractor Predicting: 141it [01:35,  1.43it/s]Extractor Predicting: 142it [01:35,  1.49it/s]Extractor Predicting: 143it [01:36,  1.46it/s]Extractor Predicting: 144it [01:37,  1.47it/s]Extractor Predicting: 145it [01:37,  1.51it/s]Extractor Predicting: 146it [01:38,  1.51it/s]Extractor Predicting: 147it [01:39,  1.50it/s]Extractor Predicting: 148it [01:40,  1.43it/s]Extractor Predicting: 149it [01:40,  1.46it/s]Extractor Predicting: 150it [01:41,  1.50it/s]Extractor Predicting: 151it [01:41,  1.54it/s]Extractor Predicting: 152it [01:42,  1.57it/s]Extractor Predicting: 153it [01:43,  1.49it/s]Extractor Predicting: 154it [01:44,  1.48it/s]Extractor Predicting: 155it [01:44,  1.50it/s]Extractor Predicting: 156it [01:45,  1.50it/s]Extractor Predicting: 157it [01:45,  1.50it/s]Extractor Predicting: 158it [01:46,  1.46it/s]Extractor Predicting: 159it [01:47,  1.47it/s]Extractor Predicting: 160it [01:48,  1.50it/s]Extractor Predicting: 161it [01:48,  1.53it/s]Extractor Predicting: 162it [01:49,  1.55it/s]Extractor Predicting: 163it [01:49,  1.50it/s]Extractor Predicting: 164it [01:50,  1.51it/s]Extractor Predicting: 165it [01:51,  1.50it/s]Extractor Predicting: 166it [01:51,  1.51it/s]Extractor Predicting: 167it [01:52,  1.54it/s]Extractor Predicting: 168it [01:53,  1.48it/s]Extractor Predicting: 169it [01:53,  1.49it/s]Extractor Predicting: 170it [01:54,  1.50it/s]Extractor Predicting: 171it [01:55,  1.47it/s]Extractor Predicting: 172it [01:56,  1.47it/s]Extractor Predicting: 173it [01:56,  1.42it/s]Extractor Predicting: 174it [01:57,  1.41it/s]Extractor Predicting: 175it [01:58,  1.39it/s]Extractor Predicting: 176it [01:58,  1.42it/s]Extractor Predicting: 177it [01:59,  1.45it/s]Extractor Predicting: 178it [02:00,  1.42it/s]Extractor Predicting: 179it [02:00,  1.46it/s]Extractor Predicting: 180it [02:01,  1.46it/s]Extractor Predicting: 181it [02:02,  1.49it/s]Extractor Predicting: 182it [02:02,  1.50it/s]Extractor Predicting: 183it [02:03,  1.51it/s]Extractor Predicting: 184it [02:04,  1.49it/s]Extractor Predicting: 185it [02:04,  1.52it/s]Extractor Predicting: 186it [02:05,  1.55it/s]Extractor Predicting: 187it [02:06,  1.55it/s]Extractor Predicting: 188it [02:06,  1.55it/s]Extractor Predicting: 189it [02:07,  1.52it/s]Extractor Predicting: 190it [02:08,  1.53it/s]Extractor Predicting: 191it [02:08,  1.50it/s]Extractor Predicting: 192it [02:09,  1.49it/s]Extractor Predicting: 193it [02:10,  1.51it/s]Extractor Predicting: 194it [02:10,  1.51it/s]Extractor Predicting: 195it [02:11,  1.53it/s]Extractor Predicting: 196it [02:12,  1.54it/s]Extractor Predicting: 197it [02:12,  1.56it/s]Extractor Predicting: 198it [02:13,  1.55it/s]Extractor Predicting: 199it [02:14,  1.50it/s]Extractor Predicting: 200it [02:14,  1.51it/s]Extractor Predicting: 201it [02:15,  1.52it/s]Extractor Predicting: 202it [02:16,  1.55it/s]Extractor Predicting: 203it [02:16,  1.58it/s]Extractor Predicting: 204it [02:17,  1.53it/s]Extractor Predicting: 205it [02:17,  1.54it/s]Extractor Predicting: 206it [02:18,  1.55it/s]Extractor Predicting: 207it [02:19,  1.52it/s]Extractor Predicting: 208it [02:19,  1.53it/s]Extractor Predicting: 209it [02:20,  1.47it/s]Extractor Predicting: 210it [02:21,  1.49it/s]Extractor Predicting: 211it [02:21,  1.53it/s]Extractor Predicting: 212it [02:22,  1.52it/s]Extractor Predicting: 213it [02:23,  1.39it/s]Extractor Predicting: 214it [02:24,  1.39it/s]Extractor Predicting: 215it [02:24,  1.46it/s]Extractor Predicting: 216it [02:25,  1.49it/s]Extractor Predicting: 217it [02:26,  1.46it/s]Extractor Predicting: 218it [02:26,  1.49it/s]Extractor Predicting: 219it [02:27,  1.43it/s]Extractor Predicting: 220it [02:28,  1.48it/s]Extractor Predicting: 221it [02:28,  1.49it/s]Extractor Predicting: 222it [02:29,  1.53it/s]Extractor Predicting: 223it [02:30,  1.55it/s]Extractor Predicting: 224it [02:30,  1.49it/s]Extractor Predicting: 225it [02:31,  1.48it/s]Extractor Predicting: 226it [02:32,  1.45it/s]Extractor Predicting: 227it [02:32,  1.46it/s]Extractor Predicting: 228it [02:33,  1.44it/s]Extractor Predicting: 229it [02:34,  1.46it/s]Extractor Predicting: 230it [02:34,  1.49it/s]Extractor Predicting: 231it [02:35,  1.43it/s]Extractor Predicting: 232it [02:36,  1.46it/s]Extractor Predicting: 233it [02:36,  1.48it/s]Extractor Predicting: 234it [02:37,  1.49it/s]Extractor Predicting: 235it [02:38,  1.46it/s]Extractor Predicting: 236it [02:39,  1.47it/s]Extractor Predicting: 237it [02:39,  1.52it/s]Extractor Predicting: 238it [02:40,  1.50it/s]Extractor Predicting: 239it [02:40,  1.49it/s]Extractor Predicting: 240it [02:41,  1.47it/s]Extractor Predicting: 241it [02:42,  1.44it/s]Extractor Predicting: 242it [02:43,  1.48it/s]Extractor Predicting: 243it [02:43,  1.49it/s]Extractor Predicting: 244it [02:44,  1.47it/s]Extractor Predicting: 245it [02:45,  1.49it/s]Extractor Predicting: 246it [02:45,  1.47it/s]Extractor Predicting: 247it [02:46,  1.50it/s]Extractor Predicting: 248it [02:47,  1.45it/s]Extractor Predicting: 249it [02:47,  1.47it/s]Extractor Predicting: 250it [02:48,  1.48it/s]Extractor Predicting: 251it [02:49,  1.41it/s]Extractor Predicting: 252it [02:49,  1.43it/s]Extractor Predicting: 253it [02:50,  1.44it/s]Extractor Predicting: 254it [02:51,  1.49it/s]Extractor Predicting: 255it [02:51,  1.49it/s]Extractor Predicting: 256it [02:52,  1.47it/s]Extractor Predicting: 257it [02:53,  1.51it/s]Extractor Predicting: 258it [02:53,  1.51it/s]Extractor Predicting: 259it [02:54,  1.51it/s]Extractor Predicting: 260it [02:55,  1.49it/s]Extractor Predicting: 261it [02:55,  1.45it/s]Extractor Predicting: 262it [02:56,  1.47it/s]Extractor Predicting: 263it [02:57,  1.43it/s]Extractor Predicting: 264it [02:58,  1.45it/s]Extractor Predicting: 265it [02:58,  1.48it/s]Extractor Predicting: 266it [02:59,  1.46it/s]Extractor Predicting: 267it [03:00,  1.48it/s]Extractor Predicting: 268it [03:00,  1.51it/s]Extractor Predicting: 269it [03:01,  1.55it/s]Extractor Predicting: 270it [03:01,  1.57it/s]Extractor Predicting: 271it [03:02,  1.49it/s]Extractor Predicting: 272it [03:03,  1.52it/s]Extractor Predicting: 273it [03:03,  1.51it/s]Extractor Predicting: 274it [03:04,  1.52it/s]Extractor Predicting: 275it [03:05,  1.53it/s]Extractor Predicting: 276it [03:05,  1.46it/s]Extractor Predicting: 277it [03:06,  1.45it/s]Extractor Predicting: 278it [03:07,  1.51it/s]Extractor Predicting: 279it [03:08,  1.47it/s]Extractor Predicting: 280it [03:08,  1.49it/s]Extractor Predicting: 281it [03:09,  1.46it/s]Extractor Predicting: 282it [03:10,  1.49it/s]Extractor Predicting: 283it [03:10,  1.51it/s]Extractor Predicting: 284it [03:11,  1.48it/s]Extractor Predicting: 285it [03:12,  1.49it/s]Extractor Predicting: 286it [03:12,  1.46it/s]Extractor Predicting: 287it [03:13,  1.50it/s]Extractor Predicting: 288it [03:14,  1.50it/s]Extractor Predicting: 289it [03:14,  1.47it/s]Extractor Predicting: 290it [03:15,  1.53it/s]Extractor Predicting: 291it [03:16,  1.45it/s]Extractor Predicting: 292it [03:16,  1.46it/s]Extractor Predicting: 293it [03:17,  1.45it/s]Extractor Predicting: 294it [03:18,  1.43it/s]Extractor Predicting: 295it [03:18,  1.48it/s]Extractor Predicting: 296it [03:19,  1.41it/s]Extractor Predicting: 297it [03:20,  1.46it/s]Extractor Predicting: 298it [03:20,  1.43it/s]Extractor Predicting: 299it [03:21,  1.45it/s]Extractor Predicting: 300it [03:22,  1.47it/s]Extractor Predicting: 301it [03:23,  1.42it/s]Extractor Predicting: 302it [03:24,  1.28it/s]Extractor Predicting: 303it [03:24,  1.32it/s]Extractor Predicting: 304it [03:25,  1.34it/s]Extractor Predicting: 305it [03:26,  1.34it/s]Extractor Predicting: 306it [03:26,  1.36it/s]Extractor Predicting: 307it [03:27,  1.37it/s]Extractor Predicting: 308it [03:28,  1.41it/s]Extractor Predicting: 309it [03:28,  1.42it/s]Extractor Predicting: 310it [03:29,  1.36it/s]Extractor Predicting: 311it [03:30,  1.37it/s]Extractor Predicting: 312it [03:31,  1.40it/s]Extractor Predicting: 313it [03:31,  1.41it/s]Extractor Predicting: 314it [03:32,  1.40it/s]Extractor Predicting: 315it [03:33,  1.39it/s]Extractor Predicting: 316it [03:34,  1.40it/s]Extractor Predicting: 317it [03:34,  1.40it/s]Extractor Predicting: 318it [03:35,  1.42it/s]Extractor Predicting: 319it [03:36,  1.43it/s]Extractor Predicting: 320it [03:36,  1.48it/s]Extractor Predicting: 321it [03:37,  1.53it/s]Extractor Predicting: 322it [03:37,  1.62it/s]Extractor Predicting: 323it [03:38,  1.66it/s]Extractor Predicting: 324it [03:39,  1.67it/s]Extractor Predicting: 325it [03:39,  1.70it/s]Extractor Predicting: 326it [03:40,  1.67it/s]Extractor Predicting: 327it [03:40,  1.67it/s]Extractor Predicting: 328it [03:41,  1.70it/s]Extractor Predicting: 329it [03:41,  1.74it/s]Extractor Predicting: 330it [03:42,  1.74it/s]Extractor Predicting: 331it [03:43,  1.78it/s]Extractor Predicting: 332it [03:43,  1.67it/s]Extractor Predicting: 333it [03:44,  1.72it/s]Extractor Predicting: 334it [03:44,  1.73it/s]Extractor Predicting: 335it [03:45,  1.74it/s]Extractor Predicting: 336it [03:45,  1.72it/s]Extractor Predicting: 337it [03:46,  1.75it/s]Extractor Predicting: 338it [03:47,  1.65it/s]Extractor Predicting: 339it [03:47,  1.66it/s]Extractor Predicting: 340it [03:48,  1.73it/s]Extractor Predicting: 341it [03:48,  1.72it/s]Extractor Predicting: 342it [03:49,  1.74it/s]Extractor Predicting: 343it [03:50,  1.75it/s]Extractor Predicting: 344it [03:50,  1.66it/s]Extractor Predicting: 345it [03:51,  1.68it/s]Extractor Predicting: 346it [03:51,  1.69it/s]Extractor Predicting: 347it [03:52,  1.68it/s]Extractor Predicting: 348it [03:53,  1.63it/s]Extractor Predicting: 349it [03:53,  1.55it/s]Extractor Predicting: 350it [03:54,  1.52it/s]Extractor Predicting: 351it [03:55,  1.51it/s]Extractor Predicting: 352it [03:55,  1.52it/s]Extractor Predicting: 353it [03:56,  1.54it/s]Extractor Predicting: 354it [03:57,  1.50it/s]Extractor Predicting: 355it [03:57,  1.49it/s]Extractor Predicting: 356it [03:58,  1.48it/s]Extractor Predicting: 357it [03:59,  1.46it/s]Extractor Predicting: 358it [03:59,  1.49it/s]Extractor Predicting: 359it [04:00,  1.47it/s]Extractor Predicting: 360it [04:01,  1.51it/s]Extractor Predicting: 361it [04:01,  1.50it/s]Extractor Predicting: 362it [04:02,  1.54it/s]Extractor Predicting: 363it [04:03,  1.52it/s]Extractor Predicting: 364it [04:03,  1.46it/s]Extractor Predicting: 365it [04:04,  1.48it/s]Extractor Predicting: 366it [04:05,  1.49it/s]Extractor Predicting: 367it [04:05,  1.49it/s]Extractor Predicting: 368it [04:06,  1.51it/s]Extractor Predicting: 369it [04:07,  1.45it/s]Extractor Predicting: 370it [04:08,  1.45it/s]Extractor Predicting: 371it [04:08,  1.47it/s]Extractor Predicting: 372it [04:09,  1.47it/s]Extractor Predicting: 373it [04:10,  1.46it/s]Extractor Predicting: 374it [04:10,  1.45it/s]Extractor Predicting: 375it [04:11,  1.48it/s]Extractor Predicting: 376it [04:12,  1.46it/s]Extractor Predicting: 377it [04:12,  1.48it/s]Extractor Predicting: 378it [04:13,  1.46it/s]Extractor Predicting: 379it [04:14,  1.42it/s]Extractor Predicting: 380it [04:14,  1.43it/s]Extractor Predicting: 381it [04:15,  1.42it/s]Extractor Predicting: 382it [04:16,  1.42it/s]Extractor Predicting: 383it [04:17,  1.44it/s]Extractor Predicting: 384it [04:17,  1.38it/s]Extractor Predicting: 385it [04:18,  1.40it/s]Extractor Predicting: 386it [04:19,  1.43it/s]Extractor Predicting: 387it [04:19,  1.42it/s]Extractor Predicting: 388it [04:20,  1.42it/s]Extractor Predicting: 389it [04:21,  1.42it/s]Extractor Predicting: 390it [04:21,  1.44it/s]Extractor Predicting: 391it [04:22,  1.45it/s]Extractor Predicting: 392it [04:23,  1.42it/s]Extractor Predicting: 393it [04:23,  1.47it/s]Extractor Predicting: 394it [04:24,  1.42it/s]Extractor Predicting: 395it [04:25,  1.41it/s]Extractor Predicting: 396it [04:26,  1.42it/s]Extractor Predicting: 397it [04:26,  1.41it/s]Extractor Predicting: 398it [04:27,  1.43it/s]Extractor Predicting: 399it [04:28,  1.36it/s]Extractor Predicting: 400it [04:29,  1.38it/s]Extractor Predicting: 401it [04:29,  1.39it/s]Extractor Predicting: 402it [04:30,  1.42it/s]Extractor Predicting: 403it [04:31,  1.42it/s]Extractor Predicting: 404it [04:31,  1.43it/s]Extractor Predicting: 405it [04:32,  1.43it/s]Extractor Predicting: 406it [04:33,  1.47it/s]Extractor Predicting: 407it [04:33,  1.47it/s]Extractor Predicting: 408it [04:34,  1.46it/s]Extractor Predicting: 409it [04:35,  1.50it/s]Extractor Predicting: 410it [04:35,  1.50it/s]Extractor Predicting: 411it [04:36,  1.52it/s]Extractor Predicting: 412it [04:37,  1.50it/s]Extractor Predicting: 413it [04:37,  1.54it/s]Extractor Predicting: 414it [04:38,  1.50it/s]Extractor Predicting: 415it [04:39,  1.51it/s]Extractor Predicting: 416it [04:39,  1.49it/s]Extractor Predicting: 417it [04:40,  1.52it/s]Extractor Predicting: 418it [04:41,  1.35it/s]Extractor Predicting: 419it [04:42,  1.36it/s]Extractor Predicting: 420it [04:42,  1.40it/s]Extractor Predicting: 421it [04:43,  1.44it/s]Extractor Predicting: 422it [04:44,  1.43it/s]Extractor Predicting: 423it [04:44,  1.45it/s]Extractor Predicting: 424it [04:45,  1.39it/s]Extractor Predicting: 425it [04:46,  1.43it/s]Extractor Predicting: 426it [04:46,  1.43it/s]Extractor Predicting: 427it [04:47,  1.46it/s]Extractor Predicting: 428it [04:48,  1.46it/s]Extractor Predicting: 429it [04:49,  1.42it/s]Extractor Predicting: 430it [04:49,  1.77it/s]Extractor Predicting: 430it [04:49,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]> [0;32m/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py[0m(514)[0;36mpredict_step[0;34m()[0m
[0;32m    513 [0;31m        [0;31m#     import ipdb[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 514 [0;31m        [0;31m#     ipdb.set_trace()[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    515 [0;31m        [0;31m#     relation_preds = self._postprocess_relations(re_tag_logits, entity_preds, labels)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Extractor Predicting: 0it [02:57, ?it/s]

Traceback (most recent call last):
  File "wrapper.py", line 821, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 381, in main
    path_test=path_test, labels=labels_test, mode='multi', is_eval=False, model_size=model_size)
  File "wrapper.py", line 762, in run_eval
    model.predict(path_in, path_out, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/extractor.py", line 233, in predict
    data = trainer.predict(model, dataloader, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/data/joint_data.py", line 172, in predict
    outputs = model.predict_step(inputs, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/base.py", line 98, in hooked_predict_step
    rets = self._predict_step(inputs, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py", line 514, in predict_step
    #     ipdb.set_trace()
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py", line 514, in predict_step
    #     ipdb.set_trace()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

If you suspect this is an IPython 7.33.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_15_seed_4', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
train vocab size: 82795
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 82895, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/model', pretrained_wv='outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=82895, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.268, loss:51788.2216
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.961, loss:2679.3330
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.957, loss:2342.0851
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.955, loss:2282.2041
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 0.971, loss:2258.7579
>> valid entity prec:0.3240, rec:0.3826, f1:0.3509
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 3.137, loss:2173.5662
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.012, loss:2006.2409
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 0.971, loss:2022.9817
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 0.974, loss:1853.0482
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 0.967, loss:1760.4072
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.3949, rec:0.5461, f1:0.4584
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 3.103, loss:1670.9484
g_step 1200, step 1200, avg_time 0.975, loss:1576.1126
g_step 1300, step 1300, avg_time 0.960, loss:1545.4849
g_step 1400, step 1400, avg_time 0.972, loss:1515.5650
g_step 1500, step 1500, avg_time 0.951, loss:1460.0428
>> valid entity prec:0.3792, rec:0.4291, f1:0.4026
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 1600, avg_time 3.020, loss:1433.7556
g_step 1700, step 1700, avg_time 0.961, loss:1319.6221
g_step 1800, step 1800, avg_time 0.952, loss:1377.5671
g_step 1900, step 1900, avg_time 0.959, loss:1336.8093
g_step 2000, step 2000, avg_time 0.957, loss:1295.6330
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4503, rec:0.3655, f1:0.4035
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 2100, avg_time 3.027, loss:1306.9234
g_step 2200, step 2200, avg_time 0.957, loss:1361.6233
g_step 2300, step 2300, avg_time 0.964, loss:1259.2794
g_step 2400, step 2400, avg_time 0.967, loss:1273.8444
g_step 2500, step 2500, avg_time 0.984, loss:1265.2452
>> valid entity prec:0.3406, rec:0.4698, f1:0.3949
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 2600, avg_time 3.066, loss:1240.6989
g_step 2700, step 2700, avg_time 0.963, loss:1173.4209
g_step 2800, step 2800, avg_time 0.964, loss:1245.4220
g_step 2900, step 2900, avg_time 0.980, loss:1252.0937
g_step 3000, step 3000, avg_time 0.984, loss:1252.1738
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4131, rec:0.4365, f1:0.4245
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 87, avg_time 3.042, loss:1188.9855
g_step 3200, step 187, avg_time 0.969, loss:1151.7017
g_step 3300, step 287, avg_time 0.969, loss:1178.2118
g_step 3400, step 387, avg_time 0.985, loss:1191.9075
g_step 3500, step 487, avg_time 0.961, loss:1124.6323
>> valid entity prec:0.3688, rec:0.4041, f1:0.3856
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 587, avg_time 3.044, loss:1250.5893
g_step 3700, step 687, avg_time 0.985, loss:1132.7045
g_step 3800, step 787, avg_time 0.974, loss:1127.8351
g_step 3900, step 887, avg_time 0.983, loss:1127.2785
g_step 4000, step 987, avg_time 0.957, loss:1132.4703
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.3791, rec:0.5742, f1:0.4567
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 1087, avg_time 3.070, loss:1155.8485
g_step 4200, step 1187, avg_time 0.974, loss:1091.0437
g_step 4300, step 1287, avg_time 0.972, loss:1090.9186
g_step 4400, step 1387, avg_time 0.971, loss:1120.2823
g_step 4500, step 1487, avg_time 0.970, loss:1114.0341
>> valid entity prec:0.4173, rec:0.5479, f1:0.4738
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 1587, avg_time 3.106, loss:1089.1910
g_step 4700, step 1687, avg_time 0.965, loss:1107.4829
g_step 4800, step 1787, avg_time 0.964, loss:1068.3493
g_step 4900, step 1887, avg_time 0.962, loss:1083.3390
g_step 5000, step 1987, avg_time 0.979, loss:1125.3127
learning rate was adjusted to 0.0008
>> valid entity prec:0.4127, rec:0.4362, f1:0.4241
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 2087, avg_time 3.045, loss:1068.3558
g_step 5200, step 2187, avg_time 0.961, loss:1051.1991
g_step 5300, step 2287, avg_time 0.968, loss:1053.7318
g_step 5400, step 2387, avg_time 0.969, loss:1066.5339
g_step 5500, step 2487, avg_time 0.963, loss:1036.7510
>> valid entity prec:0.3647, rec:0.4486, f1:0.4023
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 2587, avg_time 3.068, loss:1009.1499
g_step 5700, step 2687, avg_time 0.962, loss:1066.9580
g_step 5800, step 2787, avg_time 0.968, loss:1073.2774
g_step 5900, step 2887, avg_time 0.965, loss:1015.6860
g_step 6000, step 2987, avg_time 0.964, loss:1041.1015
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4116, rec:0.4039, f1:0.4077
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 74, avg_time 3.046, loss:976.7991
g_step 6200, step 174, avg_time 0.971, loss:1064.5291
g_step 6300, step 274, avg_time 0.971, loss:1016.0799
g_step 6400, step 374, avg_time 0.959, loss:1004.1819
g_step 6500, step 474, avg_time 0.962, loss:985.6221
>> valid entity prec:0.3446, rec:0.4270, f1:0.3814
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 574, avg_time 3.055, loss:1022.7107
g_step 6700, step 674, avg_time 0.972, loss:1033.9548
g_step 6800, step 774, avg_time 0.965, loss:1005.5596
g_step 6900, step 874, avg_time 0.974, loss:1019.2573
g_step 7000, step 974, avg_time 0.955, loss:926.3896
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4073, rec:0.5375, f1:0.4634
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 1074, avg_time 3.066, loss:1001.9461
g_step 7200, step 1174, avg_time 0.979, loss:1039.4136
g_step 7300, step 1274, avg_time 0.966, loss:1019.6799
g_step 7400, step 1374, avg_time 0.975, loss:1042.7211
g_step 7500, step 1474, avg_time 0.976, loss:1000.2367
>> valid entity prec:0.4262, rec:0.5239, f1:0.4700
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 1574, avg_time 3.056, loss:1006.1671
g_step 7700, step 1674, avg_time 0.982, loss:955.5297
g_step 7800, step 1774, avg_time 0.968, loss:991.8122
g_step 7900, step 1874, avg_time 0.963, loss:936.9329
g_step 8000, step 1974, avg_time 0.973, loss:995.2903
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.3897, rec:0.4003, f1:0.3949
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 2074, avg_time 3.045, loss:992.6262
g_step 8200, step 2174, avg_time 0.971, loss:1005.2949
g_step 8300, step 2274, avg_time 0.971, loss:971.4847
g_step 8400, step 2374, avg_time 0.967, loss:996.3232
g_step 8500, step 2474, avg_time 0.971, loss:929.7950
>> valid entity prec:0.4469, rec:0.4637, f1:0.4552
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 2574, avg_time 3.040, loss:992.0482
g_step 8700, step 2674, avg_time 0.968, loss:978.0286
g_step 8800, step 2774, avg_time 0.978, loss:977.3971
g_step 8900, step 2874, avg_time 0.968, loss:989.1348
g_step 9000, step 2974, avg_time 0.978, loss:999.8604
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.4133, rec:0.4471, f1:0.4295
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 61, avg_time 3.061, loss:929.0120
g_step 9200, step 161, avg_time 0.972, loss:958.4305
g_step 9300, step 261, avg_time 0.968, loss:927.9503
g_step 9400, step 361, avg_time 0.970, loss:981.0124
g_step 9500, step 461, avg_time 0.983, loss:932.3928
>> valid entity prec:0.4146, rec:0.4282, f1:0.4213
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 561, avg_time 3.054, loss:897.9299
g_step 9700, step 661, avg_time 0.965, loss:898.6524
g_step 9800, step 761, avg_time 0.969, loss:925.0537
g_step 9900, step 861, avg_time 0.972, loss:916.9091
g_step 10000, step 961, avg_time 0.966, loss:928.3936
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.3984, rec:0.3999, f1:0.3991
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:06,  6.24s/it]Extractor Predicting: 2it [00:06,  2.98s/it]Extractor Predicting: 3it [00:08,  2.50s/it]Extractor Predicting: 4it [00:09,  1.76s/it]Extractor Predicting: 5it [00:10,  1.36s/it]Extractor Predicting: 6it [00:10,  1.10s/it]Extractor Predicting: 7it [00:11,  1.07it/s]Extractor Predicting: 8it [00:11,  1.21it/s]Extractor Predicting: 9it [00:12,  1.31it/s]Extractor Predicting: 10it [00:13,  1.41it/s]Extractor Predicting: 11it [00:13,  1.46it/s]Extractor Predicting: 12it [00:14,  1.50it/s]Extractor Predicting: 13it [00:15,  1.48it/s]Extractor Predicting: 14it [00:15,  1.51it/s]Extractor Predicting: 15it [00:16,  1.54it/s]Extractor Predicting: 16it [00:17,  1.53it/s]Extractor Predicting: 17it [00:17,  1.54it/s]Extractor Predicting: 18it [00:18,  1.54it/s]Extractor Predicting: 19it [00:18,  1.57it/s]Extractor Predicting: 20it [00:19,  1.58it/s]Extractor Predicting: 21it [00:20,  1.61it/s]Extractor Predicting: 22it [00:20,  1.63it/s]Extractor Predicting: 23it [00:21,  1.59it/s]Extractor Predicting: 24it [00:21,  1.65it/s]Extractor Predicting: 25it [00:22,  1.67it/s]Extractor Predicting: 26it [00:23,  1.63it/s]Extractor Predicting: 27it [00:23,  1.66it/s]Extractor Predicting: 28it [00:24,  1.58it/s]Extractor Predicting: 29it [00:25,  1.62it/s]Extractor Predicting: 30it [00:25,  1.61it/s]Extractor Predicting: 31it [00:26,  1.63it/s]Extractor Predicting: 32it [00:26,  1.64it/s]Extractor Predicting: 33it [00:27,  1.56it/s]Extractor Predicting: 34it [00:28,  1.60it/s]Extractor Predicting: 35it [00:28,  1.58it/s]Extractor Predicting: 36it [00:29,  1.58it/s]Extractor Predicting: 37it [00:30,  1.58it/s]Extractor Predicting: 38it [00:30,  1.56it/s]Extractor Predicting: 39it [00:31,  1.60it/s]Extractor Predicting: 40it [00:31,  1.66it/s]Extractor Predicting: 41it [00:32,  1.60it/s]Extractor Predicting: 42it [00:33,  1.57it/s]Extractor Predicting: 43it [00:34,  1.44it/s]Extractor Predicting: 44it [00:34,  1.42it/s]Extractor Predicting: 45it [00:35,  1.22it/s]Extractor Predicting: 46it [00:36,  1.31it/s]Extractor Predicting: 47it [00:37,  1.30it/s]Extractor Predicting: 48it [00:37,  1.38it/s]Extractor Predicting: 49it [00:38,  1.41it/s]Extractor Predicting: 50it [00:39,  1.39it/s]Extractor Predicting: 51it [00:39,  1.45it/s]Extractor Predicting: 52it [00:40,  1.38it/s]Extractor Predicting: 53it [00:41,  1.43it/s]Extractor Predicting: 54it [00:42,  1.45it/s]Extractor Predicting: 55it [00:42,  1.48it/s]Extractor Predicting: 56it [00:43,  1.48it/s]Extractor Predicting: 57it [00:44,  1.49it/s]Extractor Predicting: 58it [00:44,  1.50it/s]Extractor Predicting: 59it [00:45,  1.46it/s]Extractor Predicting: 60it [00:46,  1.49it/s]Extractor Predicting: 61it [00:46,  1.51it/s]Extractor Predicting: 62it [00:47,  1.39it/s]Extractor Predicting: 63it [00:48,  1.47it/s]Extractor Predicting: 64it [00:48,  1.40it/s]Extractor Predicting: 65it [00:49,  1.44it/s]Extractor Predicting: 66it [00:50,  1.45it/s]Extractor Predicting: 67it [00:50,  1.49it/s]Extractor Predicting: 68it [00:51,  1.51it/s]Extractor Predicting: 69it [00:52,  1.46it/s]Extractor Predicting: 70it [00:52,  1.49it/s]Extractor Predicting: 71it [00:53,  1.50it/s]Extractor Predicting: 72it [00:54,  1.53it/s]Extractor Predicting: 73it [00:54,  1.52it/s]Extractor Predicting: 74it [00:55,  1.48it/s]Extractor Predicting: 75it [00:56,  1.47it/s]Extractor Predicting: 76it [00:56,  1.51it/s]Extractor Predicting: 77it [00:57,  1.54it/s]Extractor Predicting: 78it [00:58,  1.52it/s]Extractor Predicting: 79it [00:58,  1.47it/s]Extractor Predicting: 80it [00:59,  1.49it/s]Extractor Predicting: 81it [01:00,  1.52it/s]Extractor Predicting: 82it [01:00,  1.52it/s]Extractor Predicting: 83it [01:01,  1.52it/s]Extractor Predicting: 84it [01:02,  1.49it/s]Extractor Predicting: 85it [01:02,  1.52it/s]Extractor Predicting: 86it [01:03,  1.52it/s]Extractor Predicting: 87it [01:04,  1.49it/s]Extractor Predicting: 88it [01:04,  1.51it/s]Extractor Predicting: 89it [01:05,  1.50it/s]Extractor Predicting: 90it [01:06,  1.52it/s]Extractor Predicting: 91it [01:06,  1.54it/s]Extractor Predicting: 92it [01:07,  1.55it/s]Extractor Predicting: 93it [01:08,  1.53it/s]Extractor Predicting: 94it [01:08,  1.47it/s]Extractor Predicting: 95it [01:09,  1.47it/s]Extractor Predicting: 96it [01:10,  1.47it/s]Extractor Predicting: 97it [01:10,  1.47it/s]Extractor Predicting: 98it [01:11,  1.46it/s]Extractor Predicting: 99it [01:12,  1.47it/s]Extractor Predicting: 100it [01:12,  1.47it/s]Extractor Predicting: 101it [01:13,  1.49it/s]Extractor Predicting: 102it [01:14,  1.50it/s]Extractor Predicting: 103it [01:14,  1.50it/s]Extractor Predicting: 104it [01:15,  1.50it/s]Extractor Predicting: 105it [01:16,  1.45it/s]Extractor Predicting: 106it [01:16,  1.49it/s]Extractor Predicting: 107it [01:17,  1.51it/s]Extractor Predicting: 108it [01:18,  1.53it/s]Extractor Predicting: 109it [01:18,  1.53it/s]Extractor Predicting: 110it [01:19,  1.49it/s]Extractor Predicting: 111it [01:20,  1.52it/s]Extractor Predicting: 112it [01:20,  1.52it/s]Extractor Predicting: 113it [01:21,  1.53it/s]Extractor Predicting: 114it [01:22,  1.56it/s]Extractor Predicting: 115it [01:22,  1.51it/s]Extractor Predicting: 116it [01:23,  1.54it/s]Extractor Predicting: 117it [01:24,  1.54it/s]Extractor Predicting: 118it [01:24,  1.55it/s]Extractor Predicting: 119it [01:25,  1.53it/s]Extractor Predicting: 120it [01:26,  1.48it/s]Extractor Predicting: 121it [01:26,  1.52it/s]Extractor Predicting: 122it [01:27,  1.55it/s]Extractor Predicting: 123it [01:27,  1.55it/s]Extractor Predicting: 124it [01:28,  1.55it/s]Extractor Predicting: 125it [01:29,  1.48it/s]Extractor Predicting: 126it [01:30,  1.49it/s]Extractor Predicting: 127it [01:30,  1.50it/s]Extractor Predicting: 128it [01:31,  1.53it/s]Extractor Predicting: 129it [01:31,  1.54it/s]Extractor Predicting: 130it [01:32,  1.51it/s]Extractor Predicting: 131it [01:33,  1.53it/s]Extractor Predicting: 132it [01:33,  1.53it/s]Extractor Predicting: 133it [01:34,  1.56it/s]Extractor Predicting: 134it [01:35,  1.51it/s]Extractor Predicting: 135it [01:35,  1.46it/s]Extractor Predicting: 136it [01:36,  1.49it/s]Extractor Predicting: 137it [01:37,  1.36it/s]Extractor Predicting: 138it [01:38,  1.37it/s]Extractor Predicting: 139it [01:38,  1.41it/s]Extractor Predicting: 140it [01:39,  1.38it/s]Extractor Predicting: 141it [01:40,  1.40it/s]Extractor Predicting: 142it [01:41,  1.39it/s]Extractor Predicting: 143it [01:41,  1.42it/s]Extractor Predicting: 144it [01:42,  1.43it/s]Extractor Predicting: 145it [01:43,  1.42it/s]Extractor Predicting: 146it [01:45,  1.24s/it]Extractor Predicting: 147it [01:46,  1.08s/it]Extractor Predicting: 148it [01:47,  1.02it/s]Extractor Predicting: 149it [01:47,  1.11it/s]Extractor Predicting: 150it [01:48,  1.21it/s]Extractor Predicting: 151it [01:49,  1.27it/s]Extractor Predicting: 152it [01:49,  1.33it/s]Extractor Predicting: 153it [01:50,  1.37it/s]Extractor Predicting: 154it [01:51,  1.40it/s]Extractor Predicting: 155it [01:51,  1.40it/s]Extractor Predicting: 156it [01:52,  1.44it/s]Extractor Predicting: 157it [01:53,  1.46it/s]Extractor Predicting: 158it [01:53,  1.45it/s]Extractor Predicting: 159it [01:54,  1.44it/s]Extractor Predicting: 160it [01:55,  1.47it/s]Extractor Predicting: 161it [01:55,  1.49it/s]Extractor Predicting: 162it [01:56,  1.43it/s]Extractor Predicting: 163it [01:57,  1.39it/s]Extractor Predicting: 164it [01:58,  1.40it/s]Extractor Predicting: 165it [01:58,  1.42it/s]Extractor Predicting: 166it [01:59,  1.45it/s]Extractor Predicting: 167it [02:00,  1.46it/s]Extractor Predicting: 168it [02:00,  1.39it/s]Extractor Predicting: 169it [02:01,  1.42it/s]Extractor Predicting: 170it [02:02,  1.40it/s]Extractor Predicting: 171it [02:02,  1.45it/s]Extractor Predicting: 172it [02:03,  1.46it/s]Extractor Predicting: 173it [02:04,  1.40it/s]Extractor Predicting: 174it [02:05,  1.42it/s]Extractor Predicting: 175it [02:05,  1.44it/s]Extractor Predicting: 176it [02:06,  1.45it/s]Extractor Predicting: 177it [02:07,  1.43it/s]Extractor Predicting: 178it [02:07,  1.41it/s]Extractor Predicting: 179it [02:08,  1.41it/s]Extractor Predicting: 180it [02:09,  1.41it/s]Extractor Predicting: 181it [02:10,  1.40it/s]Extractor Predicting: 182it [02:10,  1.39it/s]Extractor Predicting: 183it [02:11,  1.39it/s]Extractor Predicting: 184it [02:12,  1.45it/s]Extractor Predicting: 185it [02:12,  1.46it/s]Extractor Predicting: 186it [02:13,  1.48it/s]Extractor Predicting: 187it [02:14,  1.44it/s]Extractor Predicting: 188it [02:14,  1.47it/s]Extractor Predicting: 189it [02:15,  1.51it/s]Extractor Predicting: 190it [02:16,  1.54it/s]Extractor Predicting: 191it [02:16,  1.55it/s]Extractor Predicting: 192it [02:17,  1.53it/s]Extractor Predicting: 193it [02:18,  1.51it/s]Extractor Predicting: 194it [02:18,  1.50it/s]Extractor Predicting: 195it [02:19,  1.52it/s]Extractor Predicting: 196it [02:20,  1.53it/s]Extractor Predicting: 197it [02:20,  1.54it/s]Extractor Predicting: 198it [02:21,  1.47it/s]Extractor Predicting: 199it [02:22,  1.47it/s]Extractor Predicting: 200it [02:22,  1.50it/s]Extractor Predicting: 201it [02:23,  1.51it/s]Extractor Predicting: 202it [02:24,  1.54it/s]Extractor Predicting: 203it [02:24,  1.49it/s]Extractor Predicting: 204it [02:25,  1.50it/s]Extractor Predicting: 205it [02:26,  1.51it/s]Extractor Predicting: 206it [02:26,  1.51it/s]Extractor Predicting: 207it [02:27,  1.53it/s]Extractor Predicting: 208it [02:28,  1.48it/s]Extractor Predicting: 209it [02:28,  1.52it/s]Extractor Predicting: 210it [02:29,  1.49it/s]Extractor Predicting: 211it [02:30,  1.52it/s]Extractor Predicting: 212it [02:30,  1.51it/s]Extractor Predicting: 213it [02:31,  1.46it/s]Extractor Predicting: 214it [02:32,  1.47it/s]Extractor Predicting: 215it [02:32,  1.47it/s]Extractor Predicting: 216it [02:33,  1.45it/s]Extractor Predicting: 217it [02:34,  1.47it/s]Extractor Predicting: 218it [02:34,  1.43it/s]Extractor Predicting: 219it [02:35,  1.46it/s]Extractor Predicting: 220it [02:36,  1.51it/s]Extractor Predicting: 221it [02:36,  1.54it/s]Extractor Predicting: 222it [02:37,  1.54it/s]Extractor Predicting: 223it [02:38,  1.32it/s]Extractor Predicting: 224it [02:38,  1.53it/s]Extractor Predicting: 224it [02:38,  1.41it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 30214
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30314, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:19,  1.44it/s]Extractor Predicting: 31it [00:19,  1.50it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.45it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.43it/s]Extractor Predicting: 46it [00:30,  1.44it/s]Extractor Predicting: 47it [00:30,  1.44it/s]Extractor Predicting: 48it [00:31,  1.44it/s]Extractor Predicting: 49it [00:32,  1.47it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.56it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:38,  1.65it/s]Extractor Predicting: 61it [00:39,  1.60it/s]Extractor Predicting: 62it [00:40,  1.60it/s]Extractor Predicting: 63it [00:40,  1.61it/s]Extractor Predicting: 64it [00:41,  1.67it/s]Extractor Predicting: 65it [00:42,  1.67it/s]Extractor Predicting: 66it [00:42,  1.64it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:43,  1.63it/s]Extractor Predicting: 69it [00:44,  1.62it/s]Extractor Predicting: 70it [00:45,  1.60it/s]Extractor Predicting: 71it [00:45,  1.65it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:47,  1.61it/s]Extractor Predicting: 74it [00:47,  1.63it/s]Extractor Predicting: 75it [00:48,  1.69it/s]Extractor Predicting: 76it [00:48,  1.69it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:50,  1.61it/s]Extractor Predicting: 79it [00:50,  1.62it/s]Extractor Predicting: 80it [00:51,  1.58it/s]Extractor Predicting: 81it [00:52,  1.58it/s]Extractor Predicting: 82it [00:52,  1.53it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:54,  1.64it/s]Extractor Predicting: 86it [00:55,  1.67it/s]Extractor Predicting: 87it [00:55,  1.59it/s]Extractor Predicting: 88it [00:56,  1.60it/s]Extractor Predicting: 89it [00:57,  1.61it/s]Extractor Predicting: 90it [00:57,  1.56it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [00:59,  1.60it/s]Extractor Predicting: 94it [01:00,  1.62it/s]Extractor Predicting: 95it [01:00,  1.64it/s]Extractor Predicting: 96it [01:01,  1.65it/s]Extractor Predicting: 97it [01:01,  1.73it/s]Extractor Predicting: 98it [01:02,  1.72it/s]Extractor Predicting: 99it [01:03,  1.72it/s]Extractor Predicting: 100it [01:03,  1.73it/s]Extractor Predicting: 101it [01:04,  1.74it/s]Extractor Predicting: 102it [01:04,  1.65it/s]Extractor Predicting: 103it [01:05,  1.61it/s]Extractor Predicting: 104it [01:06,  1.65it/s]Extractor Predicting: 105it [01:06,  1.67it/s]Extractor Predicting: 106it [01:07,  1.69it/s]Extractor Predicting: 107it [01:07,  1.68it/s]Extractor Predicting: 108it [01:08,  1.68it/s]Extractor Predicting: 109it [01:09,  1.70it/s]Extractor Predicting: 110it [01:09,  1.67it/s]Extractor Predicting: 111it [01:10,  1.71it/s]Extractor Predicting: 112it [01:10,  1.73it/s]Extractor Predicting: 113it [01:11,  1.67it/s]Extractor Predicting: 114it [01:12,  1.67it/s]Extractor Predicting: 115it [01:12,  1.70it/s]Extractor Predicting: 116it [01:13,  1.67it/s]Extractor Predicting: 117it [01:13,  1.72it/s]Extractor Predicting: 118it [01:14,  1.66it/s]Extractor Predicting: 119it [01:15,  1.60it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:16,  1.62it/s]Extractor Predicting: 122it [01:16,  1.62it/s]Extractor Predicting: 123it [01:17,  1.62it/s]Extractor Predicting: 124it [01:18,  1.54it/s]Extractor Predicting: 125it [01:18,  1.58it/s]Extractor Predicting: 126it [01:19,  1.60it/s]Extractor Predicting: 127it [01:20,  1.66it/s]Extractor Predicting: 128it [01:20,  1.62it/s]Extractor Predicting: 129it [01:22,  1.06s/it]Extractor Predicting: 130it [01:23,  1.06it/s]Extractor Predicting: 131it [01:24,  1.17it/s]Extractor Predicting: 132it [01:24,  1.22it/s]Extractor Predicting: 133it [01:25,  1.31it/s]Extractor Predicting: 134it [01:26,  1.39it/s]Extractor Predicting: 135it [01:26,  1.46it/s]Extractor Predicting: 136it [01:27,  1.52it/s]Extractor Predicting: 137it [01:27,  1.49it/s]Extractor Predicting: 138it [01:28,  1.51it/s]Extractor Predicting: 139it [01:29,  1.53it/s]Extractor Predicting: 140it [01:29,  1.57it/s]Extractor Predicting: 141it [01:30,  1.50it/s]Extractor Predicting: 142it [01:31,  1.45it/s]Extractor Predicting: 143it [01:33,  1.22s/it]Extractor Predicting: 144it [01:34,  1.05s/it]Extractor Predicting: 145it [01:35,  1.08it/s]Extractor Predicting: 146it [01:35,  1.13it/s]Extractor Predicting: 147it [01:36,  1.21it/s]Extractor Predicting: 148it [01:37,  1.30it/s]Extractor Predicting: 149it [01:37,  1.36it/s]Extractor Predicting: 150it [01:38,  1.40it/s]Extractor Predicting: 151it [01:39,  1.40it/s]Extractor Predicting: 152it [01:39,  1.44it/s]Extractor Predicting: 153it [01:40,  1.49it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:41,  1.52it/s]Extractor Predicting: 156it [01:42,  1.47it/s]Extractor Predicting: 157it [01:43,  1.50it/s]Extractor Predicting: 158it [01:43,  1.52it/s]Extractor Predicting: 159it [01:44,  1.52it/s]Extractor Predicting: 160it [01:45,  1.48it/s]Extractor Predicting: 161it [01:45,  1.45it/s]Extractor Predicting: 162it [01:46,  1.46it/s]Extractor Predicting: 163it [01:47,  1.47it/s]Extractor Predicting: 164it [01:47,  1.49it/s]Extractor Predicting: 165it [01:48,  1.48it/s]Extractor Predicting: 166it [01:49,  1.43it/s]Extractor Predicting: 167it [01:49,  1.45it/s]Extractor Predicting: 168it [01:50,  1.42it/s]Extractor Predicting: 169it [01:51,  1.43it/s]Extractor Predicting: 170it [01:52,  1.46it/s]Extractor Predicting: 171it [01:53,  1.24it/s]Extractor Predicting: 172it [01:53,  1.30it/s]Extractor Predicting: 173it [01:54,  1.35it/s]Extractor Predicting: 174it [01:55,  1.39it/s]Extractor Predicting: 175it [01:55,  1.42it/s]Extractor Predicting: 176it [01:56,  1.46it/s]Extractor Predicting: 177it [01:57,  1.48it/s]Extractor Predicting: 178it [01:57,  1.52it/s]Extractor Predicting: 179it [01:58,  1.54it/s]Extractor Predicting: 180it [01:59,  1.52it/s]Extractor Predicting: 181it [01:59,  1.55it/s]Extractor Predicting: 182it [02:00,  1.60it/s]Extractor Predicting: 183it [02:00,  1.57it/s]Extractor Predicting: 184it [02:01,  1.61it/s]Extractor Predicting: 185it [02:02,  1.51it/s]Extractor Predicting: 186it [02:02,  1.53it/s]Extractor Predicting: 187it [02:03,  1.56it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:04,  1.54it/s]Extractor Predicting: 190it [02:05,  1.52it/s]Extractor Predicting: 191it [02:06,  1.54it/s]Extractor Predicting: 192it [02:06,  1.52it/s]Extractor Predicting: 193it [02:07,  1.50it/s]Extractor Predicting: 194it [02:08,  1.52it/s]Extractor Predicting: 195it [02:08,  1.57it/s]Extractor Predicting: 196it [02:09,  1.58it/s]Extractor Predicting: 197it [02:10,  1.52it/s]Extractor Predicting: 198it [02:10,  1.56it/s]Extractor Predicting: 199it [02:11,  1.59it/s]Extractor Predicting: 200it [02:11,  1.58it/s]Extractor Predicting: 201it [02:12,  1.59it/s]Extractor Predicting: 202it [02:13,  1.52it/s]Extractor Predicting: 203it [02:13,  1.53it/s]Extractor Predicting: 204it [02:14,  1.53it/s]Extractor Predicting: 205it [02:15,  1.55it/s]Extractor Predicting: 206it [02:15,  1.54it/s]Extractor Predicting: 207it [02:16,  1.50it/s]Extractor Predicting: 208it [02:17,  1.53it/s]Extractor Predicting: 209it [02:17,  1.52it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:19,  1.51it/s]Extractor Predicting: 212it [02:19,  1.50it/s]Extractor Predicting: 213it [02:20,  1.56it/s]Extractor Predicting: 214it [02:21,  1.51it/s]Extractor Predicting: 215it [02:21,  1.52it/s]Extractor Predicting: 216it [02:22,  1.52it/s]Extractor Predicting: 217it [02:23,  1.51it/s]Extractor Predicting: 218it [02:23,  1.52it/s]Extractor Predicting: 219it [02:24,  1.49it/s]Extractor Predicting: 220it [02:25,  1.53it/s]Extractor Predicting: 221it [02:25,  1.55it/s]Extractor Predicting: 222it [02:26,  1.54it/s]Extractor Predicting: 223it [02:27,  1.52it/s]Extractor Predicting: 224it [02:27,  1.56it/s]Extractor Predicting: 225it [02:28,  1.54it/s]Extractor Predicting: 226it [02:28,  1.51it/s]Extractor Predicting: 227it [02:29,  1.53it/s]Extractor Predicting: 228it [02:30,  1.55it/s]Extractor Predicting: 229it [02:30,  1.54it/s]Extractor Predicting: 230it [02:31,  1.58it/s]Extractor Predicting: 231it [02:32,  1.54it/s]Extractor Predicting: 232it [02:32,  1.51it/s]Extractor Predicting: 233it [02:33,  1.54it/s]Extractor Predicting: 234it [02:34,  1.54it/s]Extractor Predicting: 235it [02:34,  1.55it/s]Extractor Predicting: 236it [02:35,  1.55it/s]Extractor Predicting: 237it [02:36,  1.59it/s]Extractor Predicting: 238it [02:36,  1.59it/s]Extractor Predicting: 239it [02:37,  1.56it/s]Extractor Predicting: 240it [02:37,  1.54it/s]Extractor Predicting: 241it [02:38,  1.53it/s]Extractor Predicting: 242it [02:39,  1.58it/s]Extractor Predicting: 243it [02:39,  1.59it/s]Extractor Predicting: 244it [02:40,  1.55it/s]Extractor Predicting: 245it [02:41,  1.53it/s]Extractor Predicting: 246it [02:41,  1.55it/s]Extractor Predicting: 247it [02:42,  1.58it/s]Extractor Predicting: 248it [02:43,  1.56it/s]Extractor Predicting: 249it [02:43,  1.56it/s]Extractor Predicting: 250it [02:44,  1.55it/s]Extractor Predicting: 251it [02:45,  1.58it/s]Extractor Predicting: 252it [02:45,  1.56it/s]Extractor Predicting: 253it [02:46,  1.55it/s]Extractor Predicting: 254it [02:47,  1.50it/s]Extractor Predicting: 255it [02:47,  1.54it/s]Extractor Predicting: 256it [02:48,  1.54it/s]Extractor Predicting: 257it [02:48,  1.56it/s]Extractor Predicting: 258it [02:49,  1.56it/s]Extractor Predicting: 259it [02:50,  1.50it/s]Extractor Predicting: 260it [02:50,  1.51it/s]Extractor Predicting: 261it [02:51,  1.51it/s]Extractor Predicting: 262it [02:52,  1.49it/s]Extractor Predicting: 263it [02:52,  1.54it/s]Extractor Predicting: 264it [02:53,  1.48it/s]Extractor Predicting: 265it [02:54,  1.48it/s]Extractor Predicting: 266it [02:54,  1.47it/s]Extractor Predicting: 267it [02:55,  1.48it/s]Extractor Predicting: 268it [02:56,  1.45it/s]Extractor Predicting: 269it [02:57,  1.44it/s]Extractor Predicting: 270it [02:57,  1.47it/s]Extractor Predicting: 271it [02:58,  1.55it/s]Extractor Predicting: 272it [02:59,  1.51it/s]Extractor Predicting: 273it [02:59,  1.50it/s]Extractor Predicting: 274it [03:00,  1.46it/s]Extractor Predicting: 275it [03:01,  1.47it/s]Extractor Predicting: 276it [03:01,  1.49it/s]Extractor Predicting: 277it [03:02,  1.50it/s]Extractor Predicting: 278it [03:03,  1.50it/s]Extractor Predicting: 279it [03:03,  1.45it/s]Extractor Predicting: 280it [03:04,  1.47it/s]Extractor Predicting: 281it [03:05,  1.50it/s]Extractor Predicting: 282it [03:05,  1.52it/s]Extractor Predicting: 283it [03:06,  1.54it/s]Extractor Predicting: 284it [03:06,  1.55it/s]Extractor Predicting: 285it [03:07,  1.47it/s]Extractor Predicting: 286it [03:08,  1.49it/s]Extractor Predicting: 287it [03:09,  1.49it/s]Extractor Predicting: 288it [03:09,  1.48it/s]Extractor Predicting: 289it [03:10,  1.49it/s]Extractor Predicting: 290it [03:11,  1.44it/s]Extractor Predicting: 291it [03:11,  1.49it/s]Extractor Predicting: 292it [03:12,  1.46it/s]Extractor Predicting: 293it [03:13,  1.48it/s]Extractor Predicting: 294it [03:13,  1.48it/s]Extractor Predicting: 295it [03:14,  1.44it/s]Extractor Predicting: 296it [03:15,  1.46it/s]Extractor Predicting: 297it [03:15,  1.46it/s]Extractor Predicting: 298it [03:16,  1.44it/s]Extractor Predicting: 299it [03:17,  1.41it/s]Extractor Predicting: 300it [03:18,  1.36it/s]Extractor Predicting: 301it [03:18,  1.38it/s]Extractor Predicting: 302it [03:19,  1.38it/s]Extractor Predicting: 303it [03:20,  1.22it/s]Extractor Predicting: 304it [03:21,  1.30it/s]Extractor Predicting: 305it [03:21,  1.37it/s]Extractor Predicting: 306it [03:22,  1.42it/s]Extractor Predicting: 307it [03:23,  1.43it/s]Extractor Predicting: 308it [03:23,  1.46it/s]Extractor Predicting: 309it [03:24,  1.42it/s]Extractor Predicting: 310it [03:25,  1.41it/s]Extractor Predicting: 311it [03:26,  1.41it/s]Extractor Predicting: 312it [03:26,  1.44it/s]Extractor Predicting: 313it [03:27,  1.43it/s]Extractor Predicting: 314it [03:28,  1.38it/s]Extractor Predicting: 315it [03:28,  1.42it/s]Extractor Predicting: 316it [03:29,  1.42it/s]Extractor Predicting: 317it [03:30,  1.43it/s]Extractor Predicting: 318it [03:30,  1.44it/s]Extractor Predicting: 319it [03:31,  1.32it/s]Extractor Predicting: 320it [03:32,  1.36it/s]Extractor Predicting: 321it [03:33,  1.36it/s]Extractor Predicting: 322it [03:34,  1.33it/s]Extractor Predicting: 323it [03:34,  1.31it/s]Extractor Predicting: 324it [03:35,  1.33it/s]Extractor Predicting: 325it [03:36,  1.35it/s]Extractor Predicting: 326it [03:37,  1.36it/s]Extractor Predicting: 327it [03:37,  1.36it/s]Extractor Predicting: 328it [03:38,  1.35it/s]Extractor Predicting: 329it [03:39,  1.38it/s]Extractor Predicting: 330it [03:39,  1.41it/s]Extractor Predicting: 331it [03:40,  1.43it/s]Extractor Predicting: 332it [03:41,  1.40it/s]Extractor Predicting: 333it [03:42,  1.39it/s]Extractor Predicting: 334it [03:42,  1.42it/s]Extractor Predicting: 335it [03:43,  1.44it/s]Extractor Predicting: 336it [03:44,  1.39it/s]Extractor Predicting: 337it [03:44,  1.39it/s]Extractor Predicting: 338it [03:45,  1.39it/s]Extractor Predicting: 339it [03:46,  1.39it/s]Extractor Predicting: 340it [03:47,  1.38it/s]Extractor Predicting: 341it [03:47,  1.38it/s]Extractor Predicting: 342it [03:48,  1.39it/s]Extractor Predicting: 343it [03:49,  1.31it/s]Extractor Predicting: 344it [03:50,  1.34it/s]Extractor Predicting: 345it [03:50,  1.38it/s]Extractor Predicting: 346it [03:51,  1.41it/s]Extractor Predicting: 347it [03:52,  1.44it/s]Extractor Predicting: 348it [03:52,  1.44it/s]Extractor Predicting: 349it [03:53,  1.43it/s]Extractor Predicting: 350it [03:54,  1.44it/s]Extractor Predicting: 351it [03:54,  1.44it/s]Extractor Predicting: 352it [03:55,  1.47it/s]Extractor Predicting: 353it [03:56,  1.47it/s]Extractor Predicting: 354it [03:56,  1.52it/s]Extractor Predicting: 355it [03:57,  1.50it/s]Extractor Predicting: 356it [03:58,  1.52it/s]Extractor Predicting: 357it [03:58,  1.55it/s]Extractor Predicting: 358it [03:59,  1.51it/s]Extractor Predicting: 359it [04:00,  1.51it/s]Extractor Predicting: 360it [04:00,  1.56it/s]Extractor Predicting: 361it [04:01,  1.54it/s]Extractor Predicting: 362it [04:02,  1.54it/s]Extractor Predicting: 363it [04:02,  1.49it/s]Extractor Predicting: 364it [04:03,  1.54it/s]Extractor Predicting: 365it [04:03,  1.55it/s]Extractor Predicting: 366it [04:04,  1.56it/s]Extractor Predicting: 367it [04:05,  1.59it/s]Extractor Predicting: 368it [04:05,  1.54it/s]Extractor Predicting: 369it [04:06,  1.53it/s]Extractor Predicting: 370it [04:07,  1.52it/s]Extractor Predicting: 371it [04:07,  1.46it/s]Extractor Predicting: 372it [04:08,  1.52it/s]Extractor Predicting: 373it [04:09,  1.49it/s]Extractor Predicting: 374it [04:09,  1.49it/s]Extractor Predicting: 375it [04:10,  1.49it/s]Extractor Predicting: 376it [04:11,  1.49it/s]Extractor Predicting: 377it [04:11,  1.49it/s]Extractor Predicting: 378it [04:12,  1.52it/s]Extractor Predicting: 379it [04:13,  1.59it/s]Extractor Predicting: 380it [04:13,  1.63it/s]Extractor Predicting: 381it [04:14,  1.60it/s]Extractor Predicting: 382it [04:15,  1.58it/s]Extractor Predicting: 383it [04:15,  1.51it/s]Extractor Predicting: 384it [04:16,  1.57it/s]Extractor Predicting: 385it [04:16,  1.63it/s]Extractor Predicting: 386it [04:17,  1.61it/s]Extractor Predicting: 387it [04:18,  1.66it/s]Extractor Predicting: 388it [04:18,  1.59it/s]Extractor Predicting: 389it [04:19,  1.57it/s]Extractor Predicting: 390it [04:20,  1.59it/s]Extractor Predicting: 391it [04:20,  1.61it/s]Extractor Predicting: 392it [04:21,  1.58it/s]Extractor Predicting: 393it [04:21,  1.57it/s]Extractor Predicting: 394it [04:22,  1.60it/s]Extractor Predicting: 395it [04:23,  1.58it/s]Extractor Predicting: 396it [04:23,  1.58it/s]Extractor Predicting: 397it [04:24,  1.60it/s]Extractor Predicting: 398it [04:25,  1.55it/s]Extractor Predicting: 399it [04:25,  1.56it/s]Extractor Predicting: 400it [04:26,  1.59it/s]Extractor Predicting: 401it [04:26,  1.62it/s]Extractor Predicting: 402it [04:27,  1.61it/s]Extractor Predicting: 403it [04:28,  1.58it/s]Extractor Predicting: 404it [04:28,  1.55it/s]Extractor Predicting: 405it [04:29,  1.56it/s]Extractor Predicting: 406it [04:30,  1.53it/s]Extractor Predicting: 407it [04:30,  1.48it/s]Extractor Predicting: 408it [04:31,  1.49it/s]Extractor Predicting: 409it [04:32,  1.54it/s]Extractor Predicting: 410it [04:32,  1.52it/s]Extractor Predicting: 411it [04:33,  1.53it/s]Extractor Predicting: 412it [04:34,  1.52it/s]Extractor Predicting: 413it [04:34,  1.47it/s]Extractor Predicting: 414it [04:35,  1.52it/s]Extractor Predicting: 415it [04:36,  1.55it/s]Extractor Predicting: 416it [04:36,  1.50it/s]Extractor Predicting: 417it [04:37,  1.49it/s]Extractor Predicting: 418it [04:38,  1.52it/s]Extractor Predicting: 419it [04:38,  1.55it/s]Extractor Predicting: 420it [04:39,  1.61it/s]Extractor Predicting: 421it [04:40,  1.55it/s]Extractor Predicting: 422it [04:40,  1.62it/s]Extractor Predicting: 423it [04:41,  1.65it/s]Extractor Predicting: 424it [04:41,  1.63it/s]Extractor Predicting: 425it [04:42,  1.66it/s]Extractor Predicting: 426it [04:43,  1.56it/s]Extractor Predicting: 427it [04:44,  1.33it/s]Extractor Predicting: 428it [04:44,  1.35it/s]Extractor Predicting: 429it [04:45,  1.39it/s]Extractor Predicting: 430it [04:46,  1.38it/s]Extractor Predicting: 431it [04:46,  1.44it/s]Extractor Predicting: 432it [04:47,  1.48it/s]Extractor Predicting: 433it [04:48,  1.48it/s]Extractor Predicting: 434it [04:48,  1.48it/s]Extractor Predicting: 435it [04:49,  1.45it/s]Extractor Predicting: 436it [04:50,  1.47it/s]Extractor Predicting: 437it [04:50,  1.46it/s]Extractor Predicting: 438it [04:51,  1.45it/s]Extractor Predicting: 439it [04:52,  1.44it/s]Extractor Predicting: 440it [04:53,  1.39it/s]Extractor Predicting: 441it [04:53,  1.45it/s]Extractor Predicting: 442it [04:54,  1.47it/s]Extractor Predicting: 443it [04:55,  1.51it/s]Extractor Predicting: 444it [04:55,  1.51it/s]Extractor Predicting: 445it [04:56,  1.44it/s]Extractor Predicting: 446it [04:57,  1.44it/s]Extractor Predicting: 447it [04:57,  1.46it/s]Extractor Predicting: 448it [04:58,  1.48it/s]Extractor Predicting: 449it [04:59,  1.49it/s]Extractor Predicting: 450it [04:59,  1.51it/s]Extractor Predicting: 451it [05:00,  1.48it/s]Extractor Predicting: 452it [05:01,  1.49it/s]Extractor Predicting: 453it [05:01,  1.50it/s]Extractor Predicting: 454it [05:02,  1.49it/s]Extractor Predicting: 455it [05:03,  1.46it/s]Extractor Predicting: 456it [05:03,  1.48it/s]Extractor Predicting: 457it [05:04,  1.46it/s]Extractor Predicting: 458it [05:05,  1.48it/s]Extractor Predicting: 459it [05:05,  1.52it/s]Extractor Predicting: 460it [05:06,  1.47it/s]Extractor Predicting: 461it [05:07,  1.48it/s]Extractor Predicting: 462it [05:07,  1.48it/s]Extractor Predicting: 463it [05:08,  1.50it/s]Extractor Predicting: 464it [05:09,  1.51it/s]Extractor Predicting: 465it [05:09,  1.53it/s]Extractor Predicting: 466it [05:10,  1.53it/s]Extractor Predicting: 467it [05:11,  1.47it/s]Extractor Predicting: 468it [05:11,  1.50it/s]Extractor Predicting: 469it [05:12,  1.48it/s]Extractor Predicting: 470it [05:13,  1.48it/s]Extractor Predicting: 471it [05:13,  1.49it/s]Extractor Predicting: 472it [05:14,  1.48it/s]Extractor Predicting: 473it [05:15,  1.49it/s]Extractor Predicting: 474it [05:15,  1.48it/s]Extractor Predicting: 475it [05:16,  1.52it/s]Extractor Predicting: 476it [05:17,  1.53it/s]Extractor Predicting: 477it [05:17,  1.52it/s]Extractor Predicting: 478it [05:18,  1.56it/s]Extractor Predicting: 479it [05:19,  1.56it/s]Extractor Predicting: 480it [05:19,  1.52it/s]Extractor Predicting: 481it [05:20,  1.47it/s]Extractor Predicting: 482it [05:21,  1.38it/s]Extractor Predicting: 483it [05:22,  1.37it/s]Extractor Predicting: 484it [05:22,  1.38it/s]Extractor Predicting: 485it [05:23,  1.39it/s]Extractor Predicting: 486it [05:24,  1.44it/s]Extractor Predicting: 487it [05:24,  1.46it/s]Extractor Predicting: 488it [05:25,  1.51it/s]Extractor Predicting: 489it [05:26,  1.48it/s]Extractor Predicting: 490it [05:26,  1.49it/s]Extractor Predicting: 491it [05:27,  1.50it/s]Extractor Predicting: 492it [05:28,  1.46it/s]Extractor Predicting: 493it [05:28,  1.47it/s]Extractor Predicting: 494it [05:29,  1.47it/s]Extractor Predicting: 495it [05:30,  1.50it/s]Extractor Predicting: 496it [05:30,  1.54it/s]Extractor Predicting: 497it [05:31,  1.53it/s]Extractor Predicting: 498it [05:32,  1.51it/s]Extractor Predicting: 499it [05:32,  1.49it/s]Extractor Predicting: 500it [05:33,  1.50it/s]Extractor Predicting: 501it [05:34,  1.50it/s]Extractor Predicting: 502it [05:34,  1.48it/s]Extractor Predicting: 503it [05:35,  1.53it/s]Extractor Predicting: 504it [05:36,  1.57it/s]Extractor Predicting: 505it [05:36,  1.58it/s]Extractor Predicting: 506it [05:37,  1.54it/s]Extractor Predicting: 507it [05:38,  1.25it/s]Extractor Predicting: 508it [05:39,  1.32it/s]Extractor Predicting: 509it [05:39,  1.36it/s]Extractor Predicting: 510it [05:40,  1.43it/s]Extractor Predicting: 511it [05:41,  1.47it/s]Extractor Predicting: 512it [05:41,  1.46it/s]Extractor Predicting: 513it [05:42,  1.46it/s]Extractor Predicting: 514it [05:43,  1.44it/s]Extractor Predicting: 515it [05:43,  1.47it/s]Extractor Predicting: 516it [05:44,  1.48it/s]Extractor Predicting: 517it [05:45,  1.40it/s]Extractor Predicting: 518it [05:45,  1.43it/s]Extractor Predicting: 519it [05:46,  1.44it/s]Extractor Predicting: 520it [05:47,  1.48it/s]Extractor Predicting: 521it [05:47,  1.50it/s]Extractor Predicting: 522it [05:48,  1.46it/s]Extractor Predicting: 523it [05:49,  1.51it/s]Extractor Predicting: 524it [05:49,  1.51it/s]Extractor Predicting: 525it [05:50,  1.48it/s]Extractor Predicting: 526it [05:51,  1.47it/s]Extractor Predicting: 527it [05:52,  1.41it/s]Extractor Predicting: 528it [05:52,  1.44it/s]Extractor Predicting: 529it [05:53,  1.43it/s]Extractor Predicting: 530it [05:54,  1.43it/s]Extractor Predicting: 531it [05:54,  1.43it/s]Extractor Predicting: 532it [05:55,  1.39it/s]Extractor Predicting: 533it [05:56,  1.41it/s]Extractor Predicting: 534it [05:56,  1.44it/s]Extractor Predicting: 535it [05:57,  1.44it/s]Extractor Predicting: 536it [05:58,  1.26it/s]Extractor Predicting: 537it [05:59,  1.28it/s]Extractor Predicting: 538it [06:00,  1.32it/s]Extractor Predicting: 539it [06:00,  1.35it/s]Extractor Predicting: 540it [06:01,  1.38it/s]Extractor Predicting: 541it [06:02,  1.37it/s]Extractor Predicting: 542it [06:03,  1.34it/s]Extractor Predicting: 543it [06:03,  1.38it/s]Extractor Predicting: 544it [06:04,  1.36it/s]Extractor Predicting: 545it [06:05,  1.38it/s]Extractor Predicting: 546it [06:05,  1.40it/s]Extractor Predicting: 547it [06:06,  1.35it/s]Extractor Predicting: 548it [06:07,  1.39it/s]Extractor Predicting: 549it [06:08,  1.39it/s]Extractor Predicting: 550it [06:08,  1.43it/s]Extractor Predicting: 551it [06:09,  1.45it/s]Extractor Predicting: 552it [06:10,  1.46it/s]Extractor Predicting: 553it [06:10,  1.50it/s]Extractor Predicting: 554it [06:11,  1.51it/s]Extractor Predicting: 555it [06:12,  1.45it/s]Extractor Predicting: 556it [06:12,  1.48it/s]Extractor Predicting: 556it [06:12,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.42it/s]Extractor Predicting: 21it [00:14,  1.37it/s]Extractor Predicting: 22it [00:14,  1.39it/s]Extractor Predicting: 23it [00:15,  1.36it/s]Extractor Predicting: 24it [00:16,  1.37it/s]Extractor Predicting: 25it [00:16,  1.40it/s]Extractor Predicting: 26it [00:17,  1.38it/s]Extractor Predicting: 27it [00:18,  1.40it/s]Extractor Predicting: 28it [00:19,  1.38it/s]Extractor Predicting: 29it [00:19,  1.37it/s]Extractor Predicting: 30it [00:20,  1.40it/s]Extractor Predicting: 31it [00:21,  1.40it/s]Extractor Predicting: 32it [00:22,  1.36it/s]Extractor Predicting: 33it [00:22,  1.42it/s]Extractor Predicting: 34it [00:23,  1.48it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.45it/s]Extractor Predicting: 40it [00:27,  1.47it/s]Extractor Predicting: 41it [00:27,  1.46it/s]Extractor Predicting: 42it [00:28,  1.44it/s]Extractor Predicting: 43it [00:29,  1.46it/s]Extractor Predicting: 44it [00:30,  1.42it/s]Extractor Predicting: 45it [00:30,  1.44it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:32,  1.47it/s]Extractor Predicting: 48it [00:32,  1.51it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:34,  1.47it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.47it/s]Extractor Predicting: 53it [00:36,  1.53it/s]Extractor Predicting: 54it [00:36,  1.60it/s]Extractor Predicting: 55it [00:37,  1.63it/s]Extractor Predicting: 56it [00:37,  1.72it/s]Extractor Predicting: 57it [00:38,  1.78it/s]Extractor Predicting: 58it [00:38,  1.79it/s]Extractor Predicting: 59it [00:39,  1.80it/s]Extractor Predicting: 60it [00:39,  1.80it/s]Extractor Predicting: 61it [00:40,  1.69it/s]Extractor Predicting: 62it [00:41,  1.73it/s]Extractor Predicting: 63it [00:41,  1.75it/s]Extractor Predicting: 64it [00:42,  1.78it/s]Extractor Predicting: 65it [00:42,  1.82it/s]Extractor Predicting: 66it [00:43,  1.77it/s]Extractor Predicting: 67it [00:43,  1.71it/s]Extractor Predicting: 68it [00:44,  1.75it/s]Extractor Predicting: 69it [00:45,  1.79it/s]Extractor Predicting: 70it [00:45,  1.83it/s]Extractor Predicting: 71it [00:46,  1.81it/s]Extractor Predicting: 72it [00:46,  1.79it/s]Extractor Predicting: 73it [00:47,  1.79it/s]Extractor Predicting: 74it [00:47,  1.75it/s]Extractor Predicting: 75it [00:48,  1.78it/s]Extractor Predicting: 76it [00:48,  1.75it/s]Extractor Predicting: 77it [00:49,  1.75it/s]Extractor Predicting: 78it [00:50,  1.78it/s]Extractor Predicting: 79it [00:50,  1.73it/s]Extractor Predicting: 80it [00:51,  1.75it/s]Extractor Predicting: 81it [00:51,  1.77it/s]Extractor Predicting: 82it [00:52,  1.73it/s]Extractor Predicting: 83it [00:53,  1.63it/s]Extractor Predicting: 84it [00:53,  1.51it/s]Extractor Predicting: 85it [00:54,  1.46it/s]Extractor Predicting: 86it [00:55,  1.43it/s]Extractor Predicting: 87it [00:56,  1.42it/s]Extractor Predicting: 88it [00:56,  1.43it/s]Extractor Predicting: 89it [00:57,  1.36it/s]Extractor Predicting: 90it [00:58,  1.38it/s]Extractor Predicting: 91it [00:58,  1.39it/s]Extractor Predicting: 92it [00:59,  1.39it/s]Extractor Predicting: 93it [01:00,  1.39it/s]Extractor Predicting: 94it [01:01,  1.35it/s]Extractor Predicting: 95it [01:01,  1.37it/s]Extractor Predicting: 96it [01:02,  1.37it/s]Extractor Predicting: 97it [01:03,  1.38it/s]Extractor Predicting: 98it [01:04,  1.41it/s]Extractor Predicting: 99it [01:04,  1.36it/s]Extractor Predicting: 100it [01:05,  1.39it/s]Extractor Predicting: 101it [01:06,  1.43it/s]Extractor Predicting: 102it [01:06,  1.44it/s]Extractor Predicting: 103it [01:07,  1.43it/s]Extractor Predicting: 104it [01:08,  1.26it/s]Extractor Predicting: 104it [01:08,  1.52it/s]
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_15_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_15_seed_4', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/fewrel/unseen_15_seed_4/generator/synthetic.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 22430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/model', pretrained_wv='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22530, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.494, loss:60435.0471
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.081, loss:2365.6460
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.055, loss:1976.2697
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.093, loss:1861.9682
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.080, loss:1768.6654
>> valid entity prec:0.5074, rec:0.3648, f1:0.4245
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.058, loss:1700.6117
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.079, loss:1589.3446
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 300, avg_time 1.092, loss:1510.5667
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 400, avg_time 1.064, loss:1397.8044
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 500, avg_time 1.107, loss:1418.2119
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6188, rec:0.4285, f1:0.5063
>> valid relation prec:0.9024, rec:0.0107, f1:0.0212
>> valid relation with NER prec:0.9024, rec:0.0107, f1:0.0212
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 1.067, loss:1323.0145
g_step 1200, step 200, avg_time 1.073, loss:1337.2920
g_step 1300, step 300, avg_time 1.068, loss:1300.6193
g_step 1400, step 400, avg_time 1.087, loss:1296.5656
g_step 1500, step 500, avg_time 1.094, loss:1234.4017
>> valid entity prec:0.5851, rec:0.6196, f1:0.6019
>> valid relation prec:0.4770, rec:0.0543, f1:0.0975
>> valid relation with NER prec:0.4770, rec:0.0543, f1:0.0975
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.071, loss:1212.4708
g_step 1700, step 200, avg_time 1.072, loss:1226.7835
g_step 1800, step 300, avg_time 1.092, loss:1209.8910
g_step 1900, step 400, avg_time 1.071, loss:1185.7730
g_step 2000, step 500, avg_time 1.075, loss:1184.8961
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5785, rec:0.5985, f1:0.5883
>> valid relation prec:0.2228, rec:0.0261, f1:0.0468
>> valid relation with NER prec:0.2228, rec:0.0261, f1:0.0468
g_step 2100, step 100, avg_time 1.076, loss:1119.7778
g_step 2200, step 200, avg_time 1.081, loss:1172.4568
g_step 2300, step 300, avg_time 1.077, loss:1184.9506
g_step 2400, step 400, avg_time 1.065, loss:1120.6595
g_step 2500, step 500, avg_time 1.084, loss:1132.8952
>> valid entity prec:0.5334, rec:0.6265, f1:0.5762
>> valid relation prec:0.3263, rec:0.0633, f1:0.1060
>> valid relation with NER prec:0.3263, rec:0.0633, f1:0.1060
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 100, avg_time 1.145, loss:1083.8209
g_step 2700, step 200, avg_time 1.063, loss:1096.6249
g_step 2800, step 300, avg_time 1.051, loss:1072.3070
g_step 2900, step 400, avg_time 1.070, loss:1138.7547
g_step 3000, step 500, avg_time 1.073, loss:1152.6313
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6113, rec:0.5702, f1:0.5901
>> valid relation prec:0.2779, rec:0.0607, f1:0.0996
>> valid relation with NER prec:0.2779, rec:0.0607, f1:0.0996
g_step 3100, step 100, avg_time 1.078, loss:1068.3797
g_step 3200, step 200, avg_time 1.085, loss:1066.4593
g_step 3300, step 300, avg_time 1.066, loss:1079.1857
g_step 3400, step 400, avg_time 1.081, loss:1048.2067
g_step 3500, step 500, avg_time 1.076, loss:1085.7780
>> valid entity prec:0.5507, rec:0.5715, f1:0.5609
>> valid relation prec:0.3072, rec:0.0647, f1:0.1069
>> valid relation with NER prec:0.3072, rec:0.0647, f1:0.1069
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 100, avg_time 1.056, loss:1025.7232
g_step 3700, step 200, avg_time 1.054, loss:1016.0093
g_step 3800, step 300, avg_time 1.075, loss:1098.2169
g_step 3900, step 400, avg_time 1.061, loss:1019.7321
g_step 4000, step 500, avg_time 1.065, loss:1074.9382
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6286, rec:0.5585, f1:0.5915
>> valid relation prec:0.2997, rec:0.0612, f1:0.1017
>> valid relation with NER prec:0.2997, rec:0.0612, f1:0.1017
g_step 4100, step 100, avg_time 1.052, loss:1005.8110
g_step 4200, step 200, avg_time 1.062, loss:1022.6500
g_step 4300, step 300, avg_time 1.063, loss:1017.8298
g_step 4400, step 400, avg_time 1.052, loss:1030.0517
g_step 4500, step 500, avg_time 1.075, loss:1016.3809
>> valid entity prec:0.5385, rec:0.6129, f1:0.5733
>> valid relation prec:0.3345, rec:0.0813, f1:0.1308
>> valid relation with NER prec:0.3345, rec:0.0813, f1:0.1308
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 100, avg_time 1.075, loss:980.1989
g_step 4700, step 200, avg_time 1.071, loss:987.5594
g_step 4800, step 300, avg_time 1.086, loss:1008.9436
g_step 4900, step 400, avg_time 1.084, loss:1008.4066
g_step 5000, step 500, avg_time 1.073, loss:992.6890
learning rate was adjusted to 0.0008
>> valid entity prec:0.5603, rec:0.6085, f1:0.5834
>> valid relation prec:0.2767, rec:0.0601, f1:0.0987
>> valid relation with NER prec:0.2767, rec:0.0601, f1:0.0987
g_step 5100, step 100, avg_time 1.080, loss:968.1514
g_step 5200, step 200, avg_time 1.106, loss:968.9099
g_step 5300, step 300, avg_time 1.080, loss:971.4542
g_step 5400, step 400, avg_time 1.095, loss:980.6382
g_step 5500, step 500, avg_time 1.071, loss:961.7489
>> valid entity prec:0.5932, rec:0.5676, f1:0.5801
>> valid relation prec:0.2557, rec:0.0586, f1:0.0954
>> valid relation with NER prec:0.2557, rec:0.0586, f1:0.0954
g_step 5600, step 100, avg_time 1.090, loss:942.6370
g_step 5700, step 200, avg_time 1.107, loss:936.2324
g_step 5800, step 300, avg_time 1.076, loss:930.2038
g_step 5900, step 400, avg_time 1.084, loss:967.3270
g_step 6000, step 500, avg_time 1.086, loss:966.3263
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5088, rec:0.5344, f1:0.5213
>> valid relation prec:0.2525, rec:0.0743, f1:0.1148
>> valid relation with NER prec:0.2525, rec:0.0743, f1:0.1148
g_step 6100, step 100, avg_time 1.081, loss:889.2488
g_step 6200, step 200, avg_time 1.071, loss:916.8105
g_step 6300, step 300, avg_time 1.095, loss:947.8209
g_step 6400, step 400, avg_time 1.090, loss:917.2300
g_step 6500, step 500, avg_time 1.099, loss:944.8246
>> valid entity prec:0.5839, rec:0.5997, f1:0.5917
>> valid relation prec:0.3048, rec:0.0662, f1:0.1088
>> valid relation with NER prec:0.3048, rec:0.0662, f1:0.1088
g_step 6600, step 100, avg_time 1.060, loss:889.0235
g_step 6700, step 200, avg_time 1.251, loss:876.4165
g_step 6800, step 300, avg_time 1.103, loss:913.9136
g_step 6900, step 400, avg_time 1.076, loss:926.0872
g_step 7000, step 500, avg_time 1.100, loss:912.6176
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5969, rec:0.5660, f1:0.5810
>> valid relation prec:0.2793, rec:0.0781, f1:0.1221
>> valid relation with NER prec:0.2793, rec:0.0781, f1:0.1221
g_step 7100, step 100, avg_time 1.081, loss:876.9807
g_step 7200, step 200, avg_time 1.083, loss:873.2972
g_step 7300, step 300, avg_time 1.105, loss:881.5487
g_step 7400, step 400, avg_time 1.082, loss:871.2218
g_step 7500, step 500, avg_time 1.104, loss:892.3558
>> valid entity prec:0.5571, rec:0.5960, f1:0.5759
>> valid relation prec:0.2699, rec:0.0819, f1:0.1256
>> valid relation with NER prec:0.2699, rec:0.0819, f1:0.1256
g_step 7600, step 100, avg_time 1.108, loss:854.0428
g_step 7700, step 200, avg_time 1.079, loss:832.2771
g_step 7800, step 300, avg_time 1.088, loss:875.1305
g_step 7900, step 400, avg_time 1.085, loss:854.1184
g_step 8000, step 500, avg_time 1.081, loss:880.1611
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5482, rec:0.6315, f1:0.5869
>> valid relation prec:0.2520, rec:0.0842, f1:0.1262
>> valid relation with NER prec:0.2520, rec:0.0842, f1:0.1262
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 8100, step 100, avg_time 1.090, loss:832.3122
g_step 8200, step 200, avg_time 1.083, loss:837.1112
g_step 8300, step 300, avg_time 1.098, loss:838.4515
g_step 8400, step 400, avg_time 1.095, loss:846.5552
g_step 8500, step 500, avg_time 1.074, loss:842.6281
>> valid entity prec:0.5538, rec:0.6316, f1:0.5902
>> valid relation prec:0.2625, rec:0.1048, f1:0.1498
>> valid relation with NER prec:0.2625, rec:0.1048, f1:0.1498
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 8600, step 100, avg_time 1.106, loss:803.7615
g_step 8700, step 200, avg_time 1.085, loss:812.6204
g_step 8800, step 300, avg_time 1.093, loss:819.2659
g_step 8900, step 400, avg_time 1.071, loss:829.1822
g_step 9000, step 500, avg_time 1.071, loss:817.7469
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5644, rec:0.6202, f1:0.5910
>> valid relation prec:0.2473, rec:0.0862, f1:0.1279
>> valid relation with NER prec:0.2473, rec:0.0862, f1:0.1279
g_step 9100, step 100, avg_time 1.080, loss:773.4827
g_step 9200, step 200, avg_time 1.079, loss:787.4430
g_step 9300, step 300, avg_time 1.096, loss:808.6773
g_step 9400, step 400, avg_time 1.088, loss:813.5920
g_step 9500, step 500, avg_time 1.069, loss:781.1498
>> valid entity prec:0.5670, rec:0.5610, f1:0.5640
>> valid relation prec:0.2620, rec:0.0839, f1:0.1271
>> valid relation with NER prec:0.2620, rec:0.0839, f1:0.1271
g_step 9600, step 100, avg_time 1.060, loss:758.8635
g_step 9700, step 200, avg_time 1.089, loss:749.8169
g_step 9800, step 300, avg_time 1.083, loss:789.6369
g_step 9900, step 400, avg_time 1.067, loss:768.9728
g_step 10000, step 500, avg_time 1.081, loss:804.3483
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.6129, rec:0.5595, f1:0.5850
>> valid relation prec:0.2667, rec:0.0755, f1:0.1176
>> valid relation with NER prec:0.2667, rec:0.0755, f1:0.1176
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:28, 28.55s/it]Extractor Predicting: 2it [00:29, 12.18s/it]Extractor Predicting: 3it [00:29,  6.90s/it]Extractor Predicting: 4it [00:30,  4.41s/it]Extractor Predicting: 5it [00:31,  3.06s/it]Extractor Predicting: 6it [00:31,  2.25s/it]Extractor Predicting: 7it [00:32,  1.76s/it]Extractor Predicting: 8it [00:33,  1.42s/it]Extractor Predicting: 9it [00:33,  1.20s/it]Extractor Predicting: 10it [00:34,  1.03s/it]Extractor Predicting: 11it [00:35,  1.10it/s]Extractor Predicting: 12it [00:35,  1.18it/s]Extractor Predicting: 13it [00:36,  1.23it/s]Extractor Predicting: 14it [00:37,  1.30it/s]Extractor Predicting: 15it [00:38,  1.34it/s]Extractor Predicting: 16it [00:38,  1.38it/s]Extractor Predicting: 17it [00:39,  1.34it/s]Extractor Predicting: 18it [00:40,  1.41it/s]Extractor Predicting: 19it [00:40,  1.46it/s]Extractor Predicting: 20it [00:41,  1.45it/s]Extractor Predicting: 21it [00:42,  1.46it/s]Extractor Predicting: 22it [00:42,  1.42it/s]Extractor Predicting: 23it [00:43,  1.46it/s]Extractor Predicting: 24it [00:44,  1.47it/s]Extractor Predicting: 25it [00:44,  1.45it/s]Extractor Predicting: 26it [00:45,  1.43it/s]Extractor Predicting: 27it [00:46,  1.42it/s]Extractor Predicting: 28it [00:47,  1.45it/s]Extractor Predicting: 29it [00:47,  1.45it/s]Extractor Predicting: 30it [00:48,  1.41it/s]Extractor Predicting: 31it [00:49,  1.42it/s]Extractor Predicting: 32it [00:49,  1.44it/s]Extractor Predicting: 33it [00:50,  1.48it/s]Extractor Predicting: 34it [00:51,  1.52it/s]Extractor Predicting: 35it [00:51,  1.52it/s]Extractor Predicting: 36it [00:52,  1.55it/s]Extractor Predicting: 37it [00:52,  1.57it/s]Extractor Predicting: 38it [00:53,  1.53it/s]Extractor Predicting: 39it [00:54,  1.56it/s]Extractor Predicting: 40it [00:54,  1.54it/s]Extractor Predicting: 41it [00:55,  1.52it/s]Extractor Predicting: 42it [00:56,  1.57it/s]Extractor Predicting: 43it [00:56,  1.54it/s]Extractor Predicting: 44it [00:57,  1.55it/s]Extractor Predicting: 45it [00:58,  1.53it/s]Extractor Predicting: 46it [00:58,  1.54it/s]Extractor Predicting: 47it [00:59,  1.59it/s]Extractor Predicting: 48it [01:00,  1.59it/s]Extractor Predicting: 49it [01:00,  1.60it/s]Extractor Predicting: 50it [01:01,  1.55it/s]Extractor Predicting: 51it [01:02,  1.54it/s]Extractor Predicting: 52it [01:02,  1.53it/s]Extractor Predicting: 53it [01:03,  1.52it/s]Extractor Predicting: 54it [01:03,  1.56it/s]Extractor Predicting: 55it [01:04,  1.53it/s]Extractor Predicting: 56it [01:05,  1.57it/s]Extractor Predicting: 57it [01:06,  1.48it/s]Extractor Predicting: 58it [01:06,  1.49it/s]Extractor Predicting: 59it [01:07,  1.56it/s]Extractor Predicting: 60it [01:07,  1.51it/s]Extractor Predicting: 61it [01:08,  1.54it/s]Extractor Predicting: 62it [01:09,  1.62it/s]Extractor Predicting: 63it [01:09,  1.64it/s]Extractor Predicting: 64it [01:10,  1.69it/s]Extractor Predicting: 65it [01:10,  1.70it/s]Extractor Predicting: 66it [01:11,  1.73it/s]Extractor Predicting: 67it [01:11,  1.71it/s]Extractor Predicting: 68it [01:12,  1.71it/s]Extractor Predicting: 69it [01:13,  1.74it/s]Extractor Predicting: 70it [01:13,  1.70it/s]Extractor Predicting: 71it [01:14,  1.60it/s]Extractor Predicting: 72it [01:15,  1.64it/s]Extractor Predicting: 73it [01:15,  1.59it/s]Extractor Predicting: 74it [01:16,  1.59it/s]Extractor Predicting: 75it [01:16,  1.63it/s]Extractor Predicting: 76it [01:17,  1.64it/s]Extractor Predicting: 77it [01:18,  1.65it/s]Extractor Predicting: 78it [01:18,  1.67it/s]Extractor Predicting: 79it [01:19,  1.60it/s]Extractor Predicting: 80it [01:19,  1.60it/s]Extractor Predicting: 81it [01:20,  1.58it/s]Extractor Predicting: 82it [01:21,  1.56it/s]Extractor Predicting: 83it [01:21,  1.60it/s]Extractor Predicting: 84it [01:22,  1.53it/s]Extractor Predicting: 85it [01:23,  1.51it/s]Extractor Predicting: 86it [01:23,  1.52it/s]Extractor Predicting: 87it [01:24,  1.53it/s]Extractor Predicting: 88it [01:25,  1.50it/s]Extractor Predicting: 89it [01:25,  1.49it/s]Extractor Predicting: 90it [01:26,  1.50it/s]Extractor Predicting: 91it [01:27,  1.52it/s]Extractor Predicting: 92it [01:27,  1.51it/s]Extractor Predicting: 93it [01:28,  1.50it/s]Extractor Predicting: 94it [01:29,  1.46it/s]Extractor Predicting: 95it [01:29,  1.47it/s]Extractor Predicting: 96it [01:30,  1.50it/s]Extractor Predicting: 97it [01:31,  1.50it/s]Extractor Predicting: 98it [01:31,  1.52it/s]Extractor Predicting: 99it [01:35,  1.41s/it]Extractor Predicting: 100it [01:35,  1.21s/it]Extractor Predicting: 101it [01:36,  1.04s/it]Extractor Predicting: 102it [01:37,  1.09it/s]Extractor Predicting: 103it [01:37,  1.20it/s]Extractor Predicting: 104it [01:38,  1.30it/s]Extractor Predicting: 105it [01:39,  1.34it/s]Extractor Predicting: 106it [01:39,  1.39it/s]Extractor Predicting: 107it [01:40,  1.43it/s]Extractor Predicting: 108it [01:41,  1.46it/s]Extractor Predicting: 109it [01:41,  1.48it/s]Extractor Predicting: 110it [01:42,  1.44it/s]Extractor Predicting: 111it [01:43,  1.46it/s]Extractor Predicting: 112it [01:43,  1.49it/s]Extractor Predicting: 113it [01:44,  1.52it/s]Extractor Predicting: 114it [01:45,  1.53it/s]Extractor Predicting: 115it [01:45,  1.50it/s]Extractor Predicting: 116it [01:46,  1.54it/s]Extractor Predicting: 117it [01:46,  1.55it/s]Extractor Predicting: 118it [01:47,  1.54it/s]Extractor Predicting: 119it [01:48,  1.54it/s]Extractor Predicting: 120it [01:48,  1.54it/s]Extractor Predicting: 121it [01:49,  1.53it/s]Extractor Predicting: 122it [01:50,  1.50it/s]Extractor Predicting: 123it [01:50,  1.50it/s]Extractor Predicting: 124it [01:51,  1.49it/s]Extractor Predicting: 125it [01:52,  1.51it/s]Extractor Predicting: 126it [01:52,  1.52it/s]Extractor Predicting: 127it [01:53,  1.47it/s]Extractor Predicting: 128it [01:54,  1.47it/s]Extractor Predicting: 129it [01:54,  1.50it/s]Extractor Predicting: 130it [01:55,  1.53it/s]Extractor Predicting: 131it [01:56,  1.51it/s]Extractor Predicting: 132it [01:56,  1.47it/s]Extractor Predicting: 133it [01:57,  1.50it/s]Extractor Predicting: 134it [01:58,  1.50it/s]Extractor Predicting: 135it [01:59,  1.38it/s]Extractor Predicting: 136it [01:59,  1.43it/s]Extractor Predicting: 137it [02:00,  1.44it/s]Extractor Predicting: 138it [02:01,  1.49it/s]Extractor Predicting: 139it [02:01,  1.51it/s]Extractor Predicting: 140it [02:02,  1.54it/s]Extractor Predicting: 141it [02:02,  1.61it/s]Extractor Predicting: 141it [02:02,  1.15it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.32139737991266376,
  "recall": 0.10679048171793383,
  "score": 0.16031365715530385,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:10,  1.52it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:19,  1.46it/s]Extractor Predicting: 32it [00:20,  1.46it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.40it/s]Extractor Predicting: 51it [00:33,  1.42it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:38,  1.57it/s]Extractor Predicting: 60it [00:38,  1.58it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.53it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:44,  1.58it/s]Extractor Predicting: 70it [00:45,  1.57it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:50,  1.52it/s]Extractor Predicting: 78it [00:50,  1.45it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.45it/s]Extractor Predicting: 84it [00:54,  1.48it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:56,  1.50it/s]Extractor Predicting: 88it [00:57,  1.46it/s]Extractor Predicting: 89it [00:58,  1.46it/s]Extractor Predicting: 90it [00:58,  1.47it/s]Extractor Predicting: 91it [00:59,  1.50it/s]Extractor Predicting: 92it [01:00,  1.50it/s]Extractor Predicting: 93it [01:01,  1.44it/s]Extractor Predicting: 94it [01:01,  1.43it/s]Extractor Predicting: 95it [01:02,  1.44it/s]Extractor Predicting: 96it [01:03,  1.46it/s]Extractor Predicting: 97it [01:03,  1.47it/s]Extractor Predicting: 98it [01:04,  1.47it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:05,  1.47it/s]Extractor Predicting: 101it [01:06,  1.51it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:07,  1.47it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:09,  1.48it/s]Extractor Predicting: 106it [01:09,  1.49it/s]Extractor Predicting: 107it [01:10,  1.46it/s]Extractor Predicting: 108it [01:11,  1.42it/s]Extractor Predicting: 109it [01:11,  1.45it/s]Extractor Predicting: 110it [01:12,  1.45it/s]Extractor Predicting: 111it [01:13,  1.45it/s]Extractor Predicting: 112it [01:13,  1.47it/s]Extractor Predicting: 113it [01:14,  1.46it/s]Extractor Predicting: 114it [01:15,  1.50it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:17,  1.56it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:18,  1.47it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:20,  1.58it/s]Extractor Predicting: 123it [01:21,  1.52it/s]Extractor Predicting: 124it [01:21,  1.53it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:23,  1.55it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:25,  1.49it/s]Extractor Predicting: 130it [01:25,  1.51it/s]Extractor Predicting: 131it [01:26,  1.51it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 134it [01:28,  1.50it/s]Extractor Predicting: 135it [01:29,  1.53it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:30,  1.54it/s]Extractor Predicting: 138it [01:31,  1.51it/s]Extractor Predicting: 139it [01:31,  1.55it/s]Extractor Predicting: 140it [01:32,  1.55it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:33,  1.57it/s]Extractor Predicting: 143it [01:34,  1.50it/s]Extractor Predicting: 144it [01:35,  1.50it/s]Extractor Predicting: 145it [01:35,  1.53it/s]Extractor Predicting: 146it [01:36,  1.53it/s]Extractor Predicting: 147it [01:36,  1.52it/s]Extractor Predicting: 148it [01:37,  1.47it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:39,  1.51it/s]Extractor Predicting: 151it [01:39,  1.54it/s]Extractor Predicting: 152it [01:40,  1.56it/s]Extractor Predicting: 153it [01:40,  1.52it/s]Extractor Predicting: 154it [01:41,  1.50it/s]Extractor Predicting: 155it [01:42,  1.51it/s]Extractor Predicting: 156it [01:43,  1.35it/s]Extractor Predicting: 157it [01:43,  1.40it/s]Extractor Predicting: 158it [01:44,  1.36it/s]Extractor Predicting: 159it [01:45,  1.40it/s]Extractor Predicting: 160it [01:45,  1.44it/s]Extractor Predicting: 161it [01:46,  1.48it/s]Extractor Predicting: 162it [01:47,  1.51it/s]Extractor Predicting: 163it [01:47,  1.45it/s]Extractor Predicting: 164it [01:48,  1.45it/s]Extractor Predicting: 165it [01:49,  1.46it/s]Extractor Predicting: 166it [01:49,  1.49it/s]Extractor Predicting: 167it [01:50,  1.54it/s]Extractor Predicting: 168it [01:51,  1.53it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:52,  1.48it/s]Extractor Predicting: 171it [01:53,  1.46it/s]Extractor Predicting: 172it [01:54,  1.46it/s]Extractor Predicting: 173it [01:54,  1.46it/s]Extractor Predicting: 174it [01:55,  1.40it/s]Extractor Predicting: 175it [01:56,  1.38it/s]Extractor Predicting: 176it [01:56,  1.42it/s]Extractor Predicting: 177it [01:57,  1.45it/s]Extractor Predicting: 178it [01:58,  1.47it/s]Extractor Predicting: 179it [01:58,  1.48it/s]Extractor Predicting: 180it [01:59,  1.48it/s]Extractor Predicting: 181it [02:00,  1.51it/s]Extractor Predicting: 182it [02:00,  1.53it/s]Extractor Predicting: 183it [02:01,  1.53it/s]Extractor Predicting: 184it [02:02,  1.49it/s]Extractor Predicting: 185it [02:02,  1.52it/s]Extractor Predicting: 186it [02:03,  1.57it/s]Extractor Predicting: 187it [02:04,  1.56it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:05,  1.53it/s]Extractor Predicting: 190it [02:06,  1.55it/s]Extractor Predicting: 191it [02:06,  1.52it/s]Extractor Predicting: 192it [02:07,  1.51it/s]Extractor Predicting: 193it [02:08,  1.53it/s]Extractor Predicting: 194it [02:08,  1.51it/s]Extractor Predicting: 195it [02:09,  1.53it/s]Extractor Predicting: 196it [02:09,  1.55it/s]Extractor Predicting: 197it [02:10,  1.57it/s]Extractor Predicting: 198it [02:11,  1.58it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.53it/s]Extractor Predicting: 201it [02:13,  1.54it/s]Extractor Predicting: 202it [02:13,  1.57it/s]Extractor Predicting: 203it [02:14,  1.59it/s]Extractor Predicting: 204it [02:15,  1.56it/s]Extractor Predicting: 205it [02:15,  1.56it/s]Extractor Predicting: 206it [02:16,  1.56it/s]Extractor Predicting: 207it [02:17,  1.52it/s]Extractor Predicting: 208it [02:17,  1.53it/s]Extractor Predicting: 209it [02:18,  1.51it/s]Extractor Predicting: 210it [02:19,  1.51it/s]Extractor Predicting: 211it [02:19,  1.54it/s]Extractor Predicting: 212it [02:20,  1.53it/s]Extractor Predicting: 213it [02:20,  1.57it/s]Extractor Predicting: 214it [02:21,  1.55it/s]Extractor Predicting: 215it [02:22,  1.58it/s]Extractor Predicting: 216it [02:22,  1.56it/s]Extractor Predicting: 217it [02:23,  1.51it/s]Extractor Predicting: 218it [02:24,  1.53it/s]Extractor Predicting: 219it [02:24,  1.56it/s]Extractor Predicting: 220it [02:25,  1.58it/s]Extractor Predicting: 221it [02:26,  1.51it/s]Extractor Predicting: 222it [02:26,  1.54it/s]Extractor Predicting: 223it [02:27,  1.57it/s]Extractor Predicting: 224it [02:28,  1.57it/s]Extractor Predicting: 225it [02:28,  1.57it/s]Extractor Predicting: 226it [02:29,  1.44it/s]Extractor Predicting: 227it [02:30,  1.46it/s]Extractor Predicting: 228it [02:30,  1.45it/s]Extractor Predicting: 229it [02:31,  1.47it/s]Extractor Predicting: 230it [02:32,  1.50it/s]Extractor Predicting: 231it [02:32,  1.46it/s]Extractor Predicting: 232it [02:33,  1.48it/s]Extractor Predicting: 233it [02:34,  1.50it/s]Extractor Predicting: 234it [02:34,  1.51it/s]Extractor Predicting: 235it [02:35,  1.49it/s]Extractor Predicting: 236it [02:36,  1.49it/s]Extractor Predicting: 237it [02:36,  1.55it/s]Extractor Predicting: 238it [02:37,  1.52it/s]Extractor Predicting: 239it [02:38,  1.50it/s]Extractor Predicting: 240it [02:38,  1.49it/s]Extractor Predicting: 241it [02:39,  1.46it/s]Extractor Predicting: 242it [02:40,  1.51it/s]Extractor Predicting: 243it [02:40,  1.52it/s]Extractor Predicting: 244it [02:41,  1.49it/s]Extractor Predicting: 245it [02:42,  1.52it/s]Extractor Predicting: 246it [02:42,  1.48it/s]Extractor Predicting: 247it [02:43,  1.52it/s]Extractor Predicting: 248it [02:44,  1.47it/s]Extractor Predicting: 249it [02:44,  1.49it/s]Extractor Predicting: 250it [02:45,  1.50it/s]Extractor Predicting: 251it [02:46,  1.44it/s]Extractor Predicting: 252it [02:46,  1.45it/s]Extractor Predicting: 253it [02:47,  1.47it/s]Extractor Predicting: 254it [02:48,  1.51it/s]Extractor Predicting: 255it [02:48,  1.52it/s]Extractor Predicting: 256it [02:49,  1.50it/s]Extractor Predicting: 257it [02:50,  1.53it/s]Extractor Predicting: 258it [02:50,  1.52it/s]Extractor Predicting: 259it [02:51,  1.53it/s]Extractor Predicting: 260it [02:52,  1.51it/s]Extractor Predicting: 261it [02:52,  1.52it/s]Extractor Predicting: 262it [02:53,  1.47it/s]Extractor Predicting: 263it [02:54,  1.44it/s]Extractor Predicting: 264it [02:54,  1.46it/s]Extractor Predicting: 265it [02:55,  1.49it/s]Extractor Predicting: 266it [02:56,  1.51it/s]Extractor Predicting: 267it [02:57,  1.32it/s]Extractor Predicting: 268it [02:57,  1.39it/s]Extractor Predicting: 269it [02:58,  1.47it/s]Extractor Predicting: 270it [02:59,  1.52it/s]Extractor Predicting: 271it [02:59,  1.51it/s]Extractor Predicting: 272it [03:00,  1.51it/s]Extractor Predicting: 273it [03:01,  1.51it/s]Extractor Predicting: 274it [03:01,  1.52it/s]Extractor Predicting: 275it [03:02,  1.54it/s]Extractor Predicting: 276it [03:02,  1.55it/s]Extractor Predicting: 277it [03:03,  1.49it/s]Extractor Predicting: 278it [03:04,  1.54it/s]Extractor Predicting: 279it [03:04,  1.51it/s]Extractor Predicting: 280it [03:05,  1.51it/s]Extractor Predicting: 281it [03:06,  1.52it/s]Extractor Predicting: 282it [03:06,  1.48it/s]Extractor Predicting: 283it [03:07,  1.51it/s]Extractor Predicting: 284it [03:08,  1.50it/s]Extractor Predicting: 285it [03:08,  1.50it/s]Extractor Predicting: 286it [03:09,  1.53it/s]Extractor Predicting: 287it [03:10,  1.50it/s]Extractor Predicting: 288it [03:10,  1.51it/s]Extractor Predicting: 289it [03:11,  1.48it/s]Extractor Predicting: 290it [03:12,  1.54it/s]Extractor Predicting: 291it [03:12,  1.54it/s]Extractor Predicting: 292it [03:13,  1.50it/s]Extractor Predicting: 293it [03:14,  1.48it/s]Extractor Predicting: 294it [03:14,  1.46it/s]Extractor Predicting: 295it [03:15,  1.51it/s]Extractor Predicting: 296it [03:16,  1.50it/s]Extractor Predicting: 297it [03:17,  1.46it/s]Extractor Predicting: 298it [03:17,  1.43it/s]Extractor Predicting: 299it [03:18,  1.46it/s]Extractor Predicting: 300it [03:19,  1.48it/s]Extractor Predicting: 301it [03:19,  1.49it/s]Extractor Predicting: 302it [03:20,  1.45it/s]Extractor Predicting: 303it [03:21,  1.46it/s]Extractor Predicting: 304it [03:21,  1.44it/s]Extractor Predicting: 305it [03:22,  1.45it/s]Extractor Predicting: 306it [03:23,  1.44it/s]Extractor Predicting: 307it [03:24,  1.38it/s]Extractor Predicting: 308it [03:24,  1.42it/s]Extractor Predicting: 309it [03:25,  1.44it/s]Extractor Predicting: 310it [03:26,  1.43it/s]Extractor Predicting: 311it [03:26,  1.43it/s]Extractor Predicting: 312it [03:27,  1.40it/s]Extractor Predicting: 313it [03:28,  1.42it/s]Extractor Predicting: 314it [03:28,  1.41it/s]Extractor Predicting: 315it [03:29,  1.47it/s]Extractor Predicting: 316it [03:30,  1.46it/s]Extractor Predicting: 317it [03:31,  1.39it/s]Extractor Predicting: 318it [03:31,  1.42it/s]Extractor Predicting: 319it [03:32,  1.43it/s]Extractor Predicting: 320it [03:32,  1.52it/s]Extractor Predicting: 321it [03:33,  1.55it/s]Extractor Predicting: 322it [03:34,  1.61it/s]Extractor Predicting: 323it [03:34,  1.65it/s]Extractor Predicting: 324it [03:35,  1.67it/s]Extractor Predicting: 325it [03:35,  1.69it/s]Extractor Predicting: 326it [03:36,  1.70it/s]Extractor Predicting: 327it [03:37,  1.69it/s]Extractor Predicting: 328it [03:37,  1.68it/s]Extractor Predicting: 329it [03:38,  1.73it/s]Extractor Predicting: 330it [03:38,  1.73it/s]Extractor Predicting: 331it [03:39,  1.77it/s]Extractor Predicting: 332it [03:39,  1.74it/s]Extractor Predicting: 333it [03:40,  1.77it/s]Extractor Predicting: 334it [03:41,  1.72it/s]Extractor Predicting: 335it [03:41,  1.72it/s]Extractor Predicting: 336it [03:42,  1.70it/s]Extractor Predicting: 337it [03:42,  1.75it/s]Extractor Predicting: 338it [03:43,  1.71it/s]Extractor Predicting: 339it [03:43,  1.71it/s]Extractor Predicting: 340it [03:44,  1.57it/s]Extractor Predicting: 341it [03:45,  1.60it/s]Extractor Predicting: 342it [03:45,  1.66it/s]Extractor Predicting: 343it [03:46,  1.69it/s]Extractor Predicting: 344it [03:47,  1.69it/s]Extractor Predicting: 345it [03:47,  1.68it/s]Extractor Predicting: 346it [03:48,  1.70it/s]Extractor Predicting: 347it [03:48,  1.68it/s]Extractor Predicting: 348it [03:49,  1.65it/s]Extractor Predicting: 349it [03:50,  1.61it/s]Extractor Predicting: 350it [03:50,  1.52it/s]Extractor Predicting: 351it [03:51,  1.46it/s]Extractor Predicting: 352it [03:52,  1.49it/s]Extractor Predicting: 353it [03:52,  1.53it/s]Extractor Predicting: 354it [03:53,  1.52it/s]Extractor Predicting: 355it [03:54,  1.52it/s]Extractor Predicting: 356it [03:54,  1.46it/s]Extractor Predicting: 357it [03:55,  1.45it/s]Extractor Predicting: 358it [03:56,  1.48it/s]Extractor Predicting: 359it [03:56,  1.50it/s]Extractor Predicting: 360it [03:57,  1.52it/s]Extractor Predicting: 361it [03:58,  1.48it/s]Extractor Predicting: 362it [03:58,  1.52it/s]Extractor Predicting: 363it [03:59,  1.51it/s]Extractor Predicting: 364it [04:00,  1.50it/s]Extractor Predicting: 365it [04:00,  1.51it/s]Extractor Predicting: 366it [04:01,  1.46it/s]Extractor Predicting: 367it [04:02,  1.47it/s]Extractor Predicting: 368it [04:02,  1.50it/s]Extractor Predicting: 369it [04:03,  1.51it/s]Extractor Predicting: 370it [04:04,  1.50it/s]Extractor Predicting: 371it [04:04,  1.47it/s]Extractor Predicting: 372it [04:05,  1.49it/s]Extractor Predicting: 373it [04:06,  1.48it/s]Extractor Predicting: 374it [04:06,  1.50it/s]Extractor Predicting: 375it [04:07,  1.52it/s]Extractor Predicting: 376it [04:08,  1.46it/s]Extractor Predicting: 377it [04:08,  1.49it/s]Extractor Predicting: 378it [04:09,  1.47it/s]Extractor Predicting: 379it [04:10,  1.47it/s]Extractor Predicting: 380it [04:11,  1.47it/s]Extractor Predicting: 381it [04:11,  1.42it/s]Extractor Predicting: 382it [04:12,  1.43it/s]Extractor Predicting: 383it [04:13,  1.45it/s]Extractor Predicting: 384it [04:13,  1.45it/s]Extractor Predicting: 385it [04:14,  1.45it/s]Extractor Predicting: 386it [04:15,  1.44it/s]Extractor Predicting: 387it [04:15,  1.44it/s]Extractor Predicting: 388it [04:16,  1.44it/s]Extractor Predicting: 389it [04:17,  1.47it/s]Extractor Predicting: 390it [04:17,  1.49it/s]Extractor Predicting: 391it [04:18,  1.46it/s]Extractor Predicting: 392it [04:19,  1.44it/s]Extractor Predicting: 393it [04:19,  1.48it/s]Extractor Predicting: 394it [04:20,  1.48it/s]Extractor Predicting: 395it [04:21,  1.46it/s]Extractor Predicting: 396it [04:22,  1.41it/s]Extractor Predicting: 397it [04:22,  1.41it/s]Extractor Predicting: 398it [04:23,  1.43it/s]Extractor Predicting: 399it [04:24,  1.41it/s]Extractor Predicting: 400it [04:25,  1.27it/s]Extractor Predicting: 401it [04:25,  1.29it/s]Extractor Predicting: 402it [04:26,  1.34it/s]Extractor Predicting: 403it [04:27,  1.38it/s]Extractor Predicting: 404it [04:27,  1.44it/s]Extractor Predicting: 405it [04:28,  1.45it/s]Extractor Predicting: 406it [04:29,  1.46it/s]Extractor Predicting: 407it [04:29,  1.47it/s]Extractor Predicting: 408it [04:30,  1.47it/s]Extractor Predicting: 409it [04:31,  1.52it/s]Extractor Predicting: 410it [04:31,  1.51it/s]Extractor Predicting: 411it [04:32,  1.52it/s]Extractor Predicting: 412it [04:33,  1.51it/s]Extractor Predicting: 413it [04:33,  1.55it/s]Extractor Predicting: 414it [04:34,  1.55it/s]Extractor Predicting: 415it [04:35,  1.55it/s]Extractor Predicting: 416it [04:35,  1.48it/s]Extractor Predicting: 417it [04:36,  1.51it/s]Extractor Predicting: 418it [04:37,  1.51it/s]Extractor Predicting: 419it [04:37,  1.50it/s]Extractor Predicting: 420it [04:38,  1.50it/s]Extractor Predicting: 421it [04:39,  1.48it/s]Extractor Predicting: 422it [04:39,  1.47it/s]Extractor Predicting: 423it [04:40,  1.49it/s]Extractor Predicting: 424it [04:41,  1.46it/s]Extractor Predicting: 425it [04:41,  1.49it/s]Extractor Predicting: 426it [04:42,  1.41it/s]Extractor Predicting: 427it [04:43,  1.45it/s]Extractor Predicting: 428it [04:44,  1.46it/s]Extractor Predicting: 429it [04:44,  1.48it/s]Extractor Predicting: 430it [04:44,  1.83it/s]Extractor Predicting: 430it [04:44,  1.51it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3464210208396255,
  "recall": 0.11138085065061176,
  "score": 0.16856492027334852,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.18it/s]Extractor Predicting: 5it [00:04,  1.24it/s]
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5172413793103449,
  "recall": 0.07425742574257425,
  "score": 0.12987012987012989,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_15_seed_4', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/wiki/unseen_15_seed_4/generator/synthetic.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
train vocab size: 26372
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26472, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/model', pretrained_wv='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26472, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.368, loss:59196.7248
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.110, loss:2196.2750
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.048, loss:1903.6058
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.037, loss:1807.7702
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.042, loss:1671.8584
>> valid entity prec:0.3348, rec:0.2009, f1:0.2511
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.052, loss:1667.1926
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.050, loss:1624.4952
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 300, avg_time 1.048, loss:1512.1893
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 400, avg_time 1.045, loss:1541.7471
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 500, avg_time 1.052, loss:1405.4964
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4970, rec:0.1197, f1:0.1929
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 100, avg_time 1.048, loss:1444.4516
g_step 1200, step 200, avg_time 1.067, loss:1426.9849
g_step 1300, step 300, avg_time 1.059, loss:1381.4759
g_step 1400, step 400, avg_time 1.062, loss:1432.7940
g_step 1500, step 500, avg_time 1.044, loss:1337.2409
>> valid entity prec:0.4975, rec:0.3684, f1:0.4233
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.052, loss:1336.6325
g_step 1700, step 200, avg_time 1.057, loss:1373.3972
g_step 1800, step 300, avg_time 1.058, loss:1354.7165
g_step 1900, step 400, avg_time 1.054, loss:1267.2323
g_step 2000, step 500, avg_time 1.061, loss:1323.5815
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4453, rec:0.3432, f1:0.3876
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 100, avg_time 1.071, loss:1293.4412
g_step 2200, step 200, avg_time 1.062, loss:1309.6804
g_step 2300, step 300, avg_time 1.043, loss:1282.8505
g_step 2400, step 400, avg_time 1.057, loss:1279.9906
g_step 2500, step 500, avg_time 1.049, loss:1277.0060
>> valid entity prec:0.5355, rec:0.3929, f1:0.4532
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 100, avg_time 1.081, loss:1286.0820
g_step 2700, step 200, avg_time 1.045, loss:1206.7634
g_step 2800, step 300, avg_time 1.051, loss:1275.5054
g_step 2900, step 400, avg_time 1.053, loss:1257.6753
g_step 3000, step 500, avg_time 1.042, loss:1243.6983
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5593, rec:0.2162, f1:0.3119
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 100, avg_time 1.041, loss:1225.2599
g_step 3200, step 200, avg_time 1.045, loss:1227.5976
g_step 3300, step 300, avg_time 1.051, loss:1242.7988
g_step 3400, step 400, avg_time 1.026, loss:1208.7754
g_step 3500, step 500, avg_time 1.068, loss:1237.5032
>> valid entity prec:0.4819, rec:0.4472, f1:0.4639
>> valid relation prec:0.6364, rec:0.0012, f1:0.0024
>> valid relation with NER prec:0.6364, rec:0.0012, f1:0.0024
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 100, avg_time 1.044, loss:1196.7643
g_step 3700, step 200, avg_time 1.034, loss:1232.4119
g_step 3800, step 300, avg_time 1.057, loss:1198.1250
g_step 3900, step 400, avg_time 1.064, loss:1226.5944
g_step 4000, step 500, avg_time 1.053, loss:1170.7351
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4627, rec:0.5459, f1:0.5009
>> valid relation prec:0.5348, rec:0.0249, f1:0.0476
>> valid relation with NER prec:0.5348, rec:0.0249, f1:0.0476
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 100, avg_time 1.045, loss:1155.0244
g_step 4200, step 200, avg_time 1.057, loss:1194.0590
g_step 4300, step 300, avg_time 1.070, loss:1207.1948
g_step 4400, step 400, avg_time 1.054, loss:1169.8700
g_step 4500, step 500, avg_time 1.039, loss:1173.6899
>> valid entity prec:0.4961, rec:0.4328, f1:0.4623
>> valid relation prec:0.3846, rec:0.0017, f1:0.0034
>> valid relation with NER prec:0.3846, rec:0.0017, f1:0.0034
g_step 4600, step 100, avg_time 1.049, loss:1145.6071
g_step 4700, step 200, avg_time 1.051, loss:1160.7771
g_step 4800, step 300, avg_time 1.045, loss:1150.0021
g_step 4900, step 400, avg_time 1.047, loss:1158.0335
g_step 5000, step 500, avg_time 1.063, loss:1190.6703
learning rate was adjusted to 0.0008
>> valid entity prec:0.5242, rec:0.3559, f1:0.4240
>> valid relation prec:0.2963, rec:0.0014, f1:0.0027
>> valid relation with NER prec:0.2963, rec:0.0014, f1:0.0027
g_step 5100, step 100, avg_time 1.050, loss:1134.8400
g_step 5200, step 200, avg_time 1.047, loss:1154.1101
g_step 5300, step 300, avg_time 1.056, loss:1153.2247
g_step 5400, step 400, avg_time 1.060, loss:1120.0259
g_step 5500, step 500, avg_time 1.066, loss:1145.5280
>> valid entity prec:0.4981, rec:0.4392, f1:0.4668
>> valid relation prec:0.2982, rec:0.0029, f1:0.0057
>> valid relation with NER prec:0.2982, rec:0.0029, f1:0.0057
g_step 5600, step 100, avg_time 1.059, loss:1125.8970
g_step 5700, step 200, avg_time 1.048, loss:1093.0126
g_step 5800, step 300, avg_time 1.064, loss:1122.3258
g_step 5900, step 400, avg_time 1.056, loss:1135.5204
g_step 6000, step 500, avg_time 1.058, loss:1124.9305
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4513, rec:0.3969, f1:0.4223
>> valid relation prec:0.2667, rec:0.0014, f1:0.0027
>> valid relation with NER prec:0.2667, rec:0.0014, f1:0.0027
g_step 6100, step 100, avg_time 1.051, loss:1098.4804
g_step 6200, step 200, avg_time 1.058, loss:1097.0902
g_step 6300, step 300, avg_time 1.049, loss:1093.1325
g_step 6400, step 400, avg_time 1.041, loss:1093.2651
g_step 6500, step 500, avg_time 1.063, loss:1124.5792
>> valid entity prec:0.5228, rec:0.3696, f1:0.4330
>> valid relation prec:0.2941, rec:0.0051, f1:0.0101
>> valid relation with NER prec:0.2941, rec:0.0051, f1:0.0101
g_step 6600, step 100, avg_time 1.065, loss:1075.6675
g_step 6700, step 200, avg_time 1.043, loss:1086.0381
g_step 6800, step 300, avg_time 1.045, loss:1070.7064
g_step 6900, step 400, avg_time 1.062, loss:1071.1848
g_step 7000, step 500, avg_time 1.044, loss:1097.8371
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4920, rec:0.4569, f1:0.4738
>> valid relation prec:0.2804, rec:0.0051, f1:0.0101
>> valid relation with NER prec:0.2804, rec:0.0051, f1:0.0101
g_step 7100, step 100, avg_time 1.040, loss:1042.3027
g_step 7200, step 200, avg_time 1.051, loss:1077.5126
g_step 7300, step 300, avg_time 1.031, loss:1055.0332
g_step 7400, step 400, avg_time 1.046, loss:1059.9722
g_step 7500, step 500, avg_time 1.043, loss:1050.4492
>> valid entity prec:0.4884, rec:0.4118, f1:0.4468
>> valid relation prec:0.3297, rec:0.0102, f1:0.0199
>> valid relation with NER prec:0.3297, rec:0.0102, f1:0.0199
g_step 7600, step 100, avg_time 1.060, loss:1033.7649
g_step 7700, step 200, avg_time 1.046, loss:1034.4072
g_step 7800, step 300, avg_time 1.031, loss:1029.3093
g_step 7900, step 400, avg_time 1.005, loss:1040.9772
g_step 8000, step 500, avg_time 1.030, loss:1060.8407
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4552, rec:0.5218, f1:0.4862
>> valid relation prec:0.3343, rec:0.0377, f1:0.0677
>> valid relation with NER prec:0.3343, rec:0.0377, f1:0.0677
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 8100, step 100, avg_time 1.052, loss:965.1938
g_step 8200, step 200, avg_time 1.056, loss:1017.8753
g_step 8300, step 300, avg_time 1.038, loss:1029.7502
g_step 8400, step 400, avg_time 1.054, loss:1021.0055
g_step 8500, step 500, avg_time 1.052, loss:1059.7193
>> valid entity prec:0.4899, rec:0.3838, f1:0.4304
>> valid relation prec:0.2941, rec:0.0179, f1:0.0338
>> valid relation with NER prec:0.2941, rec:0.0179, f1:0.0338
g_step 8600, step 100, avg_time 1.043, loss:1009.3327
g_step 8700, step 200, avg_time 1.049, loss:986.3773
g_step 8800, step 300, avg_time 1.058, loss:1013.2235
g_step 8900, step 400, avg_time 1.054, loss:1004.5543
g_step 9000, step 500, avg_time 1.062, loss:981.4990
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.4654, rec:0.4248, f1:0.4442
>> valid relation prec:0.2854, rec:0.0254, f1:0.0467
>> valid relation with NER prec:0.2854, rec:0.0254, f1:0.0467
g_step 9100, step 100, avg_time 1.045, loss:967.4052
g_step 9200, step 200, avg_time 1.047, loss:969.6686
g_step 9300, step 300, avg_time 1.040, loss:965.9159
g_step 9400, step 400, avg_time 1.060, loss:985.9215
g_step 9500, step 500, avg_time 1.050, loss:1007.7396
>> valid entity prec:0.4611, rec:0.5076, f1:0.4833
>> valid relation prec:0.3039, rec:0.0329, f1:0.0594
>> valid relation with NER prec:0.3039, rec:0.0329, f1:0.0594
g_step 9600, step 100, avg_time 1.044, loss:909.5829
g_step 9700, step 200, avg_time 1.063, loss:978.3040
g_step 9800, step 300, avg_time 1.045, loss:981.3284
g_step 9900, step 400, avg_time 1.064, loss:959.1849
g_step 10000, step 500, avg_time 1.037, loss:968.4711
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.4673, rec:0.4018, f1:0.4321
>> valid relation prec:0.3047, rec:0.0333, f1:0.0600
>> valid relation with NER prec:0.3047, rec:0.0333, f1:0.0600
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:05,  5.99s/it]Extractor Predicting: 2it [00:06,  2.89s/it]Extractor Predicting: 3it [00:08,  2.42s/it]Extractor Predicting: 4it [00:09,  1.71s/it]Extractor Predicting: 5it [00:09,  1.31s/it]Extractor Predicting: 6it [00:10,  1.06s/it]Extractor Predicting: 7it [00:10,  1.11it/s]Extractor Predicting: 8it [00:11,  1.25it/s]Extractor Predicting: 9it [00:12,  1.35it/s]Extractor Predicting: 10it [00:12,  1.45it/s]Extractor Predicting: 11it [00:13,  1.48it/s]Extractor Predicting: 12it [00:13,  1.52it/s]Extractor Predicting: 13it [00:14,  1.56it/s]Extractor Predicting: 14it [00:15,  1.58it/s]Extractor Predicting: 15it [00:15,  1.60it/s]Extractor Predicting: 16it [00:16,  1.59it/s]Extractor Predicting: 17it [00:17,  1.53it/s]Extractor Predicting: 18it [00:17,  1.60it/s]Extractor Predicting: 19it [00:18,  1.64it/s]Extractor Predicting: 20it [00:18,  1.65it/s]Extractor Predicting: 21it [00:19,  1.67it/s]Extractor Predicting: 22it [00:20,  1.65it/s]Extractor Predicting: 23it [00:20,  1.67it/s]Extractor Predicting: 24it [00:21,  1.71it/s]Extractor Predicting: 25it [00:21,  1.73it/s]Extractor Predicting: 26it [00:22,  1.67it/s]Extractor Predicting: 27it [00:22,  1.70it/s]Extractor Predicting: 28it [00:23,  1.64it/s]Extractor Predicting: 29it [00:24,  1.67it/s]Extractor Predicting: 30it [00:24,  1.54it/s]Extractor Predicting: 31it [00:25,  1.59it/s]Extractor Predicting: 32it [00:26,  1.63it/s]Extractor Predicting: 33it [00:26,  1.57it/s]Extractor Predicting: 34it [00:27,  1.62it/s]Extractor Predicting: 35it [00:28,  1.60it/s]Extractor Predicting: 36it [00:28,  1.61it/s]Extractor Predicting: 37it [00:29,  1.61it/s]Extractor Predicting: 38it [00:29,  1.59it/s]Extractor Predicting: 39it [00:30,  1.63it/s]Extractor Predicting: 40it [00:31,  1.70it/s]Extractor Predicting: 41it [00:31,  1.65it/s]Extractor Predicting: 42it [00:32,  1.61it/s]Extractor Predicting: 43it [00:33,  1.48it/s]Extractor Predicting: 44it [00:33,  1.47it/s]Extractor Predicting: 45it [00:35,  1.15it/s]Extractor Predicting: 46it [00:35,  1.26it/s]Extractor Predicting: 47it [00:36,  1.25it/s]Extractor Predicting: 48it [00:37,  1.35it/s]Extractor Predicting: 49it [00:37,  1.39it/s]Extractor Predicting: 50it [00:38,  1.39it/s]Extractor Predicting: 51it [00:39,  1.46it/s]Extractor Predicting: 52it [00:39,  1.40it/s]Extractor Predicting: 53it [00:40,  1.45it/s]Extractor Predicting: 54it [00:41,  1.47it/s]Extractor Predicting: 55it [00:41,  1.50it/s]Extractor Predicting: 56it [00:42,  1.51it/s]Extractor Predicting: 57it [00:43,  1.48it/s]Extractor Predicting: 58it [00:43,  1.49it/s]Extractor Predicting: 59it [00:44,  1.49it/s]Extractor Predicting: 60it [00:45,  1.52it/s]Extractor Predicting: 61it [00:45,  1.53it/s]Extractor Predicting: 62it [00:46,  1.54it/s]Extractor Predicting: 63it [00:47,  1.59it/s]Extractor Predicting: 64it [00:47,  1.50it/s]Extractor Predicting: 65it [00:48,  1.52it/s]Extractor Predicting: 66it [00:49,  1.51it/s]Extractor Predicting: 67it [00:49,  1.55it/s]Extractor Predicting: 68it [00:50,  1.56it/s]Extractor Predicting: 69it [00:51,  1.51it/s]Extractor Predicting: 70it [00:51,  1.54it/s]Extractor Predicting: 71it [00:52,  1.55it/s]Extractor Predicting: 72it [00:52,  1.57it/s]Extractor Predicting: 73it [00:53,  1.56it/s]Extractor Predicting: 74it [00:54,  1.54it/s]Extractor Predicting: 75it [00:54,  1.53it/s]Extractor Predicting: 76it [00:55,  1.56it/s]Extractor Predicting: 77it [00:56,  1.58it/s]Extractor Predicting: 78it [00:56,  1.56it/s]Extractor Predicting: 79it [00:57,  1.48it/s]Extractor Predicting: 80it [00:58,  1.51it/s]Extractor Predicting: 81it [00:58,  1.53it/s]Extractor Predicting: 82it [00:59,  1.54it/s]Extractor Predicting: 83it [01:00,  1.54it/s]Extractor Predicting: 84it [01:00,  1.49it/s]Extractor Predicting: 85it [01:01,  1.53it/s]Extractor Predicting: 86it [01:02,  1.54it/s]Extractor Predicting: 87it [01:02,  1.52it/s]Extractor Predicting: 88it [01:03,  1.53it/s]Extractor Predicting: 89it [01:04,  1.50it/s]Extractor Predicting: 90it [01:04,  1.53it/s]Extractor Predicting: 91it [01:05,  1.56it/s]Extractor Predicting: 92it [01:05,  1.57it/s]Extractor Predicting: 93it [01:06,  1.56it/s]Extractor Predicting: 94it [01:07,  1.51it/s]Extractor Predicting: 95it [01:07,  1.52it/s]Extractor Predicting: 96it [01:08,  1.53it/s]Extractor Predicting: 97it [01:09,  1.53it/s]Extractor Predicting: 98it [01:09,  1.51it/s]Extractor Predicting: 99it [01:10,  1.49it/s]Extractor Predicting: 100it [01:11,  1.50it/s]Extractor Predicting: 101it [01:11,  1.52it/s]Extractor Predicting: 102it [01:12,  1.53it/s]Extractor Predicting: 103it [01:13,  1.54it/s]Extractor Predicting: 104it [01:13,  1.50it/s]Extractor Predicting: 105it [01:14,  1.52it/s]Extractor Predicting: 106it [01:15,  1.53it/s]Extractor Predicting: 107it [01:15,  1.56it/s]Extractor Predicting: 108it [01:16,  1.58it/s]Extractor Predicting: 109it [01:17,  1.58it/s]Extractor Predicting: 110it [01:17,  1.59it/s]Extractor Predicting: 111it [01:18,  1.54it/s]Extractor Predicting: 112it [01:19,  1.53it/s]Extractor Predicting: 113it [01:19,  1.55it/s]Extractor Predicting: 114it [01:20,  1.59it/s]Extractor Predicting: 115it [01:20,  1.56it/s]Extractor Predicting: 116it [01:21,  1.54it/s]Extractor Predicting: 117it [01:22,  1.55it/s]Extractor Predicting: 118it [01:22,  1.56it/s]Extractor Predicting: 119it [01:23,  1.42it/s]Extractor Predicting: 120it [01:24,  1.46it/s]Extractor Predicting: 121it [01:25,  1.46it/s]Extractor Predicting: 122it [01:25,  1.51it/s]Extractor Predicting: 123it [01:26,  1.53it/s]Extractor Predicting: 124it [01:26,  1.57it/s]Extractor Predicting: 125it [01:27,  1.54it/s]Extractor Predicting: 126it [01:28,  1.47it/s]Extractor Predicting: 127it [01:28,  1.51it/s]Extractor Predicting: 128it [01:29,  1.54it/s]Extractor Predicting: 129it [01:30,  1.56it/s]Extractor Predicting: 130it [01:30,  1.57it/s]Extractor Predicting: 131it [01:31,  1.54it/s]Extractor Predicting: 132it [01:32,  1.56it/s]Extractor Predicting: 133it [01:32,  1.59it/s]Extractor Predicting: 134it [01:33,  1.54it/s]Extractor Predicting: 135it [01:34,  1.55it/s]Extractor Predicting: 136it [01:34,  1.51it/s]Extractor Predicting: 137it [01:35,  1.53it/s]Extractor Predicting: 138it [01:36,  1.50it/s]Extractor Predicting: 139it [01:36,  1.52it/s]Extractor Predicting: 140it [01:37,  1.51it/s]Extractor Predicting: 141it [01:38,  1.47it/s]Extractor Predicting: 142it [01:38,  1.45it/s]Extractor Predicting: 143it [01:39,  1.48it/s]Extractor Predicting: 144it [01:40,  1.49it/s]Extractor Predicting: 145it [01:40,  1.51it/s]Extractor Predicting: 146it [01:43,  1.17s/it]Extractor Predicting: 147it [01:43,  1.02s/it]Extractor Predicting: 148it [01:44,  1.06it/s]Extractor Predicting: 149it [01:45,  1.16it/s]Extractor Predicting: 150it [01:46,  1.18it/s]Extractor Predicting: 151it [01:46,  1.27it/s]Extractor Predicting: 152it [01:47,  1.33it/s]Extractor Predicting: 153it [01:47,  1.41it/s]Extractor Predicting: 154it [01:48,  1.45it/s]Extractor Predicting: 155it [01:49,  1.40it/s]Extractor Predicting: 156it [01:50,  1.46it/s]Extractor Predicting: 157it [01:50,  1.48it/s]Extractor Predicting: 158it [01:51,  1.51it/s]Extractor Predicting: 159it [01:51,  1.50it/s]Extractor Predicting: 160it [01:52,  1.46it/s]Extractor Predicting: 161it [01:53,  1.50it/s]Extractor Predicting: 162it [01:54,  1.45it/s]Extractor Predicting: 163it [01:54,  1.46it/s]Extractor Predicting: 164it [01:55,  1.46it/s]Extractor Predicting: 165it [01:56,  1.43it/s]Extractor Predicting: 166it [01:56,  1.47it/s]Extractor Predicting: 167it [01:57,  1.48it/s]Extractor Predicting: 168it [01:58,  1.49it/s]Extractor Predicting: 169it [01:58,  1.49it/s]Extractor Predicting: 170it [01:59,  1.41it/s]Extractor Predicting: 171it [02:00,  1.47it/s]Extractor Predicting: 172it [02:00,  1.48it/s]Extractor Predicting: 173it [02:01,  1.47it/s]Extractor Predicting: 174it [02:02,  1.47it/s]Extractor Predicting: 175it [02:02,  1.43it/s]Extractor Predicting: 176it [02:03,  1.44it/s]Extractor Predicting: 177it [02:04,  1.43it/s]Extractor Predicting: 178it [02:05,  1.45it/s]Extractor Predicting: 179it [02:05,  1.44it/s]Extractor Predicting: 180it [02:06,  1.36it/s]Extractor Predicting: 181it [02:07,  1.37it/s]Extractor Predicting: 182it [02:07,  1.39it/s]Extractor Predicting: 183it [02:08,  1.46it/s]Extractor Predicting: 184it [02:09,  1.51it/s]Extractor Predicting: 185it [02:09,  1.46it/s]Extractor Predicting: 186it [02:10,  1.49it/s]Extractor Predicting: 187it [02:11,  1.46it/s]Extractor Predicting: 188it [02:11,  1.49it/s]Extractor Predicting: 189it [02:12,  1.53it/s]Extractor Predicting: 190it [02:13,  1.52it/s]Extractor Predicting: 191it [02:13,  1.54it/s]Extractor Predicting: 192it [02:14,  1.53it/s]Extractor Predicting: 193it [02:15,  1.40it/s]Extractor Predicting: 194it [02:16,  1.43it/s]Extractor Predicting: 195it [02:16,  1.39it/s]Extractor Predicting: 196it [02:17,  1.43it/s]Extractor Predicting: 197it [02:18,  1.49it/s]Extractor Predicting: 198it [02:18,  1.48it/s]Extractor Predicting: 199it [02:19,  1.49it/s]Extractor Predicting: 200it [02:20,  1.46it/s]Extractor Predicting: 201it [02:20,  1.49it/s]Extractor Predicting: 202it [02:21,  1.56it/s]Extractor Predicting: 203it [02:21,  1.58it/s]Extractor Predicting: 204it [02:22,  1.57it/s]Extractor Predicting: 205it [02:23,  1.51it/s]Extractor Predicting: 206it [02:23,  1.53it/s]Extractor Predicting: 207it [02:24,  1.55it/s]Extractor Predicting: 208it [02:25,  1.55it/s]Extractor Predicting: 209it [02:25,  1.59it/s]Extractor Predicting: 210it [02:26,  1.47it/s]Extractor Predicting: 211it [02:27,  1.51it/s]Extractor Predicting: 212it [02:27,  1.51it/s]Extractor Predicting: 213it [02:28,  1.51it/s]Extractor Predicting: 214it [02:29,  1.51it/s]Extractor Predicting: 215it [02:29,  1.46it/s]Extractor Predicting: 216it [02:30,  1.46it/s]Extractor Predicting: 217it [02:31,  1.48it/s]Extractor Predicting: 218it [02:31,  1.50it/s]Extractor Predicting: 219it [02:32,  1.53it/s]Extractor Predicting: 220it [02:33,  1.49it/s]Extractor Predicting: 221it [02:33,  1.54it/s]Extractor Predicting: 222it [02:34,  1.55it/s]Extractor Predicting: 223it [02:35,  1.51it/s]Extractor Predicting: 224it [02:35,  1.72it/s]Extractor Predicting: 224it [02:35,  1.44it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3993231810490694,
  "recall": 0.04024556616643929,
  "score": 0.07312161115414408,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 30214
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30314, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.46it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.58it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:29,  1.48it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.59it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:36,  1.57it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:37,  1.67it/s]Extractor Predicting: 60it [00:38,  1.70it/s]Extractor Predicting: 61it [00:39,  1.65it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.60it/s]Extractor Predicting: 64it [00:40,  1.68it/s]Extractor Predicting: 65it [00:41,  1.69it/s]Extractor Predicting: 66it [00:42,  1.67it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:43,  1.65it/s]Extractor Predicting: 69it [00:43,  1.65it/s]Extractor Predicting: 70it [00:44,  1.63it/s]Extractor Predicting: 71it [00:45,  1.67it/s]Extractor Predicting: 72it [00:45,  1.66it/s]Extractor Predicting: 73it [00:46,  1.70it/s]Extractor Predicting: 74it [00:46,  1.70it/s]Extractor Predicting: 75it [00:47,  1.70it/s]Extractor Predicting: 76it [00:48,  1.70it/s]Extractor Predicting: 77it [00:48,  1.47it/s]Extractor Predicting: 78it [00:49,  1.53it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:50,  1.50it/s]Extractor Predicting: 81it [00:51,  1.53it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:52,  1.56it/s]Extractor Predicting: 84it [00:53,  1.60it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:54,  1.64it/s]Extractor Predicting: 87it [00:55,  1.68it/s]Extractor Predicting: 88it [00:55,  1.68it/s]Extractor Predicting: 89it [00:56,  1.67it/s]Extractor Predicting: 90it [00:57,  1.61it/s]Extractor Predicting: 91it [00:57,  1.53it/s]Extractor Predicting: 92it [00:58,  1.58it/s]Extractor Predicting: 93it [00:58,  1.67it/s]Extractor Predicting: 94it [00:59,  1.68it/s]Extractor Predicting: 95it [01:00,  1.69it/s]Extractor Predicting: 96it [01:00,  1.59it/s]Extractor Predicting: 97it [01:01,  1.69it/s]Extractor Predicting: 98it [01:01,  1.71it/s]Extractor Predicting: 99it [01:02,  1.73it/s]Extractor Predicting: 100it [01:02,  1.75it/s]Extractor Predicting: 101it [01:03,  1.75it/s]Extractor Predicting: 102it [01:04,  1.69it/s]Extractor Predicting: 103it [01:04,  1.65it/s]Extractor Predicting: 104it [01:05,  1.68it/s]Extractor Predicting: 105it [01:05,  1.69it/s]Extractor Predicting: 106it [01:06,  1.72it/s]Extractor Predicting: 107it [01:07,  1.74it/s]Extractor Predicting: 108it [01:07,  1.67it/s]Extractor Predicting: 109it [01:08,  1.70it/s]Extractor Predicting: 110it [01:08,  1.69it/s]Extractor Predicting: 111it [01:09,  1.74it/s]Extractor Predicting: 112it [01:09,  1.75it/s]Extractor Predicting: 113it [01:10,  1.76it/s]Extractor Predicting: 114it [01:11,  1.67it/s]Extractor Predicting: 115it [01:11,  1.72it/s]Extractor Predicting: 116it [01:12,  1.70it/s]Extractor Predicting: 117it [01:12,  1.75it/s]Extractor Predicting: 118it [01:13,  1.70it/s]Extractor Predicting: 119it [01:14,  1.69it/s]Extractor Predicting: 120it [01:14,  1.64it/s]Extractor Predicting: 121it [01:15,  1.64it/s]Extractor Predicting: 122it [01:15,  1.66it/s]Extractor Predicting: 123it [01:16,  1.66it/s]Extractor Predicting: 124it [01:17,  1.65it/s]Extractor Predicting: 125it [01:17,  1.66it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:18,  1.68it/s]Extractor Predicting: 128it [01:19,  1.65it/s]Extractor Predicting: 129it [01:21,  1.13s/it]Extractor Predicting: 130it [01:22,  1.02s/it]Extractor Predicting: 131it [01:23,  1.11it/s]Extractor Predicting: 132it [01:23,  1.21it/s]Extractor Predicting: 133it [01:24,  1.31it/s]Extractor Predicting: 134it [01:25,  1.40it/s]Extractor Predicting: 135it [01:25,  1.42it/s]Extractor Predicting: 136it [01:26,  1.49it/s]Extractor Predicting: 137it [01:27,  1.55it/s]Extractor Predicting: 138it [01:27,  1.56it/s]Extractor Predicting: 139it [01:28,  1.57it/s]Extractor Predicting: 140it [01:28,  1.54it/s]Extractor Predicting: 141it [01:29,  1.49it/s]Extractor Predicting: 142it [01:30,  1.53it/s]Extractor Predicting: 143it [01:32,  1.17s/it]Extractor Predicting: 144it [01:33,  1.04s/it]Extractor Predicting: 145it [01:34,  1.09it/s]Extractor Predicting: 146it [01:34,  1.17it/s]Extractor Predicting: 147it [01:35,  1.26it/s]Extractor Predicting: 148it [01:36,  1.35it/s]Extractor Predicting: 149it [01:36,  1.36it/s]Extractor Predicting: 150it [01:37,  1.42it/s]Extractor Predicting: 151it [01:38,  1.45it/s]Extractor Predicting: 152it [01:38,  1.48it/s]Extractor Predicting: 153it [01:39,  1.52it/s]Extractor Predicting: 154it [01:39,  1.50it/s]Extractor Predicting: 155it [01:40,  1.52it/s]Extractor Predicting: 156it [01:41,  1.53it/s]Extractor Predicting: 157it [01:41,  1.55it/s]Extractor Predicting: 158it [01:42,  1.59it/s]Extractor Predicting: 159it [01:43,  1.53it/s]Extractor Predicting: 160it [01:43,  1.49it/s]Extractor Predicting: 161it [01:44,  1.51it/s]Extractor Predicting: 162it [01:45,  1.51it/s]Extractor Predicting: 163it [01:45,  1.51it/s]Extractor Predicting: 164it [01:46,  1.53it/s]Extractor Predicting: 165it [01:47,  1.51it/s]Extractor Predicting: 166it [01:47,  1.54it/s]Extractor Predicting: 167it [01:48,  1.53it/s]Extractor Predicting: 168it [01:49,  1.44it/s]Extractor Predicting: 169it [01:49,  1.45it/s]Extractor Predicting: 170it [01:50,  1.50it/s]Extractor Predicting: 171it [01:51,  1.44it/s]Extractor Predicting: 172it [01:51,  1.47it/s]Extractor Predicting: 173it [01:52,  1.42it/s]Extractor Predicting: 174it [01:53,  1.46it/s]Extractor Predicting: 175it [01:53,  1.50it/s]Extractor Predicting: 176it [01:54,  1.54it/s]Extractor Predicting: 177it [01:55,  1.56it/s]Extractor Predicting: 178it [01:55,  1.52it/s]Extractor Predicting: 179it [01:56,  1.55it/s]Extractor Predicting: 180it [01:57,  1.57it/s]Extractor Predicting: 181it [01:57,  1.59it/s]Extractor Predicting: 182it [01:58,  1.65it/s]Extractor Predicting: 183it [01:58,  1.58it/s]Extractor Predicting: 184it [01:59,  1.62it/s]Extractor Predicting: 185it [02:00,  1.59it/s]Extractor Predicting: 186it [02:00,  1.61it/s]Extractor Predicting: 187it [02:01,  1.64it/s]Extractor Predicting: 188it [02:02,  1.56it/s]Extractor Predicting: 189it [02:02,  1.55it/s]Extractor Predicting: 190it [02:03,  1.54it/s]Extractor Predicting: 191it [02:04,  1.56it/s]Extractor Predicting: 192it [02:04,  1.61it/s]Extractor Predicting: 193it [02:05,  1.55it/s]Extractor Predicting: 194it [02:05,  1.57it/s]Extractor Predicting: 195it [02:06,  1.62it/s]Extractor Predicting: 196it [02:07,  1.43it/s]Extractor Predicting: 197it [02:08,  1.48it/s]Extractor Predicting: 198it [02:08,  1.50it/s]Extractor Predicting: 199it [02:09,  1.56it/s]Extractor Predicting: 200it [02:09,  1.56it/s]Extractor Predicting: 201it [02:10,  1.60it/s]Extractor Predicting: 202it [02:11,  1.59it/s]Extractor Predicting: 203it [02:11,  1.54it/s]Extractor Predicting: 204it [02:12,  1.55it/s]Extractor Predicting: 205it [02:13,  1.59it/s]Extractor Predicting: 206it [02:13,  1.59it/s]Extractor Predicting: 207it [02:14,  1.61it/s]Extractor Predicting: 208it [02:14,  1.56it/s]Extractor Predicting: 209it [02:15,  1.55it/s]Extractor Predicting: 210it [02:16,  1.58it/s]Extractor Predicting: 211it [02:16,  1.56it/s]Extractor Predicting: 212it [02:17,  1.58it/s]Extractor Predicting: 213it [02:18,  1.64it/s]Extractor Predicting: 214it [02:18,  1.59it/s]Extractor Predicting: 215it [02:19,  1.52it/s]Extractor Predicting: 216it [02:20,  1.53it/s]Extractor Predicting: 217it [02:20,  1.59it/s]Extractor Predicting: 218it [02:21,  1.60it/s]Extractor Predicting: 219it [02:21,  1.57it/s]Extractor Predicting: 220it [02:22,  1.55it/s]Extractor Predicting: 221it [02:23,  1.59it/s]Extractor Predicting: 222it [02:23,  1.61it/s]Extractor Predicting: 223it [02:24,  1.60it/s]Extractor Predicting: 224it [02:25,  1.64it/s]Extractor Predicting: 225it [02:25,  1.54it/s]Extractor Predicting: 226it [02:26,  1.50it/s]Extractor Predicting: 227it [02:27,  1.57it/s]Extractor Predicting: 228it [02:27,  1.59it/s]Extractor Predicting: 229it [02:28,  1.58it/s]Extractor Predicting: 230it [02:28,  1.54it/s]Extractor Predicting: 231it [02:29,  1.52it/s]Extractor Predicting: 232it [02:30,  1.55it/s]Extractor Predicting: 233it [02:30,  1.57it/s]Extractor Predicting: 234it [02:31,  1.58it/s]Extractor Predicting: 235it [02:32,  1.53it/s]Extractor Predicting: 236it [02:32,  1.55it/s]Extractor Predicting: 237it [02:33,  1.59it/s]Extractor Predicting: 238it [02:34,  1.59it/s]Extractor Predicting: 239it [02:34,  1.62it/s]Extractor Predicting: 240it [02:35,  1.54it/s]Extractor Predicting: 241it [02:36,  1.54it/s]Extractor Predicting: 242it [02:36,  1.60it/s]Extractor Predicting: 243it [02:37,  1.62it/s]Extractor Predicting: 244it [02:37,  1.62it/s]Extractor Predicting: 245it [02:38,  1.48it/s]Extractor Predicting: 246it [02:39,  1.52it/s]Extractor Predicting: 247it [02:39,  1.56it/s]Extractor Predicting: 248it [02:40,  1.57it/s]Extractor Predicting: 249it [02:41,  1.61it/s]Extractor Predicting: 250it [02:41,  1.52it/s]Extractor Predicting: 251it [02:42,  1.57it/s]Extractor Predicting: 252it [02:43,  1.57it/s]Extractor Predicting: 253it [02:43,  1.57it/s]Extractor Predicting: 254it [02:44,  1.56it/s]Extractor Predicting: 255it [02:44,  1.53it/s]Extractor Predicting: 256it [02:45,  1.54it/s]Extractor Predicting: 257it [02:46,  1.58it/s]Extractor Predicting: 258it [02:46,  1.58it/s]Extractor Predicting: 259it [02:47,  1.58it/s]Extractor Predicting: 260it [02:48,  1.58it/s]Extractor Predicting: 261it [02:48,  1.58it/s]Extractor Predicting: 262it [02:49,  1.55it/s]Extractor Predicting: 263it [02:50,  1.52it/s]Extractor Predicting: 264it [02:50,  1.52it/s]Extractor Predicting: 265it [02:51,  1.52it/s]Extractor Predicting: 266it [02:52,  1.52it/s]Extractor Predicting: 267it [02:52,  1.52it/s]Extractor Predicting: 268it [02:53,  1.45it/s]Extractor Predicting: 269it [02:54,  1.50it/s]Extractor Predicting: 270it [02:54,  1.52it/s]Extractor Predicting: 271it [02:55,  1.60it/s]Extractor Predicting: 272it [02:56,  1.55it/s]Extractor Predicting: 273it [02:56,  1.49it/s]Extractor Predicting: 274it [02:57,  1.53it/s]Extractor Predicting: 275it [02:57,  1.54it/s]Extractor Predicting: 276it [02:58,  1.54it/s]Extractor Predicting: 277it [02:59,  1.54it/s]Extractor Predicting: 278it [03:00,  1.48it/s]Extractor Predicting: 279it [03:00,  1.50it/s]Extractor Predicting: 280it [03:01,  1.51it/s]Extractor Predicting: 281it [03:01,  1.54it/s]Extractor Predicting: 282it [03:02,  1.56it/s]Extractor Predicting: 283it [03:03,  1.49it/s]Extractor Predicting: 284it [03:03,  1.52it/s]Extractor Predicting: 285it [03:04,  1.52it/s]Extractor Predicting: 286it [03:05,  1.53it/s]Extractor Predicting: 287it [03:05,  1.52it/s]Extractor Predicting: 288it [03:06,  1.48it/s]Extractor Predicting: 289it [03:07,  1.49it/s]Extractor Predicting: 290it [03:07,  1.50it/s]Extractor Predicting: 291it [03:08,  1.53it/s]Extractor Predicting: 292it [03:09,  1.51it/s]Extractor Predicting: 293it [03:09,  1.47it/s]Extractor Predicting: 294it [03:10,  1.49it/s]Extractor Predicting: 295it [03:11,  1.49it/s]Extractor Predicting: 296it [03:11,  1.50it/s]Extractor Predicting: 297it [03:12,  1.50it/s]Extractor Predicting: 298it [03:13,  1.40it/s]Extractor Predicting: 299it [03:14,  1.40it/s]Extractor Predicting: 300it [03:14,  1.39it/s]Extractor Predicting: 301it [03:15,  1.41it/s]Extractor Predicting: 302it [03:16,  1.41it/s]Extractor Predicting: 303it [03:17,  1.35it/s]Extractor Predicting: 304it [03:17,  1.37it/s]Extractor Predicting: 305it [03:18,  1.43it/s]Extractor Predicting: 306it [03:19,  1.47it/s]Extractor Predicting: 307it [03:19,  1.49it/s]Extractor Predicting: 308it [03:20,  1.51it/s]Extractor Predicting: 309it [03:21,  1.44it/s]Extractor Predicting: 310it [03:21,  1.44it/s]Extractor Predicting: 311it [03:22,  1.44it/s]Extractor Predicting: 312it [03:23,  1.47it/s]Extractor Predicting: 313it [03:23,  1.47it/s]Extractor Predicting: 314it [03:24,  1.36it/s]Extractor Predicting: 315it [03:25,  1.41it/s]Extractor Predicting: 316it [03:26,  1.42it/s]Extractor Predicting: 317it [03:26,  1.45it/s]Extractor Predicting: 318it [03:27,  1.46it/s]Extractor Predicting: 319it [03:28,  1.41it/s]Extractor Predicting: 320it [03:28,  1.44it/s]Extractor Predicting: 321it [03:29,  1.43it/s]Extractor Predicting: 322it [03:30,  1.38it/s]Extractor Predicting: 323it [03:30,  1.41it/s]Extractor Predicting: 324it [03:31,  1.36it/s]Extractor Predicting: 325it [03:32,  1.38it/s]Extractor Predicting: 326it [03:33,  1.24it/s]Extractor Predicting: 327it [03:34,  1.29it/s]Extractor Predicting: 328it [03:34,  1.30it/s]Extractor Predicting: 329it [03:35,  1.35it/s]Extractor Predicting: 330it [03:36,  1.40it/s]Extractor Predicting: 331it [03:36,  1.44it/s]Extractor Predicting: 332it [03:37,  1.41it/s]Extractor Predicting: 333it [03:38,  1.39it/s]Extractor Predicting: 334it [03:39,  1.43it/s]Extractor Predicting: 335it [03:39,  1.45it/s]Extractor Predicting: 336it [03:40,  1.41it/s]Extractor Predicting: 337it [03:41,  1.42it/s]Extractor Predicting: 338it [03:41,  1.42it/s]Extractor Predicting: 339it [03:42,  1.42it/s]Extractor Predicting: 340it [03:43,  1.42it/s]Extractor Predicting: 341it [03:43,  1.42it/s]Extractor Predicting: 342it [03:44,  1.42it/s]Extractor Predicting: 343it [03:45,  1.33it/s]Extractor Predicting: 344it [03:46,  1.36it/s]Extractor Predicting: 345it [03:46,  1.40it/s]Extractor Predicting: 346it [03:47,  1.44it/s]Extractor Predicting: 347it [03:48,  1.48it/s]Extractor Predicting: 348it [03:48,  1.48it/s]Extractor Predicting: 349it [03:49,  1.47it/s]Extractor Predicting: 350it [03:50,  1.47it/s]Extractor Predicting: 351it [03:50,  1.48it/s]Extractor Predicting: 352it [03:51,  1.52it/s]Extractor Predicting: 353it [03:52,  1.48it/s]Extractor Predicting: 354it [03:52,  1.54it/s]Extractor Predicting: 355it [03:53,  1.54it/s]Extractor Predicting: 356it [03:54,  1.57it/s]Extractor Predicting: 357it [03:54,  1.61it/s]Extractor Predicting: 358it [03:55,  1.57it/s]Extractor Predicting: 359it [03:55,  1.56it/s]Extractor Predicting: 360it [03:56,  1.61it/s]Extractor Predicting: 361it [03:57,  1.58it/s]Extractor Predicting: 362it [03:57,  1.58it/s]Extractor Predicting: 363it [03:58,  1.55it/s]Extractor Predicting: 364it [03:59,  1.59it/s]Extractor Predicting: 365it [03:59,  1.59it/s]Extractor Predicting: 366it [04:00,  1.62it/s]Extractor Predicting: 367it [04:00,  1.64it/s]Extractor Predicting: 368it [04:01,  1.55it/s]Extractor Predicting: 369it [04:02,  1.55it/s]Extractor Predicting: 370it [04:02,  1.54it/s]Extractor Predicting: 371it [04:03,  1.50it/s]Extractor Predicting: 372it [04:04,  1.56it/s]Extractor Predicting: 373it [04:04,  1.51it/s]Extractor Predicting: 374it [04:05,  1.52it/s]Extractor Predicting: 375it [04:06,  1.52it/s]Extractor Predicting: 376it [04:06,  1.52it/s]Extractor Predicting: 377it [04:07,  1.53it/s]Extractor Predicting: 378it [04:08,  1.55it/s]Extractor Predicting: 379it [04:08,  1.63it/s]Extractor Predicting: 380it [04:09,  1.68it/s]Extractor Predicting: 381it [04:09,  1.64it/s]Extractor Predicting: 382it [04:10,  1.62it/s]Extractor Predicting: 383it [04:11,  1.61it/s]Extractor Predicting: 384it [04:11,  1.62it/s]Extractor Predicting: 385it [04:12,  1.67it/s]Extractor Predicting: 386it [04:12,  1.65it/s]Extractor Predicting: 387it [04:13,  1.71it/s]Extractor Predicting: 388it [04:14,  1.69it/s]Extractor Predicting: 389it [04:14,  1.64it/s]Extractor Predicting: 390it [04:15,  1.59it/s]Extractor Predicting: 391it [04:16,  1.63it/s]Extractor Predicting: 392it [04:16,  1.59it/s]Extractor Predicting: 393it [04:17,  1.63it/s]Extractor Predicting: 394it [04:17,  1.66it/s]Extractor Predicting: 395it [04:18,  1.57it/s]Extractor Predicting: 396it [04:19,  1.57it/s]Extractor Predicting: 397it [04:19,  1.61it/s]Extractor Predicting: 398it [04:20,  1.61it/s]Extractor Predicting: 399it [04:21,  1.62it/s]Extractor Predicting: 400it [04:21,  1.64it/s]Extractor Predicting: 401it [04:22,  1.67it/s]Extractor Predicting: 402it [04:22,  1.61it/s]Extractor Predicting: 403it [04:23,  1.62it/s]Extractor Predicting: 404it [04:24,  1.60it/s]Extractor Predicting: 405it [04:24,  1.60it/s]Extractor Predicting: 406it [04:25,  1.56it/s]Extractor Predicting: 407it [04:26,  1.45it/s]Extractor Predicting: 408it [04:26,  1.50it/s]Extractor Predicting: 409it [04:27,  1.55it/s]Extractor Predicting: 410it [04:28,  1.56it/s]Extractor Predicting: 411it [04:28,  1.56it/s]Extractor Predicting: 412it [04:29,  1.48it/s]Extractor Predicting: 413it [04:30,  1.53it/s]Extractor Predicting: 414it [04:30,  1.56it/s]Extractor Predicting: 415it [04:31,  1.58it/s]Extractor Predicting: 416it [04:31,  1.56it/s]Extractor Predicting: 417it [04:32,  1.47it/s]Extractor Predicting: 418it [04:33,  1.51it/s]Extractor Predicting: 419it [04:33,  1.56it/s]Extractor Predicting: 420it [04:34,  1.62it/s]Extractor Predicting: 421it [04:35,  1.64it/s]Extractor Predicting: 422it [04:35,  1.62it/s]Extractor Predicting: 423it [04:36,  1.67it/s]Extractor Predicting: 424it [04:36,  1.66it/s]Extractor Predicting: 425it [04:37,  1.69it/s]Extractor Predicting: 426it [04:38,  1.66it/s]Extractor Predicting: 427it [04:38,  1.56it/s]Extractor Predicting: 428it [04:39,  1.53it/s]Extractor Predicting: 429it [04:40,  1.55it/s]Extractor Predicting: 430it [04:40,  1.58it/s]Extractor Predicting: 431it [04:41,  1.61it/s]Extractor Predicting: 432it [04:41,  1.57it/s]Extractor Predicting: 433it [04:42,  1.57it/s]Extractor Predicting: 434it [04:43,  1.55it/s]Extractor Predicting: 435it [04:43,  1.57it/s]Extractor Predicting: 436it [04:44,  1.59it/s]Extractor Predicting: 437it [04:45,  1.51it/s]Extractor Predicting: 438it [04:45,  1.52it/s]Extractor Predicting: 439it [04:46,  1.53it/s]Extractor Predicting: 440it [04:47,  1.52it/s]Extractor Predicting: 441it [04:47,  1.56it/s]Extractor Predicting: 442it [04:48,  1.48it/s]Extractor Predicting: 443it [04:49,  1.53it/s]Extractor Predicting: 444it [04:49,  1.52it/s]Extractor Predicting: 445it [04:50,  1.52it/s]Extractor Predicting: 446it [04:51,  1.51it/s]Extractor Predicting: 447it [04:52,  1.34it/s]Extractor Predicting: 448it [04:52,  1.40it/s]Extractor Predicting: 449it [04:53,  1.39it/s]Extractor Predicting: 450it [04:54,  1.47it/s]Extractor Predicting: 451it [04:54,  1.47it/s]Extractor Predicting: 452it [04:55,  1.49it/s]Extractor Predicting: 453it [04:56,  1.51it/s]Extractor Predicting: 454it [04:56,  1.44it/s]Extractor Predicting: 455it [04:57,  1.48it/s]Extractor Predicting: 456it [04:58,  1.50it/s]Extractor Predicting: 457it [04:58,  1.50it/s]Extractor Predicting: 458it [04:59,  1.52it/s]Extractor Predicting: 459it [05:00,  1.52it/s]Extractor Predicting: 460it [05:00,  1.52it/s]Extractor Predicting: 461it [05:01,  1.53it/s]Extractor Predicting: 462it [05:01,  1.53it/s]Extractor Predicting: 463it [05:02,  1.55it/s]Extractor Predicting: 464it [05:03,  1.51it/s]Extractor Predicting: 465it [05:03,  1.54it/s]Extractor Predicting: 466it [05:04,  1.53it/s]Extractor Predicting: 467it [05:05,  1.55it/s]Extractor Predicting: 468it [05:05,  1.57it/s]Extractor Predicting: 469it [05:06,  1.48it/s]Extractor Predicting: 470it [05:07,  1.49it/s]Extractor Predicting: 471it [05:07,  1.51it/s]Extractor Predicting: 472it [05:08,  1.56it/s]Extractor Predicting: 473it [05:09,  1.55it/s]Extractor Predicting: 474it [05:09,  1.48it/s]Extractor Predicting: 475it [05:10,  1.53it/s]Extractor Predicting: 476it [05:11,  1.56it/s]Extractor Predicting: 477it [05:11,  1.61it/s]Extractor Predicting: 478it [05:12,  1.65it/s]Extractor Predicting: 479it [05:12,  1.57it/s]Extractor Predicting: 480it [05:13,  1.54it/s]Extractor Predicting: 481it [05:14,  1.51it/s]Extractor Predicting: 482it [05:15,  1.50it/s]Extractor Predicting: 483it [05:15,  1.45it/s]Extractor Predicting: 484it [05:16,  1.43it/s]Extractor Predicting: 485it [05:17,  1.44it/s]Extractor Predicting: 486it [05:17,  1.49it/s]Extractor Predicting: 487it [05:18,  1.54it/s]Extractor Predicting: 488it [05:18,  1.59it/s]Extractor Predicting: 489it [05:19,  1.47it/s]Extractor Predicting: 490it [05:20,  1.50it/s]Extractor Predicting: 491it [05:21,  1.48it/s]Extractor Predicting: 492it [05:21,  1.51it/s]Extractor Predicting: 493it [05:22,  1.52it/s]Extractor Predicting: 494it [05:23,  1.52it/s]Extractor Predicting: 495it [05:23,  1.54it/s]Extractor Predicting: 496it [05:24,  1.51it/s]Extractor Predicting: 497it [05:24,  1.55it/s]Extractor Predicting: 498it [05:25,  1.54it/s]Extractor Predicting: 499it [05:26,  1.55it/s]Extractor Predicting: 500it [05:26,  1.57it/s]Extractor Predicting: 501it [05:27,  1.50it/s]Extractor Predicting: 502it [05:28,  1.56it/s]Extractor Predicting: 503it [05:28,  1.61it/s]Extractor Predicting: 504it [05:29,  1.63it/s]Extractor Predicting: 505it [05:29,  1.64it/s]Extractor Predicting: 506it [05:30,  1.53it/s]Extractor Predicting: 507it [05:31,  1.48it/s]Extractor Predicting: 508it [05:32,  1.49it/s]Extractor Predicting: 509it [05:32,  1.50it/s]Extractor Predicting: 510it [05:33,  1.56it/s]Extractor Predicting: 511it [05:34,  1.41it/s]Extractor Predicting: 512it [05:34,  1.47it/s]Extractor Predicting: 513it [05:35,  1.48it/s]Extractor Predicting: 514it [05:36,  1.48it/s]Extractor Predicting: 515it [05:36,  1.51it/s]Extractor Predicting: 516it [05:37,  1.47it/s]Extractor Predicting: 517it [05:38,  1.46it/s]Extractor Predicting: 518it [05:38,  1.48it/s]Extractor Predicting: 519it [05:39,  1.50it/s]Extractor Predicting: 520it [05:40,  1.53it/s]Extractor Predicting: 521it [05:40,  1.48it/s]Extractor Predicting: 522it [05:41,  1.50it/s]Extractor Predicting: 523it [05:42,  1.55it/s]Extractor Predicting: 524it [05:42,  1.55it/s]Extractor Predicting: 525it [05:43,  1.53it/s]Extractor Predicting: 526it [05:44,  1.45it/s]Extractor Predicting: 527it [05:44,  1.44it/s]Extractor Predicting: 528it [05:45,  1.47it/s]Extractor Predicting: 529it [05:46,  1.47it/s]Extractor Predicting: 530it [05:46,  1.47it/s]Extractor Predicting: 531it [05:47,  1.42it/s]Extractor Predicting: 532it [05:48,  1.44it/s]Extractor Predicting: 533it [05:49,  1.46it/s]Extractor Predicting: 534it [05:49,  1.49it/s]Extractor Predicting: 535it [05:50,  1.48it/s]Extractor Predicting: 536it [05:51,  1.41it/s]Extractor Predicting: 537it [05:51,  1.41it/s]Extractor Predicting: 538it [05:52,  1.43it/s]Extractor Predicting: 539it [05:53,  1.44it/s]Extractor Predicting: 540it [05:53,  1.46it/s]Extractor Predicting: 541it [05:54,  1.44it/s]Extractor Predicting: 542it [05:55,  1.41it/s]Extractor Predicting: 543it [05:55,  1.45it/s]Extractor Predicting: 544it [05:56,  1.41it/s]Extractor Predicting: 545it [05:57,  1.44it/s]Extractor Predicting: 546it [05:58,  1.45it/s]Extractor Predicting: 547it [05:58,  1.38it/s]Extractor Predicting: 548it [05:59,  1.42it/s]Extractor Predicting: 549it [06:00,  1.43it/s]Extractor Predicting: 550it [06:00,  1.47it/s]Extractor Predicting: 551it [06:01,  1.49it/s]Extractor Predicting: 552it [06:02,  1.45it/s]Extractor Predicting: 553it [06:02,  1.50it/s]Extractor Predicting: 554it [06:03,  1.51it/s]Extractor Predicting: 555it [06:04,  1.49it/s]Extractor Predicting: 556it [06:04,  1.53it/s]Extractor Predicting: 556it [06:04,  1.52it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3482758620689655,
  "recall": 0.03785323439022562,
  "score": 0.06828476776418092,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.45it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:13,  1.42it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:15,  1.39it/s]Extractor Predicting: 25it [00:16,  1.42it/s]Extractor Predicting: 26it [00:17,  1.41it/s]Extractor Predicting: 27it [00:17,  1.44it/s]Extractor Predicting: 28it [00:18,  1.42it/s]Extractor Predicting: 29it [00:19,  1.39it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:20,  1.43it/s]Extractor Predicting: 32it [00:21,  1.40it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:24,  1.53it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.47it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.45it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.54it/s]Extractor Predicting: 49it [00:32,  1.38it/s]Extractor Predicting: 50it [00:33,  1.45it/s]Extractor Predicting: 51it [00:34,  1.47it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.54it/s]Extractor Predicting: 54it [00:36,  1.57it/s]Extractor Predicting: 55it [00:36,  1.67it/s]Extractor Predicting: 56it [00:37,  1.77it/s]Extractor Predicting: 57it [00:37,  1.83it/s]Extractor Predicting: 58it [00:38,  1.86it/s]Extractor Predicting: 59it [00:38,  1.86it/s]Extractor Predicting: 60it [00:39,  1.77it/s]Extractor Predicting: 61it [00:39,  1.79it/s]Extractor Predicting: 62it [00:40,  1.82it/s]Extractor Predicting: 63it [00:40,  1.82it/s]Extractor Predicting: 64it [00:41,  1.83it/s]Extractor Predicting: 65it [00:41,  1.87it/s]Extractor Predicting: 66it [00:42,  1.76it/s]Extractor Predicting: 67it [00:43,  1.78it/s]Extractor Predicting: 68it [00:43,  1.82it/s]Extractor Predicting: 69it [00:44,  1.87it/s]Extractor Predicting: 70it [00:44,  1.91it/s]Extractor Predicting: 71it [00:45,  1.89it/s]Extractor Predicting: 72it [00:45,  1.78it/s]Extractor Predicting: 73it [00:46,  1.89it/s]Extractor Predicting: 74it [00:46,  1.84it/s]Extractor Predicting: 75it [00:47,  1.87it/s]Extractor Predicting: 76it [00:47,  1.84it/s]Extractor Predicting: 77it [00:48,  1.84it/s]Extractor Predicting: 78it [00:49,  1.79it/s]Extractor Predicting: 79it [00:49,  1.82it/s]Extractor Predicting: 80it [00:50,  1.84it/s]Extractor Predicting: 81it [00:50,  1.86it/s]Extractor Predicting: 82it [00:51,  1.81it/s]Extractor Predicting: 83it [00:51,  1.72it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.53it/s]Extractor Predicting: 86it [00:54,  1.50it/s]Extractor Predicting: 87it [00:54,  1.48it/s]Extractor Predicting: 88it [00:55,  1.48it/s]Extractor Predicting: 89it [00:56,  1.44it/s]Extractor Predicting: 90it [00:56,  1.44it/s]Extractor Predicting: 91it [00:57,  1.45it/s]Extractor Predicting: 92it [00:58,  1.45it/s]Extractor Predicting: 93it [00:58,  1.45it/s]Extractor Predicting: 94it [00:59,  1.40it/s]Extractor Predicting: 95it [01:00,  1.43it/s]Extractor Predicting: 96it [01:01,  1.42it/s]Extractor Predicting: 97it [01:01,  1.44it/s]Extractor Predicting: 98it [01:02,  1.46it/s]Extractor Predicting: 99it [01:03,  1.41it/s]Extractor Predicting: 100it [01:03,  1.43it/s]Extractor Predicting: 101it [01:04,  1.46it/s]Extractor Predicting: 102it [01:05,  1.47it/s]Extractor Predicting: 103it [01:05,  1.46it/s]Extractor Predicting: 104it [01:07,  1.10it/s]Extractor Predicting: 104it [01:07,  1.55it/s]
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7297619047619047,
  "recall": 0.11316226693741924,
  "score": 0.19594054658782165,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_4/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_15_seed_4', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/fewrel/unseen_15_seed_4/generator/synthetic.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000, 'num_pseudo_per_label': 50, 'num_train_per_label': 66}
num of filtered data: 4782 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/filtered.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 21951
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22051, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/model', pretrained_wv='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22051, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.342, loss:54892.2184
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.997, loss:2571.0540
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 100, avg_time 0.986, loss:2356.4426
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 200, avg_time 0.989, loss:2197.1156
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 100, avg_time 0.988, loss:2151.2725
>> valid entity prec:0.4614, rec:0.5498, f1:0.5017
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 200, avg_time 2.405, loss:2047.6094
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 100, avg_time 0.991, loss:1946.8039
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 200, avg_time 0.985, loss:1796.5792
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 100, avg_time 1.002, loss:1715.5199
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 200, avg_time 0.979, loss:1646.1774
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6099, rec:0.4925, f1:0.5450
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 0.980, loss:1539.6295
g_step 1200, step 200, avg_time 0.987, loss:1530.1763
g_step 1300, step 100, avg_time 0.974, loss:1455.0239
g_step 1400, step 200, avg_time 0.992, loss:1392.8736
g_step 1500, step 100, avg_time 0.985, loss:1341.5625
>> valid entity prec:0.5764, rec:0.5554, f1:0.5657
>> valid relation prec:0.0070, rec:0.0015, f1:0.0024
>> valid relation with NER prec:0.0070, rec:0.0015, f1:0.0024
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 200, avg_time 2.290, loss:1339.6047
g_step 1700, step 100, avg_time 0.956, loss:1290.4845
g_step 1800, step 200, avg_time 0.973, loss:1266.7193
g_step 1900, step 100, avg_time 0.996, loss:1193.2212
g_step 2000, step 200, avg_time 0.983, loss:1231.8590
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4854, rec:0.5446, f1:0.5133
>> valid relation prec:0.0221, rec:0.0026, f1:0.0047
>> valid relation with NER prec:0.0221, rec:0.0026, f1:0.0047
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 100, avg_time 0.996, loss:1151.3164
g_step 2200, step 200, avg_time 0.989, loss:1155.8125
g_step 2300, step 100, avg_time 0.988, loss:1090.9041
g_step 2400, step 200, avg_time 0.989, loss:1132.5294
g_step 2500, step 100, avg_time 0.992, loss:1061.9691
>> valid entity prec:0.5479, rec:0.5488, f1:0.5484
>> valid relation prec:0.1431, rec:0.0264, f1:0.0446
>> valid relation with NER prec:0.1431, rec:0.0264, f1:0.0446
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 200, avg_time 2.315, loss:1061.6854
g_step 2700, step 100, avg_time 0.982, loss:1005.2077
g_step 2800, step 200, avg_time 0.990, loss:1037.7212
g_step 2900, step 100, avg_time 0.979, loss:978.4663
g_step 3000, step 200, avg_time 0.994, loss:984.8754
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5291, rec:0.6585, f1:0.5867
>> valid relation prec:0.0935, rec:0.0319, f1:0.0476
>> valid relation with NER prec:0.0935, rec:0.0319, f1:0.0476
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 100, avg_time 0.986, loss:954.4793
g_step 3200, step 200, avg_time 0.985, loss:948.0442
g_step 3300, step 100, avg_time 0.987, loss:903.4439
g_step 3400, step 200, avg_time 0.988, loss:905.9928
g_step 3500, step 100, avg_time 0.984, loss:852.1522
>> valid entity prec:0.5827, rec:0.4494, f1:0.5074
>> valid relation prec:0.0925, rec:0.0235, f1:0.0375
>> valid relation with NER prec:0.0925, rec:0.0235, f1:0.0375
g_step 3600, step 200, avg_time 2.270, loss:877.9817
g_step 3700, step 100, avg_time 1.000, loss:833.7999
g_step 3800, step 200, avg_time 0.982, loss:832.9518
g_step 3900, step 100, avg_time 0.989, loss:788.2670
g_step 4000, step 200, avg_time 0.990, loss:816.2119
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5794, rec:0.4760, f1:0.5226
>> valid relation prec:0.0984, rec:0.0305, f1:0.0465
>> valid relation with NER prec:0.0984, rec:0.0305, f1:0.0465
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.18s/it]Extractor Predicting: 2it [00:07,  3.39s/it]Extractor Predicting: 3it [00:08,  2.12s/it]Extractor Predicting: 4it [00:09,  1.52s/it]Extractor Predicting: 5it [00:09,  1.21s/it]Extractor Predicting: 6it [00:10,  1.03s/it]Extractor Predicting: 7it [00:11,  1.07it/s]Extractor Predicting: 8it [00:11,  1.16it/s]Extractor Predicting: 9it [00:12,  1.22it/s]Extractor Predicting: 10it [00:13,  1.22it/s]Extractor Predicting: 11it [00:14,  1.31it/s]Extractor Predicting: 12it [00:14,  1.32it/s]Extractor Predicting: 13it [00:15,  1.32it/s]Extractor Predicting: 14it [00:16,  1.36it/s]Extractor Predicting: 15it [00:16,  1.38it/s]Extractor Predicting: 16it [00:17,  1.40it/s]Extractor Predicting: 17it [00:18,  1.38it/s]Extractor Predicting: 18it [00:19,  1.43it/s]Extractor Predicting: 19it [00:19,  1.48it/s]Extractor Predicting: 20it [00:20,  1.46it/s]Extractor Predicting: 21it [00:21,  1.46it/s]Extractor Predicting: 22it [00:21,  1.48it/s]Extractor Predicting: 23it [00:22,  1.49it/s]Extractor Predicting: 24it [00:23,  1.48it/s]Extractor Predicting: 25it [00:23,  1.45it/s]Extractor Predicting: 26it [00:24,  1.43it/s]Extractor Predicting: 27it [00:25,  1.42it/s]Extractor Predicting: 28it [00:25,  1.44it/s]Extractor Predicting: 29it [00:26,  1.41it/s]Extractor Predicting: 30it [00:27,  1.41it/s]Extractor Predicting: 31it [00:28,  1.42it/s]Extractor Predicting: 32it [00:28,  1.44it/s]Extractor Predicting: 33it [00:29,  1.47it/s]Extractor Predicting: 34it [00:30,  1.44it/s]Extractor Predicting: 35it [00:30,  1.49it/s]Extractor Predicting: 36it [00:31,  1.52it/s]Extractor Predicting: 37it [00:31,  1.54it/s]Extractor Predicting: 38it [00:32,  1.51it/s]Extractor Predicting: 39it [00:33,  1.51it/s]Extractor Predicting: 40it [00:33,  1.52it/s]Extractor Predicting: 41it [00:34,  1.50it/s]Extractor Predicting: 42it [00:35,  1.55it/s]Extractor Predicting: 43it [00:35,  1.53it/s]Extractor Predicting: 44it [00:36,  1.48it/s]Extractor Predicting: 45it [00:37,  1.50it/s]Extractor Predicting: 46it [00:37,  1.53it/s]Extractor Predicting: 47it [00:38,  1.58it/s]Extractor Predicting: 48it [00:39,  1.59it/s]Extractor Predicting: 49it [00:39,  1.53it/s]Extractor Predicting: 50it [00:40,  1.52it/s]Extractor Predicting: 51it [00:41,  1.52it/s]Extractor Predicting: 52it [00:41,  1.52it/s]Extractor Predicting: 53it [00:42,  1.51it/s]Extractor Predicting: 54it [00:43,  1.48it/s]Extractor Predicting: 55it [00:43,  1.50it/s]Extractor Predicting: 56it [00:44,  1.53it/s]Extractor Predicting: 57it [00:45,  1.57it/s]Extractor Predicting: 58it [00:45,  1.55it/s]Extractor Predicting: 59it [00:46,  1.54it/s]Extractor Predicting: 60it [00:47,  1.51it/s]Extractor Predicting: 61it [00:47,  1.54it/s]Extractor Predicting: 62it [00:48,  1.60it/s]Extractor Predicting: 63it [00:48,  1.61it/s]Extractor Predicting: 64it [00:49,  1.57it/s]Extractor Predicting: 65it [00:50,  1.60it/s]Extractor Predicting: 66it [00:50,  1.61it/s]Extractor Predicting: 67it [00:51,  1.62it/s]Extractor Predicting: 68it [00:51,  1.63it/s]Extractor Predicting: 69it [00:52,  1.67it/s]Extractor Predicting: 70it [00:53,  1.65it/s]Extractor Predicting: 71it [00:53,  1.56it/s]Extractor Predicting: 72it [00:54,  1.61it/s]Extractor Predicting: 73it [00:55,  1.58it/s]Extractor Predicting: 74it [00:55,  1.58it/s]Extractor Predicting: 75it [00:56,  1.63it/s]Extractor Predicting: 76it [00:56,  1.60it/s]Extractor Predicting: 77it [00:57,  1.63it/s]Extractor Predicting: 78it [00:58,  1.65it/s]Extractor Predicting: 79it [00:58,  1.66it/s]Extractor Predicting: 80it [00:59,  1.65it/s]Extractor Predicting: 81it [01:00,  1.55it/s]Extractor Predicting: 82it [01:00,  1.53it/s]Extractor Predicting: 83it [01:01,  1.57it/s]Extractor Predicting: 84it [01:02,  1.57it/s]Extractor Predicting: 85it [01:02,  1.53it/s]Extractor Predicting: 86it [01:03,  1.45it/s]Extractor Predicting: 87it [01:04,  1.49it/s]Extractor Predicting: 88it [01:04,  1.48it/s]Extractor Predicting: 89it [01:05,  1.48it/s]Extractor Predicting: 90it [01:06,  1.49it/s]Extractor Predicting: 91it [01:06,  1.45it/s]Extractor Predicting: 92it [01:07,  1.45it/s]Extractor Predicting: 93it [01:08,  1.46it/s]Extractor Predicting: 94it [01:08,  1.49it/s]Extractor Predicting: 95it [01:09,  1.48it/s]Extractor Predicting: 96it [01:10,  1.43it/s]Extractor Predicting: 97it [01:10,  1.45it/s]Extractor Predicting: 98it [01:11,  1.47it/s]Extractor Predicting: 99it [01:12,  1.49it/s]Extractor Predicting: 100it [01:12,  1.51it/s]Extractor Predicting: 101it [01:13,  1.36it/s]Extractor Predicting: 102it [01:14,  1.42it/s]Extractor Predicting: 103it [01:15,  1.46it/s]Extractor Predicting: 104it [01:15,  1.50it/s]Extractor Predicting: 105it [01:16,  1.54it/s]Extractor Predicting: 106it [01:17,  1.46it/s]Extractor Predicting: 107it [01:17,  1.47it/s]Extractor Predicting: 108it [01:18,  1.49it/s]Extractor Predicting: 109it [01:19,  1.49it/s]Extractor Predicting: 110it [01:19,  1.49it/s]Extractor Predicting: 111it [01:20,  1.44it/s]Extractor Predicting: 112it [01:21,  1.47it/s]Extractor Predicting: 113it [01:21,  1.51it/s]Extractor Predicting: 114it [01:22,  1.51it/s]Extractor Predicting: 115it [01:23,  1.52it/s]Extractor Predicting: 116it [01:23,  1.55it/s]Extractor Predicting: 117it [01:24,  1.48it/s]Extractor Predicting: 118it [01:25,  1.49it/s]Extractor Predicting: 119it [01:25,  1.51it/s]Extractor Predicting: 120it [01:26,  1.52it/s]Extractor Predicting: 121it [01:27,  1.51it/s]Extractor Predicting: 122it [01:27,  1.44it/s]Extractor Predicting: 123it [01:28,  1.46it/s]Extractor Predicting: 124it [01:29,  1.46it/s]Extractor Predicting: 125it [01:29,  1.49it/s]Extractor Predicting: 126it [01:30,  1.50it/s]Extractor Predicting: 127it [01:31,  1.45it/s]Extractor Predicting: 128it [01:31,  1.44it/s]Extractor Predicting: 129it [01:32,  1.47it/s]Extractor Predicting: 130it [01:33,  1.51it/s]Extractor Predicting: 131it [01:33,  1.49it/s]Extractor Predicting: 132it [01:34,  1.45it/s]Extractor Predicting: 133it [01:35,  1.49it/s]Extractor Predicting: 134it [01:35,  1.47it/s]Extractor Predicting: 135it [01:36,  1.47it/s]Extractor Predicting: 136it [01:37,  1.49it/s]Extractor Predicting: 137it [01:37,  1.47it/s]Extractor Predicting: 138it [01:38,  1.50it/s]Extractor Predicting: 139it [01:39,  1.51it/s]Extractor Predicting: 140it [01:39,  1.54it/s]Extractor Predicting: 141it [01:40,  1.61it/s]Extractor Predicting: 141it [01:40,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2766990291262136,
  "recall": 0.04962275101567034,
  "score": 0.08415354330708662,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.42it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:20,  1.42it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:21,  1.46it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:23,  1.49it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.55it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.54it/s]Extractor Predicting: 59it [00:39,  1.55it/s]Extractor Predicting: 60it [00:39,  1.56it/s]Extractor Predicting: 61it [00:40,  1.48it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:49,  1.51it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.44it/s]Extractor Predicting: 78it [00:51,  1.46it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.49it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:54,  1.46it/s]Extractor Predicting: 83it [00:55,  1.46it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:56,  1.50it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:57,  1.47it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.47it/s]Extractor Predicting: 90it [00:59,  1.48it/s]Extractor Predicting: 91it [01:00,  1.49it/s]Extractor Predicting: 92it [01:01,  1.44it/s]Extractor Predicting: 93it [01:02,  1.44it/s]Extractor Predicting: 94it [01:02,  1.30it/s]Extractor Predicting: 95it [01:03,  1.34it/s]Extractor Predicting: 96it [01:04,  1.35it/s]Extractor Predicting: 97it [01:05,  1.39it/s]Extractor Predicting: 98it [01:05,  1.42it/s]Extractor Predicting: 99it [01:06,  1.43it/s]Extractor Predicting: 100it [01:07,  1.44it/s]Extractor Predicting: 101it [01:07,  1.44it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:09,  1.46it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:11,  1.43it/s]Extractor Predicting: 107it [01:12,  1.40it/s]Extractor Predicting: 108it [01:12,  1.42it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:14,  1.40it/s]Extractor Predicting: 112it [01:15,  1.42it/s]Extractor Predicting: 113it [01:16,  1.47it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:18,  1.50it/s]Extractor Predicting: 118it [01:19,  1.50it/s]Extractor Predicting: 119it [01:20,  1.52it/s]Extractor Predicting: 120it [01:20,  1.53it/s]Extractor Predicting: 121it [01:21,  1.47it/s]Extractor Predicting: 122it [01:22,  1.52it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:23,  1.54it/s]Extractor Predicting: 125it [01:24,  1.54it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:25,  1.51it/s]Extractor Predicting: 128it [01:26,  1.55it/s]Extractor Predicting: 129it [01:26,  1.51it/s]Extractor Predicting: 130it [01:27,  1.52it/s]Extractor Predicting: 131it [01:28,  1.47it/s]Extractor Predicting: 132it [01:28,  1.50it/s]Extractor Predicting: 133it [01:29,  1.50it/s]Extractor Predicting: 134it [01:30,  1.51it/s]Extractor Predicting: 135it [01:30,  1.53it/s]Extractor Predicting: 136it [01:31,  1.48it/s]Extractor Predicting: 137it [01:32,  1.50it/s]Extractor Predicting: 138it [01:32,  1.51it/s]Extractor Predicting: 139it [01:33,  1.55it/s]Extractor Predicting: 140it [01:33,  1.55it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:35,  1.53it/s]Extractor Predicting: 143it [01:35,  1.53it/s]Extractor Predicting: 144it [01:36,  1.50it/s]Extractor Predicting: 145it [01:37,  1.52it/s]Extractor Predicting: 146it [01:38,  1.45it/s]Extractor Predicting: 147it [01:38,  1.46it/s]Extractor Predicting: 148it [01:39,  1.46it/s]Extractor Predicting: 149it [01:40,  1.48it/s]Extractor Predicting: 150it [01:40,  1.50it/s]Extractor Predicting: 151it [01:41,  1.46it/s]Extractor Predicting: 152it [01:42,  1.50it/s]Extractor Predicting: 153it [01:42,  1.52it/s]Extractor Predicting: 154it [01:43,  1.50it/s]Extractor Predicting: 155it [01:44,  1.51it/s]Extractor Predicting: 156it [01:44,  1.44it/s]Extractor Predicting: 157it [01:45,  1.46it/s]Extractor Predicting: 158it [01:46,  1.46it/s]Extractor Predicting: 159it [01:46,  1.47it/s]Extractor Predicting: 160it [01:47,  1.48it/s]Extractor Predicting: 161it [01:48,  1.51it/s]Extractor Predicting: 162it [01:48,  1.53it/s]Extractor Predicting: 163it [01:49,  1.48it/s]Extractor Predicting: 164it [01:50,  1.50it/s]Extractor Predicting: 165it [01:50,  1.49it/s]Extractor Predicting: 166it [01:51,  1.50it/s]Extractor Predicting: 167it [01:52,  1.53it/s]Extractor Predicting: 168it [01:52,  1.45it/s]Extractor Predicting: 169it [01:53,  1.46it/s]Extractor Predicting: 170it [01:54,  1.47it/s]Extractor Predicting: 171it [01:54,  1.45it/s]Extractor Predicting: 172it [01:55,  1.45it/s]Extractor Predicting: 173it [01:56,  1.41it/s]Extractor Predicting: 174it [01:57,  1.40it/s]Extractor Predicting: 175it [01:57,  1.38it/s]Extractor Predicting: 176it [01:58,  1.41it/s]Extractor Predicting: 177it [01:59,  1.44it/s]Extractor Predicting: 178it [01:59,  1.41it/s]Extractor Predicting: 179it [02:00,  1.46it/s]Extractor Predicting: 180it [02:01,  1.43it/s]Extractor Predicting: 181it [02:20,  6.39s/it]Extractor Predicting: 182it [02:21,  4.68s/it]Extractor Predicting: 183it [02:22,  3.48s/it]Extractor Predicting: 184it [02:22,  2.62s/it]Extractor Predicting: 185it [02:23,  2.03s/it]Extractor Predicting: 186it [02:24,  1.60s/it]Extractor Predicting: 187it [02:24,  1.32s/it]Extractor Predicting: 188it [02:25,  1.11s/it]Extractor Predicting: 189it [02:26,  1.03it/s]Extractor Predicting: 190it [02:26,  1.15it/s]Extractor Predicting: 191it [02:27,  1.22it/s]Extractor Predicting: 192it [02:28,  1.27it/s]Extractor Predicting: 193it [02:28,  1.34it/s]Extractor Predicting: 194it [02:29,  1.43it/s]Extractor Predicting: 195it [02:30,  1.46it/s]Extractor Predicting: 196it [02:30,  1.49it/s]Extractor Predicting: 197it [02:31,  1.50it/s]Extractor Predicting: 198it [02:31,  1.51it/s]Extractor Predicting: 199it [02:32,  1.51it/s]Extractor Predicting: 200it [02:33,  1.51it/s]Extractor Predicting: 201it [02:33,  1.52it/s]Extractor Predicting: 202it [02:34,  1.51it/s]Extractor Predicting: 203it [02:35,  1.55it/s]Extractor Predicting: 204it [02:35,  1.57it/s]Extractor Predicting: 205it [02:36,  1.57it/s]Extractor Predicting: 206it [02:37,  1.56it/s]Extractor Predicting: 207it [02:37,  1.46it/s]Extractor Predicting: 208it [02:38,  1.48it/s]Extractor Predicting: 209it [02:39,  1.51it/s]Extractor Predicting: 210it [02:39,  1.51it/s]Extractor Predicting: 211it [02:40,  1.54it/s]Extractor Predicting: 212it [02:41,  1.46it/s]Extractor Predicting: 213it [02:41,  1.51it/s]Extractor Predicting: 214it [02:42,  1.51it/s]Extractor Predicting: 215it [02:43,  1.53it/s]Extractor Predicting: 216it [02:43,  1.54it/s]Extractor Predicting: 217it [02:44,  1.43it/s]Extractor Predicting: 218it [02:45,  1.47it/s]Extractor Predicting: 219it [02:45,  1.51it/s]Extractor Predicting: 220it [02:46,  1.54it/s]Extractor Predicting: 221it [02:47,  1.53it/s]Extractor Predicting: 222it [02:47,  1.49it/s]Extractor Predicting: 223it [02:48,  1.52it/s]Extractor Predicting: 224it [02:49,  1.54it/s]Extractor Predicting: 225it [02:49,  1.53it/s]Extractor Predicting: 226it [02:50,  1.35it/s]Extractor Predicting: 227it [02:51,  1.34it/s]Extractor Predicting: 228it [02:52,  1.35it/s]Extractor Predicting: 229it [02:52,  1.39it/s]Extractor Predicting: 230it [02:53,  1.44it/s]Extractor Predicting: 231it [02:54,  1.44it/s]Extractor Predicting: 232it [02:54,  1.46it/s]Extractor Predicting: 233it [02:55,  1.49it/s]Extractor Predicting: 234it [02:56,  1.40it/s]Extractor Predicting: 235it [02:57,  1.41it/s]Extractor Predicting: 236it [02:57,  1.46it/s]Extractor Predicting: 237it [02:58,  1.51it/s]Extractor Predicting: 238it [02:58,  1.49it/s]Extractor Predicting: 239it [02:59,  1.45it/s]Extractor Predicting: 240it [03:00,  1.45it/s]Extractor Predicting: 241it [03:01,  1.47it/s]Extractor Predicting: 242it [03:01,  1.50it/s]Extractor Predicting: 243it [03:02,  1.51it/s]Extractor Predicting: 244it [03:03,  1.43it/s]Extractor Predicting: 245it [03:03,  1.47it/s]Extractor Predicting: 246it [03:04,  1.49it/s]Extractor Predicting: 247it [03:05,  1.52it/s]Extractor Predicting: 248it [03:05,  1.46it/s]Extractor Predicting: 249it [03:06,  1.43it/s]Extractor Predicting: 250it [03:07,  1.45it/s]Extractor Predicting: 251it [03:07,  1.44it/s]Extractor Predicting: 252it [03:08,  1.45it/s]Extractor Predicting: 253it [03:09,  1.46it/s]Extractor Predicting: 254it [03:09,  1.47it/s]Extractor Predicting: 255it [03:10,  1.49it/s]Extractor Predicting: 256it [03:11,  1.53it/s]Extractor Predicting: 257it [03:11,  1.56it/s]Extractor Predicting: 258it [03:12,  1.54it/s]Extractor Predicting: 259it [03:13,  1.51it/s]Extractor Predicting: 260it [03:13,  1.49it/s]Extractor Predicting: 261it [03:14,  1.50it/s]Extractor Predicting: 262it [03:15,  1.51it/s]Extractor Predicting: 263it [03:15,  1.46it/s]Extractor Predicting: 264it [03:16,  1.46it/s]Extractor Predicting: 265it [03:17,  1.48it/s]Extractor Predicting: 266it [03:17,  1.50it/s]Extractor Predicting: 267it [03:18,  1.50it/s]Extractor Predicting: 268it [03:19,  1.52it/s]Extractor Predicting: 269it [03:19,  1.54it/s]Extractor Predicting: 270it [03:20,  1.57it/s]Extractor Predicting: 271it [03:21,  1.54it/s]Extractor Predicting: 272it [03:21,  1.56it/s]Extractor Predicting: 273it [03:22,  1.54it/s]Extractor Predicting: 274it [03:23,  1.50it/s]Extractor Predicting: 275it [03:23,  1.53it/s]Extractor Predicting: 276it [03:24,  1.54it/s]Extractor Predicting: 277it [03:25,  1.52it/s]Extractor Predicting: 278it [03:25,  1.55it/s]Extractor Predicting: 279it [03:26,  1.51it/s]Extractor Predicting: 280it [03:27,  1.49it/s]Extractor Predicting: 281it [03:27,  1.51it/s]Extractor Predicting: 282it [03:28,  1.52it/s]Extractor Predicting: 283it [03:28,  1.53it/s]Extractor Predicting: 284it [03:29,  1.50it/s]Extractor Predicting: 285it [03:30,  1.45it/s]Extractor Predicting: 286it [03:31,  1.48it/s]Extractor Predicting: 287it [03:31,  1.52it/s]Extractor Predicting: 288it [03:32,  1.51it/s]Extractor Predicting: 289it [03:33,  1.48it/s]Extractor Predicting: 290it [03:33,  1.50it/s]Extractor Predicting: 291it [03:34,  1.50it/s]Extractor Predicting: 292it [03:35,  1.49it/s]Extractor Predicting: 293it [03:35,  1.47it/s]Extractor Predicting: 294it [03:36,  1.43it/s]Extractor Predicting: 295it [03:37,  1.45it/s]Extractor Predicting: 296it [03:37,  1.44it/s]Extractor Predicting: 297it [03:38,  1.48it/s]Extractor Predicting: 298it [03:39,  1.44it/s]Extractor Predicting: 299it [03:39,  1.45it/s]Extractor Predicting: 300it [03:40,  1.43it/s]Extractor Predicting: 301it [03:41,  1.45it/s]Extractor Predicting: 302it [03:41,  1.45it/s]Extractor Predicting: 303it [03:42,  1.44it/s]Extractor Predicting: 304it [03:43,  1.42it/s]Extractor Predicting: 305it [03:44,  1.40it/s]Extractor Predicting: 306it [03:44,  1.39it/s]Extractor Predicting: 307it [03:45,  1.39it/s]Extractor Predicting: 308it [03:46,  1.42it/s]Extractor Predicting: 309it [03:46,  1.44it/s]Extractor Predicting: 310it [03:47,  1.36it/s]Extractor Predicting: 311it [03:48,  1.38it/s]Extractor Predicting: 312it [03:49,  1.41it/s]Extractor Predicting: 313it [03:49,  1.41it/s]Extractor Predicting: 314it [03:50,  1.39it/s]Extractor Predicting: 315it [03:51,  1.40it/s]Extractor Predicting: 316it [03:52,  1.40it/s]Extractor Predicting: 317it [03:52,  1.40it/s]Extractor Predicting: 318it [03:53,  1.41it/s]Extractor Predicting: 319it [03:54,  1.38it/s]Extractor Predicting: 320it [03:54,  1.48it/s]Extractor Predicting: 321it [03:55,  1.52it/s]Extractor Predicting: 322it [03:55,  1.60it/s]Extractor Predicting: 323it [03:56,  1.64it/s]Extractor Predicting: 324it [03:57,  1.61it/s]Extractor Predicting: 325it [03:57,  1.46it/s]Extractor Predicting: 326it [03:58,  1.52it/s]Extractor Predicting: 327it [03:59,  1.55it/s]Extractor Predicting: 328it [03:59,  1.60it/s]Extractor Predicting: 329it [04:00,  1.61it/s]Extractor Predicting: 330it [04:00,  1.63it/s]Extractor Predicting: 331it [04:01,  1.69it/s]Extractor Predicting: 332it [04:02,  1.68it/s]Extractor Predicting: 333it [04:02,  1.72it/s]Extractor Predicting: 334it [04:03,  1.73it/s]Extractor Predicting: 335it [04:03,  1.67it/s]Extractor Predicting: 336it [04:04,  1.67it/s]Extractor Predicting: 337it [04:05,  1.73it/s]Extractor Predicting: 338it [04:05,  1.69it/s]Extractor Predicting: 339it [04:06,  1.69it/s]Extractor Predicting: 340it [04:06,  1.75it/s]Extractor Predicting: 341it [04:07,  1.68it/s]Extractor Predicting: 342it [04:07,  1.71it/s]Extractor Predicting: 343it [04:08,  1.71it/s]Extractor Predicting: 344it [04:09,  1.69it/s]Extractor Predicting: 345it [04:09,  1.72it/s]Extractor Predicting: 346it [04:10,  1.72it/s]Extractor Predicting: 347it [04:10,  1.64it/s]Extractor Predicting: 348it [04:11,  1.61it/s]Extractor Predicting: 349it [04:12,  1.58it/s]Extractor Predicting: 350it [04:12,  1.55it/s]Extractor Predicting: 351it [04:13,  1.54it/s]Extractor Predicting: 352it [04:14,  1.49it/s]Extractor Predicting: 353it [04:14,  1.52it/s]Extractor Predicting: 354it [04:15,  1.51it/s]Extractor Predicting: 355it [04:16,  1.50it/s]Extractor Predicting: 356it [04:16,  1.49it/s]Extractor Predicting: 357it [04:17,  1.44it/s]Extractor Predicting: 358it [04:18,  1.47it/s]Extractor Predicting: 359it [04:19,  1.49it/s]Extractor Predicting: 360it [04:19,  1.52it/s]Extractor Predicting: 361it [04:20,  1.51it/s]Extractor Predicting: 362it [04:20,  1.52it/s]Extractor Predicting: 363it [04:21,  1.51it/s]Extractor Predicting: 364it [04:22,  1.50it/s]Extractor Predicting: 365it [04:23,  1.50it/s]Extractor Predicting: 366it [04:23,  1.50it/s]Extractor Predicting: 367it [04:24,  1.46it/s]Extractor Predicting: 368it [04:25,  1.49it/s]Extractor Predicting: 369it [04:25,  1.49it/s]Extractor Predicting: 370it [04:26,  1.48it/s]Extractor Predicting: 371it [04:27,  1.48it/s]Extractor Predicting: 372it [04:27,  1.49it/s]Extractor Predicting: 373it [04:28,  1.43it/s]Extractor Predicting: 374it [04:29,  1.46it/s]Extractor Predicting: 375it [04:29,  1.48it/s]Extractor Predicting: 376it [04:30,  1.47it/s]Extractor Predicting: 377it [04:31,  1.49it/s]Extractor Predicting: 378it [04:31,  1.45it/s]Extractor Predicting: 379it [04:32,  1.46it/s]Extractor Predicting: 380it [04:33,  1.46it/s]Extractor Predicting: 381it [04:33,  1.45it/s]Extractor Predicting: 382it [04:34,  1.45it/s]Extractor Predicting: 383it [04:35,  1.42it/s]Extractor Predicting: 384it [04:36,  1.42it/s]Extractor Predicting: 385it [04:36,  1.44it/s]Extractor Predicting: 386it [04:37,  1.46it/s]Extractor Predicting: 387it [04:38,  1.44it/s]Extractor Predicting: 388it [04:38,  1.38it/s]Extractor Predicting: 389it [04:39,  1.43it/s]Extractor Predicting: 390it [04:40,  1.45it/s]Extractor Predicting: 391it [04:40,  1.46it/s]Extractor Predicting: 392it [04:41,  1.44it/s]Extractor Predicting: 393it [04:42,  1.43it/s]Extractor Predicting: 394it [04:43,  1.43it/s]Extractor Predicting: 395it [04:43,  1.42it/s]Extractor Predicting: 396it [04:44,  1.43it/s]Extractor Predicting: 397it [04:45,  1.42it/s]Extractor Predicting: 398it [04:45,  1.35it/s]Extractor Predicting: 399it [04:46,  1.35it/s]Extractor Predicting: 400it [04:47,  1.37it/s]Extractor Predicting: 401it [04:48,  1.39it/s]Extractor Predicting: 402it [04:48,  1.43it/s]Extractor Predicting: 403it [04:49,  1.37it/s]Extractor Predicting: 404it [04:50,  1.43it/s]Extractor Predicting: 405it [04:50,  1.43it/s]Extractor Predicting: 406it [04:51,  1.48it/s]Extractor Predicting: 407it [04:52,  1.48it/s]Extractor Predicting: 408it [04:52,  1.41it/s]Extractor Predicting: 409it [04:53,  1.47it/s]Extractor Predicting: 410it [04:54,  1.47it/s]Extractor Predicting: 411it [04:54,  1.51it/s]Extractor Predicting: 412it [04:55,  1.46it/s]Extractor Predicting: 413it [04:56,  1.50it/s]Extractor Predicting: 414it [04:56,  1.50it/s]Extractor Predicting: 415it [04:57,  1.50it/s]Extractor Predicting: 416it [04:58,  1.48it/s]Extractor Predicting: 417it [04:58,  1.46it/s]Extractor Predicting: 418it [04:59,  1.46it/s]Extractor Predicting: 419it [05:00,  1.46it/s]Extractor Predicting: 420it [05:01,  1.48it/s]Extractor Predicting: 421it [05:01,  1.48it/s]Extractor Predicting: 422it [05:02,  1.41it/s]Extractor Predicting: 423it [05:03,  1.44it/s]Extractor Predicting: 424it [05:03,  1.42it/s]Extractor Predicting: 425it [05:04,  1.45it/s]Extractor Predicting: 426it [05:05,  1.45it/s]Extractor Predicting: 427it [05:05,  1.41it/s]Extractor Predicting: 428it [05:06,  1.42it/s]Extractor Predicting: 429it [05:07,  1.45it/s]Extractor Predicting: 430it [05:07,  1.80it/s]Extractor Predicting: 430it [05:07,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4017857142857143,
  "recall": 0.026218683239463973,
  "score": 0.04922515952597994,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.27it/s]Extractor Predicting: 5it [00:03,  1.28it/s]
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.009900990099009901,
  "score": 0.019417475728155338,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_4/extractor/results_multi_is_eval_False.json"
}
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_15_seed_4', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/wiki/unseen_15_seed_4/generator/synthetic.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_4/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_4/extractor/filtered.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl'}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/dev.jsonl', 'labels': ['inception', 'located on terrain feature', 'military branch', 'occupant', 'occupation'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_4/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_4/test.jsonl', 'labels': ['applies to jurisdiction', 'family name', 'father', 'follows', 'genre', 'is a list of', 'located in the administrative territorial entity', 'located on astronomical body', 'lyrics by', 'manufacturer', 'member of', 'part of the series', 'place of birth', 'production company', 'use'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
