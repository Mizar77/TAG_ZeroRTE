/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4991 mean pseudo reward: 0.9749362422843003
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 23696
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23796, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_5_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23796, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.378, loss:3063.4264
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.042, loss:2078.4945
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.042, loss:1764.5297
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.039, loss:1657.1411
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.046, loss:1491.1375
>> valid entity prec:0.7085, rec:0.4677, f1:0.5635
>> valid relation prec:0.6653, rec:0.0921, f1:0.1618
>> valid relation with NER prec:0.6653, rec:0.0921, f1:0.1618
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.487, loss:1496.9627
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.043, loss:1380.3627
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.044, loss:1261.7803
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.049, loss:1224.2458
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.043, loss:1159.1696
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6509, rec:0.5914, f1:0.6197
>> valid relation prec:0.3940, rec:0.0941, f1:0.1519
>> valid relation with NER prec:0.3940, rec:0.0941, f1:0.1519
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.449, loss:1105.6865
g_step 1200, step 160, avg_time 1.052, loss:1074.8784
g_step 1300, step 52, avg_time 1.039, loss:1027.9583
g_step 1400, step 152, avg_time 1.037, loss:961.6148
g_step 1500, step 44, avg_time 1.039, loss:965.9545
>> valid entity prec:0.6798, rec:0.5244, f1:0.5921
>> valid relation prec:0.3330, rec:0.0858, f1:0.1365
>> valid relation with NER prec:0.3330, rec:0.0858, f1:0.1365
g_step 1600, step 144, avg_time 2.459, loss:921.2792
g_step 1700, step 36, avg_time 1.053, loss:924.3203
g_step 1800, step 136, avg_time 1.036, loss:871.4902
g_step 1900, step 28, avg_time 1.038, loss:869.9882
g_step 2000, step 128, avg_time 1.037, loss:815.6428
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6141, rec:0.6266, f1:0.6203
>> valid relation prec:0.2364, rec:0.0695, f1:0.1074
>> valid relation with NER prec:0.2364, rec:0.0695, f1:0.1074
new max entity f1 on valid!
g_step 2100, step 20, avg_time 2.460, loss:827.4511
g_step 2200, step 120, avg_time 1.042, loss:762.7495
g_step 2300, step 12, avg_time 1.045, loss:805.4575
g_step 2400, step 112, avg_time 1.048, loss:738.4290
g_step 2500, step 4, avg_time 1.041, loss:739.4187
>> valid entity prec:0.6119, rec:0.5461, f1:0.5771
>> valid relation prec:0.2489, rec:0.0778, f1:0.1185
>> valid relation with NER prec:0.2489, rec:0.0778, f1:0.1185
g_step 2600, step 104, avg_time 2.451, loss:684.6822
g_step 2700, step 204, avg_time 1.043, loss:705.3207
g_step 2800, step 96, avg_time 1.033, loss:661.7907
g_step 2900, step 196, avg_time 1.051, loss:662.9698
g_step 3000, step 88, avg_time 1.054, loss:638.1024
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6627, rec:0.5101, f1:0.5765
>> valid relation prec:0.2455, rec:0.0749, f1:0.1148
>> valid relation with NER prec:0.2455, rec:0.0749, f1:0.1148
g_step 3100, step 188, avg_time 2.441, loss:647.5760
g_step 3200, step 80, avg_time 1.031, loss:591.1332
g_step 3300, step 180, avg_time 1.049, loss:603.4981
g_step 3400, step 72, avg_time 1.046, loss:569.0444
g_step 3500, step 172, avg_time 1.044, loss:582.0854
>> valid entity prec:0.5580, rec:0.6386, f1:0.5956
>> valid relation prec:0.1884, rec:0.0735, f1:0.1058
>> valid relation with NER prec:0.1884, rec:0.0735, f1:0.1058
g_step 3600, step 64, avg_time 2.458, loss:560.9632
g_step 3700, step 164, avg_time 1.043, loss:533.5178
g_step 3800, step 56, avg_time 1.040, loss:539.2381
g_step 3900, step 156, avg_time 1.039, loss:516.2087
g_step 4000, step 48, avg_time 1.055, loss:519.2968
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6222, rec:0.5953, f1:0.6085
>> valid relation prec:0.1614, rec:0.0626, f1:0.0903
>> valid relation with NER prec:0.1614, rec:0.0626, f1:0.0903
g_step 4100, step 148, avg_time 2.450, loss:487.9118
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:22:28 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:22:28 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-22-28_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:22:29 - WARNING - datasets.builder -   Using custom data configuration default-e9cbe3eeaa05447b
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e9cbe3eeaa05447b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  7.80 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:22:29,845 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:22:29,846 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:22:29,848 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:22:29,849 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:22:29,858 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:22:29,862 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:22:29,862 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:22:29,862 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:22:29,862 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:22:29,863 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:22:29,863 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:22:30,032 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:22:35,110 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:22:35,113 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e9cbe3eeaa05447b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:22:35 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1534f2374830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.57ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.48ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.91ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.14ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.61ba/s]100%|██████████| 6/6 [00:01<00:00,  4.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.95ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.21ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.29ba/s]100%|██████████| 4/4 [00:00<00:00,  5.34ba/s]100%|██████████| 4/4 [00:00<00:00,  4.85ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.90ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.12ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.51ba/s]100%|██████████| 6/6 [00:00<00:00, 12.34ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.86ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.11ba/s]100%|██████████| 4/4 [00:00<00:00, 11.47ba/s]
[INFO|trainer.py:414] 2023-08-27 22:22:38,514 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:22:38,523 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:22:38,523 >>   Num examples = 5005
[INFO|trainer.py:1149] 2023-08-27 22:22:38,523 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:22:38,523 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:22:38,524 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:22:38,524 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:22:38,524 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<03:26,  1.88it/s]  1%|          | 2/390 [00:00<02:29,  2.59it/s]  1%|          | 3/390 [00:01<02:11,  2.94it/s]  1%|          | 4/390 [00:01<02:02,  3.14it/s]  1%|▏         | 5/390 [00:01<01:57,  3.27it/s]  2%|▏         | 6/390 [00:01<01:55,  3.34it/s]  2%|▏         | 7/390 [00:02<01:52,  3.39it/s]  2%|▏         | 8/390 [00:02<01:51,  3.43it/s]  2%|▏         | 9/390 [00:02<01:50,  3.46it/s]  3%|▎         | 10/390 [00:03<01:49,  3.47it/s]  3%|▎         | 11/390 [00:03<01:48,  3.48it/s]  3%|▎         | 12/390 [00:03<01:48,  3.49it/s]  3%|▎         | 13/390 [00:03<01:47,  3.50it/s]  4%|▎         | 14/390 [00:04<01:47,  3.50it/s]  4%|▍         | 15/390 [00:04<01:47,  3.50it/s]  4%|▍         | 16/390 [00:04<01:46,  3.50it/s]  4%|▍         | 17/390 [00:05<01:46,  3.51it/s]  5%|▍         | 18/390 [00:05<01:45,  3.51it/s]  5%|▍         | 19/390 [00:05<01:45,  3.51it/s]  5%|▌         | 20/390 [00:05<01:45,  3.51it/s]  5%|▌         | 21/390 [00:06<01:45,  3.51it/s]  6%|▌         | 22/390 [00:06<01:44,  3.51it/s]  6%|▌         | 23/390 [00:06<01:44,  3.51it/s]  6%|▌         | 24/390 [00:07<01:44,  3.51it/s]  6%|▋         | 25/390 [00:07<01:44,  3.51it/s]  7%|▋         | 26/390 [00:07<01:43,  3.51it/s]  7%|▋         | 27/390 [00:07<01:43,  3.51it/s]  7%|▋         | 28/390 [00:08<01:43,  3.51it/s]  7%|▋         | 29/390 [00:08<01:42,  3.51it/s]  8%|▊         | 30/390 [00:08<01:42,  3.51it/s]  8%|▊         | 31/390 [00:09<01:42,  3.51it/s]  8%|▊         | 32/390 [00:09<01:41,  3.51it/s]  8%|▊         | 33/390 [00:09<01:41,  3.51it/s]  9%|▊         | 34/390 [00:09<01:41,  3.51it/s]  9%|▉         | 35/390 [00:10<01:41,  3.51it/s]  9%|▉         | 36/390 [00:10<01:40,  3.51it/s]  9%|▉         | 37/390 [00:10<01:40,  3.51it/s] 10%|▉         | 38/390 [00:11<01:40,  3.51it/s] 10%|█         | 39/390 [00:11<01:40,  3.51it/s] 10%|█         | 40/390 [00:11<01:39,  3.51it/s] 11%|█         | 41/390 [00:11<01:39,  3.51it/s] 11%|█         | 42/390 [00:12<01:39,  3.51it/s] 11%|█         | 43/390 [00:12<01:39,  3.50it/s] 11%|█▏        | 44/390 [00:12<01:38,  3.50it/s] 12%|█▏        | 45/390 [00:13<01:38,  3.50it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.50it/s] 12%|█▏        | 47/390 [00:13<01:37,  3.50it/s] 12%|█▏        | 48/390 [00:13<01:37,  3.50it/s] 13%|█▎        | 49/390 [00:14<01:37,  3.50it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.50it/s] 13%|█▎        | 51/390 [00:14<01:36,  3.50it/s] 13%|█▎        | 52/390 [00:15<01:36,  3.50it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.49it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.49it/s] 14%|█▍        | 55/390 [00:15<01:35,  3.50it/s] 14%|█▍        | 56/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 58/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 59/390 [00:17<01:34,  3.50it/s] 15%|█▌        | 60/390 [00:17<01:34,  3.50it/s] 16%|█▌        | 61/390 [00:17<01:33,  3.50it/s] 16%|█▌        | 62/390 [00:17<01:33,  3.50it/s] 16%|█▌        | 63/390 [00:18<01:33,  3.50it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.50it/s] 17%|█▋        | 65/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 66/390 [00:19<01:32,  3.50it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.50it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.50it/s] 18%|█▊        | 69/390 [00:19<01:31,  3.50it/s] 18%|█▊        | 70/390 [00:20<01:31,  3.50it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.50it/s] 18%|█▊        | 72/390 [00:20<01:30,  3.50it/s] 19%|█▊        | 73/390 [00:21<01:30,  3.50it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.50it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.50it/s] 19%|█▉        | 76/390 [00:21<01:29,  3.50it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.50it/s] 20%|██        | 78/390 [00:22<01:29,  3.50it/s][INFO|trainer.py:2140] 2023-08-27 22:23:01,085 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:23:01,085 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-27 22:23:01,085 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.50it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.07it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.27it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.56it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.09it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.81it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.37it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.43it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.40it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.35it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.34it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.23it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.33it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.42it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.44it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.40it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.17it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.33it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.31it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.30it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.36it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.33it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.22it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.25it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.34it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.33it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.31it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.35it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.21it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.17it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.28it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.35it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.19it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.18it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.90it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.19it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.22it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.30it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.32it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.19it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.27it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.19it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.23it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.24it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.23it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.27it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.30it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.29it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.34it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.29it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.19it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.21it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.17it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.13it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.17it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.29it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.28it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.20it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.28it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.24it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.22it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.21it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.19it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.13it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.12it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.26it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.27it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.24it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.29it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.26it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.22it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.23it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.13it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.19it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.24it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.24it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.24it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.32it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.24it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.13it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.22it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.15it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.11it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.13it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:31<01:29,  3.50it/s]
100%|██████████| 437/437 [00:09<00:00, 47.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:23:10,366 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-27 22:23:10,391 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:23:12,706 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:23:12,721 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:23:12,779 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:00,  5.21s/it] 21%|██        | 80/390 [00:39<19:17,  3.73s/it] 21%|██        | 81/390 [00:39<13:54,  2.70s/it] 21%|██        | 82/390 [00:40<10:08,  1.97s/it] 21%|██▏       | 83/390 [00:40<07:30,  1.47s/it] 22%|██▏       | 84/390 [00:40<05:40,  1.11s/it] 22%|██▏       | 85/390 [00:40<04:23,  1.16it/s] 22%|██▏       | 86/390 [00:41<03:30,  1.45it/s] 22%|██▏       | 87/390 [00:41<02:52,  1.76it/s] 23%|██▎       | 88/390 [00:41<02:26,  2.07it/s] 23%|██▎       | 89/390 [00:42<02:07,  2.36it/s] 23%|██▎       | 90/390 [00:42<01:54,  2.61it/s] 23%|██▎       | 91/390 [00:42<01:45,  2.83it/s] 24%|██▎       | 92/390 [00:42<01:39,  3.00it/s] 24%|██▍       | 93/390 [00:43<01:34,  3.14it/s] 24%|██▍       | 94/390 [00:43<01:31,  3.24it/s] 24%|██▍       | 95/390 [00:43<01:29,  3.31it/s] 25%|██▍       | 96/390 [00:44<01:27,  3.37it/s] 25%|██▍       | 97/390 [00:44<01:26,  3.41it/s] 25%|██▌       | 98/390 [00:44<01:24,  3.44it/s] 25%|██▌       | 99/390 [00:44<01:24,  3.45it/s] 26%|██▌       | 100/390 [00:45<01:23,  3.46it/s] 26%|██▌       | 101/390 [00:45<01:23,  3.47it/s] 26%|██▌       | 102/390 [00:45<01:22,  3.48it/s] 26%|██▋       | 103/390 [00:46<01:22,  3.49it/s] 27%|██▋       | 104/390 [00:46<01:21,  3.49it/s] 27%|██▋       | 105/390 [00:46<01:21,  3.50it/s] 27%|██▋       | 106/390 [00:46<01:21,  3.50it/s] 27%|██▋       | 107/390 [00:47<01:20,  3.50it/s] 28%|██▊       | 108/390 [00:47<01:20,  3.50it/s] 28%|██▊       | 109/390 [00:47<01:20,  3.50it/s] 28%|██▊       | 110/390 [00:48<01:20,  3.49it/s] 28%|██▊       | 111/390 [00:48<01:19,  3.49it/s] 29%|██▊       | 112/390 [00:48<01:19,  3.49it/s] 29%|██▉       | 113/390 [00:48<01:23,  3.31it/s] 29%|██▉       | 114/390 [00:49<01:22,  3.36it/s] 29%|██▉       | 115/390 [00:49<01:20,  3.40it/s] 30%|██▉       | 116/390 [00:49<01:26,  3.16it/s] 30%|███       | 117/390 [00:50<01:24,  3.25it/s] 30%|███       | 118/390 [00:50<01:21,  3.32it/s] 31%|███       | 119/390 [00:50<01:20,  3.37it/s] 31%|███       | 120/390 [00:51<01:19,  3.38it/s] 31%|███       | 121/390 [00:51<01:18,  3.41it/s] 31%|███▏      | 122/390 [00:51<01:17,  3.44it/s] 32%|███▏      | 123/390 [00:51<01:17,  3.46it/s] 32%|███▏      | 124/390 [00:52<01:16,  3.47it/s] 32%|███▏      | 125/390 [00:52<01:16,  3.48it/s] 32%|███▏      | 126/390 [00:52<01:15,  3.49it/s] 33%|███▎      | 127/390 [00:53<01:15,  3.49it/s] 33%|███▎      | 128/390 [00:53<01:14,  3.49it/s] 33%|███▎      | 129/390 [00:53<01:14,  3.50it/s] 33%|███▎      | 130/390 [00:53<01:14,  3.50it/s] 34%|███▎      | 131/390 [00:54<01:14,  3.50it/s] 34%|███▍      | 132/390 [00:54<01:13,  3.50it/s] 34%|███▍      | 133/390 [00:54<01:13,  3.49it/s] 34%|███▍      | 134/390 [00:55<01:13,  3.49it/s] 35%|███▍      | 135/390 [00:55<01:13,  3.49it/s] 35%|███▍      | 136/390 [00:55<01:12,  3.50it/s] 35%|███▌      | 137/390 [00:55<01:12,  3.49it/s] 35%|███▌      | 138/390 [00:56<01:12,  3.49it/s] 36%|███▌      | 139/390 [00:56<01:11,  3.49it/s] 36%|███▌      | 140/390 [00:56<01:11,  3.49it/s] 36%|███▌      | 141/390 [00:57<01:11,  3.49it/s] 36%|███▋      | 142/390 [00:57<01:11,  3.49it/s] 37%|███▋      | 143/390 [00:57<01:10,  3.49it/s] 37%|███▋      | 144/390 [00:57<01:10,  3.49it/s] 37%|███▋      | 145/390 [00:58<01:10,  3.49it/s] 37%|███▋      | 146/390 [00:58<01:09,  3.49it/s] 38%|███▊      | 147/390 [00:58<01:09,  3.49it/s] 38%|███▊      | 148/390 [00:59<01:09,  3.49it/s] 38%|███▊      | 149/390 [00:59<01:08,  3.49it/s] 38%|███▊      | 150/390 [00:59<01:08,  3.49it/s] 39%|███▊      | 151/390 [00:59<01:08,  3.49it/s] 39%|███▉      | 152/390 [01:00<01:08,  3.49it/s] 39%|███▉      | 153/390 [01:00<01:07,  3.49it/s] 39%|███▉      | 154/390 [01:00<01:07,  3.49it/s] 40%|███▉      | 155/390 [01:01<01:07,  3.48it/s] 40%|████      | 156/390 [01:01<01:07,  3.48it/s][INFO|trainer.py:2140] 2023-08-27 22:23:39,943 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:23:39,944 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-27 22:23:39,944 >>   Batch size = 8
{'eval_loss': 1.0319582223892212, 'eval_runtime': 9.2526, 'eval_samples_per_second': 377.839, 'eval_steps_per_second': 47.23, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.12it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.17it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.49it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.65it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.20it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.98it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.58it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.29it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.27it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.26it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.34it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.41it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.34it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.33it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.37it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.27it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.17it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.10it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.08it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.11it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.23it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.32it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.31it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.20it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.24it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.10it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.07it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.19it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.19it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.11it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.13it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.14it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.22it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.17it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.13it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.10it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.97it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.11it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.14it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.08it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.25it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.25it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.07it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.08it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.09it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.01it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.18it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.11it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.13it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.12it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.18it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.12it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.09it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.11it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.03it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.05it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.23it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.27it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.20it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.13it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.07it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.04it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.02it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.07it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.05it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.15it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.24it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.29it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.29it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.14it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.11it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.02it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.09it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.09it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.11it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.78it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.17it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.26it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.19it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.18it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.96it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.95it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.09it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.12it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.14it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.24it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:10<01:07,  3.48it/s]
100%|██████████| 437/437 [00:09<00:00, 47.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:23:49,244 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-27 22:23:49,263 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:23:52,529 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:23:52,544 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:23:52,557 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:18<21:13,  5.47s/it] 41%|████      | 158/390 [01:19<15:07,  3.91s/it] 41%|████      | 159/390 [01:19<10:52,  2.82s/it] 41%|████      | 160/390 [01:19<07:54,  2.06s/it] 41%|████▏     | 161/390 [01:20<05:50,  1.53s/it] 42%|████▏     | 162/390 [01:20<04:23,  1.16s/it] 42%|████▏     | 163/390 [01:20<03:23,  1.12it/s] 42%|████▏     | 164/390 [01:20<02:41,  1.40it/s] 42%|████▏     | 165/390 [01:21<02:11,  1.71it/s] 43%|████▎     | 166/390 [01:21<01:50,  2.02it/s] 43%|████▎     | 167/390 [01:21<01:36,  2.31it/s] 43%|████▎     | 168/390 [01:22<01:26,  2.57it/s] 43%|████▎     | 169/390 [01:22<01:19,  2.80it/s] 44%|████▎     | 170/390 [01:22<01:13,  2.97it/s] 44%|████▍     | 171/390 [01:22<01:10,  3.12it/s] 44%|████▍     | 172/390 [01:23<01:07,  3.22it/s] 44%|████▍     | 173/390 [01:23<01:05,  3.30it/s] 45%|████▍     | 174/390 [01:23<01:04,  3.36it/s] 45%|████▍     | 175/390 [01:24<01:03,  3.40it/s] 45%|████▌     | 176/390 [01:24<01:02,  3.43it/s] 45%|████▌     | 177/390 [01:24<01:01,  3.45it/s] 46%|████▌     | 178/390 [01:24<01:01,  3.46it/s] 46%|████▌     | 179/390 [01:25<01:00,  3.47it/s] 46%|████▌     | 180/390 [01:25<01:00,  3.47it/s] 46%|████▋     | 181/390 [01:25<01:00,  3.48it/s] 47%|████▋     | 182/390 [01:26<00:59,  3.48it/s] 47%|████▋     | 183/390 [01:26<00:59,  3.49it/s] 47%|████▋     | 184/390 [01:26<00:59,  3.49it/s] 47%|████▋     | 185/390 [01:26<00:58,  3.49it/s] 48%|████▊     | 186/390 [01:27<00:58,  3.50it/s] 48%|████▊     | 187/390 [01:27<00:58,  3.50it/s] 48%|████▊     | 188/390 [01:27<00:57,  3.49it/s] 48%|████▊     | 189/390 [01:28<00:57,  3.49it/s] 49%|████▊     | 190/390 [01:28<00:57,  3.49it/s] 49%|████▉     | 191/390 [01:28<00:57,  3.49it/s] 49%|████▉     | 192/390 [01:28<00:56,  3.49it/s] 49%|████▉     | 193/390 [01:29<00:56,  3.49it/s] 50%|████▉     | 194/390 [01:29<00:56,  3.49it/s] 50%|█████     | 195/390 [01:29<00:55,  3.49it/s] 50%|█████     | 196/390 [01:30<00:55,  3.49it/s] 51%|█████     | 197/390 [01:30<00:55,  3.49it/s] 51%|█████     | 198/390 [01:30<00:54,  3.49it/s] 51%|█████     | 199/390 [01:30<00:54,  3.50it/s] 51%|█████▏    | 200/390 [01:31<00:54,  3.49it/s] 52%|█████▏    | 201/390 [01:31<00:54,  3.50it/s] 52%|█████▏    | 202/390 [01:31<00:54,  3.48it/s] 52%|█████▏    | 203/390 [01:32<00:53,  3.49it/s] 52%|█████▏    | 204/390 [01:32<00:53,  3.49it/s] 53%|█████▎    | 205/390 [01:32<00:52,  3.49it/s] 53%|█████▎    | 206/390 [01:32<00:52,  3.49it/s] 53%|█████▎    | 207/390 [01:33<00:52,  3.49it/s] 53%|█████▎    | 208/390 [01:33<00:52,  3.49it/s] 54%|█████▎    | 209/390 [01:33<00:51,  3.49it/s] 54%|█████▍    | 210/390 [01:34<00:51,  3.49it/s] 54%|█████▍    | 211/390 [01:34<00:51,  3.49it/s] 54%|█████▍    | 212/390 [01:34<00:50,  3.49it/s] 55%|█████▍    | 213/390 [01:34<00:50,  3.48it/s] 55%|█████▍    | 214/390 [01:35<00:50,  3.48it/s] 55%|█████▌    | 215/390 [01:35<00:50,  3.49it/s] 55%|█████▌    | 216/390 [01:35<00:49,  3.49it/s] 56%|█████▌    | 217/390 [01:36<00:49,  3.49it/s] 56%|█████▌    | 218/390 [01:36<00:49,  3.49it/s] 56%|█████▌    | 219/390 [01:36<00:48,  3.49it/s] 56%|█████▋    | 220/390 [01:36<00:48,  3.49it/s] 57%|█████▋    | 221/390 [01:37<00:48,  3.49it/s] 57%|█████▋    | 222/390 [01:37<00:48,  3.49it/s] 57%|█████▋    | 223/390 [01:37<00:47,  3.49it/s] 57%|█████▋    | 224/390 [01:38<00:47,  3.48it/s] 58%|█████▊    | 225/390 [01:38<00:47,  3.48it/s] 58%|█████▊    | 226/390 [01:38<00:47,  3.49it/s] 58%|█████▊    | 227/390 [01:38<00:46,  3.49it/s] 58%|█████▊    | 228/390 [01:39<00:46,  3.49it/s] 59%|█████▊    | 229/390 [01:39<00:46,  3.49it/s] 59%|█████▉    | 230/390 [01:39<00:45,  3.49it/s] 59%|█████▉    | 231/390 [01:40<00:45,  3.49it/s] 59%|█████▉    | 232/390 [01:40<00:45,  3.49it/s] 60%|█████▉    | 233/390 [01:40<00:44,  3.49it/s] 60%|██████    | 234/390 [01:40<00:44,  3.49it/s][INFO|trainer.py:2140] 2023-08-27 22:24:19,542 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:24:19,542 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-27 22:24:19,542 >>   Batch size = 8
{'eval_loss': 1.0304350852966309, 'eval_runtime': 9.2835, 'eval_samples_per_second': 376.583, 'eval_steps_per_second': 47.073, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.86it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.97it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.23it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.60it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.07it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.83it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.47it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.22it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.29it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.30it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.19it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.24it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.15it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.14it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.01it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.03it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.93it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.99it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.07it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.12it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.13it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.26it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.28it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.05it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.10it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.03it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.99it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.09it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.15it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.15it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.16it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.14it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.12it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.02it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.89it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.00it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.14it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.16it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.13it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.25it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.14it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.10it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.06it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.02it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.90it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.11it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.10it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.23it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.21it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.16it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.07it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.07it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.00it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.96it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.01it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.10it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.07it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.16it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.14it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.12it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.08it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.98it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.00it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.01it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.95it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.13it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.16it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.04it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.15it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.11it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.02it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.03it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.04it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.98it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.04it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.11it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.15it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.15it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.14it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.01it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.99it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 44.55it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 45.70it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.16it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.50it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:50<00:44,  3.49it/s]
100%|██████████| 437/437 [00:09<00:00, 46.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:24:28,887 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 22:24:28,910 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:24:31,385 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:24:31,398 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:24:31,424 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:58<13:45,  5.32s/it] 61%|██████    | 236/390 [01:58<09:47,  3.81s/it] 61%|██████    | 237/390 [01:58<07:01,  2.76s/it] 61%|██████    | 238/390 [01:58<05:06,  2.01s/it] 61%|██████▏   | 239/390 [01:59<03:45,  1.50s/it] 62%|██████▏   | 240/390 [01:59<02:49,  1.13s/it] 62%|██████▏   | 241/390 [01:59<02:10,  1.14it/s] 62%|██████▏   | 242/390 [02:00<01:43,  1.43it/s] 62%|██████▏   | 243/390 [02:00<01:24,  1.73it/s] 63%|██████▎   | 244/390 [02:00<01:11,  2.04it/s] 63%|██████▎   | 245/390 [02:00<01:02,  2.33it/s] 63%|██████▎   | 246/390 [02:01<00:55,  2.59it/s] 63%|██████▎   | 247/390 [02:01<00:51,  2.80it/s] 64%|██████▎   | 248/390 [02:01<00:47,  2.98it/s] 64%|██████▍   | 249/390 [02:02<00:45,  3.11it/s] 64%|██████▍   | 250/390 [02:02<00:43,  3.22it/s] 64%|██████▍   | 251/390 [02:02<00:42,  3.30it/s] 65%|██████▍   | 252/390 [02:02<00:41,  3.35it/s] 65%|██████▍   | 253/390 [02:03<00:40,  3.39it/s] 65%|██████▌   | 254/390 [02:03<00:39,  3.42it/s] 65%|██████▌   | 255/390 [02:03<00:39,  3.44it/s] 66%|██████▌   | 256/390 [02:04<00:38,  3.46it/s] 66%|██████▌   | 257/390 [02:04<00:38,  3.47it/s] 66%|██████▌   | 258/390 [02:04<00:38,  3.47it/s] 66%|██████▋   | 259/390 [02:04<00:37,  3.48it/s] 67%|██████▋   | 260/390 [02:05<00:37,  3.48it/s] 67%|██████▋   | 261/390 [02:05<00:37,  3.49it/s] 67%|██████▋   | 262/390 [02:05<00:36,  3.49it/s] 67%|██████▋   | 263/390 [02:06<00:36,  3.49it/s] 68%|██████▊   | 264/390 [02:06<00:36,  3.49it/s] 68%|██████▊   | 265/390 [02:06<00:35,  3.48it/s] 68%|██████▊   | 266/390 [02:06<00:35,  3.48it/s] 68%|██████▊   | 267/390 [02:07<00:35,  3.49it/s] 69%|██████▊   | 268/390 [02:07<00:34,  3.49it/s] 69%|██████▉   | 269/390 [02:07<00:34,  3.46it/s] 69%|██████▉   | 270/390 [02:08<00:34,  3.47it/s] 69%|██████▉   | 271/390 [02:08<00:34,  3.47it/s] 70%|██████▉   | 272/390 [02:08<00:33,  3.48it/s] 70%|███████   | 273/390 [02:08<00:33,  3.48it/s] 70%|███████   | 274/390 [02:09<00:33,  3.49it/s] 71%|███████   | 275/390 [02:09<00:32,  3.49it/s] 71%|███████   | 276/390 [02:09<00:32,  3.49it/s] 71%|███████   | 277/390 [02:10<00:32,  3.49it/s] 71%|███████▏  | 278/390 [02:10<00:32,  3.49it/s] 72%|███████▏  | 279/390 [02:10<00:31,  3.49it/s] 72%|███████▏  | 280/390 [02:10<00:31,  3.48it/s] 72%|███████▏  | 281/390 [02:11<00:31,  3.48it/s] 72%|███████▏  | 282/390 [02:11<00:31,  3.48it/s] 73%|███████▎  | 283/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 284/390 [02:12<00:30,  3.49it/s] 73%|███████▎  | 285/390 [02:12<00:30,  3.49it/s] 73%|███████▎  | 286/390 [02:12<00:29,  3.49it/s] 74%|███████▎  | 287/390 [02:12<00:29,  3.49it/s] 74%|███████▍  | 288/390 [02:13<00:29,  3.49it/s] 74%|███████▍  | 289/390 [02:13<00:28,  3.49it/s] 74%|███████▍  | 290/390 [02:13<00:28,  3.49it/s] 75%|███████▍  | 291/390 [02:14<00:28,  3.46it/s] 75%|███████▍  | 292/390 [02:14<00:28,  3.47it/s] 75%|███████▌  | 293/390 [02:14<00:27,  3.48it/s] 75%|███████▌  | 294/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 295/390 [02:15<00:27,  3.48it/s] 76%|███████▌  | 296/390 [02:15<00:26,  3.48it/s] 76%|███████▌  | 297/390 [02:15<00:26,  3.49it/s] 76%|███████▋  | 298/390 [02:16<00:26,  3.49it/s] 77%|███████▋  | 299/390 [02:16<00:26,  3.49it/s] 77%|███████▋  | 300/390 [02:16<00:25,  3.49it/s] 77%|███████▋  | 301/390 [02:16<00:25,  3.49it/s] 77%|███████▋  | 302/390 [02:17<00:25,  3.47it/s] 78%|███████▊  | 303/390 [02:17<00:24,  3.48it/s] 78%|███████▊  | 304/390 [02:17<00:24,  3.48it/s] 78%|███████▊  | 305/390 [02:18<00:24,  3.49it/s] 78%|███████▊  | 306/390 [02:18<00:24,  3.49it/s] 79%|███████▊  | 307/390 [02:18<00:23,  3.49it/s] 79%|███████▉  | 308/390 [02:18<00:23,  3.49it/s] 79%|███████▉  | 309/390 [02:19<00:23,  3.49it/s] 79%|███████▉  | 310/390 [02:19<00:22,  3.49it/s] 80%|███████▉  | 311/390 [02:19<00:22,  3.49it/s] 80%|████████  | 312/390 [02:20<00:22,  3.49it/s][INFO|trainer.py:2140] 2023-08-27 22:24:58,713 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:24:58,713 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-27 22:24:58,713 >>   Batch size = 8
{'eval_loss': 1.0382918119430542, 'eval_runtime': 9.3224, 'eval_samples_per_second': 375.01, 'eval_steps_per_second': 46.876, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.08it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.28it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.45it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.06it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.80it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.57it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.21it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.13it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.19it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.15it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.27it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.25it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.16it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.15it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.00it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.94it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.90it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.05it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.00it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.09it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.17it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.03it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.06it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.10it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.08it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.99it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.05it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.86it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.01it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.15it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.16it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.10it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.15it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.00it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.00it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.04it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.08it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.02it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.11it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.98it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.04it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.02it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.07it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.97it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.01it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.96it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.01it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.99it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.08it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.17it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.01it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.13it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.09it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.02it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.04it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.99it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.94it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.04it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.17it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.14it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.06it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.02it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.03it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.03it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.02it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.05it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.88it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.12it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.11it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.07it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.11it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.08it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.00it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.99it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.02it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.01it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.04it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.03it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.01it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.04it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.01it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.99it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.02it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.05it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.98it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:29<00:22,  3.49it/s]
100%|██████████| 437/437 [00:09<00:00, 46.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:25:08,054 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-27 22:25:08,074 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:25:10,148 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:25:10,162 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:25:10,171 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:36<06:35,  5.13s/it] 81%|████████  | 314/390 [02:36<04:39,  3.68s/it] 81%|████████  | 315/390 [02:37<03:19,  2.66s/it] 81%|████████  | 316/390 [02:37<02:24,  1.95s/it] 81%|████████▏ | 317/390 [02:37<01:45,  1.45s/it] 82%|████████▏ | 318/390 [02:38<01:19,  1.10s/it] 82%|████████▏ | 319/390 [02:38<01:00,  1.17it/s] 82%|████████▏ | 320/390 [02:38<00:47,  1.46it/s] 82%|████████▏ | 321/390 [02:38<00:39,  1.77it/s] 83%|████████▎ | 322/390 [02:39<00:32,  2.08it/s] 83%|████████▎ | 323/390 [02:39<00:28,  2.36it/s] 83%|████████▎ | 324/390 [02:39<00:25,  2.62it/s] 83%|████████▎ | 325/390 [02:40<00:23,  2.82it/s] 84%|████████▎ | 326/390 [02:40<00:21,  3.00it/s] 84%|████████▍ | 327/390 [02:40<00:20,  3.13it/s] 84%|████████▍ | 328/390 [02:40<00:19,  3.23it/s] 84%|████████▍ | 329/390 [02:41<00:18,  3.30it/s] 85%|████████▍ | 330/390 [02:41<00:17,  3.36it/s] 85%|████████▍ | 331/390 [02:41<00:17,  3.40it/s] 85%|████████▌ | 332/390 [02:42<00:16,  3.43it/s] 85%|████████▌ | 333/390 [02:42<00:16,  3.44it/s] 86%|████████▌ | 334/390 [02:42<00:16,  3.46it/s] 86%|████████▌ | 335/390 [02:42<00:15,  3.47it/s] 86%|████████▌ | 336/390 [02:43<00:15,  3.46it/s] 86%|████████▋ | 337/390 [02:43<00:15,  3.47it/s] 87%|████████▋ | 338/390 [02:43<00:14,  3.47it/s] 87%|████████▋ | 339/390 [02:44<00:14,  3.48it/s] 87%|████████▋ | 340/390 [02:44<00:14,  3.48it/s] 87%|████████▋ | 341/390 [02:44<00:14,  3.49it/s] 88%|████████▊ | 342/390 [02:44<00:13,  3.49it/s] 88%|████████▊ | 343/390 [02:45<00:13,  3.49it/s] 88%|████████▊ | 344/390 [02:45<00:13,  3.49it/s] 88%|████████▊ | 345/390 [02:45<00:12,  3.49it/s] 89%|████████▊ | 346/390 [02:46<00:12,  3.49it/s] 89%|████████▉ | 347/390 [02:46<00:12,  3.48it/s] 89%|████████▉ | 348/390 [02:46<00:12,  3.48it/s] 89%|████████▉ | 349/390 [02:46<00:11,  3.49it/s] 90%|████████▉ | 350/390 [02:47<00:11,  3.49it/s] 90%|█████████ | 351/390 [02:47<00:11,  3.49it/s] 90%|█████████ | 352/390 [02:47<00:10,  3.49it/s] 91%|█████████ | 353/390 [02:48<00:10,  3.49it/s] 91%|█████████ | 354/390 [02:48<00:10,  3.49it/s] 91%|█████████ | 355/390 [02:48<00:10,  3.49it/s] 91%|█████████▏| 356/390 [02:48<00:09,  3.49it/s] 92%|█████████▏| 357/390 [02:49<00:09,  3.48it/s] 92%|█████████▏| 358/390 [02:49<00:09,  3.48it/s] 92%|█████████▏| 359/390 [02:49<00:08,  3.48it/s] 92%|█████████▏| 360/390 [02:50<00:08,  3.43it/s] 93%|█████████▎| 361/390 [02:50<00:08,  3.44it/s] 93%|█████████▎| 362/390 [02:50<00:08,  3.45it/s] 93%|█████████▎| 363/390 [02:50<00:07,  3.46it/s] 93%|█████████▎| 364/390 [02:51<00:07,  3.47it/s] 94%|█████████▎| 365/390 [02:51<00:07,  3.48it/s] 94%|█████████▍| 366/390 [02:51<00:06,  3.48it/s] 94%|█████████▍| 367/390 [02:52<00:06,  3.48it/s] 94%|█████████▍| 368/390 [02:52<00:06,  3.48it/s] 95%|█████████▍| 369/390 [02:52<00:06,  3.47it/s] 95%|█████████▍| 370/390 [02:52<00:05,  3.47it/s] 95%|█████████▌| 371/390 [02:53<00:05,  3.48it/s] 95%|█████████▌| 372/390 [02:53<00:05,  3.48it/s] 96%|█████████▌| 373/390 [02:53<00:04,  3.48it/s] 96%|█████████▌| 374/390 [02:54<00:04,  3.48it/s] 96%|█████████▌| 375/390 [02:54<00:04,  3.48it/s] 96%|█████████▋| 376/390 [02:54<00:04,  3.48it/s] 97%|█████████▋| 377/390 [02:54<00:03,  3.48it/s] 97%|█████████▋| 378/390 [02:55<00:03,  3.49it/s] 97%|█████████▋| 379/390 [02:55<00:03,  3.49it/s] 97%|█████████▋| 380/390 [02:55<00:02,  3.49it/s] 98%|█████████▊| 381/390 [02:56<00:02,  3.49it/s] 98%|█████████▊| 382/390 [02:56<00:02,  3.49it/s] 98%|█████████▊| 383/390 [02:56<00:02,  3.49it/s] 98%|█████████▊| 384/390 [02:56<00:01,  3.49it/s] 99%|█████████▊| 385/390 [02:57<00:01,  3.49it/s] 99%|█████████▉| 386/390 [02:57<00:01,  3.47it/s] 99%|█████████▉| 387/390 [02:57<00:00,  3.48it/s] 99%|█████████▉| 388/390 [02:58<00:00,  3.48it/s]100%|█████████▉| 389/390 [02:58<00:00,  3.48it/s]100%|██████████| 390/390 [02:58<00:00,  3.48it/s][INFO|trainer.py:2140] 2023-08-27 22:25:37,203 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:25:37,203 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-27 22:25:37,203 >>   Batch size = 8
{'eval_loss': 1.0430231094360352, 'eval_runtime': 9.3044, 'eval_samples_per_second': 375.735, 'eval_steps_per_second': 46.967, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.50it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.07it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.16it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.48it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.07it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.73it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.42it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.10it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.09it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.07it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.16it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.15it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.13it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.15it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.20it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.14it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.00it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.98it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.90it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.06it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.07it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.00it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.09it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.16it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.00it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.97it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.00it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.89it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.97it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.04it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.94it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.95it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.08it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.08it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.99it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.99it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.02it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.98it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.11it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.98it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.98it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.12it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.01it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.00it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.05it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.04it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.99it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.09it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.95it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.04it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.11it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.04it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.98it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.06it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.01it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.01it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.05it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.02it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.02it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.09it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.03it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.03it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.09it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.93it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.91it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.06it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.02it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.98it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.08it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.97it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.98it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.03it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.01it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.03it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.02it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.94it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.97it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.03it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.07it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.04it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.95it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.01it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.97it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.93it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.00it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:07<00:00,  3.48it/s]
100%|██████████| 437/437 [00:09<00:00, 47.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:25:46,510 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-27 22:25:46,527 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:25:50,444 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:25:50,469 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:25:50,486 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:25:54,848 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:25:54,851 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156 (score: 1.0304350852966309).
                                                 100%|██████████| 390/390 [03:18<00:00,  3.48it/s]100%|██████████| 390/390 [03:18<00:00,  1.97it/s]
[INFO|trainer.py:1894] 2023-08-27 22:25:56,535 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:25:56,548 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:25:59,694 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:25:59,713 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:25:59,725 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:25:59,920 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:25:59,920 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:25:59,920 >>   train_loss               =     0.8031
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:25:59,920 >>   train_runtime            = 0:03:18.00
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:25:59,920 >>   train_samples            =       5005
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:25:59,920 >>   train_samples_per_second =    126.383
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:25:59,920 >>   train_steps_per_second   =       1.97
{'eval_loss': 1.043765664100647, 'eval_runtime': 9.2945, 'eval_samples_per_second': 376.135, 'eval_steps_per_second': 47.017, 'epoch': 4.99}
{'train_runtime': 198.0085, 'train_samples_per_second': 126.383, 'train_steps_per_second': 1.97, 'train_loss': 0.8031253912510016, 'epoch': 4.99}
08/27/2023 22:25:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:25:59,954 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:25:59,955 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-27 22:25:59,955 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.31it/s]  3%|▎         | 12/437 [00:00<00:08, 51.63it/s]  4%|▍         | 18/437 [00:00<00:08, 49.61it/s]  5%|▌         | 24/437 [00:00<00:08, 48.78it/s]  7%|▋         | 29/437 [00:00<00:08, 48.30it/s]  8%|▊         | 34/437 [00:00<00:08, 48.05it/s]  9%|▉         | 39/437 [00:00<00:08, 47.81it/s] 10%|█         | 44/437 [00:00<00:08, 47.61it/s] 11%|█         | 49/437 [00:01<00:08, 47.58it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.54it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.47it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.48it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.48it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.39it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.43it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.47it/s] 20%|██        | 89/437 [00:01<00:07, 47.35it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.36it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.41it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.36it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.28it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.37it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.39it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.41it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.41it/s] 31%|███       | 134/437 [00:02<00:06, 47.44it/s] 32%|███▏      | 139/437 [00:03<00:08, 36.21it/s] 33%|███▎      | 144/437 [00:03<00:07, 38.99it/s] 34%|███▍      | 149/437 [00:03<00:06, 41.17it/s] 35%|███▌      | 154/437 [00:03<00:06, 42.85it/s] 36%|███▋      | 159/437 [00:03<00:06, 44.12it/s] 38%|███▊      | 164/437 [00:03<00:06, 45.04it/s] 39%|███▊      | 169/437 [00:03<00:05, 45.74it/s] 40%|███▉      | 174/437 [00:03<00:05, 46.24it/s] 41%|████      | 179/437 [00:03<00:05, 46.39it/s] 42%|████▏     | 184/437 [00:03<00:05, 46.64it/s] 43%|████▎     | 189/437 [00:04<00:05, 46.73it/s] 44%|████▍     | 194/437 [00:04<00:05, 46.90it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.08it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.21it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.25it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.30it/s] 50%|█████     | 219/437 [00:04<00:04, 47.33it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.26it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.21it/s] 54%|█████▎    | 234/437 [00:05<00:04, 47.26it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.17it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.05it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.18it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.16it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.24it/s] 60%|██████    | 264/437 [00:05<00:03, 47.26it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.32it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.21it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.22it/s] 65%|██████▍   | 284/437 [00:06<00:03, 47.24it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.24it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.19it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.15it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.29it/s] 71%|███████   | 309/437 [00:06<00:02, 47.28it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.32it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.34it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.23it/s] 75%|███████▌  | 329/437 [00:07<00:02, 47.27it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.26it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.22it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.23it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.24it/s] 81%|████████  | 354/437 [00:07<00:01, 47.24it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.28it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.35it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.26it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.25it/s] 87%|████████▋ | 379/437 [00:08<00:01, 47.24it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.21it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.16it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.13it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.26it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.19it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.14it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.13it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.12it/s] 97%|█████████▋| 424/437 [00:09<00:00, 47.06it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.10it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.16it/s]100%|██████████| 437/437 [00:09<00:00, 46.81it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:26:09,314 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   eval_loss               =     1.0304
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   eval_runtime            = 0:00:09.35
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   eval_samples_per_second =    373.533
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   eval_steps_per_second   =     46.692
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:26:09,314 >>   perplexity              =     2.8023
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_3', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 11540
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11640, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_5_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.21s/it]Extractor Estimating: 2it [00:17,  7.40s/it]Extractor Estimating: 3it [00:19,  4.89s/it]Extractor Estimating: 4it [00:19,  3.19s/it]Extractor Estimating: 5it [00:20,  2.28s/it]Extractor Estimating: 6it [00:21,  1.74s/it]Extractor Estimating: 7it [00:21,  1.38s/it]Extractor Estimating: 8it [00:22,  1.15s/it]Extractor Estimating: 9it [00:23,  1.00s/it]Extractor Estimating: 10it [00:23,  1.12it/s]Extractor Estimating: 11it [00:24,  1.21it/s]Extractor Estimating: 12it [00:25,  1.29it/s]Extractor Estimating: 13it [00:25,  1.34it/s]Extractor Estimating: 14it [00:27,  1.09it/s]Extractor Estimating: 15it [00:27,  1.17it/s]Extractor Estimating: 16it [00:28,  1.26it/s]Extractor Estimating: 17it [00:29,  1.31it/s]Extractor Estimating: 18it [00:30,  1.09it/s]Extractor Estimating: 19it [00:31,  1.20it/s]Extractor Estimating: 20it [00:31,  1.30it/s]Extractor Estimating: 21it [00:32,  1.36it/s]Extractor Estimating: 22it [00:33,  1.42it/s]Extractor Estimating: 23it [00:33,  1.43it/s]Extractor Estimating: 24it [00:34,  1.45it/s]Extractor Estimating: 25it [00:35,  1.49it/s]Extractor Estimating: 26it [00:35,  1.48it/s]Extractor Estimating: 27it [00:36,  1.43it/s]Extractor Estimating: 28it [00:37,  1.45it/s]Extractor Estimating: 29it [00:37,  1.45it/s]Extractor Estimating: 30it [00:38,  1.47it/s]Extractor Estimating: 31it [00:39,  1.47it/s]Extractor Estimating: 32it [00:39,  1.53it/s]Extractor Estimating: 33it [00:40,  1.55it/s]Extractor Estimating: 34it [00:41,  1.54it/s]Extractor Estimating: 35it [00:41,  1.55it/s]Extractor Estimating: 36it [00:42,  1.51it/s]Extractor Estimating: 37it [00:43,  1.54it/s]Extractor Estimating: 38it [00:43,  1.47it/s]Extractor Estimating: 39it [00:44,  1.42it/s]Extractor Estimating: 40it [00:45,  1.43it/s]Extractor Estimating: 41it [00:45,  1.42it/s]Extractor Estimating: 42it [00:47,  1.11it/s]Extractor Estimating: 43it [00:48,  1.12it/s]Extractor Estimating: 44it [00:48,  1.19it/s]Extractor Estimating: 45it [00:49,  1.29it/s]Extractor Estimating: 46it [00:50,  1.31it/s]Extractor Estimating: 47it [00:50,  1.38it/s]Extractor Estimating: 48it [00:51,  1.30it/s]Extractor Estimating: 49it [00:52,  1.39it/s]Extractor Estimating: 50it [00:53,  1.41it/s]Extractor Estimating: 51it [00:53,  1.44it/s]Extractor Estimating: 52it [00:54,  1.48it/s]Extractor Estimating: 53it [00:55,  1.50it/s]Extractor Estimating: 54it [00:55,  1.52it/s]Extractor Estimating: 55it [00:56,  1.50it/s]Extractor Estimating: 56it [00:56,  1.53it/s]Extractor Estimating: 57it [00:57,  1.51it/s]Extractor Estimating: 58it [00:58,  1.50it/s]Extractor Estimating: 59it [00:58,  1.53it/s]Extractor Estimating: 60it [00:59,  1.53it/s]Extractor Estimating: 61it [01:00,  1.55it/s]Extractor Estimating: 62it [01:00,  1.53it/s]Extractor Estimating: 63it [01:01,  1.44it/s]Extractor Estimating: 64it [01:02,  1.46it/s]Extractor Estimating: 65it [01:03,  1.46it/s]Extractor Estimating: 66it [01:03,  1.51it/s]Extractor Estimating: 67it [01:04,  1.49it/s]Extractor Estimating: 68it [01:05,  1.45it/s]Extractor Estimating: 69it [01:05,  1.52it/s]Extractor Estimating: 70it [01:06,  1.55it/s]Extractor Estimating: 71it [01:06,  1.55it/s]Extractor Estimating: 72it [01:07,  1.53it/s]Extractor Estimating: 73it [01:08,  1.55it/s]Extractor Estimating: 74it [01:08,  1.51it/s]Extractor Estimating: 75it [01:09,  1.54it/s]Extractor Estimating: 76it [01:10,  1.53it/s]Extractor Estimating: 77it [01:10,  1.53it/s]Extractor Estimating: 78it [01:11,  1.58it/s]Extractor Estimating: 79it [01:12,  1.53it/s]Extractor Estimating: 80it [01:12,  1.54it/s]Extractor Estimating: 81it [01:13,  1.52it/s]Extractor Estimating: 82it [01:14,  1.53it/s]Extractor Estimating: 83it [01:14,  1.54it/s]Extractor Estimating: 84it [01:15,  1.57it/s]Extractor Estimating: 85it [01:15,  1.60it/s]Extractor Estimating: 86it [01:16,  1.50it/s]Extractor Estimating: 87it [01:18,  1.17it/s]Extractor Estimating: 88it [01:18,  1.22it/s]Extractor Estimating: 89it [01:21,  1.31s/it]Extractor Estimating: 90it [01:21,  1.10s/it]Extractor Estimating: 91it [01:22,  1.02it/s]Extractor Estimating: 92it [01:23,  1.17it/s]Extractor Estimating: 93it [01:23,  1.27it/s]Extractor Estimating: 94it [01:24,  1.35it/s]Extractor Estimating: 95it [01:24,  1.44it/s]Extractor Estimating: 96it [01:25,  1.47it/s]Extractor Estimating: 97it [01:26,  1.51it/s]Extractor Estimating: 98it [01:26,  1.55it/s]Extractor Estimating: 99it [01:27,  1.54it/s]Extractor Estimating: 100it [01:28,  1.55it/s]Extractor Estimating: 101it [01:28,  1.52it/s]Extractor Estimating: 102it [01:29,  1.51it/s]Extractor Estimating: 103it [01:30,  1.54it/s]Extractor Estimating: 104it [01:30,  1.50it/s]Extractor Estimating: 105it [01:31,  1.52it/s]Extractor Estimating: 106it [01:32,  1.49it/s]Extractor Estimating: 107it [01:32,  1.54it/s]Extractor Estimating: 108it [01:33,  1.55it/s]Extractor Estimating: 109it [01:34,  1.49it/s]Extractor Estimating: 110it [01:34,  1.51it/s]Extractor Estimating: 111it [01:35,  1.51it/s]Extractor Estimating: 112it [01:36,  1.49it/s]Extractor Estimating: 113it [01:36,  1.51it/s]Extractor Estimating: 114it [01:37,  1.56it/s]Extractor Estimating: 115it [01:37,  1.52it/s]Extractor Estimating: 116it [01:38,  1.53it/s]Extractor Estimating: 117it [01:39,  1.56it/s]Extractor Estimating: 118it [01:39,  1.54it/s]Extractor Estimating: 119it [01:40,  1.56it/s]Extractor Estimating: 120it [01:41,  1.50it/s]Extractor Estimating: 121it [01:41,  1.50it/s]Extractor Estimating: 122it [01:42,  1.54it/s]Extractor Estimating: 123it [01:43,  1.49it/s]Extractor Estimating: 124it [01:43,  1.51it/s]Extractor Estimating: 125it [01:44,  1.50it/s]Extractor Estimating: 126it [01:45,  1.43it/s]Extractor Estimating: 127it [01:46,  1.45it/s]Extractor Estimating: 128it [01:46,  1.47it/s]Extractor Estimating: 129it [01:47,  1.47it/s]Extractor Estimating: 130it [01:47,  1.52it/s]Extractor Estimating: 131it [01:48,  1.53it/s]Extractor Estimating: 132it [01:49,  1.55it/s]Extractor Estimating: 133it [01:49,  1.51it/s]Extractor Estimating: 134it [01:50,  1.52it/s]Extractor Estimating: 135it [01:51,  1.54it/s]Extractor Estimating: 136it [01:51,  1.58it/s]Extractor Estimating: 137it [01:52,  1.55it/s]Extractor Estimating: 138it [01:53,  1.59it/s]Extractor Estimating: 139it [01:53,  1.59it/s]Extractor Estimating: 140it [01:54,  1.57it/s]Extractor Estimating: 141it [01:55,  1.54it/s]Extractor Estimating: 142it [01:55,  1.51it/s]Extractor Estimating: 143it [01:56,  1.52it/s]Extractor Estimating: 144it [01:57,  1.46it/s]Extractor Estimating: 145it [01:57,  1.48it/s]Extractor Estimating: 146it [01:58,  1.51it/s]Extractor Estimating: 147it [01:59,  1.52it/s]Extractor Estimating: 148it [01:59,  1.52it/s]Extractor Estimating: 149it [02:00,  1.53it/s]Extractor Estimating: 150it [02:00,  1.56it/s]Extractor Estimating: 151it [02:01,  1.54it/s]Extractor Estimating: 152it [02:02,  1.55it/s]Extractor Estimating: 153it [02:02,  1.55it/s]Extractor Estimating: 154it [02:03,  1.45it/s]Extractor Estimating: 155it [02:04,  1.43it/s]Extractor Estimating: 156it [02:05,  1.48it/s]Extractor Estimating: 157it [02:05,  1.55it/s]Extractor Estimating: 158it [02:06,  1.55it/s]Extractor Estimating: 159it [02:06,  1.52it/s]Extractor Estimating: 160it [02:07,  1.50it/s]Extractor Estimating: 161it [02:08,  1.47it/s]Extractor Estimating: 162it [02:09,  1.45it/s]Extractor Estimating: 163it [02:09,  1.44it/s]Extractor Estimating: 164it [02:10,  1.44it/s]Extractor Estimating: 165it [02:11,  1.42it/s]Extractor Estimating: 166it [02:12,  1.37it/s]Extractor Estimating: 167it [02:12,  1.37it/s]Extractor Estimating: 168it [02:13,  1.38it/s]Extractor Estimating: 169it [02:14,  1.43it/s]Extractor Estimating: 170it [02:14,  1.44it/s]Extractor Estimating: 171it [02:15,  1.47it/s]Extractor Estimating: 172it [02:16,  1.44it/s]Extractor Estimating: 173it [02:16,  1.44it/s]Extractor Estimating: 174it [02:17,  1.47it/s]Extractor Estimating: 175it [02:18,  1.37it/s]Extractor Estimating: 176it [02:18,  1.45it/s]Extractor Estimating: 177it [02:19,  1.47it/s]Extractor Estimating: 178it [02:20,  1.43it/s]Extractor Estimating: 179it [02:21,  1.45it/s]Extractor Estimating: 180it [02:21,  1.46it/s]Extractor Estimating: 181it [02:22,  1.46it/s]Extractor Estimating: 182it [02:23,  1.48it/s]Extractor Estimating: 183it [02:23,  1.51it/s]Extractor Estimating: 184it [02:24,  1.53it/s]Extractor Estimating: 185it [02:24,  1.60it/s]Extractor Estimating: 186it [02:25,  1.58it/s]Extractor Estimating: 187it [02:26,  1.52it/s]Extractor Estimating: 188it [02:26,  1.53it/s]Extractor Estimating: 189it [02:27,  1.55it/s]Extractor Estimating: 190it [02:28,  1.56it/s]Extractor Estimating: 191it [02:28,  1.51it/s]Extractor Estimating: 192it [02:29,  1.54it/s]Extractor Estimating: 193it [02:30,  1.43it/s]Extractor Estimating: 194it [02:30,  1.44it/s]Extractor Estimating: 195it [02:31,  1.48it/s]Extractor Estimating: 196it [02:32,  1.53it/s]Extractor Estimating: 197it [02:32,  1.53it/s]Extractor Estimating: 198it [02:33,  1.53it/s]Extractor Estimating: 199it [02:34,  1.53it/s]Extractor Estimating: 200it [02:34,  1.53it/s]Extractor Estimating: 201it [02:35,  1.45it/s]Extractor Estimating: 202it [02:36,  1.47it/s]Extractor Estimating: 203it [02:36,  1.50it/s]Extractor Estimating: 204it [02:37,  1.38it/s]Extractor Estimating: 205it [02:38,  1.45it/s]Extractor Estimating: 206it [02:39,  1.44it/s]Extractor Estimating: 207it [02:39,  1.46it/s]Extractor Estimating: 208it [02:40,  1.46it/s]Extractor Estimating: 209it [02:41,  1.45it/s]Extractor Estimating: 210it [02:41,  1.48it/s]Extractor Estimating: 211it [02:42,  1.44it/s]Extractor Estimating: 212it [02:43,  1.45it/s]Extractor Estimating: 213it [02:43,  1.49it/s]Extractor Estimating: 214it [02:44,  1.47it/s]Extractor Estimating: 215it [02:45,  1.46it/s]Extractor Estimating: 216it [02:45,  1.43it/s]Extractor Estimating: 217it [02:46,  1.46it/s]Extractor Estimating: 218it [02:47,  1.47it/s]Extractor Estimating: 219it [02:47,  1.49it/s]Extractor Estimating: 220it [02:48,  1.46it/s]Extractor Estimating: 221it [02:49,  1.47it/s]Extractor Estimating: 222it [02:49,  1.55it/s]Extractor Estimating: 223it [02:50,  1.50it/s]Extractor Estimating: 224it [02:51,  1.49it/s]Extractor Estimating: 225it [02:51,  1.49it/s]Extractor Estimating: 226it [02:52,  1.48it/s]Extractor Estimating: 227it [02:53,  1.46it/s]Extractor Estimating: 228it [02:53,  1.43it/s]Extractor Estimating: 229it [02:54,  1.47it/s]Extractor Estimating: 230it [02:55,  1.49it/s]Extractor Estimating: 231it [02:55,  1.53it/s]Extractor Estimating: 232it [02:56,  1.51it/s]Extractor Estimating: 233it [02:57,  1.49it/s]Extractor Estimating: 234it [02:57,  1.48it/s]Extractor Estimating: 235it [02:58,  1.53it/s]Extractor Estimating: 236it [02:59,  1.47it/s]Extractor Estimating: 237it [03:00,  1.44it/s]Extractor Estimating: 238it [03:00,  1.49it/s]Extractor Estimating: 239it [03:01,  1.47it/s]Extractor Estimating: 240it [03:02,  1.48it/s]Extractor Estimating: 241it [03:02,  1.41it/s]Extractor Estimating: 242it [03:03,  1.42it/s]Extractor Estimating: 243it [03:04,  1.45it/s]Extractor Estimating: 244it [03:04,  1.51it/s]Extractor Estimating: 245it [03:05,  1.45it/s]Extractor Estimating: 246it [03:06,  1.48it/s]Extractor Estimating: 247it [03:06,  1.50it/s]Extractor Estimating: 248it [03:07,  1.48it/s]Extractor Estimating: 249it [03:08,  1.50it/s]Extractor Estimating: 250it [03:08,  1.48it/s]Extractor Estimating: 250it [03:08,  1.32it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5139 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 24362
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24462, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_5_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24462, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.447, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 85, avg_time 1.106, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 185, avg_time 1.124, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 70, avg_time 1.095, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 170, avg_time 2.243, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 55, avg_time 1.099, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 155, avg_time 1.111, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 40, avg_time 1.097, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 140, avg_time 1.110, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 25, avg_time 2.229, loss:nan
g_step 1200, step 125, avg_time 1.096, loss:nan
g_step 1300, step 10, avg_time 1.117, loss:nan
g_step 1400, step 110, avg_time 1.104, loss:nan
g_step 1500, step 210, avg_time 1.113, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 95, avg_time 2.217, loss:nan
g_step 1700, step 195, avg_time 1.118, loss:nan
g_step 1800, step 80, avg_time 1.100, loss:nan
g_step 1900, step 180, avg_time 1.106, loss:nan
g_step 2000, step 65, avg_time 1.108, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 165, avg_time 2.235, loss:nan
g_step 2200, step 50, avg_time 1.094, loss:nan
g_step 2300, step 150, avg_time 1.117, loss:nan
g_step 2400, step 35, avg_time 1.093, loss:nan
g_step 2500, step 135, avg_time 1.114, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 20, avg_time 2.231, loss:nan
g_step 2700, step 120, avg_time 1.115, loss:nan
g_step 2800, step 5, avg_time 1.088, loss:nan
g_step 2900, step 105, avg_time 1.114, loss:nan
g_step 3000, step 205, avg_time 1.104, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 90, avg_time 2.227, loss:nan
g_step 3200, step 190, avg_time 1.118, loss:nan
g_step 3300, step 75, avg_time 1.095, loss:nan
g_step 3400, step 175, avg_time 1.122, loss:nan
g_step 3500, step 60, avg_time 1.088, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 160, avg_time 2.252, loss:nan
g_step 3700, step 45, avg_time 1.104, loss:nan
g_step 3800, step 145, avg_time 1.103, loss:nan
g_step 3900, step 30, avg_time 1.098, loss:nan
g_step 4000, step 130, avg_time 1.097, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 15, avg_time 2.228, loss:nan
g_step 4200, step 115, avg_time 1.111, loss:nan
g_step 4300, step 215, avg_time 1.106, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:05:19 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:05:19 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-05-19_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:05:20 - WARNING - datasets.builder -   Using custom data configuration default-2f575ffad8af2175
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2f575ffad8af2175/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:05:20,660 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:05:20,661 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:05:20,661 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:05:20,662 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:05:20,670 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:05:20,672 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:05:20,672 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:05:20,672 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:05:20,673 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:05:20,673 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:05:20,673 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:05:20,782 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:05:24,000 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:05:24,006 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2f575ffad8af2175/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:05:24 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1481f53f4290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.22ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.90ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.15ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.27ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.34ba/s]100%|██████████| 6/6 [00:01<00:00,  4.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.75ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.22ba/s]100%|██████████| 4/4 [00:00<00:00,  5.27ba/s]100%|██████████| 4/4 [00:00<00:00,  4.76ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.31ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.53ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.86ba/s]100%|██████████| 6/6 [00:00<00:00, 11.25ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.92ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.85ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.24ba/s]100%|██████████| 4/4 [00:00<00:00, 10.39ba/s]
[INFO|trainer.py:414] 2023-08-28 00:05:27,361 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:05:27,371 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:05:27,372 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:05:27,372 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:05:27,372 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:05:27,372 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:05:27,372 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:05:27,372 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:56,  3.48it/s]  0%|          | 2/405 [00:00<01:51,  3.60it/s]  1%|          | 3/405 [00:00<01:50,  3.63it/s]  1%|          | 4/405 [00:01<01:49,  3.65it/s]  1%|          | 5/405 [00:01<01:49,  3.66it/s]  1%|▏         | 6/405 [00:01<01:48,  3.67it/s]  2%|▏         | 7/405 [00:01<01:48,  3.67it/s]  2%|▏         | 8/405 [00:02<01:48,  3.67it/s]  2%|▏         | 9/405 [00:02<01:47,  3.67it/s]  2%|▏         | 10/405 [00:02<01:47,  3.67it/s]  3%|▎         | 11/405 [00:03<01:47,  3.66it/s]  3%|▎         | 12/405 [00:03<01:47,  3.66it/s]  3%|▎         | 13/405 [00:03<01:46,  3.67it/s]  3%|▎         | 14/405 [00:03<01:46,  3.67it/s]  4%|▎         | 15/405 [00:04<01:46,  3.67it/s]  4%|▍         | 16/405 [00:04<01:46,  3.67it/s]  4%|▍         | 17/405 [00:04<01:45,  3.67it/s]  4%|▍         | 18/405 [00:04<01:45,  3.67it/s]  5%|▍         | 19/405 [00:05<01:45,  3.67it/s]  5%|▍         | 20/405 [00:05<01:45,  3.66it/s]  5%|▌         | 21/405 [00:05<01:44,  3.67it/s]  5%|▌         | 22/405 [00:06<01:44,  3.67it/s]  6%|▌         | 23/405 [00:06<01:44,  3.66it/s]  6%|▌         | 24/405 [00:06<01:43,  3.66it/s]  6%|▌         | 25/405 [00:06<01:43,  3.66it/s]  6%|▋         | 26/405 [00:07<01:43,  3.66it/s]  7%|▋         | 27/405 [00:07<01:43,  3.67it/s]  7%|▋         | 28/405 [00:07<01:42,  3.67it/s]  7%|▋         | 29/405 [00:07<01:42,  3.67it/s]  7%|▋         | 30/405 [00:08<01:42,  3.67it/s]  8%|▊         | 31/405 [00:08<01:41,  3.67it/s]  8%|▊         | 32/405 [00:08<01:41,  3.67it/s]  8%|▊         | 33/405 [00:09<01:41,  3.67it/s]  8%|▊         | 34/405 [00:09<01:41,  3.66it/s]  9%|▊         | 35/405 [00:09<01:40,  3.66it/s]  9%|▉         | 36/405 [00:09<01:40,  3.66it/s]  9%|▉         | 37/405 [00:10<01:40,  3.67it/s]  9%|▉         | 38/405 [00:10<01:40,  3.66it/s] 10%|▉         | 39/405 [00:10<01:39,  3.67it/s] 10%|▉         | 40/405 [00:10<01:39,  3.66it/s] 10%|█         | 41/405 [00:11<01:39,  3.66it/s] 10%|█         | 42/405 [00:11<01:39,  3.66it/s] 11%|█         | 43/405 [00:11<01:38,  3.66it/s] 11%|█         | 44/405 [00:12<01:38,  3.66it/s] 11%|█         | 45/405 [00:12<01:38,  3.67it/s] 11%|█▏        | 46/405 [00:12<01:37,  3.66it/s] 12%|█▏        | 47/405 [00:12<01:37,  3.67it/s] 12%|█▏        | 48/405 [00:13<01:37,  3.67it/s] 12%|█▏        | 49/405 [00:13<01:37,  3.67it/s] 12%|█▏        | 50/405 [00:13<01:36,  3.66it/s] 13%|█▎        | 51/405 [00:13<01:36,  3.67it/s] 13%|█▎        | 52/405 [00:14<01:36,  3.66it/s] 13%|█▎        | 53/405 [00:14<01:36,  3.67it/s] 13%|█▎        | 54/405 [00:14<01:35,  3.66it/s] 14%|█▎        | 55/405 [00:15<01:35,  3.65it/s] 14%|█▍        | 56/405 [00:15<01:35,  3.66it/s] 14%|█▍        | 57/405 [00:15<01:35,  3.66it/s] 14%|█▍        | 58/405 [00:15<01:34,  3.66it/s] 15%|█▍        | 59/405 [00:16<01:34,  3.66it/s] 15%|█▍        | 60/405 [00:16<01:34,  3.66it/s] 15%|█▌        | 61/405 [00:16<01:33,  3.66it/s] 15%|█▌        | 62/405 [00:16<01:33,  3.66it/s] 16%|█▌        | 63/405 [00:17<01:33,  3.66it/s] 16%|█▌        | 64/405 [00:17<01:33,  3.66it/s] 16%|█▌        | 65/405 [00:17<01:32,  3.66it/s] 16%|█▋        | 66/405 [00:18<01:32,  3.66it/s] 17%|█▋        | 67/405 [00:18<01:32,  3.66it/s] 17%|█▋        | 68/405 [00:18<01:32,  3.66it/s] 17%|█▋        | 69/405 [00:18<01:31,  3.66it/s] 17%|█▋        | 70/405 [00:19<01:31,  3.66it/s] 18%|█▊        | 71/405 [00:19<01:31,  3.66it/s] 18%|█▊        | 72/405 [00:19<01:31,  3.66it/s] 18%|█▊        | 73/405 [00:19<01:30,  3.66it/s] 18%|█▊        | 74/405 [00:20<01:30,  3.66it/s] 19%|█▊        | 75/405 [00:20<01:30,  3.66it/s] 19%|█▉        | 76/405 [00:20<01:29,  3.66it/s] 19%|█▉        | 77/405 [00:21<01:29,  3.66it/s] 19%|█▉        | 78/405 [00:21<01:29,  3.66it/s] 20%|█▉        | 79/405 [00:21<01:29,  3.66it/s] 20%|█▉        | 80/405 [00:21<01:28,  3.66it/s] 20%|██        | 81/405 [00:22<01:20,  4.05it/s][INFO|trainer.py:2140] 2023-08-28 00:05:49,406 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:05:49,406 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 00:05:49,406 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.10it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.31it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.49it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.69it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.35it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.09it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.81it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.63it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.44it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.49it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.42it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.54it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.48it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.40it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.43it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.52it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.46it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.37it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.43it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.43it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.40it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.38it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.48it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.50it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.41it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.42it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.46it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.40it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.47it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.40it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.45it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.42it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.42it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.49it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.49it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.44it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.43it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.51it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.43it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.40it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.44it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.41it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.46it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.47it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.47it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.45it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.43it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.41it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.36it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.41it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.46it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.43it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.46it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.49it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.37it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.44it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.41it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.30it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.39it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.34it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.37it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.36it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.33it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.34it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.29it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.33it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.46it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.38it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.32it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.40it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.38it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.35it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.34it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.35it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.24it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.34it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.40it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.32it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.37it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.41it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.09it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.12it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.17it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.19it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.22it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:31<01:20,  4.05it/s]
100%|██████████| 437/437 [00:09<00:00, 47.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:05:58,661 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:05:58,682 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:06:01,014 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:06:01,032 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:06:01,042 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:34<20:36,  3.83s/it] 20%|██        | 83/405 [00:34<14:49,  2.76s/it] 21%|██        | 84/405 [00:34<10:46,  2.01s/it] 21%|██        | 85/405 [00:35<07:57,  1.49s/it] 21%|██        | 86/405 [00:35<05:59,  1.13s/it] 21%|██▏       | 87/405 [00:35<04:36,  1.15it/s] 22%|██▏       | 88/405 [00:35<03:39,  1.45it/s] 22%|██▏       | 89/405 [00:36<02:58,  1.77it/s] 22%|██▏       | 90/405 [00:36<02:30,  2.09it/s] 22%|██▏       | 91/405 [00:36<02:10,  2.40it/s] 23%|██▎       | 92/405 [00:36<01:57,  2.67it/s] 23%|██▎       | 93/405 [00:37<01:47,  2.90it/s] 23%|██▎       | 94/405 [00:37<01:40,  3.09it/s] 23%|██▎       | 95/405 [00:37<01:35,  3.24it/s] 24%|██▎       | 96/405 [00:38<01:32,  3.36it/s] 24%|██▍       | 97/405 [00:38<01:29,  3.44it/s] 24%|██▍       | 98/405 [00:38<01:27,  3.50it/s] 24%|██▍       | 99/405 [00:38<01:26,  3.55it/s] 25%|██▍       | 100/405 [00:39<01:25,  3.58it/s] 25%|██▍       | 101/405 [00:39<01:24,  3.60it/s] 25%|██▌       | 102/405 [00:39<01:23,  3.62it/s] 25%|██▌       | 103/405 [00:39<01:23,  3.63it/s] 26%|██▌       | 104/405 [00:40<01:22,  3.63it/s] 26%|██▌       | 105/405 [00:40<01:22,  3.64it/s] 26%|██▌       | 106/405 [00:40<01:22,  3.64it/s] 26%|██▋       | 107/405 [00:41<01:21,  3.64it/s] 27%|██▋       | 108/405 [00:41<01:21,  3.65it/s] 27%|██▋       | 109/405 [00:41<01:21,  3.65it/s] 27%|██▋       | 110/405 [00:41<01:20,  3.65it/s] 27%|██▋       | 111/405 [00:42<01:20,  3.65it/s] 28%|██▊       | 112/405 [00:42<01:20,  3.65it/s] 28%|██▊       | 113/405 [00:42<01:19,  3.65it/s] 28%|██▊       | 114/405 [00:42<01:19,  3.65it/s] 28%|██▊       | 115/405 [00:43<01:19,  3.65it/s] 29%|██▊       | 116/405 [00:43<01:19,  3.65it/s] 29%|██▉       | 117/405 [00:43<01:18,  3.65it/s] 29%|██▉       | 118/405 [00:44<01:18,  3.65it/s] 29%|██▉       | 119/405 [00:44<01:18,  3.65it/s] 30%|██▉       | 120/405 [00:44<01:18,  3.65it/s] 30%|██▉       | 121/405 [00:44<01:17,  3.65it/s] 30%|███       | 122/405 [00:45<01:17,  3.65it/s] 30%|███       | 123/405 [00:45<01:17,  3.65it/s] 31%|███       | 124/405 [00:45<01:16,  3.65it/s] 31%|███       | 125/405 [00:45<01:16,  3.65it/s] 31%|███       | 126/405 [00:46<01:16,  3.65it/s] 31%|███▏      | 127/405 [00:46<01:16,  3.65it/s] 32%|███▏      | 128/405 [00:46<01:15,  3.65it/s] 32%|███▏      | 129/405 [00:47<01:15,  3.63it/s] 32%|███▏      | 130/405 [00:47<01:15,  3.64it/s] 32%|███▏      | 131/405 [00:47<01:15,  3.64it/s] 33%|███▎      | 132/405 [00:47<01:14,  3.65it/s] 33%|███▎      | 133/405 [00:48<01:14,  3.65it/s] 33%|███▎      | 134/405 [00:48<01:14,  3.65it/s] 33%|███▎      | 135/405 [00:48<01:13,  3.65it/s] 34%|███▎      | 136/405 [00:49<01:13,  3.65it/s] 34%|███▍      | 137/405 [00:49<01:13,  3.65it/s] 34%|███▍      | 138/405 [00:49<01:13,  3.65it/s] 34%|███▍      | 139/405 [00:49<01:12,  3.65it/s] 35%|███▍      | 140/405 [00:50<01:13,  3.63it/s] 35%|███▍      | 141/405 [00:50<01:12,  3.64it/s] 35%|███▌      | 142/405 [00:50<01:12,  3.64it/s] 35%|███▌      | 143/405 [00:50<01:11,  3.65it/s] 36%|███▌      | 144/405 [00:51<01:11,  3.64it/s] 36%|███▌      | 145/405 [00:51<01:11,  3.64it/s] 36%|███▌      | 146/405 [00:51<01:11,  3.65it/s] 36%|███▋      | 147/405 [00:52<01:12,  3.55it/s] 37%|███▋      | 148/405 [00:52<01:12,  3.55it/s] 37%|███▋      | 149/405 [00:52<01:11,  3.58it/s] 37%|███▋      | 150/405 [00:52<01:10,  3.60it/s] 37%|███▋      | 151/405 [00:53<01:10,  3.61it/s] 38%|███▊      | 152/405 [00:53<01:09,  3.62it/s] 38%|███▊      | 153/405 [00:53<01:09,  3.63it/s] 38%|███▊      | 154/405 [00:53<01:09,  3.64it/s] 38%|███▊      | 155/405 [00:54<01:08,  3.64it/s] 39%|███▊      | 156/405 [00:54<01:08,  3.65it/s] 39%|███▉      | 157/405 [00:54<01:07,  3.65it/s] 39%|███▉      | 158/405 [00:55<01:07,  3.65it/s] 39%|███▉      | 159/405 [00:55<01:07,  3.65it/s] 40%|███▉      | 160/405 [00:55<01:07,  3.65it/s] 40%|███▉      | 161/405 [00:55<01:06,  3.65it/s] 40%|████      | 162/405 [00:56<01:00,  4.03it/s][INFO|trainer.py:2140] 2023-08-28 00:06:23,452 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:06:23,452 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 00:06:23,452 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2234, 'eval_samples_per_second': 379.035, 'eval_steps_per_second': 47.379, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.15it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.19it/s][A
  4%|▍         | 18/437 [00:00<00:09, 45.55it/s][A
  5%|▌         | 23/437 [00:00<00:08, 46.12it/s][A
  6%|▋         | 28/437 [00:00<00:08, 46.39it/s][A
  8%|▊         | 33/437 [00:00<00:08, 46.73it/s][A
  9%|▊         | 38/437 [00:00<00:08, 46.99it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.12it/s][A
 11%|█         | 48/437 [00:01<00:08, 47.25it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.22it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.14it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.20it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.20it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.24it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.23it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.34it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.25it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.26it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.33it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.20it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.19it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.19it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.22it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.26it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.24it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.15it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.21it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.30it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.31it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.20it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.22it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.24it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.18it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.26it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.17it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.22it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.29it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.29it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.28it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.16it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.23it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.24it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.13it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.24it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.25it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.16it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.24it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.27it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.24it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.19it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.21it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.23it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.83it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.25it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.17it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.06it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.18it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.22it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.25it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.20it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.18it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.22it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.17it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.22it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.21it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.19it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.17it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.08it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.08it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.09it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.10it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.13it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.06it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.11it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.13it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.19it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.18it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.02it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.12it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.08it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.85it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.98it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.96it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:05<01:00,  4.03it/s]
100%|██████████| 437/437 [00:09<00:00, 46.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:06:32,764 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:06:32,784 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:06:35,406 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:06:35,429 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:06:35,438 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:08<15:52,  3.94s/it] 40%|████      | 164/405 [01:08<11:24,  2.84s/it] 41%|████      | 165/405 [01:09<08:17,  2.07s/it] 41%|████      | 166/405 [01:09<06:06,  1.53s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:09<04:35,  1.16s/it] 41%|████▏     | 168/405 [01:10<03:31,  1.12it/s] 42%|████▏     | 169/405 [01:10<02:46,  1.41it/s] 42%|████▏     | 170/405 [01:10<02:15,  1.73it/s] 42%|████▏     | 171/405 [01:10<01:53,  2.06it/s] 42%|████▏     | 172/405 [01:11<01:38,  2.37it/s] 43%|████▎     | 173/405 [01:11<01:27,  2.65it/s] 43%|████▎     | 174/405 [01:11<01:20,  2.88it/s] 43%|████▎     | 175/405 [01:11<01:15,  3.06it/s] 43%|████▎     | 176/405 [01:12<01:11,  3.22it/s] 44%|████▎     | 177/405 [01:12<01:08,  3.34it/s] 44%|████▍     | 178/405 [01:12<01:06,  3.43it/s] 44%|████▍     | 179/405 [01:13<01:04,  3.49it/s] 44%|████▍     | 180/405 [01:13<01:03,  3.54it/s] 45%|████▍     | 181/405 [01:13<01:02,  3.57it/s] 45%|████▍     | 182/405 [01:13<01:02,  3.60it/s] 45%|████▌     | 183/405 [01:14<01:01,  3.61it/s] 45%|████▌     | 184/405 [01:14<01:00,  3.62it/s] 46%|████▌     | 185/405 [01:14<01:00,  3.63it/s] 46%|████▌     | 186/405 [01:14<01:00,  3.62it/s] 46%|████▌     | 187/405 [01:15<01:00,  3.62it/s] 46%|████▋     | 188/405 [01:15<00:59,  3.63it/s] 47%|████▋     | 189/405 [01:15<00:59,  3.64it/s] 47%|████▋     | 190/405 [01:16<00:59,  3.64it/s] 47%|████▋     | 191/405 [01:16<00:58,  3.64it/s] 47%|████▋     | 192/405 [01:16<00:58,  3.65it/s] 48%|████▊     | 193/405 [01:16<00:58,  3.65it/s] 48%|████▊     | 194/405 [01:17<00:57,  3.65it/s] 48%|████▊     | 195/405 [01:17<00:57,  3.65it/s] 48%|████▊     | 196/405 [01:17<00:57,  3.65it/s] 49%|████▊     | 197/405 [01:17<00:57,  3.63it/s] 49%|████▉     | 198/405 [01:18<00:56,  3.64it/s] 49%|████▉     | 199/405 [01:18<00:56,  3.64it/s] 49%|████▉     | 200/405 [01:18<00:56,  3.64it/s] 50%|████▉     | 201/405 [01:19<00:55,  3.64it/s] 50%|████▉     | 202/405 [01:19<00:55,  3.64it/s] 50%|█████     | 203/405 [01:19<00:55,  3.64it/s] 50%|█████     | 204/405 [01:19<00:55,  3.64it/s] 51%|█████     | 205/405 [01:20<00:54,  3.64it/s] 51%|█████     | 206/405 [01:20<00:54,  3.64it/s] 51%|█████     | 207/405 [01:20<00:54,  3.64it/s] 51%|█████▏    | 208/405 [01:20<00:54,  3.63it/s] 52%|█████▏    | 209/405 [01:21<00:53,  3.63it/s] 52%|█████▏    | 210/405 [01:21<00:53,  3.63it/s] 52%|█████▏    | 211/405 [01:21<00:53,  3.64it/s] 52%|█████▏    | 212/405 [01:22<00:53,  3.64it/s] 53%|█████▎    | 213/405 [01:22<00:52,  3.64it/s] 53%|█████▎    | 214/405 [01:22<00:52,  3.64it/s] 53%|█████▎    | 215/405 [01:22<00:52,  3.64it/s] 53%|█████▎    | 216/405 [01:23<00:51,  3.64it/s] 54%|█████▎    | 217/405 [01:23<00:51,  3.64it/s] 54%|█████▍    | 218/405 [01:23<00:51,  3.64it/s] 54%|█████▍    | 219/405 [01:24<00:51,  3.63it/s] 54%|█████▍    | 220/405 [01:24<00:50,  3.63it/s] 55%|█████▍    | 221/405 [01:24<00:50,  3.63it/s] 55%|█████▍    | 222/405 [01:24<00:50,  3.64it/s] 55%|█████▌    | 223/405 [01:25<00:49,  3.64it/s] 55%|█████▌    | 224/405 [01:25<00:49,  3.64it/s] 56%|█████▌    | 225/405 [01:25<00:49,  3.64it/s] 56%|█████▌    | 226/405 [01:25<00:49,  3.64it/s] 56%|█████▌    | 227/405 [01:26<00:48,  3.65it/s] 56%|█████▋    | 228/405 [01:26<00:48,  3.64it/s] 57%|█████▋    | 229/405 [01:26<00:48,  3.64it/s] 57%|█████▋    | 230/405 [01:27<00:48,  3.64it/s] 57%|█████▋    | 231/405 [01:27<00:48,  3.62it/s] 57%|█████▋    | 232/405 [01:27<00:47,  3.62it/s] 58%|█████▊    | 233/405 [01:27<00:47,  3.63it/s] 58%|█████▊    | 234/405 [01:28<00:47,  3.63it/s] 58%|█████▊    | 235/405 [01:28<00:46,  3.63it/s] 58%|█████▊    | 236/405 [01:28<00:46,  3.64it/s] 59%|█████▊    | 237/405 [01:28<00:46,  3.63it/s] 59%|█████▉    | 238/405 [01:29<00:45,  3.63it/s] 59%|█████▉    | 239/405 [01:29<00:45,  3.64it/s] 59%|█████▉    | 240/405 [01:29<00:45,  3.63it/s] 60%|█████▉    | 241/405 [01:30<00:45,  3.63it/s] 60%|█████▉    | 242/405 [01:30<00:44,  3.64it/s] 60%|██████    | 243/405 [01:30<00:40,  4.02it/s][INFO|trainer.py:2140] 2023-08-28 00:06:57,911 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:06:57,911 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 00:06:57,911 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2846, 'eval_samples_per_second': 376.536, 'eval_steps_per_second': 47.067, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.52it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.86it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.19it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.45it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.08it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.76it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.62it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.21it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.06it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.08it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.98it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.90it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.08it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.08it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.07it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.06it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.91it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.88it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.81it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.92it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.87it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.98it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.06it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.08it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.05it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.99it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.89it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.91it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.95it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.92it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.02it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.07it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.07it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.99it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.04it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.96it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.87it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.00it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.03it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.01it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.13it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.10it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.98it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.96it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.88it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.82it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.00it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.01it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.98it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.02it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.09it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.11it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.03it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.88it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.80it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.84it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.94it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.97it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.01it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.03it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.04it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.94it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.73it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.87it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.99it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.03it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.03it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.09it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.09it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.93it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.00it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.92it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.84it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.98it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.99it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.96it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.10it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.06it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.92it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.87it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.86it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.84it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.93it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.94it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.01it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [01:39<00:40,  4.02it/s]
100%|██████████| 437/437 [00:09<00:00, 47.01it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:07:07,261 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:07:07,284 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:07:09,910 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:07:09,938 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:07:09,956 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [01:43<10:36,  3.95s/it] 60%|██████    | 245/405 [01:43<07:35,  2.85s/it] 61%|██████    | 246/405 [01:43<05:30,  2.08s/it] 61%|██████    | 247/405 [01:43<04:02,  1.54s/it] 61%|██████    | 248/405 [01:44<03:01,  1.16s/it] 61%|██████▏   | 249/405 [01:44<02:19,  1.12it/s] 62%|██████▏   | 250/405 [01:44<01:49,  1.41it/s] 62%|██████▏   | 251/405 [01:45<01:28,  1.73it/s] 62%|██████▏   | 252/405 [01:45<01:14,  2.05it/s] 62%|██████▏   | 253/405 [01:45<01:04,  2.36it/s] 63%|██████▎   | 254/405 [01:45<00:57,  2.64it/s] 63%|██████▎   | 255/405 [01:46<00:52,  2.87it/s] 63%|██████▎   | 256/405 [01:46<00:48,  3.07it/s] 63%|██████▎   | 257/405 [01:46<00:45,  3.22it/s] 64%|██████▎   | 258/405 [01:46<00:44,  3.34it/s] 64%|██████▍   | 259/405 [01:47<00:42,  3.42it/s] 64%|██████▍   | 260/405 [01:47<00:41,  3.48it/s] 64%|██████▍   | 261/405 [01:47<00:40,  3.53it/s] 65%|██████▍   | 262/405 [01:48<00:40,  3.56it/s] 65%|██████▍   | 263/405 [01:48<00:39,  3.59it/s] 65%|██████▌   | 264/405 [01:48<00:39,  3.60it/s] 65%|██████▌   | 265/405 [01:48<00:38,  3.60it/s] 66%|██████▌   | 266/405 [01:49<00:38,  3.61it/s] 66%|██████▌   | 267/405 [01:49<00:38,  3.62it/s] 66%|██████▌   | 268/405 [01:49<00:37,  3.63it/s] 66%|██████▋   | 269/405 [01:50<00:37,  3.63it/s] 67%|██████▋   | 270/405 [01:50<00:37,  3.64it/s] 67%|██████▋   | 271/405 [01:50<00:36,  3.63it/s] 67%|██████▋   | 272/405 [01:50<00:36,  3.64it/s] 67%|██████▋   | 273/405 [01:51<00:36,  3.64it/s] 68%|██████▊   | 274/405 [01:51<00:36,  3.61it/s] 68%|██████▊   | 275/405 [01:51<00:35,  3.61it/s] 68%|██████▊   | 276/405 [01:51<00:35,  3.60it/s] 68%|██████▊   | 277/405 [01:52<00:36,  3.53it/s] 69%|██████▊   | 278/405 [01:52<00:35,  3.54it/s] 69%|██████▉   | 279/405 [01:52<00:35,  3.57it/s] 69%|██████▉   | 280/405 [01:53<00:34,  3.59it/s] 69%|██████▉   | 281/405 [01:53<00:34,  3.60it/s] 70%|██████▉   | 282/405 [01:53<00:34,  3.62it/s] 70%|██████▉   | 283/405 [01:53<00:33,  3.62it/s] 70%|███████   | 284/405 [01:54<00:33,  3.63it/s] 70%|███████   | 285/405 [01:54<00:33,  3.63it/s] 71%|███████   | 286/405 [01:54<00:32,  3.64it/s] 71%|███████   | 287/405 [01:54<00:32,  3.62it/s] 71%|███████   | 288/405 [01:55<00:32,  3.63it/s] 71%|███████▏  | 289/405 [01:55<00:31,  3.63it/s] 72%|███████▏  | 290/405 [01:55<00:31,  3.64it/s] 72%|███████▏  | 291/405 [01:56<00:31,  3.64it/s] 72%|███████▏  | 292/405 [01:56<00:31,  3.64it/s] 72%|███████▏  | 293/405 [01:56<00:30,  3.64it/s] 73%|███████▎  | 294/405 [01:56<00:30,  3.64it/s] 73%|███████▎  | 295/405 [01:57<00:30,  3.64it/s] 73%|███████▎  | 296/405 [01:57<00:29,  3.64it/s] 73%|███████▎  | 297/405 [01:57<00:29,  3.64it/s] 74%|███████▎  | 298/405 [01:58<00:29,  3.63it/s] 74%|███████▍  | 299/405 [01:58<00:29,  3.63it/s] 74%|███████▍  | 300/405 [01:58<00:28,  3.64it/s] 74%|███████▍  | 301/405 [01:58<00:28,  3.64it/s] 75%|███████▍  | 302/405 [01:59<00:28,  3.64it/s] 75%|███████▍  | 303/405 [01:59<00:28,  3.64it/s] 75%|███████▌  | 304/405 [01:59<00:27,  3.64it/s] 75%|███████▌  | 305/405 [01:59<00:27,  3.64it/s] 76%|███████▌  | 306/405 [02:00<00:27,  3.64it/s] 76%|███████▌  | 307/405 [02:00<00:26,  3.64it/s] 76%|███████▌  | 308/405 [02:00<00:26,  3.64it/s] 76%|███████▋  | 309/405 [02:01<00:26,  3.63it/s] 77%|███████▋  | 310/405 [02:01<00:26,  3.64it/s] 77%|███████▋  | 311/405 [02:01<00:25,  3.64it/s] 77%|███████▋  | 312/405 [02:01<00:25,  3.64it/s] 77%|███████▋  | 313/405 [02:02<00:25,  3.64it/s] 78%|███████▊  | 314/405 [02:02<00:25,  3.63it/s] 78%|███████▊  | 315/405 [02:02<00:24,  3.64it/s] 78%|███████▊  | 316/405 [02:02<00:24,  3.64it/s] 78%|███████▊  | 317/405 [02:03<00:24,  3.64it/s] 79%|███████▊  | 318/405 [02:03<00:23,  3.64it/s] 79%|███████▉  | 319/405 [02:03<00:23,  3.64it/s] 79%|███████▉  | 320/405 [02:04<00:23,  3.59it/s] 79%|███████▉  | 321/405 [02:04<00:23,  3.60it/s] 80%|███████▉  | 322/405 [02:04<00:22,  3.61it/s] 80%|███████▉  | 323/405 [02:04<00:22,  3.62it/s] 80%|████████  | 324/405 [02:05<00:20,  4.01it/s][INFO|trainer.py:2140] 2023-08-28 00:07:32,461 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:07:32,461 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 00:07:32,461 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.3029, 'eval_samples_per_second': 375.797, 'eval_steps_per_second': 46.975, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.69it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.99it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.33it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.00it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.69it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.45it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.99it/s][A
 11%|█         | 48/437 [00:00<00:08, 46.96it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.96it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.93it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.00it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.09it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.08it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.08it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.13it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.95it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.74it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.80it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.82it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.88it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.02it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.09it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.01it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.94it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.92it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.84it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.86it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.80it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.81it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.89it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.93it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.00it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.06it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.96it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.85it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.88it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.86it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.78it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.95it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.97it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.99it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.93it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.91it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.77it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.87it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.86it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.80it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.97it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.92it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.92it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.02it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.89it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.80it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.83it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.77it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.71it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.94it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.96it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.90it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.01it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.88it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.86it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.80it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.83it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.97it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.91it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.91it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.95it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.89it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.99it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.91it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.67it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.41it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.68it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.79it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.83it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.85it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.85it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.85it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.83it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.85it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.85it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.72it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [02:14<00:20,  4.01it/s]
100%|██████████| 437/437 [00:09<00:00, 46.72it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:07:41,802 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:07:41,829 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:07:46,107 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:07:46,122 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:07:46,130 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [02:19<05:55,  4.45s/it] 80%|████████  | 326/405 [02:19<04:12,  3.19s/it] 81%|████████  | 327/405 [02:19<03:00,  2.32s/it] 81%|████████  | 328/405 [02:20<02:11,  1.70s/it] 81%|████████  | 329/405 [02:20<01:36,  1.28s/it] 81%|████████▏ | 330/405 [02:20<01:13,  1.03it/s] 82%|████████▏ | 331/405 [02:20<00:56,  1.31it/s] 82%|████████▏ | 332/405 [02:21<00:45,  1.62it/s] 82%|████████▏ | 333/405 [02:21<00:37,  1.94it/s] 82%|████████▏ | 334/405 [02:21<00:31,  2.26it/s] 83%|████████▎ | 335/405 [02:22<00:27,  2.54it/s] 83%|████████▎ | 336/405 [02:22<00:24,  2.80it/s] 83%|████████▎ | 337/405 [02:22<00:22,  3.01it/s] 83%|████████▎ | 338/405 [02:22<00:21,  3.17it/s] 84%|████████▎ | 339/405 [02:23<00:20,  3.30it/s] 84%|████████▍ | 340/405 [02:23<00:19,  3.39it/s] 84%|████████▍ | 341/405 [02:23<00:18,  3.46it/s] 84%|████████▍ | 342/405 [02:23<00:17,  3.52it/s] 85%|████████▍ | 343/405 [02:24<00:17,  3.55it/s] 85%|████████▍ | 344/405 [02:24<00:17,  3.58it/s] 85%|████████▌ | 345/405 [02:24<00:16,  3.60it/s] 85%|████████▌ | 346/405 [02:25<00:16,  3.60it/s] 86%|████████▌ | 347/405 [02:25<00:16,  3.61it/s] 86%|████████▌ | 348/405 [02:25<00:15,  3.62it/s] 86%|████████▌ | 349/405 [02:25<00:15,  3.63it/s] 86%|████████▋ | 350/405 [02:26<00:15,  3.63it/s] 87%|████████▋ | 351/405 [02:26<00:14,  3.63it/s] 87%|████████▋ | 352/405 [02:26<00:14,  3.63it/s] 87%|████████▋ | 353/405 [02:27<00:14,  3.64it/s] 87%|████████▋ | 354/405 [02:27<00:14,  3.64it/s] 88%|████████▊ | 355/405 [02:27<00:13,  3.64it/s] 88%|████████▊ | 356/405 [02:27<00:13,  3.64it/s] 88%|████████▊ | 357/405 [02:28<00:13,  3.59it/s] 88%|████████▊ | 358/405 [02:28<00:13,  3.61it/s] 89%|████████▊ | 359/405 [02:28<00:12,  3.62it/s] 89%|████████▉ | 360/405 [02:28<00:12,  3.62it/s] 89%|████████▉ | 361/405 [02:29<00:12,  3.63it/s] 89%|████████▉ | 362/405 [02:29<00:11,  3.63it/s] 90%|████████▉ | 363/405 [02:29<00:11,  3.63it/s] 90%|████████▉ | 364/405 [02:30<00:11,  3.64it/s] 90%|█████████ | 365/405 [02:30<00:10,  3.64it/s] 90%|█████████ | 366/405 [02:30<00:10,  3.64it/s] 91%|█████████ | 367/405 [02:30<00:10,  3.64it/s] 91%|█████████ | 368/405 [02:31<00:10,  3.63it/s] 91%|█████████ | 369/405 [02:31<00:09,  3.63it/s] 91%|█████████▏| 370/405 [02:31<00:09,  3.63it/s] 92%|█████████▏| 371/405 [02:31<00:09,  3.64it/s] 92%|█████████▏| 372/405 [02:32<00:09,  3.64it/s] 92%|█████████▏| 373/405 [02:32<00:08,  3.64it/s] 92%|█████████▏| 374/405 [02:32<00:08,  3.64it/s] 93%|█████████▎| 375/405 [02:33<00:08,  3.64it/s] 93%|█████████▎| 376/405 [02:33<00:07,  3.64it/s] 93%|█████████▎| 377/405 [02:33<00:07,  3.64it/s] 93%|█████████▎| 378/405 [02:33<00:07,  3.64it/s] 94%|█████████▎| 379/405 [02:34<00:07,  3.63it/s] 94%|█████████▍| 380/405 [02:34<00:06,  3.63it/s] 94%|█████████▍| 381/405 [02:34<00:06,  3.64it/s] 94%|█████████▍| 382/405 [02:34<00:06,  3.64it/s] 95%|█████████▍| 383/405 [02:35<00:06,  3.64it/s] 95%|█████████▍| 384/405 [02:35<00:05,  3.64it/s] 95%|█████████▌| 385/405 [02:35<00:05,  3.64it/s] 95%|█████████▌| 386/405 [02:36<00:05,  3.64it/s] 96%|█████████▌| 387/405 [02:36<00:04,  3.64it/s] 96%|█████████▌| 388/405 [02:36<00:04,  3.64it/s] 96%|█████████▌| 389/405 [02:36<00:04,  3.64it/s] 96%|█████████▋| 390/405 [02:37<00:04,  3.62it/s] 97%|█████████▋| 391/405 [02:37<00:03,  3.62it/s] 97%|█████████▋| 392/405 [02:37<00:03,  3.63it/s] 97%|█████████▋| 393/405 [02:38<00:03,  3.63it/s] 97%|█████████▋| 394/405 [02:38<00:03,  3.64it/s] 98%|█████████▊| 395/405 [02:38<00:02,  3.64it/s] 98%|█████████▊| 396/405 [02:38<00:02,  3.64it/s] 98%|█████████▊| 397/405 [02:39<00:02,  3.64it/s] 98%|█████████▊| 398/405 [02:39<00:01,  3.64it/s] 99%|█████████▊| 399/405 [02:39<00:01,  3.64it/s] 99%|█████████▉| 400/405 [02:39<00:01,  3.64it/s] 99%|█████████▉| 401/405 [02:40<00:01,  3.63it/s] 99%|█████████▉| 402/405 [02:40<00:00,  3.63it/s]100%|█████████▉| 403/405 [02:40<00:00,  3.63it/s]100%|█████████▉| 404/405 [02:41<00:00,  3.64it/s]100%|██████████| 405/405 [02:41<00:00,  4.02it/s][INFO|trainer.py:2140] 2023-08-28 00:08:08,611 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:08:08,611 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 00:08:08,611 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.3209, 'eval_samples_per_second': 375.071, 'eval_steps_per_second': 46.884, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.76it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.86it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.03it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.35it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.87it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.55it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.47it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.21it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.15it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.19it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.12it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.03it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.06it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.09it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.06it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.97it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.84it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.81it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.93it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.02it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.94it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.04it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.05it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.96it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.05it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.87it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.81it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.89it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.92it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.86it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.99it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.02it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.05it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.03it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.90it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.84it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.87it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.91it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.86it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.99it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.05it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.09it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.10it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.08it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.02it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.01it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.91it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.91it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.83it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.98it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.03it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.95it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.99it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.91it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.95it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.97it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.83it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.86it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.97it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.05it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.94it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.92it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.86it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.86it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.01it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.98it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.81it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.94it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.03it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.95it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.00it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.84it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.78it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.93it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.98it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.88it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.93it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.92it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.96it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.99it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.89it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.76it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.88it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.94it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.89it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.87it/s][A                                                 
                                                 [A100%|██████████| 405/405 [02:50<00:00,  4.02it/s]
100%|██████████| 437/437 [00:09<00:00, 46.87it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:08:17,943 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:08:17,961 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:08:20,421 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:08:20,438 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:08:20,456 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:08:20,791 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:08:20,792 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81 (score: 1.1523417234420776).
                                                 100%|██████████| 405/405 [02:55<00:00,  4.02it/s]100%|██████████| 405/405 [02:55<00:00,  2.31it/s]
[INFO|trainer.py:1894] 2023-08-28 00:08:22,532 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:08:22,552 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:08:24,885 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:08:24,896 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:08:24,912 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:08:25,137 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:25,137 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:25,137 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:25,137 >>   train_runtime            = 0:02:55.14
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:25,137 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:25,137 >>   train_samples_per_second =    147.307
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:25,137 >>   train_steps_per_second   =      2.312
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.3081, 'eval_samples_per_second': 375.587, 'eval_steps_per_second': 46.948, 'epoch': 5.0}
{'train_runtime': 175.1449, 'train_samples_per_second': 147.307, 'train_steps_per_second': 2.312, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:08:25 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:08:25,174 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:08:25,174 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 00:08:25,174 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.81it/s]  3%|▎         | 12/437 [00:00<00:08, 51.52it/s]  4%|▍         | 18/437 [00:00<00:08, 49.49it/s]  5%|▌         | 23/437 [00:00<00:08, 48.72it/s]  6%|▋         | 28/437 [00:00<00:08, 48.36it/s]  8%|▊         | 33/437 [00:00<00:08, 47.93it/s]  9%|▊         | 38/437 [00:00<00:08, 47.66it/s] 10%|▉         | 43/437 [00:00<00:08, 47.64it/s] 11%|█         | 48/437 [00:00<00:08, 47.29it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.27it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.27it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.24it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.28it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.29it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.37it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.32it/s] 20%|██        | 88/437 [00:01<00:07, 47.24it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.10it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.06it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.09it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.15it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.22it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.16it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.24it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.35it/s] 30%|███       | 133/437 [00:02<00:06, 47.29it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.14it/s] 33%|███▎      | 143/437 [00:03<00:06, 47.13it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.08it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.09it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.13it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.19it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.22it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.22it/s] 41%|████      | 178/437 [00:03<00:05, 47.12it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.28it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.10it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.15it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.09it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.11it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.20it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.10it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.19it/s] 51%|█████     | 223/437 [00:04<00:04, 47.25it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.22it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.21it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.05it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.08it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.12it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.15it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.20it/s] 60%|██████    | 263/437 [00:05<00:03, 47.14it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.17it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.24it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.18it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.20it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.09it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.09it/s] 68%|██████▊   | 298/437 [00:06<00:02, 47.17it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.20it/s] 70%|███████   | 308/437 [00:06<00:02, 47.20it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.08it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.17it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.12it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.24it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.18it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.10it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.11it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.13it/s] 81%|████████  | 353/437 [00:07<00:01, 47.16it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.19it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.19it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.17it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.22it/s] 86%|████████▋ | 378/437 [00:07<00:01, 47.12it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.14it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.03it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.14it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.07it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.00it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.04it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.02it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.09it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.08it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.02it/s]100%|██████████| 437/437 [00:09<00:00, 47.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:08:34,449 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   eval_loss               =     1.1523
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   eval_runtime            = 0:00:09.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   eval_samples_per_second =    376.922
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   eval_steps_per_second   =     47.115
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:08:34,449 >>   perplexity              =     3.1656
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:40,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:40,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:40,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:40,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:40,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:08:41,753 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:08:41,754 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:08:42,314 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:08:43,365 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:08:43,365 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:46,420 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:46,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:46,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:46,424 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:08:46,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:08:47,048 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:08:47,049 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:08:47,617 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:08:47,776 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:08:47,776 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-81
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-324
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-162
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-243
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/checkpoint-405
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:13,  1.75it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.64it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:18,  1.64it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:19,  1.62it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:22,  1.66it/s]Extractor Predicting: 39it [00:23,  1.70it/s]Extractor Predicting: 40it [00:23,  1.72it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:25,  1.71it/s]Extractor Predicting: 43it [00:25,  1.73it/s]Extractor Predicting: 44it [00:26,  1.72it/s]Extractor Predicting: 45it [00:26,  1.67it/s]Extractor Predicting: 46it [00:27,  1.60it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.66it/s]Extractor Predicting: 50it [00:29,  1.69it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:31,  1.76it/s]Extractor Predicting: 53it [00:31,  1.72it/s]Extractor Predicting: 54it [00:32,  1.73it/s]Extractor Predicting: 55it [00:32,  1.74it/s]Extractor Predicting: 56it [00:33,  1.72it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:34,  1.68it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:35,  1.67it/s]Extractor Predicting: 61it [00:36,  1.67it/s]Extractor Predicting: 62it [00:37,  1.67it/s]Extractor Predicting: 63it [00:37,  1.65it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:38,  1.67it/s]Extractor Predicting: 66it [00:39,  1.64it/s]Extractor Predicting: 67it [00:40,  1.61it/s]Extractor Predicting: 68it [00:40,  1.62it/s]Extractor Predicting: 69it [00:41,  1.64it/s]Extractor Predicting: 70it [00:41,  1.70it/s]Extractor Predicting: 71it [00:42,  1.68it/s]Extractor Predicting: 72it [00:43,  1.66it/s]Extractor Predicting: 73it [00:43,  1.68it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:44,  1.64it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:46,  1.65it/s]Extractor Predicting: 78it [00:46,  1.64it/s]Extractor Predicting: 79it [00:47,  1.62it/s]Extractor Predicting: 80it [00:48,  1.63it/s]Extractor Predicting: 81it [00:48,  1.64it/s]Extractor Predicting: 82it [00:49,  1.66it/s]Extractor Predicting: 83it [00:49,  1.68it/s]Extractor Predicting: 84it [00:50,  1.61it/s]Extractor Predicting: 85it [00:51,  1.61it/s]Extractor Predicting: 86it [00:51,  1.64it/s]Extractor Predicting: 87it [00:52,  1.61it/s]Extractor Predicting: 88it [00:52,  1.66it/s]Extractor Predicting: 89it [00:53,  1.66it/s]Extractor Predicting: 90it [00:54,  1.65it/s]Extractor Predicting: 91it [00:54,  1.70it/s]Extractor Predicting: 92it [00:55,  1.69it/s]Extractor Predicting: 93it [00:55,  1.72it/s]Extractor Predicting: 94it [00:56,  1.68it/s]Extractor Predicting: 95it [00:56,  1.69it/s]Extractor Predicting: 96it [00:57,  1.68it/s]Extractor Predicting: 97it [00:58,  1.65it/s]Extractor Predicting: 98it [00:58,  1.63it/s]Extractor Predicting: 99it [00:59,  1.66it/s]Extractor Predicting: 100it [01:00,  1.64it/s]Extractor Predicting: 101it [01:00,  1.63it/s]Extractor Predicting: 102it [01:01,  1.59it/s]Extractor Predicting: 103it [01:01,  1.61it/s]Extractor Predicting: 104it [01:02,  1.62it/s]Extractor Predicting: 105it [01:03,  1.63it/s]Extractor Predicting: 106it [01:03,  1.67it/s]Extractor Predicting: 107it [01:04,  1.65it/s]Extractor Predicting: 108it [01:04,  1.66it/s]Extractor Predicting: 109it [01:05,  1.66it/s]Extractor Predicting: 110it [01:06,  1.65it/s]Extractor Predicting: 111it [01:06,  1.64it/s]Extractor Predicting: 112it [01:07,  1.66it/s]Extractor Predicting: 113it [01:07,  1.66it/s]Extractor Predicting: 114it [01:08,  1.65it/s]Extractor Predicting: 115it [01:09,  1.65it/s]Extractor Predicting: 116it [01:09,  1.71it/s]Extractor Predicting: 117it [01:10,  1.68it/s]Extractor Predicting: 118it [01:10,  1.66it/s]Extractor Predicting: 119it [01:11,  1.62it/s]Extractor Predicting: 120it [01:12,  1.64it/s]Extractor Predicting: 121it [01:12,  1.66it/s]Extractor Predicting: 122it [01:13,  1.63it/s]Extractor Predicting: 123it [01:14,  1.62it/s]Extractor Predicting: 124it [01:14,  1.60it/s]Extractor Predicting: 125it [01:15,  1.57it/s]Extractor Predicting: 126it [01:15,  1.61it/s]Extractor Predicting: 127it [01:16,  1.63it/s]Extractor Predicting: 128it [01:17,  1.64it/s]Extractor Predicting: 129it [01:17,  1.66it/s]Extractor Predicting: 130it [01:18,  1.70it/s]Extractor Predicting: 131it [01:18,  1.71it/s]Extractor Predicting: 132it [01:19,  1.69it/s]Extractor Predicting: 133it [01:20,  1.68it/s]Extractor Predicting: 134it [01:20,  1.69it/s]Extractor Predicting: 135it [01:21,  1.69it/s]Extractor Predicting: 136it [01:21,  1.67it/s]Extractor Predicting: 137it [01:22,  1.66it/s]Extractor Predicting: 138it [01:23,  1.51it/s]Extractor Predicting: 139it [01:23,  1.56it/s]Extractor Predicting: 140it [01:24,  1.58it/s]Extractor Predicting: 141it [01:25,  1.60it/s]Extractor Predicting: 142it [01:25,  1.59it/s]Extractor Predicting: 143it [01:26,  1.60it/s]Extractor Predicting: 144it [01:26,  1.81it/s]Extractor Predicting: 144it [01:26,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:21,871 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:21,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:21,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:21,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:21,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:10:22,607 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:10:22,608 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:10:23,204 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:10:24,243 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:10:24,243 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:27,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:27,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:27,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:27,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:10:27,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:10:27,876 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:10:27,877 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:10:28,494 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:10:28,648 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:10:28,649 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.83it/s]Extractor Predicting: 4it [00:02,  1.85it/s]Extractor Predicting: 5it [00:02,  1.86it/s]Extractor Predicting: 6it [00:03,  1.87it/s]Extractor Predicting: 7it [00:03,  1.88it/s]Extractor Predicting: 8it [00:04,  1.87it/s]Extractor Predicting: 9it [00:04,  1.90it/s]Extractor Predicting: 10it [00:05,  1.91it/s]Extractor Predicting: 11it [00:05,  1.96it/s]Extractor Predicting: 12it [00:06,  1.91it/s]Extractor Predicting: 13it [00:06,  1.92it/s]Extractor Predicting: 14it [00:07,  1.93it/s]Extractor Predicting: 15it [00:07,  1.94it/s]Extractor Predicting: 16it [00:08,  1.89it/s]Extractor Predicting: 17it [00:09,  1.90it/s]Extractor Predicting: 18it [00:09,  1.94it/s]Extractor Predicting: 19it [00:10,  1.93it/s]Extractor Predicting: 20it [00:10,  1.92it/s]Extractor Predicting: 21it [00:11,  1.95it/s]Extractor Predicting: 22it [00:11,  1.98it/s]Extractor Predicting: 23it [00:12,  1.93it/s]Extractor Predicting: 24it [00:12,  1.89it/s]Extractor Predicting: 25it [00:13,  1.84it/s]Extractor Predicting: 26it [00:13,  1.88it/s]Extractor Predicting: 27it [00:14,  1.90it/s]Extractor Predicting: 28it [00:14,  1.81it/s]Extractor Predicting: 29it [00:15,  1.78it/s]Extractor Predicting: 30it [00:16,  1.70it/s]Extractor Predicting: 31it [00:16,  1.66it/s]Extractor Predicting: 32it [00:17,  1.60it/s]Extractor Predicting: 33it [00:18,  1.60it/s]Extractor Predicting: 34it [00:18,  1.57it/s]Extractor Predicting: 35it [00:19,  1.61it/s]Extractor Predicting: 36it [00:19,  1.61it/s]Extractor Predicting: 37it [00:20,  1.61it/s]Extractor Predicting: 38it [00:21,  1.56it/s]Extractor Predicting: 39it [00:21,  1.57it/s]Extractor Predicting: 40it [00:22,  1.54it/s]Extractor Predicting: 41it [00:23,  1.53it/s]Extractor Predicting: 42it [00:23,  1.54it/s]Extractor Predicting: 43it [00:24,  1.55it/s]Extractor Predicting: 44it [00:25,  1.48it/s]Extractor Predicting: 45it [00:25,  1.51it/s]Extractor Predicting: 46it [00:26,  1.55it/s]Extractor Predicting: 47it [00:27,  1.54it/s]Extractor Predicting: 48it [00:27,  1.53it/s]Extractor Predicting: 49it [00:28,  1.54it/s]Extractor Predicting: 50it [00:29,  1.55it/s]Extractor Predicting: 51it [00:29,  1.54it/s]Extractor Predicting: 52it [00:30,  1.56it/s]Extractor Predicting: 53it [00:30,  1.56it/s]Extractor Predicting: 54it [00:31,  1.57it/s]Extractor Predicting: 55it [00:32,  1.56it/s]Extractor Predicting: 56it [00:32,  1.54it/s]Extractor Predicting: 57it [00:33,  1.56it/s]Extractor Predicting: 58it [00:34,  1.57it/s]Extractor Predicting: 59it [00:34,  1.60it/s]Extractor Predicting: 60it [00:35,  1.62it/s]Extractor Predicting: 61it [00:36,  1.58it/s]Extractor Predicting: 62it [00:36,  1.55it/s]Extractor Predicting: 63it [00:37,  1.59it/s]Extractor Predicting: 64it [00:37,  1.62it/s]Extractor Predicting: 65it [00:38,  1.64it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:40,  1.69it/s]Extractor Predicting: 69it [00:40,  1.68it/s]Extractor Predicting: 70it [00:41,  1.71it/s]Extractor Predicting: 71it [00:41,  1.76it/s]Extractor Predicting: 72it [00:42,  1.71it/s]Extractor Predicting: 73it [00:43,  1.67it/s]Extractor Predicting: 74it [00:43,  1.64it/s]Extractor Predicting: 75it [00:44,  1.65it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:45,  1.62it/s]Extractor Predicting: 78it [00:46,  1.62it/s]Extractor Predicting: 79it [00:46,  1.63it/s]Extractor Predicting: 80it [00:47,  1.62it/s]Extractor Predicting: 81it [00:48,  1.60it/s]Extractor Predicting: 82it [00:48,  1.63it/s]Extractor Predicting: 83it [00:49,  1.62it/s]Extractor Predicting: 84it [00:49,  1.61it/s]Extractor Predicting: 85it [00:50,  1.63it/s]Extractor Predicting: 86it [00:51,  1.65it/s]Extractor Predicting: 87it [00:51,  1.70it/s]Extractor Predicting: 88it [00:52,  1.75it/s]Extractor Predicting: 89it [00:52,  1.76it/s]Extractor Predicting: 90it [00:53,  1.75it/s]Extractor Predicting: 91it [00:53,  1.81it/s]Extractor Predicting: 92it [00:54,  1.79it/s]Extractor Predicting: 93it [00:54,  1.85it/s]Extractor Predicting: 94it [00:55,  1.82it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:56,  1.80it/s]Extractor Predicting: 97it [00:57,  1.84it/s]Extractor Predicting: 98it [00:57,  1.86it/s]Extractor Predicting: 99it [00:58,  1.86it/s]Extractor Predicting: 100it [00:58,  1.92it/s]Extractor Predicting: 101it [00:59,  1.89it/s]Extractor Predicting: 102it [00:59,  1.84it/s]Extractor Predicting: 103it [01:00,  1.81it/s]Extractor Predicting: 104it [01:00,  1.83it/s]Extractor Predicting: 105it [01:01,  1.86it/s]Extractor Predicting: 106it [01:02,  1.85it/s]Extractor Predicting: 107it [01:02,  1.92it/s]Extractor Predicting: 108it [01:03,  1.86it/s]Extractor Predicting: 109it [01:03,  1.88it/s]Extractor Predicting: 110it [01:04,  1.88it/s]Extractor Predicting: 111it [01:04,  1.88it/s]Extractor Predicting: 112it [01:05,  1.90it/s]Extractor Predicting: 113it [01:05,  1.88it/s]Extractor Predicting: 114it [01:06,  1.85it/s]Extractor Predicting: 115it [01:06,  1.82it/s]Extractor Predicting: 116it [01:07,  1.77it/s]Extractor Predicting: 117it [01:08,  1.74it/s]Extractor Predicting: 118it [01:08,  1.74it/s]Extractor Predicting: 119it [01:09,  1.73it/s]Extractor Predicting: 120it [01:09,  1.55it/s]Extractor Predicting: 121it [01:10,  1.61it/s]Extractor Predicting: 122it [01:11,  1.64it/s]Extractor Predicting: 123it [01:11,  1.63it/s]Extractor Predicting: 124it [01:12,  1.65it/s]Extractor Predicting: 125it [01:12,  1.63it/s]Extractor Predicting: 126it [01:13,  1.67it/s]Extractor Predicting: 127it [01:14,  1.66it/s]Extractor Predicting: 128it [01:14,  1.65it/s]Extractor Predicting: 129it [01:15,  1.66it/s]Extractor Predicting: 130it [01:15,  1.64it/s]Extractor Predicting: 131it [01:16,  1.63it/s]Extractor Predicting: 132it [01:17,  1.64it/s]Extractor Predicting: 133it [01:17,  1.66it/s]Extractor Predicting: 134it [01:18,  1.66it/s]Extractor Predicting: 135it [01:19,  1.65it/s]Extractor Predicting: 136it [01:19,  1.61it/s]Extractor Predicting: 137it [01:20,  1.65it/s]Extractor Predicting: 138it [01:20,  1.64it/s]Extractor Predicting: 139it [01:21,  1.65it/s]Extractor Predicting: 140it [01:22,  1.64it/s]Extractor Predicting: 141it [01:22,  1.66it/s]Extractor Predicting: 142it [01:23,  1.66it/s]Extractor Predicting: 143it [01:23,  2.10it/s]Extractor Predicting: 143it [01:23,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:58,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:58,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:58,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:58,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:58,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:11:58,988 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:11:58,989 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:11:59,567 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:12:00,577 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:12:00,578 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:12:03,527 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:12:03,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:12:03,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:12:03,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:12:03,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:12:04,189 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:12:04,190 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:12:04,838 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:12:04,998 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:12:04,998 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 2it [00:01,  1.40it/s]
[INFO|configuration_utils.py:515] 2023-08-28 00:12:06,795 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:12:06,796 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:12:06,803 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:12:06,804 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 00:12:06,806 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:12:09,810 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 00:12:09,810 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 00:12:09,833 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:12:09,834 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:12:09,842 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:09,847 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:09,847 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:09,847 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:09,847 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:09,847 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:09,847 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 00:12:10,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:10,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:11,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:12,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:13,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:14,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:14,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:15,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:16,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:17,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:18,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:18,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:19,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:20,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:21,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:22,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:23,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:24,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:24,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:25,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:26,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:27,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:27,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:28,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:54, 19.42s/it][WARNING|generation_utils.py:914] 2023-08-28 00:12:29,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:30,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:31,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:32,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:32,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:33,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:34,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:34,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:35,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:36,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:37,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:38,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:39,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:39,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:40,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:41,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:43,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:43,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:44,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:45,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:46,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:47,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:48,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:49,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:39<02:40, 20.00s/it][WARNING|generation_utils.py:914] 2023-08-28 00:12:49,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:50,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:51,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:52,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:53,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:53,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:54,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:55,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:56,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:57,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:58,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:58,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:59,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:00,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:01,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:02,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:02,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:03,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:04,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:05,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:06,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:06,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:57<02:12, 18.96s/it][WARNING|generation_utils.py:914] 2023-08-28 00:13:07,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:08,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:09,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:10,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:10,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:11,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:12,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:12,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:13,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:14,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:14,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:15,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:16,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:17,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:18,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:18,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:19,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:20,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:20,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:21,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:22,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:23,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:24,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:25,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:26,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:26,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:27,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:28,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:28,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:29,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:30,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:31,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:31,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:32,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:33,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:34,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:34,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:35,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:35,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:36,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:37,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:37,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:38,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:39,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:30<02:25, 24.30s/it][WARNING|generation_utils.py:914] 2023-08-28 00:13:40,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:41,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:41,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:42,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:43,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:44,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:45,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:45,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:46,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:47,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:48,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:48,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:49,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:50,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:51,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:52,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:52,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:53,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:54,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:55,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:56,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:56,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:57,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:58,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:59,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:49<01:53, 22.67s/it][WARNING|generation_utils.py:914] 2023-08-28 00:13:59,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:00,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:01,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:02,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:03,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:03,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:04,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:05,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:06,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:07,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:08,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:08,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:09,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:10,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:11,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:12,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:12,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:13,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:14,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:15,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:16,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:16,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:17,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:18,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:09<01:26, 21.55s/it][WARNING|generation_utils.py:914] 2023-08-28 00:14:19,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:20,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:20,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:21,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:22,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:23,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:24,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:25,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:25,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:26,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:27,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:28,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:29,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:30,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:31,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:32,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:32,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:33,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:34,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:35,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:36,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:37,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:37,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:38,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:39,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:40,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:31<01:05, 21.76s/it][WARNING|generation_utils.py:914] 2023-08-28 00:14:41,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:42,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:42,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:43,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:44,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:45,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:46,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:46,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:47,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:48,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:49,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:49,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:50,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:51,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:52,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:52,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:53,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:54,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:54,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:55,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:56,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:56,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:57,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:48<00:40, 20.28s/it][WARNING|generation_utils.py:914] 2023-08-28 00:14:58,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:59,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:00,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:00,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:01,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:02,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:03,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:04,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:04,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:05,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:06,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:07,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:08,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:08,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:09,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:10,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:11,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:12,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:12,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:13,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:14,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:15,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:15,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:16,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:17,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:08<00:20, 20.08s/it][WARNING|generation_utils.py:914] 2023-08-28 00:15:18,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:18,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:19,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:20,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:21,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:22,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:22,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:23,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:24,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:25,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:25,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:26,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:27,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:28,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:29,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:29,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:30,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:31,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:32,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:32,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:33,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:34,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:35,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:36,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:36,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:15:37,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:28<00:00, 20.23s/it]Generating: 100%|██████████| 10/10 [03:28<00:00, 20.87s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:44,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:44,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:44,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:44,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:44,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:15:45,510 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:15:45,511 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:15:46,087 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:15:47,154 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:15:47,154 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:50,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:50,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:50,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:50,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:15:50,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:15:50,723 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:15:50,724 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:15:51,319 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:15:51,473 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:15:51,473 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
{'target': 600, 'success': 9, 'raw': 32}
{'target': 600, 'success': 26, 'raw': 64}
{'target': 600, 'success': 37, 'raw': 96}
{'target': 600, 'success': 54, 'raw': 128}
{'target': 600, 'success': 67, 'raw': 160}
{'target': 600, 'success': 82, 'raw': 192}
{'target': 600, 'success': 97, 'raw': 224}
{'target': 600, 'success': 111, 'raw': 256}
{'target': 600, 'success': 125, 'raw': 288}
{'target': 600, 'success': 137, 'raw': 320}
{'target': 600, 'success': 152, 'raw': 352}
{'target': 600, 'success': 166, 'raw': 384}
{'target': 600, 'success': 180, 'raw': 416}
{'target': 600, 'success': 196, 'raw': 448}
{'target': 600, 'success': 214, 'raw': 480}
{'target': 600, 'success': 229, 'raw': 512}
{'target': 600, 'success': 246, 'raw': 544}
{'target': 600, 'success': 260, 'raw': 576}
{'target': 600, 'success': 273, 'raw': 608}
{'target': 600, 'success': 285, 'raw': 640}
{'target': 600, 'success': 301, 'raw': 672}
{'target': 600, 'success': 315, 'raw': 704}
{'target': 600, 'success': 327, 'raw': 736}
{'target': 600, 'success': 334, 'raw': 768}
{'target': 600, 'success': 355, 'raw': 800}
{'target': 600, 'success': 369, 'raw': 832}
{'target': 600, 'success': 386, 'raw': 864}
{'target': 600, 'success': 399, 'raw': 896}
{'target': 600, 'success': 412, 'raw': 928}
{'target': 600, 'success': 425, 'raw': 960}
{'target': 600, 'success': 433, 'raw': 992}
{'target': 600, 'success': 443, 'raw': 1024}
{'target': 600, 'success': 460, 'raw': 1056}
{'target': 600, 'success': 475, 'raw': 1088}
{'target': 600, 'success': 485, 'raw': 1120}
{'target': 600, 'success': 497, 'raw': 1152}
{'target': 600, 'success': 511, 'raw': 1184}
{'target': 600, 'success': 525, 'raw': 1216}
{'target': 600, 'success': 542, 'raw': 1248}
{'target': 600, 'success': 556, 'raw': 1280}
{'target': 600, 'success': 573, 'raw': 1312}
{'target': 600, 'success': 586, 'raw': 1344}
{'target': 600, 'success': 596, 'raw': 1376}
{'target': 600, 'success': 605, 'raw': 1408}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4296875, 'errors': {'', '(\'Love Is One of the Most Important Facts You Can Learn About Yourself\', \'original language of film or TV show\', \'\', \'On 3 November 2001 , he made the film " Love Is One of the Most Important Facts You Can Learn About Yourself " , about an alcoholic father .\')', '(\'The Hound of the Baskervilles\', \'original language of film or TV show\', \'\', \'His best work came in the 1983 horror movie " The Hound of the Baskervilles " directed by Gene Luen Yang .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 496, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8033854166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 254, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 450, 'raw': 608}
{'target': 600, 'success': 469, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.7271634615384616, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 526, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7525, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 428, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 595, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 11540
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11640, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:02,  1.36it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:06,  1.49it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.49it/s]Extractor Estimating: 12it [00:08,  1.49it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.41it/s]Extractor Estimating: 16it [00:10,  1.45it/s]Extractor Estimating: 17it [00:11,  1.44it/s]Extractor Estimating: 18it [00:12,  1.41it/s]Extractor Estimating: 19it [00:13,  1.45it/s]Extractor Estimating: 20it [00:13,  1.50it/s]Extractor Estimating: 21it [00:14,  1.51it/s]Extractor Estimating: 22it [00:14,  1.52it/s]Extractor Estimating: 23it [00:15,  1.51it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:16,  1.53it/s]Extractor Estimating: 26it [00:17,  1.50it/s]Extractor Estimating: 27it [00:18,  1.44it/s]Extractor Estimating: 28it [00:19,  1.39it/s]Extractor Estimating: 29it [00:19,  1.42it/s]Extractor Estimating: 30it [00:20,  1.44it/s]Extractor Estimating: 31it [00:21,  1.45it/s]Extractor Estimating: 32it [00:21,  1.51it/s]Extractor Estimating: 33it [00:22,  1.53it/s]Extractor Estimating: 34it [00:23,  1.54it/s]Extractor Estimating: 35it [00:23,  1.54it/s]Extractor Estimating: 36it [00:24,  1.51it/s]Extractor Estimating: 37it [00:25,  1.53it/s]Extractor Estimating: 38it [00:25,  1.46it/s]Extractor Estimating: 39it [00:26,  1.43it/s]Extractor Estimating: 40it [00:27,  1.44it/s]Extractor Estimating: 41it [00:27,  1.43it/s]Extractor Estimating: 42it [00:28,  1.36it/s]Extractor Estimating: 43it [00:29,  1.32it/s]Extractor Estimating: 44it [00:30,  1.35it/s]Extractor Estimating: 45it [00:30,  1.41it/s]Extractor Estimating: 46it [00:31,  1.40it/s]Extractor Estimating: 47it [00:32,  1.44it/s]Extractor Estimating: 48it [00:33,  1.39it/s]Extractor Estimating: 49it [00:33,  1.47it/s]Extractor Estimating: 50it [00:34,  1.47it/s]Extractor Estimating: 51it [00:34,  1.48it/s]Extractor Estimating: 52it [00:35,  1.51it/s]Extractor Estimating: 53it [00:36,  1.52it/s]Extractor Estimating: 54it [00:36,  1.53it/s]Extractor Estimating: 55it [00:37,  1.51it/s]Extractor Estimating: 56it [00:38,  1.54it/s]Extractor Estimating: 57it [00:38,  1.52it/s]Extractor Estimating: 58it [00:39,  1.51it/s]Extractor Estimating: 59it [00:40,  1.53it/s]Extractor Estimating: 60it [00:40,  1.54it/s]Extractor Estimating: 61it [00:41,  1.55it/s]Extractor Estimating: 62it [00:42,  1.53it/s]Extractor Estimating: 63it [00:42,  1.54it/s]Extractor Estimating: 64it [00:43,  1.53it/s]Extractor Estimating: 65it [00:44,  1.51it/s]Extractor Estimating: 66it [00:44,  1.54it/s]Extractor Estimating: 67it [00:45,  1.51it/s]Extractor Estimating: 68it [00:46,  1.47it/s]Extractor Estimating: 69it [00:46,  1.51it/s]Extractor Estimating: 70it [00:47,  1.55it/s]Extractor Estimating: 71it [00:47,  1.54it/s]Extractor Estimating: 72it [00:48,  1.53it/s]Extractor Estimating: 73it [00:49,  1.54it/s]Extractor Estimating: 74it [00:50,  1.50it/s]Extractor Estimating: 75it [00:50,  1.53it/s]Extractor Estimating: 76it [00:51,  1.52it/s]Extractor Estimating: 77it [00:51,  1.51it/s]Extractor Estimating: 78it [00:52,  1.57it/s]Extractor Estimating: 79it [00:53,  1.52it/s]Extractor Estimating: 80it [00:53,  1.53it/s]Extractor Estimating: 81it [00:54,  1.51it/s]Extractor Estimating: 82it [00:55,  1.52it/s]Extractor Estimating: 83it [00:55,  1.56it/s]Extractor Estimating: 84it [00:56,  1.58it/s]Extractor Estimating: 85it [00:57,  1.60it/s]Extractor Estimating: 86it [00:57,  1.50it/s]Extractor Estimating: 87it [00:58,  1.43it/s]Extractor Estimating: 88it [00:59,  1.40it/s]Extractor Estimating: 89it [01:00,  1.32it/s]Extractor Estimating: 90it [01:00,  1.40it/s]Extractor Estimating: 91it [01:01,  1.41it/s]Extractor Estimating: 92it [01:02,  1.49it/s]Extractor Estimating: 93it [01:02,  1.52it/s]Extractor Estimating: 94it [01:03,  1.54it/s]Extractor Estimating: 95it [01:03,  1.57it/s]Extractor Estimating: 96it [01:04,  1.57it/s]Extractor Estimating: 97it [01:05,  1.58it/s]Extractor Estimating: 98it [01:05,  1.59it/s]Extractor Estimating: 99it [01:06,  1.57it/s]Extractor Estimating: 100it [01:07,  1.44it/s]Extractor Estimating: 101it [01:07,  1.45it/s]Extractor Estimating: 102it [01:08,  1.45it/s]Extractor Estimating: 103it [01:09,  1.50it/s]Extractor Estimating: 104it [01:10,  1.47it/s]Extractor Estimating: 105it [01:10,  1.49it/s]Extractor Estimating: 106it [01:11,  1.48it/s]Extractor Estimating: 107it [01:11,  1.52it/s]Extractor Estimating: 108it [01:12,  1.53it/s]Extractor Estimating: 109it [01:13,  1.48it/s]Extractor Estimating: 110it [01:13,  1.50it/s]Extractor Estimating: 111it [01:14,  1.50it/s]Extractor Estimating: 112it [01:15,  1.49it/s]Extractor Estimating: 113it [01:15,  1.50it/s]Extractor Estimating: 114it [01:16,  1.55it/s]Extractor Estimating: 115it [01:17,  1.51it/s]Extractor Estimating: 116it [01:17,  1.53it/s]Extractor Estimating: 117it [01:18,  1.56it/s]Extractor Estimating: 118it [01:19,  1.53it/s]Extractor Estimating: 119it [01:19,  1.55it/s]Extractor Estimating: 120it [01:20,  1.49it/s]Extractor Estimating: 121it [01:21,  1.49it/s]Extractor Estimating: 122it [01:21,  1.53it/s]Extractor Estimating: 123it [01:22,  1.48it/s]Extractor Estimating: 124it [01:23,  1.50it/s]Extractor Estimating: 125it [01:23,  1.49it/s]Extractor Estimating: 126it [01:24,  1.43it/s]Extractor Estimating: 127it [01:25,  1.44it/s]Extractor Estimating: 128it [01:26,  1.46it/s]Extractor Estimating: 129it [01:26,  1.46it/s]Extractor Estimating: 130it [01:27,  1.52it/s]Extractor Estimating: 131it [01:27,  1.52it/s]Extractor Estimating: 132it [01:28,  1.54it/s]Extractor Estimating: 133it [01:29,  1.51it/s]Extractor Estimating: 134it [01:29,  1.52it/s]Extractor Estimating: 135it [01:30,  1.54it/s]Extractor Estimating: 136it [01:31,  1.58it/s]Extractor Estimating: 137it [01:31,  1.55it/s]Extractor Estimating: 138it [01:32,  1.59it/s]Extractor Estimating: 139it [01:33,  1.59it/s]Extractor Estimating: 140it [01:33,  1.57it/s]Extractor Estimating: 141it [01:34,  1.53it/s]Extractor Estimating: 142it [01:35,  1.50it/s]Extractor Estimating: 143it [01:35,  1.52it/s]Extractor Estimating: 144it [01:36,  1.58it/s]Extractor Estimating: 145it [01:36,  1.57it/s]Extractor Estimating: 146it [01:37,  1.57it/s]Extractor Estimating: 147it [01:38,  1.56it/s]Extractor Estimating: 148it [01:38,  1.58it/s]Extractor Estimating: 149it [01:39,  1.57it/s]Extractor Estimating: 150it [01:40,  1.58it/s]Extractor Estimating: 151it [01:40,  1.55it/s]Extractor Estimating: 152it [01:41,  1.56it/s]Extractor Estimating: 153it [01:42,  1.55it/s]Extractor Estimating: 154it [01:42,  1.44it/s]Extractor Estimating: 155it [01:43,  1.41it/s]Extractor Estimating: 156it [01:44,  1.47it/s]Extractor Estimating: 157it [01:44,  1.53it/s]Extractor Estimating: 158it [01:45,  1.54it/s]Extractor Estimating: 159it [01:46,  1.49it/s]Extractor Estimating: 160it [01:46,  1.49it/s]Extractor Estimating: 161it [01:47,  1.46it/s]Extractor Estimating: 162it [01:48,  1.43it/s]Extractor Estimating: 163it [01:49,  1.43it/s]Extractor Estimating: 164it [01:49,  1.43it/s]Extractor Estimating: 165it [01:50,  1.42it/s]Extractor Estimating: 166it [01:51,  1.27it/s]Extractor Estimating: 167it [01:52,  1.30it/s]Extractor Estimating: 168it [01:52,  1.33it/s]Extractor Estimating: 169it [01:53,  1.39it/s]Extractor Estimating: 170it [01:54,  1.41it/s]Extractor Estimating: 171it [01:54,  1.44it/s]Extractor Estimating: 172it [01:55,  1.42it/s]Extractor Estimating: 173it [01:56,  1.42it/s]Extractor Estimating: 174it [01:56,  1.46it/s]Extractor Estimating: 175it [01:57,  1.37it/s]Extractor Estimating: 176it [01:58,  1.44it/s]Extractor Estimating: 177it [01:59,  1.46it/s]Extractor Estimating: 178it [01:59,  1.43it/s]Extractor Estimating: 179it [02:00,  1.46it/s]Extractor Estimating: 180it [02:01,  1.46it/s]Extractor Estimating: 181it [02:01,  1.46it/s]Extractor Estimating: 182it [02:02,  1.47it/s]Extractor Estimating: 183it [02:03,  1.52it/s]Extractor Estimating: 184it [02:03,  1.53it/s]Extractor Estimating: 185it [02:04,  1.60it/s]Extractor Estimating: 186it [02:04,  1.58it/s]Extractor Estimating: 187it [02:05,  1.51it/s]Extractor Estimating: 188it [02:06,  1.53it/s]Extractor Estimating: 189it [02:06,  1.55it/s]Extractor Estimating: 190it [02:07,  1.56it/s]Extractor Estimating: 191it [02:08,  1.49it/s]Extractor Estimating: 192it [02:08,  1.53it/s]Extractor Estimating: 193it [02:09,  1.52it/s]Extractor Estimating: 194it [02:10,  1.50it/s]Extractor Estimating: 195it [02:10,  1.52it/s]Extractor Estimating: 196it [02:11,  1.56it/s]Extractor Estimating: 197it [02:12,  1.55it/s]Extractor Estimating: 198it [02:12,  1.54it/s]Extractor Estimating: 199it [02:13,  1.54it/s]Extractor Estimating: 200it [02:14,  1.53it/s]Extractor Estimating: 201it [02:14,  1.46it/s]Extractor Estimating: 202it [02:15,  1.48it/s]Extractor Estimating: 203it [02:16,  1.50it/s]Extractor Estimating: 204it [02:16,  1.50it/s]Extractor Estimating: 205it [02:17,  1.54it/s]Extractor Estimating: 206it [02:18,  1.50it/s]Extractor Estimating: 207it [02:18,  1.50it/s]Extractor Estimating: 208it [02:19,  1.50it/s]Extractor Estimating: 209it [02:20,  1.47it/s]Extractor Estimating: 210it [02:20,  1.49it/s]Extractor Estimating: 211it [02:21,  1.45it/s]Extractor Estimating: 212it [02:22,  1.46it/s]Extractor Estimating: 213it [02:22,  1.50it/s]Extractor Estimating: 214it [02:23,  1.47it/s]Extractor Estimating: 215it [02:24,  1.46it/s]Extractor Estimating: 216it [02:24,  1.43it/s]Extractor Estimating: 217it [02:25,  1.45it/s]Extractor Estimating: 218it [02:26,  1.47it/s]Extractor Estimating: 219it [02:26,  1.48it/s]Extractor Estimating: 220it [02:27,  1.46it/s]Extractor Estimating: 221it [02:28,  1.47it/s]Extractor Estimating: 222it [02:28,  1.54it/s]Extractor Estimating: 223it [02:29,  1.50it/s]Extractor Estimating: 224it [02:30,  1.49it/s]Extractor Estimating: 225it [02:31,  1.48it/s]Extractor Estimating: 226it [02:31,  1.48it/s]Extractor Estimating: 227it [02:32,  1.46it/s]Extractor Estimating: 228it [02:33,  1.43it/s]Extractor Estimating: 229it [02:33,  1.47it/s]Extractor Estimating: 230it [02:34,  1.49it/s]Extractor Estimating: 231it [02:35,  1.52it/s]Extractor Estimating: 232it [02:35,  1.39it/s]Extractor Estimating: 233it [02:36,  1.41it/s]Extractor Estimating: 234it [02:37,  1.42it/s]Extractor Estimating: 235it [02:37,  1.48it/s]Extractor Estimating: 236it [02:38,  1.44it/s]Extractor Estimating: 237it [02:39,  1.42it/s]Extractor Estimating: 238it [02:39,  1.47it/s]Extractor Estimating: 239it [02:40,  1.45it/s]Extractor Estimating: 240it [02:41,  1.46it/s]Extractor Estimating: 241it [02:42,  1.40it/s]Extractor Estimating: 242it [02:42,  1.41it/s]Extractor Estimating: 243it [02:43,  1.44it/s]Extractor Estimating: 244it [02:44,  1.50it/s]Extractor Estimating: 245it [02:44,  1.45it/s]Extractor Estimating: 246it [02:45,  1.48it/s]Extractor Estimating: 247it [02:46,  1.50it/s]Extractor Estimating: 248it [02:46,  1.48it/s]Extractor Estimating: 249it [02:47,  1.50it/s]Extractor Estimating: 250it [02:48,  1.47it/s]Extractor Estimating: 250it [02:48,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:51,949 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:51,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:51,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:51,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:51,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:18:52,707 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:18:52,708 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:18:53,117 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:18:54,193 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:18:54,193 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:57,367 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:57,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:57,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:57,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:18:57,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:18:57,811 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:18:57,813 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:18:58,187 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:18:58,356 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:18:58,356 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 01:57:03,152 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 01:57:03,177 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5227 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 23176
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23276, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23276, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.131, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.141, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.140, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.155, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.120, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.271, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.152, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.140, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.141, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.127, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.292, loss:nan
g_step 1200, step 110, avg_time 1.141, loss:nan
g_step 1300, step 210, avg_time 1.137, loss:nan
g_step 1400, step 92, avg_time 1.150, loss:nan
g_step 1500, step 192, avg_time 1.140, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 2.272, loss:nan
g_step 1700, step 174, avg_time 1.148, loss:nan
g_step 1800, step 56, avg_time 1.144, loss:nan
g_step 1900, step 156, avg_time 1.143, loss:nan
g_step 2000, step 38, avg_time 1.144, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 2.274, loss:nan
g_step 2200, step 20, avg_time 1.146, loss:nan
g_step 2300, step 120, avg_time 1.136, loss:nan
g_step 2400, step 2, avg_time 1.139, loss:nan
g_step 2500, step 102, avg_time 1.155, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 2.257, loss:nan
g_step 2700, step 84, avg_time 1.129, loss:nan
g_step 2800, step 184, avg_time 1.149, loss:nan
g_step 2900, step 66, avg_time 1.134, loss:nan
g_step 3000, step 166, avg_time 1.157, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.265, loss:nan
g_step 3200, step 148, avg_time 1.141, loss:nan
g_step 3300, step 30, avg_time 1.138, loss:nan
g_step 3400, step 130, avg_time 1.135, loss:nan
g_step 3500, step 12, avg_time 1.138, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.266, loss:nan
g_step 3700, step 212, avg_time 1.156, loss:nan
g_step 3800, step 94, avg_time 1.147, loss:nan
g_step 3900, step 194, avg_time 1.141, loss:nan
g_step 4000, step 76, avg_time 1.141, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.287, loss:nan
g_step 4200, step 58, avg_time 1.125, loss:nan
g_step 4300, step 158, avg_time 1.148, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 01:57:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 01:57:03 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_01-57-03_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 01:57:04 - WARNING - datasets.builder -   Using custom data configuration default-867696c5681f8615
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-867696c5681f8615/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 01:57:04,452 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:57:04,453 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:57:04,454 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:57:04,455 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:57:04,465 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:57:04,471 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:57:04,471 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:57:04,471 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:57:04,471 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:57:04,471 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:57:04,471 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 01:57:04,635 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:57:07,813 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 01:57:07,816 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-867696c5681f8615/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.33ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.32ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.76ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.02ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.16ba/s]100%|██████████| 6/6 [00:01<00:00,  4.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.16ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.27ba/s]100%|██████████| 4/4 [00:00<00:00,  5.30ba/s]100%|██████████| 4/4 [00:00<00:00,  4.82ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.85ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.46ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.83ba/s]100%|██████████| 6/6 [00:00<00:00, 11.02ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.60ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.32ba/s]100%|██████████| 4/4 [00:00<00:00, 10.55ba/s]
[INFO|trainer.py:414] 2023-08-28 01:57:11,326 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 01:57:11,338 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 01:57:11,338 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 01:57:11,338 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 01:57:11,338 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 01:57:11,338 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 01:57:11,338 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 01:57:11,338 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:55,  3.53it/s]  0%|          | 2/410 [00:00<01:53,  3.61it/s]  1%|          | 3/410 [00:00<01:51,  3.64it/s]  1%|          | 4/410 [00:01<01:51,  3.65it/s]  1%|          | 5/410 [00:01<01:50,  3.65it/s]  1%|▏         | 6/410 [00:01<01:50,  3.66it/s]  2%|▏         | 7/410 [00:01<01:50,  3.66it/s]  2%|▏         | 8/410 [00:02<01:49,  3.66it/s]  2%|▏         | 9/410 [00:02<01:49,  3.67it/s]  2%|▏         | 10/410 [00:02<01:49,  3.67it/s]  3%|▎         | 11/410 [00:03<01:49,  3.64it/s]  3%|▎         | 12/410 [00:03<01:49,  3.65it/s]  3%|▎         | 13/410 [00:03<01:48,  3.65it/s]  3%|▎         | 14/410 [00:03<01:48,  3.66it/s]  4%|▎         | 15/410 [00:04<01:47,  3.66it/s]  4%|▍         | 16/410 [00:04<01:47,  3.66it/s]  4%|▍         | 17/410 [00:04<01:47,  3.67it/s]  4%|▍         | 18/410 [00:04<01:46,  3.67it/s]  5%|▍         | 19/410 [00:05<01:46,  3.67it/s]  5%|▍         | 20/410 [00:05<01:46,  3.67it/s]  5%|▌         | 21/410 [00:05<01:46,  3.67it/s]  5%|▌         | 22/410 [00:06<01:46,  3.66it/s]  6%|▌         | 23/410 [00:06<01:45,  3.66it/s]  6%|▌         | 24/410 [00:06<01:45,  3.66it/s]  6%|▌         | 25/410 [00:06<01:45,  3.66it/s]  6%|▋         | 26/410 [00:07<01:44,  3.66it/s]  7%|▋         | 27/410 [00:07<01:44,  3.66it/s]  7%|▋         | 28/410 [00:07<01:44,  3.66it/s]  7%|▋         | 29/410 [00:07<01:44,  3.66it/s]  7%|▋         | 30/410 [00:08<01:43,  3.66it/s]  8%|▊         | 31/410 [00:08<01:43,  3.66it/s]  8%|▊         | 32/410 [00:08<01:43,  3.66it/s]  8%|▊         | 33/410 [00:09<01:43,  3.65it/s]  8%|▊         | 34/410 [00:09<01:43,  3.65it/s]  9%|▊         | 35/410 [00:09<01:42,  3.65it/s]  9%|▉         | 36/410 [00:09<01:42,  3.66it/s]  9%|▉         | 37/410 [00:10<01:42,  3.66it/s]  9%|▉         | 38/410 [00:10<01:41,  3.66it/s] 10%|▉         | 39/410 [00:10<01:41,  3.66it/s] 10%|▉         | 40/410 [00:10<01:41,  3.66it/s] 10%|█         | 41/410 [00:11<01:40,  3.66it/s] 10%|█         | 42/410 [00:11<01:40,  3.66it/s] 10%|█         | 43/410 [00:11<01:40,  3.66it/s] 11%|█         | 44/410 [00:12<01:45,  3.48it/s] 11%|█         | 45/410 [00:12<01:43,  3.53it/s] 11%|█         | 46/410 [00:12<01:41,  3.57it/s] 11%|█▏        | 47/410 [00:12<01:40,  3.60it/s] 12%|█▏        | 48/410 [00:13<01:40,  3.61it/s] 12%|█▏        | 49/410 [00:13<01:39,  3.63it/s] 12%|█▏        | 50/410 [00:13<01:39,  3.64it/s] 12%|█▏        | 51/410 [00:13<01:38,  3.64it/s] 13%|█▎        | 52/410 [00:14<01:38,  3.65it/s] 13%|█▎        | 53/410 [00:14<01:37,  3.65it/s] 13%|█▎        | 54/410 [00:14<01:37,  3.65it/s] 13%|█▎        | 55/410 [00:15<01:37,  3.63it/s] 14%|█▎        | 56/410 [00:15<01:37,  3.64it/s] 14%|█▍        | 57/410 [00:15<01:36,  3.65it/s] 14%|█▍        | 58/410 [00:15<01:36,  3.65it/s] 14%|█▍        | 59/410 [00:16<01:36,  3.65it/s] 15%|█▍        | 60/410 [00:16<01:35,  3.65it/s] 15%|█▍        | 61/410 [00:16<01:35,  3.66it/s] 15%|█▌        | 62/410 [00:17<01:35,  3.66it/s] 15%|█▌        | 63/410 [00:17<01:34,  3.66it/s] 16%|█▌        | 64/410 [00:17<01:34,  3.66it/s] 16%|█▌        | 65/410 [00:17<01:34,  3.66it/s] 16%|█▌        | 66/410 [00:18<01:34,  3.64it/s] 16%|█▋        | 67/410 [00:18<01:34,  3.65it/s] 17%|█▋        | 68/410 [00:18<01:33,  3.65it/s] 17%|█▋        | 69/410 [00:18<01:33,  3.65it/s] 17%|█▋        | 70/410 [00:19<01:33,  3.65it/s] 17%|█▋        | 71/410 [00:19<01:32,  3.65it/s] 18%|█▊        | 72/410 [00:19<01:32,  3.65it/s] 18%|█▊        | 73/410 [00:20<01:32,  3.66it/s] 18%|█▊        | 74/410 [00:20<01:32,  3.65it/s] 18%|█▊        | 75/410 [00:20<01:31,  3.65it/s] 19%|█▊        | 76/410 [00:20<01:31,  3.65it/s] 19%|█▉        | 77/410 [00:21<01:31,  3.64it/s] 19%|█▉        | 78/410 [00:21<01:31,  3.64it/s] 19%|█▉        | 79/410 [00:21<01:30,  3.65it/s] 20%|█▉        | 80/410 [00:21<01:30,  3.65it/s] 20%|█▉        | 81/410 [00:22<01:30,  3.65it/s] 20%|██        | 82/410 [00:22<01:26,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 01:57:33,796 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:57:33,796 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 01:57:33,796 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.03it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.33it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.46it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.87it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.38it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.12it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.87it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.57it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.54it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.58it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.59it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.54it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.52it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.38it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.43it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.44it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.28it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.24it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.34it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.43it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.23it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.41it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.41it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.40it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.46it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.46it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.48it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.31it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.39it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.50it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.46it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.48it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.43it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.44it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.48it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.46it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.47it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.43it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.47it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.44it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.47it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.51it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.44it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.49it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.41it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.38it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.41it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.43it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.39it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.37it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.46it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.42it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.42it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.38it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.39it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.42it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.47it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.42it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.48it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.43it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.34it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.34it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.31it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.35it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.39it/s][A
 76%|███████▌  | 333/437 [00:06<00:02, 47.43it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.40it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.38it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.44it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.45it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.40it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.44it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.42it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.42it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.46it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.46it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.46it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.33it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.38it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.35it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.38it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.41it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.30it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.38it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.32it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.34it/s][A                                                
                                                 [A 20%|██        | 82/410 [00:31<01:26,  3.79it/s]
100%|██████████| 437/437 [00:09<00:00, 47.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:57:43,035 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 01:57:43,059 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:57:45,546 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:57:45,558 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:57:45,567 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:34<21:09,  3.88s/it] 20%|██        | 84/410 [00:35<15:12,  2.80s/it] 21%|██        | 85/410 [00:35<11:03,  2.04s/it] 21%|██        | 86/410 [00:35<08:09,  1.51s/it] 21%|██        | 87/410 [00:35<06:08,  1.14s/it] 21%|██▏       | 88/410 [00:36<04:43,  1.14it/s] 22%|██▏       | 89/410 [00:36<03:44,  1.43it/s] 22%|██▏       | 90/410 [00:36<03:02,  1.75it/s] 22%|██▏       | 91/410 [00:36<02:33,  2.08it/s] 22%|██▏       | 92/410 [00:37<02:13,  2.39it/s] 23%|██▎       | 93/410 [00:37<01:59,  2.66it/s] 23%|██▎       | 94/410 [00:37<01:49,  2.90it/s] 23%|██▎       | 95/410 [00:38<01:42,  3.08it/s] 23%|██▎       | 96/410 [00:38<01:37,  3.23it/s] 24%|██▎       | 97/410 [00:38<01:33,  3.34it/s] 24%|██▍       | 98/410 [00:38<01:30,  3.43it/s] 24%|██▍       | 99/410 [00:39<01:29,  3.49it/s] 24%|██▍       | 100/410 [00:39<01:27,  3.53it/s] 25%|██▍       | 101/410 [00:39<01:26,  3.57it/s] 25%|██▍       | 102/410 [00:39<01:25,  3.59it/s] 25%|██▌       | 103/410 [00:40<01:25,  3.61it/s] 25%|██▌       | 104/410 [00:40<01:24,  3.62it/s] 26%|██▌       | 105/410 [00:40<01:24,  3.63it/s] 26%|██▌       | 106/410 [00:41<01:23,  3.62it/s] 26%|██▌       | 107/410 [00:41<01:23,  3.63it/s] 26%|██▋       | 108/410 [00:41<01:23,  3.63it/s] 27%|██▋       | 109/410 [00:41<01:22,  3.64it/s] 27%|██▋       | 110/410 [00:42<01:22,  3.64it/s] 27%|██▋       | 111/410 [00:42<01:22,  3.64it/s] 27%|██▋       | 112/410 [00:42<01:21,  3.64it/s] 28%|██▊       | 113/410 [00:43<01:21,  3.64it/s] 28%|██▊       | 114/410 [00:43<01:21,  3.64it/s] 28%|██▊       | 115/410 [00:43<01:20,  3.65it/s] 28%|██▊       | 116/410 [00:43<01:20,  3.65it/s] 29%|██▊       | 117/410 [00:44<01:20,  3.63it/s] 29%|██▉       | 118/410 [00:44<01:20,  3.64it/s] 29%|██▉       | 119/410 [00:44<01:19,  3.64it/s] 29%|██▉       | 120/410 [00:44<01:19,  3.65it/s] 30%|██▉       | 121/410 [00:45<01:19,  3.64it/s] 30%|██▉       | 122/410 [00:45<01:19,  3.65it/s] 30%|███       | 123/410 [00:45<01:18,  3.64it/s] 30%|███       | 124/410 [00:46<01:18,  3.65it/s] 30%|███       | 125/410 [00:46<01:18,  3.64it/s] 31%|███       | 126/410 [00:46<01:17,  3.65it/s] 31%|███       | 127/410 [00:46<01:17,  3.64it/s] 31%|███       | 128/410 [00:47<01:17,  3.63it/s] 31%|███▏      | 129/410 [00:47<01:17,  3.64it/s] 32%|███▏      | 130/410 [00:47<01:16,  3.64it/s] 32%|███▏      | 131/410 [00:47<01:16,  3.64it/s] 32%|███▏      | 132/410 [00:48<01:16,  3.64it/s] 32%|███▏      | 133/410 [00:48<01:16,  3.64it/s] 33%|███▎      | 134/410 [00:48<01:15,  3.64it/s] 33%|███▎      | 135/410 [00:49<01:15,  3.65it/s] 33%|███▎      | 136/410 [00:49<01:15,  3.64it/s] 33%|███▎      | 137/410 [00:49<01:14,  3.65it/s] 34%|███▎      | 138/410 [00:49<01:14,  3.65it/s] 34%|███▍      | 139/410 [00:50<01:14,  3.64it/s] 34%|███▍      | 140/410 [00:50<01:14,  3.64it/s] 34%|███▍      | 141/410 [00:50<01:13,  3.64it/s] 35%|███▍      | 142/410 [00:50<01:13,  3.63it/s] 35%|███▍      | 143/410 [00:51<01:13,  3.64it/s] 35%|███▌      | 144/410 [00:51<01:13,  3.64it/s] 35%|███▌      | 145/410 [00:51<01:12,  3.64it/s] 36%|███▌      | 146/410 [00:52<01:14,  3.56it/s] 36%|███▌      | 147/410 [00:52<01:13,  3.57it/s] 36%|███▌      | 148/410 [00:52<01:12,  3.59it/s] 36%|███▋      | 149/410 [00:52<01:12,  3.61it/s] 37%|███▋      | 150/410 [00:53<01:12,  3.60it/s] 37%|███▋      | 151/410 [00:53<01:11,  3.61it/s] 37%|███▋      | 152/410 [00:53<01:11,  3.62it/s] 37%|███▋      | 153/410 [00:54<01:10,  3.63it/s] 38%|███▊      | 154/410 [00:54<01:10,  3.64it/s] 38%|███▊      | 155/410 [00:54<01:10,  3.64it/s] 38%|███▊      | 156/410 [00:54<01:09,  3.64it/s] 38%|███▊      | 157/410 [00:55<01:09,  3.64it/s] 39%|███▊      | 158/410 [00:55<01:09,  3.65it/s] 39%|███▉      | 159/410 [00:55<01:08,  3.65it/s] 39%|███▉      | 160/410 [00:55<01:08,  3.64it/s] 39%|███▉      | 161/410 [00:56<01:08,  3.64it/s] 40%|███▉      | 162/410 [00:56<01:08,  3.64it/s] 40%|███▉      | 163/410 [00:56<01:07,  3.64it/s] 40%|████      | 164/410 [00:57<01:05,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 01:58:08,351 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:58:08,351 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 01:58:08,351 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.219, 'eval_samples_per_second': 379.215, 'eval_steps_per_second': 47.402, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.73it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.04it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.46it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.79it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.36it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.00it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.69it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.44it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.32it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.41it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.30it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.29it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.37it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.41it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.39it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.25it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.11it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.12it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.20it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.27it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.17it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.20it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.33it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.34it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.28it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.18it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.18it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.07it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.18it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.28it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.23it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.28it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.24it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.23it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.20it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.07it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.14it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.14it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.22it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.26it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.28it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.21it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.19it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.19it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.14it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.19it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.22it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.12it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.17it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.31it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.30it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.26it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.12it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.08it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.11it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.14it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.21it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.18it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.18it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.32it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.22it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.15it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.21it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.11it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.10it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.22it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.24it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.28it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.17it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.22it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.14it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.14it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.14it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.04it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.15it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.17it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.29it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.34it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.19it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.18it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.10it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.06it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.14it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.19it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.11it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.18it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:06<01:05,  3.78it/s]
100%|██████████| 437/437 [00:09<00:00, 47.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:58:17,634 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 01:58:17,649 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:58:20,199 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:58:20,217 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:58:20,226 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:09<15:58,  3.91s/it] 40%|████      | 166/410 [01:09<11:28,  2.82s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:09<08:19,  2.06s/it] 41%|████      | 168/410 [01:10<06:08,  1.52s/it] 41%|████      | 169/410 [01:10<04:36,  1.15s/it] 41%|████▏     | 170/410 [01:10<03:32,  1.13it/s] 42%|████▏     | 171/410 [01:11<02:47,  1.42it/s] 42%|████▏     | 172/410 [01:11<02:16,  1.74it/s] 42%|████▏     | 173/410 [01:11<01:54,  2.07it/s] 42%|████▏     | 174/410 [01:11<01:39,  2.37it/s] 43%|████▎     | 175/410 [01:12<01:28,  2.65it/s] 43%|████▎     | 176/410 [01:12<01:21,  2.88it/s] 43%|████▎     | 177/410 [01:12<01:15,  3.08it/s] 43%|████▎     | 178/410 [01:13<01:11,  3.22it/s] 44%|████▎     | 179/410 [01:13<01:09,  3.34it/s] 44%|████▍     | 180/410 [01:13<01:07,  3.42it/s] 44%|████▍     | 181/410 [01:13<01:05,  3.49it/s] 44%|████▍     | 182/410 [01:14<01:04,  3.53it/s] 45%|████▍     | 183/410 [01:14<01:03,  3.57it/s] 45%|████▍     | 184/410 [01:14<01:02,  3.59it/s] 45%|████▌     | 185/410 [01:14<01:02,  3.61it/s] 45%|████▌     | 186/410 [01:15<01:02,  3.61it/s] 46%|████▌     | 187/410 [01:15<01:01,  3.62it/s] 46%|████▌     | 188/410 [01:15<01:01,  3.63it/s] 46%|████▌     | 189/410 [01:16<01:00,  3.63it/s] 46%|████▋     | 190/410 [01:16<01:00,  3.63it/s] 47%|████▋     | 191/410 [01:16<01:00,  3.64it/s] 47%|████▋     | 192/410 [01:16<00:59,  3.64it/s] 47%|████▋     | 193/410 [01:17<00:59,  3.64it/s] 47%|████▋     | 194/410 [01:17<00:59,  3.64it/s] 48%|████▊     | 195/410 [01:17<00:58,  3.64it/s] 48%|████▊     | 196/410 [01:17<00:58,  3.64it/s] 48%|████▊     | 197/410 [01:18<00:58,  3.63it/s] 48%|████▊     | 198/410 [01:18<00:58,  3.63it/s] 49%|████▊     | 199/410 [01:18<00:58,  3.64it/s] 49%|████▉     | 200/410 [01:19<00:57,  3.64it/s] 49%|████▉     | 201/410 [01:19<00:57,  3.64it/s] 49%|████▉     | 202/410 [01:19<00:57,  3.64it/s] 50%|████▉     | 203/410 [01:19<00:56,  3.64it/s] 50%|████▉     | 204/410 [01:20<00:56,  3.64it/s] 50%|█████     | 205/410 [01:20<00:56,  3.64it/s] 50%|█████     | 206/410 [01:20<00:56,  3.64it/s] 50%|█████     | 207/410 [01:20<00:55,  3.64it/s] 51%|█████     | 208/410 [01:21<00:55,  3.63it/s] 51%|█████     | 209/410 [01:21<00:55,  3.63it/s] 51%|█████     | 210/410 [01:21<00:54,  3.64it/s] 51%|█████▏    | 211/410 [01:22<00:54,  3.64it/s] 52%|█████▏    | 212/410 [01:22<00:54,  3.64it/s] 52%|█████▏    | 213/410 [01:22<00:54,  3.64it/s] 52%|█████▏    | 214/410 [01:22<00:53,  3.64it/s] 52%|█████▏    | 215/410 [01:23<00:53,  3.64it/s] 53%|█████▎    | 216/410 [01:23<00:53,  3.64it/s] 53%|█████▎    | 217/410 [01:23<00:53,  3.64it/s] 53%|█████▎    | 218/410 [01:23<00:52,  3.64it/s] 53%|█████▎    | 219/410 [01:24<00:52,  3.64it/s] 54%|█████▎    | 220/410 [01:24<00:52,  3.64it/s] 54%|█████▍    | 221/410 [01:24<00:51,  3.64it/s] 54%|█████▍    | 222/410 [01:25<00:51,  3.64it/s] 54%|█████▍    | 223/410 [01:25<00:51,  3.64it/s] 55%|█████▍    | 224/410 [01:25<00:51,  3.64it/s] 55%|█████▍    | 225/410 [01:25<00:50,  3.64it/s] 55%|█████▌    | 226/410 [01:26<00:50,  3.64it/s] 55%|█████▌    | 227/410 [01:26<00:50,  3.64it/s] 56%|█████▌    | 228/410 [01:26<00:50,  3.64it/s] 56%|█████▌    | 229/410 [01:27<00:49,  3.64it/s] 56%|█████▌    | 230/410 [01:27<00:49,  3.62it/s] 56%|█████▋    | 231/410 [01:27<00:49,  3.63it/s] 57%|█████▋    | 232/410 [01:27<00:49,  3.63it/s] 57%|█████▋    | 233/410 [01:28<00:48,  3.63it/s] 57%|█████▋    | 234/410 [01:28<00:48,  3.64it/s] 57%|█████▋    | 235/410 [01:28<00:48,  3.64it/s] 58%|█████▊    | 236/410 [01:28<00:47,  3.64it/s] 58%|█████▊    | 237/410 [01:29<00:47,  3.64it/s] 58%|█████▊    | 238/410 [01:29<00:47,  3.64it/s] 58%|█████▊    | 239/410 [01:29<00:46,  3.64it/s] 59%|█████▊    | 240/410 [01:30<00:46,  3.64it/s] 59%|█████▉    | 241/410 [01:30<00:46,  3.63it/s] 59%|█████▉    | 242/410 [01:30<00:46,  3.63it/s] 59%|█████▉    | 243/410 [01:30<00:45,  3.63it/s] 60%|█████▉    | 244/410 [01:31<00:45,  3.64it/s] 60%|█████▉    | 245/410 [01:31<00:45,  3.64it/s] 60%|██████    | 246/410 [01:31<00:43,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 01:58:43,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:58:43,003 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 01:58:43,003 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2585, 'eval_samples_per_second': 377.6, 'eval_steps_per_second': 47.2, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.64it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.79it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.19it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.50it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.08it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.73it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.58it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.36it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.35it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.30it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.20it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.19it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.28it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.27it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.26it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.20it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.12it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.10it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.20it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.19it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.13it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.14it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.16it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.09it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.11it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.02it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.89it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.03it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.09it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.03it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.08it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.12it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.14it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.09it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.03it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.00it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.91it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.90it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.99it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.05it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.05it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.14it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.10it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.12it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.03it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.95it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.96it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.00it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.96it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.07it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.01it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.02it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.07it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.98it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.95it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.98it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.97it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.97it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.00it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.06it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.06it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.89it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.97it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.88it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.84it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.97it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.95it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.92it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.00it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.99it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.06it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.97it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.90it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.97it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.98it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.99it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.93it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.92it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.95it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.00it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.04it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.96it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.92it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.99it/s][A                                                 
                                                 [A 60%|██████    | 246/410 [01:40<00:43,  3.77it/s]
100%|██████████| 437/437 [00:09<00:00, 46.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:58:52,309 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 01:58:52,327 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:58:54,782 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:58:54,802 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:58:54,809 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:44<10:33,  3.89s/it] 60%|██████    | 248/410 [01:44<07:34,  2.81s/it] 61%|██████    | 249/410 [01:44<05:29,  2.05s/it] 61%|██████    | 250/410 [01:44<04:02,  1.51s/it] 61%|██████    | 251/410 [01:45<03:01,  1.14s/it] 61%|██████▏   | 252/410 [01:45<02:19,  1.13it/s] 62%|██████▏   | 253/410 [01:45<01:49,  1.43it/s] 62%|██████▏   | 254/410 [01:45<01:29,  1.75it/s] 62%|██████▏   | 255/410 [01:46<01:14,  2.07it/s] 62%|██████▏   | 256/410 [01:46<01:04,  2.38it/s] 63%|██████▎   | 257/410 [01:46<00:57,  2.65it/s] 63%|██████▎   | 258/410 [01:47<00:52,  2.88it/s] 63%|██████▎   | 259/410 [01:47<00:49,  3.07it/s] 63%|██████▎   | 260/410 [01:47<00:46,  3.23it/s] 64%|██████▎   | 261/410 [01:47<00:44,  3.34it/s] 64%|██████▍   | 262/410 [01:48<00:43,  3.42it/s] 64%|██████▍   | 263/410 [01:48<00:42,  3.48it/s] 64%|██████▍   | 264/410 [01:48<00:41,  3.53it/s] 65%|██████▍   | 265/410 [01:48<00:40,  3.56it/s] 65%|██████▍   | 266/410 [01:49<00:40,  3.58it/s] 65%|██████▌   | 267/410 [01:49<00:39,  3.60it/s] 65%|██████▌   | 268/410 [01:49<00:39,  3.60it/s] 66%|██████▌   | 269/410 [01:50<00:39,  3.61it/s] 66%|██████▌   | 270/410 [01:50<00:38,  3.62it/s] 66%|██████▌   | 271/410 [01:50<00:38,  3.62it/s] 66%|██████▋   | 272/410 [01:50<00:38,  3.63it/s] 67%|██████▋   | 273/410 [01:51<00:37,  3.63it/s] 67%|██████▋   | 274/410 [01:51<00:37,  3.63it/s] 67%|██████▋   | 275/410 [01:51<00:37,  3.63it/s] 67%|██████▋   | 276/410 [01:51<00:36,  3.64it/s] 68%|██████▊   | 277/410 [01:52<00:37,  3.55it/s] 68%|██████▊   | 278/410 [01:52<00:37,  3.56it/s] 68%|██████▊   | 279/410 [01:52<00:36,  3.57it/s] 68%|██████▊   | 280/410 [01:53<00:36,  3.59it/s] 69%|██████▊   | 281/410 [01:53<00:35,  3.61it/s] 69%|██████▉   | 282/410 [01:53<00:35,  3.61it/s] 69%|██████▉   | 283/410 [01:53<00:35,  3.62it/s] 69%|██████▉   | 284/410 [01:54<00:34,  3.63it/s] 70%|██████▉   | 285/410 [01:54<00:34,  3.63it/s] 70%|██████▉   | 286/410 [01:54<00:34,  3.63it/s] 70%|███████   | 287/410 [01:55<00:33,  3.63it/s] 70%|███████   | 288/410 [01:55<00:33,  3.63it/s] 70%|███████   | 289/410 [01:55<00:33,  3.63it/s] 71%|███████   | 290/410 [01:55<00:33,  3.62it/s] 71%|███████   | 291/410 [01:56<00:32,  3.63it/s] 71%|███████   | 292/410 [01:56<00:32,  3.63it/s] 71%|███████▏  | 293/410 [01:56<00:32,  3.63it/s] 72%|███████▏  | 294/410 [01:56<00:31,  3.63it/s] 72%|███████▏  | 295/410 [01:57<00:31,  3.63it/s] 72%|███████▏  | 296/410 [01:57<00:31,  3.64it/s] 72%|███████▏  | 297/410 [01:57<00:31,  3.64it/s] 73%|███████▎  | 298/410 [01:58<00:30,  3.64it/s] 73%|███████▎  | 299/410 [01:58<00:30,  3.64it/s] 73%|███████▎  | 300/410 [01:58<00:30,  3.64it/s] 73%|███████▎  | 301/410 [01:58<00:30,  3.62it/s] 74%|███████▎  | 302/410 [01:59<00:29,  3.63it/s] 74%|███████▍  | 303/410 [01:59<00:29,  3.63it/s] 74%|███████▍  | 304/410 [01:59<00:29,  3.63it/s] 74%|███████▍  | 305/410 [01:59<00:28,  3.63it/s] 75%|███████▍  | 306/410 [02:00<00:28,  3.63it/s] 75%|███████▍  | 307/410 [02:00<00:28,  3.63it/s] 75%|███████▌  | 308/410 [02:00<00:28,  3.64it/s] 75%|███████▌  | 309/410 [02:01<00:27,  3.64it/s] 76%|███████▌  | 310/410 [02:01<00:27,  3.64it/s] 76%|███████▌  | 311/410 [02:01<00:27,  3.64it/s] 76%|███████▌  | 312/410 [02:01<00:27,  3.62it/s] 76%|███████▋  | 313/410 [02:02<00:26,  3.62it/s] 77%|███████▋  | 314/410 [02:02<00:26,  3.63it/s] 77%|███████▋  | 315/410 [02:02<00:26,  3.63it/s] 77%|███████▋  | 316/410 [02:03<00:25,  3.63it/s] 77%|███████▋  | 317/410 [02:03<00:25,  3.63it/s] 78%|███████▊  | 318/410 [02:03<00:25,  3.63it/s] 78%|███████▊  | 319/410 [02:03<00:25,  3.63it/s] 78%|███████▊  | 320/410 [02:04<00:24,  3.63it/s] 78%|███████▊  | 321/410 [02:04<00:24,  3.63it/s] 79%|███████▊  | 322/410 [02:04<00:24,  3.63it/s] 79%|███████▉  | 323/410 [02:04<00:24,  3.62it/s] 79%|███████▉  | 324/410 [02:05<00:23,  3.62it/s] 79%|███████▉  | 325/410 [02:05<00:23,  3.63it/s] 80%|███████▉  | 326/410 [02:05<00:23,  3.63it/s] 80%|███████▉  | 327/410 [02:06<00:22,  3.63it/s] 80%|████████  | 328/410 [02:06<00:21,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 01:59:17,638 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:59:17,638 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 01:59:17,638 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2919, 'eval_samples_per_second': 376.24, 'eval_steps_per_second': 47.03, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.52it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.03it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.23it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.31it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.96it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.72it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.39it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.13it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.10it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.03it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.10it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.10it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.10it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.05it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.04it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.99it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.86it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.85it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.86it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.95it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.98it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.99it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.05it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.00it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.95it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.85it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.82it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.64it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.86it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.86it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.90it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.01it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.03it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.97it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.90it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.72it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.79it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.89it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.91it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.96it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.97it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.97it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.98it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.98it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.91it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.73it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.90it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.90it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.93it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.04it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.99it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.93it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.02it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.92it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.76it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.88it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.89it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.89it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.94it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.96it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.92it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.92it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.95it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.80it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.81it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.93it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.79it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.92it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.94it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.93it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.91it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.85it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.84it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.95it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.93it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.88it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.92it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.88it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.93it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.91it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.85it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.89it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.90it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.96it/s][A                                                 
                                                 [A 80%|████████  | 328/410 [02:15<00:21,  3.77it/s]
100%|██████████| 437/437 [00:09<00:00, 46.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:59:26,974 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 01:59:26,990 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:59:30,546 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:59:30,558 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:59:30,566 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:19<05:42,  4.23s/it] 80%|████████  | 330/410 [02:20<04:03,  3.04s/it] 81%|████████  | 331/410 [02:20<02:54,  2.21s/it] 81%|████████  | 332/410 [02:20<02:07,  1.63s/it] 81%|████████  | 333/410 [02:20<01:34,  1.22s/it] 81%|████████▏ | 334/410 [02:21<01:11,  1.06it/s] 82%|████████▏ | 335/410 [02:21<00:55,  1.35it/s] 82%|████████▏ | 336/410 [02:21<00:44,  1.67it/s] 82%|████████▏ | 337/410 [02:21<00:36,  1.99it/s] 82%|████████▏ | 338/410 [02:22<00:31,  2.30it/s] 83%|████████▎ | 339/410 [02:22<00:27,  2.58it/s] 83%|████████▎ | 340/410 [02:22<00:24,  2.83it/s] 83%|████████▎ | 341/410 [02:23<00:22,  3.03it/s] 83%|████████▎ | 342/410 [02:23<00:21,  3.19it/s] 84%|████████▎ | 343/410 [02:23<00:20,  3.31it/s] 84%|████████▍ | 344/410 [02:23<00:19,  3.40it/s] 84%|████████▍ | 345/410 [02:24<00:18,  3.47it/s] 84%|████████▍ | 346/410 [02:24<00:18,  3.52it/s] 85%|████████▍ | 347/410 [02:24<00:17,  3.55it/s] 85%|████████▍ | 348/410 [02:24<00:17,  3.58it/s] 85%|████████▌ | 349/410 [02:25<00:16,  3.60it/s] 85%|████████▌ | 350/410 [02:25<00:16,  3.60it/s] 86%|████████▌ | 351/410 [02:25<00:16,  3.61it/s] 86%|████████▌ | 352/410 [02:26<00:16,  3.62it/s] 86%|████████▌ | 353/410 [02:26<00:15,  3.63it/s] 86%|████████▋ | 354/410 [02:26<00:15,  3.63it/s] 87%|████████▋ | 355/410 [02:26<00:15,  3.63it/s] 87%|████████▋ | 356/410 [02:27<00:14,  3.63it/s] 87%|████████▋ | 357/410 [02:27<00:14,  3.63it/s] 87%|████████▋ | 358/410 [02:27<00:14,  3.64it/s] 88%|████████▊ | 359/410 [02:28<00:14,  3.64it/s] 88%|████████▊ | 360/410 [02:28<00:13,  3.64it/s] 88%|████████▊ | 361/410 [02:28<00:13,  3.63it/s] 88%|████████▊ | 362/410 [02:28<00:13,  3.63it/s] 89%|████████▊ | 363/410 [02:29<00:12,  3.63it/s] 89%|████████▉ | 364/410 [02:29<00:12,  3.64it/s] 89%|████████▉ | 365/410 [02:29<00:12,  3.64it/s] 89%|████████▉ | 366/410 [02:29<00:12,  3.64it/s] 90%|████████▉ | 367/410 [02:30<00:11,  3.64it/s] 90%|████████▉ | 368/410 [02:30<00:11,  3.64it/s] 90%|█████████ | 369/410 [02:30<00:11,  3.64it/s] 90%|█████████ | 370/410 [02:31<00:10,  3.64it/s] 90%|█████████ | 371/410 [02:31<00:10,  3.64it/s] 91%|█████████ | 372/410 [02:31<00:10,  3.62it/s] 91%|█████████ | 373/410 [02:31<00:10,  3.63it/s] 91%|█████████ | 374/410 [02:32<00:09,  3.63it/s] 91%|█████████▏| 375/410 [02:32<00:09,  3.63it/s] 92%|█████████▏| 376/410 [02:32<00:09,  3.63it/s] 92%|█████████▏| 377/410 [02:32<00:09,  3.63it/s] 92%|█████████▏| 378/410 [02:33<00:08,  3.64it/s] 92%|█████████▏| 379/410 [02:33<00:08,  3.64it/s] 93%|█████████▎| 380/410 [02:33<00:08,  3.64it/s] 93%|█████████▎| 381/410 [02:34<00:07,  3.63it/s] 93%|█████████▎| 382/410 [02:34<00:07,  3.64it/s] 93%|█████████▎| 383/410 [02:34<00:07,  3.62it/s] 94%|█████████▎| 384/410 [02:34<00:07,  3.63it/s] 94%|█████████▍| 385/410 [02:35<00:06,  3.63it/s] 94%|█████████▍| 386/410 [02:35<00:06,  3.63it/s] 94%|█████████▍| 387/410 [02:35<00:06,  3.63it/s] 95%|█████████▍| 388/410 [02:36<00:06,  3.63it/s] 95%|█████████▍| 389/410 [02:36<00:05,  3.63it/s] 95%|█████████▌| 390/410 [02:36<00:05,  3.64it/s] 95%|█████████▌| 391/410 [02:36<00:05,  3.64it/s] 96%|█████████▌| 392/410 [02:37<00:04,  3.64it/s] 96%|█████████▌| 393/410 [02:37<00:04,  3.64it/s] 96%|█████████▌| 394/410 [02:37<00:04,  3.61it/s] 96%|█████████▋| 395/410 [02:37<00:04,  3.61it/s] 97%|█████████▋| 396/410 [02:38<00:03,  3.62it/s] 97%|█████████▋| 397/410 [02:38<00:03,  3.63it/s] 97%|█████████▋| 398/410 [02:38<00:03,  3.63it/s] 97%|█████████▋| 399/410 [02:39<00:03,  3.63it/s] 98%|█████████▊| 400/410 [02:39<00:02,  3.63it/s] 98%|█████████▊| 401/410 [02:39<00:02,  3.64it/s] 98%|█████████▊| 402/410 [02:39<00:02,  3.64it/s] 98%|█████████▊| 403/410 [02:40<00:01,  3.63it/s] 99%|█████████▊| 404/410 [02:40<00:01,  3.64it/s] 99%|█████████▉| 405/410 [02:40<00:01,  3.62it/s] 99%|█████████▉| 406/410 [02:40<00:01,  3.63it/s] 99%|█████████▉| 407/410 [02:41<00:00,  3.63it/s]100%|█████████▉| 408/410 [02:41<00:00,  3.63it/s]100%|█████████▉| 409/410 [02:41<00:00,  3.63it/s]100%|██████████| 410/410 [02:42<00:00,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 01:59:53,374 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:59:53,374 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 01:59:53,374 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.3148, 'eval_samples_per_second': 375.318, 'eval_steps_per_second': 46.915, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.85it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.08it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.16it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.48it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.03it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.80it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.50it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.23it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.12it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.25it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.22it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.21it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.15it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.12it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.14it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.12it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.97it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.94it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.00it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.12it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.12it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.08it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.09it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.05it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.92it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.85it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.86it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.85it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.00it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.89it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.90it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.05it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.95it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.90it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.93it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.82it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.86it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.86it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.89it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.92it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.07it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.05it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.92it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.93it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.88it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.85it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.98it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.96it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.89it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.96it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.05it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.92it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.00it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.86it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.77it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.93it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.99it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.96it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.88it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.00it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.93it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.94it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.81it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.91it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.88it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.96it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.96it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.92it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.96it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.92it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.94it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.86it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.89it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.92it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.90it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.93it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.96it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.89it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.92it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.90it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.86it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.85it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.89it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.94it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.92it/s][A                                                 
                                                 [A100%|██████████| 410/410 [02:51<00:00,  3.77it/s]
100%|██████████| 437/437 [00:09<00:00, 46.92it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:00:02,691 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 02:00:02,710 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:00:05,808 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:00:05,823 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:00:05,831 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:00:06,118 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:00:06,118 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82 (score: 1.1523417234420776).
                                                 100%|██████████| 410/410 [02:56<00:00,  3.77it/s]100%|██████████| 410/410 [02:56<00:00,  2.32it/s]
[INFO|trainer.py:1894] 2023-08-28 02:00:07,898 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 02:00:07,916 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:00:10,247 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:00:10,263 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:00:10,277 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:00:10,451 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:10,452 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:10,452 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:10,452 >>   train_runtime            = 0:02:56.55
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:10,452 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:10,452 >>   train_samples_per_second =    148.395
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:10,452 >>   train_steps_per_second   =      2.322
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.3064, 'eval_samples_per_second': 375.657, 'eval_steps_per_second': 46.957, 'epoch': 5.0}
{'train_runtime': 176.5564, 'train_samples_per_second': 148.395, 'train_steps_per_second': 2.322, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:00:10 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:00:10,479 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:00:10,479 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 02:00:10,479 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.12it/s]  3%|▎         | 12/437 [00:00<00:08, 51.30it/s]  4%|▍         | 18/437 [00:00<00:08, 49.53it/s]  5%|▌         | 23/437 [00:00<00:08, 48.77it/s]  6%|▋         | 28/437 [00:00<00:08, 48.21it/s]  8%|▊         | 33/437 [00:00<00:08, 47.98it/s]  9%|▊         | 38/437 [00:00<00:08, 47.89it/s] 10%|▉         | 43/437 [00:00<00:08, 47.65it/s] 11%|█         | 48/437 [00:00<00:08, 47.47it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.34it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.28it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.32it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.33it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.35it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.32it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.37it/s] 20%|██        | 88/437 [00:01<00:07, 47.30it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.14it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.10it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.83it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.21it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.32it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.34it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.30it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.29it/s] 30%|███       | 133/437 [00:02<00:06, 47.25it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.18it/s] 33%|███▎      | 143/437 [00:03<00:06, 47.15it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.16it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.14it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.10it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.28it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.28it/s] 41%|████      | 178/437 [00:03<00:05, 47.33it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.29it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.09it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.16it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.24it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.19it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.16it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.24it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.27it/s] 51%|█████     | 223/437 [00:04<00:04, 47.21it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.21it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.17it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.04it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.15it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.11it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.18it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.06it/s] 60%|██████    | 263/437 [00:05<00:03, 47.24it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.25it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.17it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.19it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.12it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.10it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.10it/s] 68%|██████▊   | 298/437 [00:06<00:02, 47.20it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.08it/s] 70%|███████   | 308/437 [00:06<00:02, 47.10it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.23it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.10it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.09it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.17it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.16it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.13it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.16it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.10it/s] 81%|████████  | 353/437 [00:07<00:01, 47.15it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.09it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.17it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.09it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.04it/s] 86%|████████▋ | 378/437 [00:07<00:01, 47.15it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.11it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.08it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.18it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.20it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.21it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.18it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.10it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.07it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.09it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.16it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.10it/s]100%|██████████| 437/437 [00:09<00:00, 47.25it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:00:19,745 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   eval_loss               =     1.1523
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   eval_runtime            = 0:00:09.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   eval_samples_per_second =    377.287
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   eval_steps_per_second   =     47.161
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:00:19,746 >>   perplexity              =     3.1656
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:25,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:25,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:25,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:25,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:25,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:00:25,730 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:00:25,731 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:00:26,295 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:00:27,334 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:00:27,334 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:30,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:30,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:30,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:30,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:30,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:00:30,835 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:00:30,836 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:00:31,421 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:00:31,587 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:00:31,587 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.69it/s]Extractor Predicting: 23it [00:14,  1.73it/s]Extractor Predicting: 24it [00:14,  1.74it/s]Extractor Predicting: 25it [00:15,  1.73it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:20,  1.63it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.69it/s]Extractor Predicting: 40it [00:24,  1.71it/s]Extractor Predicting: 41it [00:24,  1.67it/s]Extractor Predicting: 42it [00:25,  1.71it/s]Extractor Predicting: 43it [00:25,  1.72it/s]Extractor Predicting: 44it [00:26,  1.72it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.72it/s]Extractor Predicting: 48it [00:28,  1.74it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:30,  1.72it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:31,  1.76it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:32,  1.72it/s]Extractor Predicting: 55it [00:32,  1.74it/s]Extractor Predicting: 56it [00:33,  1.71it/s]Extractor Predicting: 57it [00:34,  1.67it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:35,  1.66it/s]Extractor Predicting: 61it [00:36,  1.66it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:38,  1.65it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:39,  1.62it/s]Extractor Predicting: 67it [00:40,  1.60it/s]Extractor Predicting: 68it [00:40,  1.62it/s]Extractor Predicting: 69it [00:41,  1.63it/s]Extractor Predicting: 70it [00:42,  1.68it/s]Extractor Predicting: 71it [00:42,  1.67it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:43,  1.68it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:45,  1.61it/s]Extractor Predicting: 77it [00:46,  1.65it/s]Extractor Predicting: 78it [00:47,  1.50it/s]Extractor Predicting: 79it [00:47,  1.51it/s]Extractor Predicting: 80it [00:48,  1.54it/s]Extractor Predicting: 81it [00:49,  1.57it/s]Extractor Predicting: 82it [00:49,  1.60it/s]Extractor Predicting: 83it [00:50,  1.64it/s]Extractor Predicting: 84it [00:50,  1.57it/s]Extractor Predicting: 85it [00:51,  1.58it/s]Extractor Predicting: 86it [00:52,  1.61it/s]Extractor Predicting: 87it [00:52,  1.59it/s]Extractor Predicting: 88it [00:53,  1.62it/s]Extractor Predicting: 89it [00:53,  1.63it/s]Extractor Predicting: 90it [00:54,  1.62it/s]Extractor Predicting: 91it [00:55,  1.67it/s]Extractor Predicting: 92it [00:55,  1.68it/s]Extractor Predicting: 93it [00:56,  1.70it/s]Extractor Predicting: 94it [00:56,  1.67it/s]Extractor Predicting: 95it [00:57,  1.67it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:58,  1.63it/s]Extractor Predicting: 98it [00:59,  1.62it/s]Extractor Predicting: 99it [00:59,  1.64it/s]Extractor Predicting: 100it [01:00,  1.62it/s]Extractor Predicting: 101it [01:01,  1.62it/s]Extractor Predicting: 102it [01:01,  1.58it/s]Extractor Predicting: 103it [01:02,  1.60it/s]Extractor Predicting: 104it [01:03,  1.61it/s]Extractor Predicting: 105it [01:03,  1.62it/s]Extractor Predicting: 106it [01:04,  1.66it/s]Extractor Predicting: 107it [01:04,  1.64it/s]Extractor Predicting: 108it [01:05,  1.65it/s]Extractor Predicting: 109it [01:06,  1.65it/s]Extractor Predicting: 110it [01:06,  1.64it/s]Extractor Predicting: 111it [01:07,  1.64it/s]Extractor Predicting: 112it [01:07,  1.66it/s]Extractor Predicting: 113it [01:08,  1.66it/s]Extractor Predicting: 114it [01:09,  1.65it/s]Extractor Predicting: 115it [01:09,  1.65it/s]Extractor Predicting: 116it [01:10,  1.71it/s]Extractor Predicting: 117it [01:10,  1.68it/s]Extractor Predicting: 118it [01:11,  1.66it/s]Extractor Predicting: 119it [01:12,  1.61it/s]Extractor Predicting: 120it [01:12,  1.64it/s]Extractor Predicting: 121it [01:13,  1.65it/s]Extractor Predicting: 122it [01:13,  1.63it/s]Extractor Predicting: 123it [01:14,  1.63it/s]Extractor Predicting: 124it [01:15,  1.60it/s]Extractor Predicting: 125it [01:15,  1.57it/s]Extractor Predicting: 126it [01:16,  1.61it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:17,  1.64it/s]Extractor Predicting: 129it [01:18,  1.65it/s]Extractor Predicting: 130it [01:18,  1.69it/s]Extractor Predicting: 131it [01:19,  1.70it/s]Extractor Predicting: 132it [01:20,  1.69it/s]Extractor Predicting: 133it [01:20,  1.68it/s]Extractor Predicting: 134it [01:21,  1.69it/s]Extractor Predicting: 135it [01:21,  1.69it/s]Extractor Predicting: 136it [01:22,  1.66it/s]Extractor Predicting: 137it [01:23,  1.65it/s]Extractor Predicting: 138it [01:23,  1.64it/s]Extractor Predicting: 139it [01:24,  1.65it/s]Extractor Predicting: 140it [01:24,  1.63it/s]Extractor Predicting: 141it [01:25,  1.62it/s]Extractor Predicting: 142it [01:26,  1.61it/s]Extractor Predicting: 143it [01:26,  1.61it/s]Extractor Predicting: 144it [01:27,  1.80it/s]Extractor Predicting: 144it [01:27,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:06,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:06,238 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:06,238 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:06,238 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:06,238 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:02:06,850 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:02:06,851 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:02:07,448 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:02:08,464 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:02:08,468 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:11,309 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:11,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:11,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:11,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:02:11,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:02:11,957 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:02:11,962 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:02:12,550 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:02:12,710 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:02:12,711 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.80it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.83it/s]Extractor Predicting: 6it [00:03,  1.84it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.84it/s]Extractor Predicting: 9it [00:04,  1.87it/s]Extractor Predicting: 10it [00:05,  1.89it/s]Extractor Predicting: 11it [00:05,  1.94it/s]Extractor Predicting: 12it [00:06,  1.78it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:07,  1.85it/s]Extractor Predicting: 15it [00:08,  1.87it/s]Extractor Predicting: 16it [00:08,  1.84it/s]Extractor Predicting: 17it [00:09,  1.85it/s]Extractor Predicting: 18it [00:09,  1.89it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.88it/s]Extractor Predicting: 21it [00:11,  1.92it/s]Extractor Predicting: 22it [00:11,  1.95it/s]Extractor Predicting: 23it [00:12,  1.90it/s]Extractor Predicting: 24it [00:12,  1.87it/s]Extractor Predicting: 25it [00:13,  1.83it/s]Extractor Predicting: 26it [00:13,  1.87it/s]Extractor Predicting: 27it [00:14,  1.88it/s]Extractor Predicting: 28it [00:15,  1.79it/s]Extractor Predicting: 29it [00:15,  1.76it/s]Extractor Predicting: 30it [00:16,  1.68it/s]Extractor Predicting: 31it [00:17,  1.64it/s]Extractor Predicting: 32it [00:17,  1.59it/s]Extractor Predicting: 33it [00:18,  1.58it/s]Extractor Predicting: 34it [00:18,  1.56it/s]Extractor Predicting: 35it [00:19,  1.60it/s]Extractor Predicting: 36it [00:20,  1.60it/s]Extractor Predicting: 37it [00:20,  1.60it/s]Extractor Predicting: 38it [00:21,  1.53it/s]Extractor Predicting: 39it [00:22,  1.55it/s]Extractor Predicting: 40it [00:22,  1.53it/s]Extractor Predicting: 41it [00:23,  1.51it/s]Extractor Predicting: 42it [00:24,  1.53it/s]Extractor Predicting: 43it [00:24,  1.54it/s]Extractor Predicting: 44it [00:25,  1.58it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:26,  1.60it/s]Extractor Predicting: 47it [00:27,  1.57it/s]Extractor Predicting: 48it [00:27,  1.55it/s]Extractor Predicting: 49it [00:28,  1.55it/s]Extractor Predicting: 50it [00:29,  1.55it/s]Extractor Predicting: 51it [00:29,  1.54it/s]Extractor Predicting: 52it [00:30,  1.56it/s]Extractor Predicting: 53it [00:31,  1.56it/s]Extractor Predicting: 54it [00:31,  1.57it/s]Extractor Predicting: 55it [00:32,  1.56it/s]Extractor Predicting: 56it [00:33,  1.53it/s]Extractor Predicting: 57it [00:33,  1.56it/s]Extractor Predicting: 58it [00:34,  1.55it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:35,  1.61it/s]Extractor Predicting: 61it [00:36,  1.57it/s]Extractor Predicting: 62it [00:36,  1.57it/s]Extractor Predicting: 63it [00:37,  1.60it/s]Extractor Predicting: 64it [00:38,  1.62it/s]Extractor Predicting: 65it [00:38,  1.63it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:40,  1.69it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:41,  1.70it/s]Extractor Predicting: 71it [00:42,  1.75it/s]Extractor Predicting: 72it [00:42,  1.71it/s]Extractor Predicting: 73it [00:43,  1.67it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:44,  1.63it/s]Extractor Predicting: 76it [00:45,  1.61it/s]Extractor Predicting: 77it [00:45,  1.62it/s]Extractor Predicting: 78it [00:46,  1.62it/s]Extractor Predicting: 79it [00:47,  1.62it/s]Extractor Predicting: 80it [00:47,  1.61it/s]Extractor Predicting: 81it [00:48,  1.60it/s]Extractor Predicting: 82it [00:49,  1.63it/s]Extractor Predicting: 83it [00:49,  1.62it/s]Extractor Predicting: 84it [00:50,  1.60it/s]Extractor Predicting: 85it [00:50,  1.63it/s]Extractor Predicting: 86it [00:51,  1.65it/s]Extractor Predicting: 87it [00:52,  1.70it/s]Extractor Predicting: 88it [00:52,  1.74it/s]Extractor Predicting: 89it [00:53,  1.76it/s]Extractor Predicting: 90it [00:53,  1.76it/s]Extractor Predicting: 91it [00:54,  1.82it/s]Extractor Predicting: 92it [00:54,  1.79it/s]Extractor Predicting: 93it [00:55,  1.83it/s]Extractor Predicting: 94it [00:55,  1.80it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:56,  1.79it/s]Extractor Predicting: 97it [00:57,  1.66it/s]Extractor Predicting: 98it [00:58,  1.72it/s]Extractor Predicting: 99it [00:58,  1.75it/s]Extractor Predicting: 100it [00:59,  1.83it/s]Extractor Predicting: 101it [00:59,  1.83it/s]Extractor Predicting: 102it [01:00,  1.79it/s]Extractor Predicting: 103it [01:00,  1.77it/s]Extractor Predicting: 104it [01:01,  1.79it/s]Extractor Predicting: 105it [01:02,  1.82it/s]Extractor Predicting: 106it [01:02,  1.82it/s]Extractor Predicting: 107it [01:03,  1.90it/s]Extractor Predicting: 108it [01:03,  1.84it/s]Extractor Predicting: 109it [01:04,  1.85it/s]Extractor Predicting: 110it [01:04,  1.85it/s]Extractor Predicting: 111it [01:05,  1.85it/s]Extractor Predicting: 112it [01:05,  1.87it/s]Extractor Predicting: 113it [01:06,  1.86it/s]Extractor Predicting: 114it [01:06,  1.83it/s]Extractor Predicting: 115it [01:07,  1.80it/s]Extractor Predicting: 116it [01:08,  1.76it/s]Extractor Predicting: 117it [01:08,  1.72it/s]Extractor Predicting: 118it [01:09,  1.73it/s]Extractor Predicting: 119it [01:09,  1.72it/s]Extractor Predicting: 120it [01:10,  1.67it/s]Extractor Predicting: 121it [01:11,  1.70it/s]Extractor Predicting: 122it [01:11,  1.70it/s]Extractor Predicting: 123it [01:12,  1.67it/s]Extractor Predicting: 124it [01:12,  1.72it/s]Extractor Predicting: 125it [01:13,  1.68it/s]Extractor Predicting: 126it [01:13,  1.70it/s]Extractor Predicting: 127it [01:14,  1.68it/s]Extractor Predicting: 128it [01:15,  1.66it/s]Extractor Predicting: 129it [01:15,  1.67it/s]Extractor Predicting: 130it [01:16,  1.65it/s]Extractor Predicting: 131it [01:17,  1.63it/s]Extractor Predicting: 132it [01:17,  1.64it/s]Extractor Predicting: 133it [01:18,  1.66it/s]Extractor Predicting: 134it [01:18,  1.66it/s]Extractor Predicting: 135it [01:19,  1.65it/s]Extractor Predicting: 136it [01:20,  1.61it/s]Extractor Predicting: 137it [01:20,  1.65it/s]Extractor Predicting: 138it [01:21,  1.64it/s]Extractor Predicting: 139it [01:21,  1.65it/s]Extractor Predicting: 140it [01:22,  1.63it/s]Extractor Predicting: 141it [01:23,  1.65it/s]Extractor Predicting: 142it [01:23,  1.64it/s]Extractor Predicting: 143it [01:23,  2.08it/s]Extractor Predicting: 143it [01:23,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:43,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:43,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:43,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:43,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:43,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:03:44,149 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:03:44,150 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:03:44,751 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:03:45,751 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:03:45,755 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:48,620 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:48,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:48,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:48,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:03:48,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:03:49,243 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:03:49,248 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:03:49,833 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:03:49,980 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:03:49,980 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 2it [00:01,  1.37it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:03:52,111 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:03:52,112 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:03:52,115 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:03:52,116 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:03:52,118 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:03:55,283 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 02:03:55,284 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 02:03:55,300 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:03:55,300 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:03:55,305 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:03:55,314 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:03:55,314 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:03:55,314 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:03:55,314 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:03:55,314 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:03:55,314 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 02:03:55,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:03:56,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:03:57,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:03:58,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:03:58,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:03:59,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:00,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:01,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:01,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:02,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:03,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:04,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:05,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:06,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:07,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:07,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:08,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:09,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:10,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:11,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:11,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:12,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:13,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:14,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:54, 19.41s/it][WARNING|generation_utils.py:914] 2023-08-28 02:04:14,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:15,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:16,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:17,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:18,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:19,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:19,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:20,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:21,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:22,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:22,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:23,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:24,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:25,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:26,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:27,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:28,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:29,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:30,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:31,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:32,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:33,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:34,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:34,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:40<02:40, 20.11s/it][WARNING|generation_utils.py:914] 2023-08-28 02:04:35,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:36,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:37,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:38,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:38,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:39,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:40,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:41,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:42,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:42,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:43,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:44,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:45,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:46,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:46,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:47,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:48,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:49,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:50,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:51,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:51,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:52,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:57<02:13, 19.05s/it][WARNING|generation_utils.py:914] 2023-08-28 02:04:53,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:54,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:55,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:56,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:57,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:57,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:58,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:59,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:04:59,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:00,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:01,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:02,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:02,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:03,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:04,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:05,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:05,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:06,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:07,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:08,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:09,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:10,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:11,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:12,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:12,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:13,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:14,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:14,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:15,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:16,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:17,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:17,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:18,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:19,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:20,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:20,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:21,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:22,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:22,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:23,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:23,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:24,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:25,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:30<02:27, 24.50s/it][WARNING|generation_utils.py:914] 2023-08-28 02:05:26,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:27,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:28,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:28,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:29,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:30,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:31,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:31,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:32,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:33,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:34,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:35,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:35,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:36,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:37,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:38,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:39,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:39,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:40,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:41,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:42,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:43,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:43,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:44,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:45,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:50<01:54, 22.83s/it][WARNING|generation_utils.py:914] 2023-08-28 02:05:46,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:47,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:47,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:48,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:49,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:50,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:50,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:51,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:52,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:53,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:54,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:55,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:55,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:56,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:57,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:58,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:59,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:05:59,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:00,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:01,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:02,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:03,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:03,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:04,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:09<01:26, 21.68s/it][WARNING|generation_utils.py:914] 2023-08-28 02:06:05,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:06,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:07,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:08,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:08,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:09,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:10,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:11,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:12,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:13,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:14,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:14,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:15,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:16,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:17,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:18,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:19,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:20,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:21,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:21,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:22,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:23,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:24,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:25,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:25,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:26,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:32<01:05, 21.89s/it][WARNING|generation_utils.py:914] 2023-08-28 02:06:27,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:28,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:29,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:30,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:30,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:31,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:32,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:33,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:33,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:34,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:35,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:36,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:36,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:37,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:38,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:39,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:39,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:40,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:41,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:41,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:42,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:43,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:44,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:49<00:40, 20.34s/it][WARNING|generation_utils.py:914] 2023-08-28 02:06:44,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:45,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:46,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:47,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:47,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:48,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:49,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:50,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:51,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:52,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:52,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:53,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:54,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:55,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:55,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:56,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:57,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:58,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:59,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:06:59,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:00,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:01,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:02,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:02,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:03,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:08<00:20, 20.12s/it][WARNING|generation_utils.py:914] 2023-08-28 02:07:04,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:05,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:06,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:06,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:07,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:08,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:09,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:09,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:10,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:11,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:12,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:12,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:13,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:14,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:15,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:16,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:17,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:17,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:18,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:19,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:20,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:21,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:21,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:22,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:23,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:07:24,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:29<00:00, 20.29s/it]Generating: 100%|██████████| 10/10 [03:29<00:00, 20.96s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:31,134 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:31,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:31,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:31,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:31,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:07:31,774 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:07:31,775 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:07:32,329 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:07:33,386 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:07:33,386 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:36,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:36,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:36,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:36,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:07:36,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:07:36,937 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:07:36,939 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:07:37,495 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:07:37,648 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:07:37,649 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
{'target': 600, 'success': 9, 'raw': 32}
{'target': 600, 'success': 26, 'raw': 64}
{'target': 600, 'success': 37, 'raw': 96}
{'target': 600, 'success': 54, 'raw': 128}
{'target': 600, 'success': 67, 'raw': 160}
{'target': 600, 'success': 82, 'raw': 192}
{'target': 600, 'success': 97, 'raw': 224}
{'target': 600, 'success': 111, 'raw': 256}
{'target': 600, 'success': 125, 'raw': 288}
{'target': 600, 'success': 137, 'raw': 320}
{'target': 600, 'success': 152, 'raw': 352}
{'target': 600, 'success': 166, 'raw': 384}
{'target': 600, 'success': 180, 'raw': 416}
{'target': 600, 'success': 196, 'raw': 448}
{'target': 600, 'success': 214, 'raw': 480}
{'target': 600, 'success': 229, 'raw': 512}
{'target': 600, 'success': 246, 'raw': 544}
{'target': 600, 'success': 260, 'raw': 576}
{'target': 600, 'success': 273, 'raw': 608}
{'target': 600, 'success': 285, 'raw': 640}
{'target': 600, 'success': 301, 'raw': 672}
{'target': 600, 'success': 315, 'raw': 704}
{'target': 600, 'success': 327, 'raw': 736}
{'target': 600, 'success': 334, 'raw': 768}
{'target': 600, 'success': 355, 'raw': 800}
{'target': 600, 'success': 369, 'raw': 832}
{'target': 600, 'success': 386, 'raw': 864}
{'target': 600, 'success': 399, 'raw': 896}
{'target': 600, 'success': 412, 'raw': 928}
{'target': 600, 'success': 425, 'raw': 960}
{'target': 600, 'success': 433, 'raw': 992}
{'target': 600, 'success': 443, 'raw': 1024}
{'target': 600, 'success': 460, 'raw': 1056}
{'target': 600, 'success': 475, 'raw': 1088}
{'target': 600, 'success': 485, 'raw': 1120}
{'target': 600, 'success': 497, 'raw': 1152}
{'target': 600, 'success': 511, 'raw': 1184}
{'target': 600, 'success': 525, 'raw': 1216}
{'target': 600, 'success': 542, 'raw': 1248}
{'target': 600, 'success': 556, 'raw': 1280}
{'target': 600, 'success': 573, 'raw': 1312}
{'target': 600, 'success': 586, 'raw': 1344}
{'target': 600, 'success': 596, 'raw': 1376}
{'target': 600, 'success': 605, 'raw': 1408}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4296875, 'errors': {'', '(\'Love Is One of the Most Important Facts You Can Learn About Yourself\', \'original language of film or TV show\', \'\', \'On 3 November 2001 , he made the film " Love Is One of the Most Important Facts You Can Learn About Yourself " , about an alcoholic father .\')', '(\'The Hound of the Baskervilles\', \'original language of film or TV show\', \'\', \'His best work came in the 1983 horror movie " The Hound of the Baskervilles " directed by Gene Luen Yang .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 496, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8033854166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 254, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 450, 'raw': 608}
{'target': 600, 'success': 469, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.7271634615384616, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 526, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7525, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 428, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 595, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 11540
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11640, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.39it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.34it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:06,  1.49it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:08,  1.48it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.40it/s]Extractor Estimating: 15it [00:10,  1.40it/s]Extractor Estimating: 16it [00:11,  1.44it/s]Extractor Estimating: 17it [00:11,  1.43it/s]Extractor Estimating: 18it [00:12,  1.40it/s]Extractor Estimating: 19it [00:13,  1.44it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:15,  1.52it/s]Extractor Estimating: 23it [00:15,  1.50it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:17,  1.53it/s]Extractor Estimating: 26it [00:17,  1.50it/s]Extractor Estimating: 27it [00:18,  1.44it/s]Extractor Estimating: 28it [00:19,  1.45it/s]Extractor Estimating: 29it [00:19,  1.46it/s]Extractor Estimating: 30it [00:20,  1.48it/s]Extractor Estimating: 31it [00:21,  1.48it/s]Extractor Estimating: 32it [00:21,  1.53it/s]Extractor Estimating: 33it [00:22,  1.46it/s]Extractor Estimating: 34it [00:23,  1.48it/s]Extractor Estimating: 35it [00:23,  1.49it/s]Extractor Estimating: 36it [00:24,  1.47it/s]Extractor Estimating: 37it [00:25,  1.50it/s]Extractor Estimating: 38it [00:25,  1.44it/s]Extractor Estimating: 39it [00:26,  1.40it/s]Extractor Estimating: 40it [00:27,  1.41it/s]Extractor Estimating: 41it [00:28,  1.41it/s]Extractor Estimating: 42it [00:28,  1.34it/s]Extractor Estimating: 43it [00:29,  1.30it/s]Extractor Estimating: 44it [00:30,  1.32it/s]Extractor Estimating: 45it [00:31,  1.39it/s]Extractor Estimating: 46it [00:31,  1.38it/s]Extractor Estimating: 47it [00:32,  1.42it/s]Extractor Estimating: 48it [00:33,  1.38it/s]Extractor Estimating: 49it [00:33,  1.45it/s]Extractor Estimating: 50it [00:34,  1.45it/s]Extractor Estimating: 51it [00:35,  1.46it/s]Extractor Estimating: 52it [00:35,  1.49it/s]Extractor Estimating: 53it [00:36,  1.50it/s]Extractor Estimating: 54it [00:37,  1.52it/s]Extractor Estimating: 55it [00:37,  1.50it/s]Extractor Estimating: 56it [00:38,  1.53it/s]Extractor Estimating: 57it [00:39,  1.51it/s]Extractor Estimating: 58it [00:39,  1.50it/s]Extractor Estimating: 59it [00:40,  1.52it/s]Extractor Estimating: 60it [00:41,  1.52it/s]Extractor Estimating: 61it [00:41,  1.54it/s]Extractor Estimating: 62it [00:42,  1.52it/s]Extractor Estimating: 63it [00:43,  1.54it/s]Extractor Estimating: 64it [00:43,  1.53it/s]Extractor Estimating: 65it [00:44,  1.50it/s]Extractor Estimating: 66it [00:45,  1.53it/s]Extractor Estimating: 67it [00:45,  1.50it/s]Extractor Estimating: 68it [00:46,  1.45it/s]Extractor Estimating: 69it [00:47,  1.51it/s]Extractor Estimating: 70it [00:47,  1.54it/s]Extractor Estimating: 71it [00:48,  1.54it/s]Extractor Estimating: 72it [00:49,  1.52it/s]Extractor Estimating: 73it [00:49,  1.53it/s]Extractor Estimating: 74it [00:50,  1.49it/s]Extractor Estimating: 75it [00:51,  1.52it/s]Extractor Estimating: 76it [00:51,  1.51it/s]Extractor Estimating: 77it [00:52,  1.52it/s]Extractor Estimating: 78it [00:52,  1.57it/s]Extractor Estimating: 79it [00:53,  1.52it/s]Extractor Estimating: 80it [00:54,  1.53it/s]Extractor Estimating: 81it [00:55,  1.50it/s]Extractor Estimating: 82it [00:55,  1.52it/s]Extractor Estimating: 83it [00:56,  1.56it/s]Extractor Estimating: 84it [00:56,  1.59it/s]Extractor Estimating: 85it [00:57,  1.61it/s]Extractor Estimating: 86it [00:58,  1.50it/s]Extractor Estimating: 87it [00:59,  1.43it/s]Extractor Estimating: 88it [00:59,  1.40it/s]Extractor Estimating: 89it [01:00,  1.32it/s]Extractor Estimating: 90it [01:01,  1.40it/s]Extractor Estimating: 91it [01:01,  1.41it/s]Extractor Estimating: 92it [01:02,  1.49it/s]Extractor Estimating: 93it [01:03,  1.52it/s]Extractor Estimating: 94it [01:03,  1.54it/s]Extractor Estimating: 95it [01:04,  1.57it/s]Extractor Estimating: 96it [01:05,  1.57it/s]Extractor Estimating: 97it [01:05,  1.58it/s]Extractor Estimating: 98it [01:06,  1.59it/s]Extractor Estimating: 99it [01:06,  1.57it/s]Extractor Estimating: 100it [01:07,  1.56it/s]Extractor Estimating: 101it [01:08,  1.53it/s]Extractor Estimating: 102it [01:08,  1.51it/s]Extractor Estimating: 103it [01:09,  1.53it/s]Extractor Estimating: 104it [01:10,  1.49it/s]Extractor Estimating: 105it [01:10,  1.51it/s]Extractor Estimating: 106it [01:11,  1.37it/s]Extractor Estimating: 107it [01:12,  1.44it/s]Extractor Estimating: 108it [01:13,  1.46it/s]Extractor Estimating: 109it [01:13,  1.43it/s]Extractor Estimating: 110it [01:14,  1.45it/s]Extractor Estimating: 111it [01:15,  1.46it/s]Extractor Estimating: 112it [01:15,  1.45it/s]Extractor Estimating: 113it [01:16,  1.47it/s]Extractor Estimating: 114it [01:17,  1.52it/s]Extractor Estimating: 115it [01:17,  1.49it/s]Extractor Estimating: 116it [01:18,  1.51it/s]Extractor Estimating: 117it [01:19,  1.54it/s]Extractor Estimating: 118it [01:19,  1.52it/s]Extractor Estimating: 119it [01:20,  1.53it/s]Extractor Estimating: 120it [01:21,  1.46it/s]Extractor Estimating: 121it [01:21,  1.47it/s]Extractor Estimating: 122it [01:22,  1.51it/s]Extractor Estimating: 123it [01:23,  1.47it/s]Extractor Estimating: 124it [01:23,  1.48it/s]Extractor Estimating: 125it [01:24,  1.47it/s]Extractor Estimating: 126it [01:25,  1.40it/s]Extractor Estimating: 127it [01:26,  1.42it/s]Extractor Estimating: 128it [01:26,  1.44it/s]Extractor Estimating: 129it [01:27,  1.44it/s]Extractor Estimating: 130it [01:27,  1.50it/s]Extractor Estimating: 131it [01:28,  1.51it/s]Extractor Estimating: 132it [01:29,  1.53it/s]Extractor Estimating: 133it [01:29,  1.50it/s]Extractor Estimating: 134it [01:30,  1.51it/s]Extractor Estimating: 135it [01:31,  1.53it/s]Extractor Estimating: 136it [01:31,  1.57it/s]Extractor Estimating: 137it [01:32,  1.54it/s]Extractor Estimating: 138it [01:33,  1.58it/s]Extractor Estimating: 139it [01:33,  1.58it/s]Extractor Estimating: 140it [01:34,  1.56it/s]Extractor Estimating: 141it [01:35,  1.52it/s]Extractor Estimating: 142it [01:35,  1.49it/s]Extractor Estimating: 143it [01:36,  1.52it/s]Extractor Estimating: 144it [01:37,  1.58it/s]Extractor Estimating: 145it [01:37,  1.56it/s]Extractor Estimating: 146it [01:38,  1.56it/s]Extractor Estimating: 147it [01:38,  1.55it/s]Extractor Estimating: 148it [01:39,  1.57it/s]Extractor Estimating: 149it [01:40,  1.57it/s]Extractor Estimating: 150it [01:40,  1.57it/s]Extractor Estimating: 151it [01:41,  1.54it/s]Extractor Estimating: 152it [01:42,  1.56it/s]Extractor Estimating: 153it [01:42,  1.54it/s]Extractor Estimating: 154it [01:43,  1.44it/s]Extractor Estimating: 155it [01:44,  1.42it/s]Extractor Estimating: 156it [01:44,  1.47it/s]Extractor Estimating: 157it [01:45,  1.54it/s]Extractor Estimating: 158it [01:46,  1.54it/s]Extractor Estimating: 159it [01:46,  1.50it/s]Extractor Estimating: 160it [01:47,  1.49it/s]Extractor Estimating: 161it [01:48,  1.46it/s]Extractor Estimating: 162it [01:49,  1.42it/s]Extractor Estimating: 163it [01:49,  1.43it/s]Extractor Estimating: 164it [01:50,  1.43it/s]Extractor Estimating: 165it [01:51,  1.42it/s]Extractor Estimating: 166it [01:51,  1.36it/s]Extractor Estimating: 167it [01:52,  1.36it/s]Extractor Estimating: 168it [01:53,  1.37it/s]Extractor Estimating: 169it [01:54,  1.42it/s]Extractor Estimating: 170it [01:54,  1.43it/s]Extractor Estimating: 171it [01:55,  1.46it/s]Extractor Estimating: 172it [01:56,  1.42it/s]Extractor Estimating: 173it [01:57,  1.32it/s]Extractor Estimating: 174it [01:57,  1.37it/s]Extractor Estimating: 175it [01:58,  1.31it/s]Extractor Estimating: 176it [01:59,  1.39it/s]Extractor Estimating: 177it [01:59,  1.42it/s]Extractor Estimating: 178it [02:00,  1.40it/s]Extractor Estimating: 179it [02:01,  1.43it/s]Extractor Estimating: 180it [02:01,  1.44it/s]Extractor Estimating: 181it [02:02,  1.44it/s]Extractor Estimating: 182it [02:03,  1.45it/s]Extractor Estimating: 183it [02:03,  1.50it/s]Extractor Estimating: 184it [02:04,  1.51it/s]Extractor Estimating: 185it [02:05,  1.59it/s]Extractor Estimating: 186it [02:05,  1.56it/s]Extractor Estimating: 187it [02:06,  1.50it/s]Extractor Estimating: 188it [02:07,  1.52it/s]Extractor Estimating: 189it [02:07,  1.55it/s]Extractor Estimating: 190it [02:08,  1.56it/s]Extractor Estimating: 191it [02:09,  1.50it/s]Extractor Estimating: 192it [02:09,  1.53it/s]Extractor Estimating: 193it [02:10,  1.51it/s]Extractor Estimating: 194it [02:11,  1.50it/s]Extractor Estimating: 195it [02:11,  1.52it/s]Extractor Estimating: 196it [02:12,  1.54it/s]Extractor Estimating: 197it [02:13,  1.53it/s]Extractor Estimating: 198it [02:13,  1.53it/s]Extractor Estimating: 199it [02:14,  1.53it/s]Extractor Estimating: 200it [02:14,  1.52it/s]Extractor Estimating: 201it [02:15,  1.45it/s]Extractor Estimating: 202it [02:16,  1.47it/s]Extractor Estimating: 203it [02:17,  1.50it/s]Extractor Estimating: 204it [02:17,  1.49it/s]Extractor Estimating: 205it [02:18,  1.53it/s]Extractor Estimating: 206it [02:19,  1.49it/s]Extractor Estimating: 207it [02:19,  1.48it/s]Extractor Estimating: 208it [02:20,  1.48it/s]Extractor Estimating: 209it [02:21,  1.46it/s]Extractor Estimating: 210it [02:21,  1.48it/s]Extractor Estimating: 211it [02:22,  1.44it/s]Extractor Estimating: 212it [02:23,  1.45it/s]Extractor Estimating: 213it [02:23,  1.48it/s]Extractor Estimating: 214it [02:24,  1.46it/s]Extractor Estimating: 215it [02:25,  1.45it/s]Extractor Estimating: 216it [02:25,  1.43it/s]Extractor Estimating: 217it [02:26,  1.44it/s]Extractor Estimating: 218it [02:27,  1.46it/s]Extractor Estimating: 219it [02:27,  1.48it/s]Extractor Estimating: 220it [02:28,  1.45it/s]Extractor Estimating: 221it [02:29,  1.46it/s]Extractor Estimating: 222it [02:29,  1.54it/s]Extractor Estimating: 223it [02:30,  1.50it/s]Extractor Estimating: 224it [02:31,  1.48it/s]Extractor Estimating: 225it [02:32,  1.48it/s]Extractor Estimating: 226it [02:32,  1.47it/s]Extractor Estimating: 227it [02:33,  1.45it/s]Extractor Estimating: 228it [02:34,  1.42it/s]Extractor Estimating: 229it [02:34,  1.46it/s]Extractor Estimating: 230it [02:35,  1.48it/s]Extractor Estimating: 231it [02:36,  1.52it/s]Extractor Estimating: 232it [02:36,  1.50it/s]Extractor Estimating: 233it [02:37,  1.49it/s]Extractor Estimating: 234it [02:38,  1.48it/s]Extractor Estimating: 235it [02:38,  1.52it/s]Extractor Estimating: 236it [02:39,  1.47it/s]Extractor Estimating: 237it [02:40,  1.43it/s]Extractor Estimating: 238it [02:40,  1.48it/s]Extractor Estimating: 239it [02:41,  1.46it/s]Extractor Estimating: 240it [02:42,  1.46it/s]Extractor Estimating: 241it [02:42,  1.40it/s]Extractor Estimating: 242it [02:43,  1.40it/s]Extractor Estimating: 243it [02:44,  1.44it/s]Extractor Estimating: 244it [02:44,  1.50it/s]Extractor Estimating: 245it [02:45,  1.45it/s]Extractor Estimating: 246it [02:46,  1.48it/s]Extractor Estimating: 247it [02:46,  1.50it/s]Extractor Estimating: 248it [02:47,  1.48it/s]Extractor Estimating: 249it [02:48,  1.49it/s]Extractor Estimating: 250it [02:49,  1.46it/s]Extractor Estimating: 250it [02:49,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:39,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:39,672 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:39,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:39,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:39,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:10:40,410 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:10:40,411 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:10:41,121 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:10:42,169 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:10:42,169 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:45,719 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:45,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:45,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:45,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:10:45,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:10:46,506 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:10:46,507 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:10:46,903 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:10:47,042 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:10:47,042 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 03:52:40,065 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 03:52:40,089 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5295 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 22029
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22129, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22129, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.193, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.198, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.203, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.169, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.159, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 2.309, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.179, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.177, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.181, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.185, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 2.292, loss:nan
g_step 1200, step 95, avg_time 1.197, loss:nan
g_step 1300, step 195, avg_time 1.159, loss:nan
g_step 1400, step 74, avg_time 1.174, loss:nan
g_step 1500, step 174, avg_time 1.173, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 2.324, loss:nan
g_step 1700, step 153, avg_time 1.194, loss:nan
g_step 1800, step 32, avg_time 1.168, loss:nan
g_step 1900, step 132, avg_time 1.175, loss:nan
g_step 2000, step 11, avg_time 1.172, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 2.328, loss:nan
g_step 2200, step 211, avg_time 1.154, loss:nan
g_step 2300, step 90, avg_time 1.190, loss:nan
g_step 2400, step 190, avg_time 1.161, loss:nan
g_step 2500, step 69, avg_time 1.175, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 2.329, loss:nan
g_step 2700, step 48, avg_time 1.155, loss:nan
g_step 2800, step 148, avg_time 1.186, loss:nan
g_step 2900, step 27, avg_time 1.173, loss:nan
g_step 3000, step 127, avg_time 1.156, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 2.326, loss:nan
g_step 3200, step 106, avg_time 1.169, loss:nan
g_step 3300, step 206, avg_time 1.188, loss:nan
g_step 3400, step 85, avg_time 1.174, loss:nan
g_step 3500, step 185, avg_time 1.174, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 2.307, loss:nan
g_step 3700, step 164, avg_time 1.181, loss:nan
g_step 3800, step 43, avg_time 1.180, loss:nan
g_step 3900, step 143, avg_time 1.184, loss:nan
g_step 4000, step 22, avg_time 1.165, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 2.309, loss:nan
g_step 4200, step 1, avg_time 1.176, loss:nan
g_step 4300, step 101, avg_time 1.176, loss:nan
g_step 4400, step 201, avg_time 1.169, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:52:40 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:52:40 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-52-40_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:52:41 - WARNING - datasets.builder -   Using custom data configuration default-5d0e58aa406c495d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5d0e58aa406c495d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:52:41,375 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:52:41,376 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:52:41,377 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:52:41,378 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:52:41,386 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:52:41,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:52:41,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:52:41,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:52:41,390 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:52:41,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:52:41,390 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:52:41,522 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:52:44,683 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:52:44,686 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5d0e58aa406c495d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.17ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.89ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.17ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.31ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.38ba/s]100%|██████████| 6/6 [00:01<00:00,  4.75ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.81ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.56ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.90ba/s]100%|██████████| 4/4 [00:00<00:00,  4.97ba/s]100%|██████████| 4/4 [00:00<00:00,  4.31ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  7.83ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.43ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.75ba/s]100%|██████████| 6/6 [00:00<00:00, 10.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.62ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.69ba/s]100%|██████████| 4/4 [00:00<00:00, 11.00ba/s]
[INFO|trainer.py:414] 2023-08-28 03:52:48,189 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:52:48,201 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:52:48,201 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 03:52:48,201 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:52:48,201 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:52:48,202 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:52:48,202 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:52:48,202 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<02:01,  3.39it/s]  0%|          | 2/415 [00:00<01:56,  3.55it/s]  1%|          | 3/415 [00:00<01:54,  3.60it/s]  1%|          | 4/415 [00:01<01:53,  3.62it/s]  1%|          | 5/415 [00:01<01:52,  3.64it/s]  1%|▏         | 6/415 [00:01<01:52,  3.65it/s]  2%|▏         | 7/415 [00:01<01:51,  3.65it/s]  2%|▏         | 8/415 [00:02<01:51,  3.66it/s]  2%|▏         | 9/415 [00:02<01:50,  3.66it/s]  2%|▏         | 10/415 [00:02<01:50,  3.66it/s]  3%|▎         | 11/415 [00:03<01:50,  3.66it/s]  3%|▎         | 12/415 [00:03<01:50,  3.66it/s]  3%|▎         | 13/415 [00:03<01:49,  3.66it/s]  3%|▎         | 14/415 [00:03<01:49,  3.67it/s]  4%|▎         | 15/415 [00:04<01:49,  3.67it/s]  4%|▍         | 16/415 [00:04<01:48,  3.66it/s]  4%|▍         | 17/415 [00:04<01:48,  3.66it/s]  4%|▍         | 18/415 [00:04<01:48,  3.67it/s]  5%|▍         | 19/415 [00:05<01:48,  3.67it/s]  5%|▍         | 20/415 [00:05<01:48,  3.66it/s]  5%|▌         | 21/415 [00:05<01:47,  3.66it/s]  5%|▌         | 22/415 [00:06<01:47,  3.66it/s]  6%|▌         | 23/415 [00:06<01:47,  3.66it/s]  6%|▌         | 24/415 [00:06<01:46,  3.66it/s]  6%|▌         | 25/415 [00:06<01:46,  3.66it/s]  6%|▋         | 26/415 [00:07<01:46,  3.66it/s]  7%|▋         | 27/415 [00:07<01:45,  3.66it/s]  7%|▋         | 28/415 [00:07<01:45,  3.66it/s]  7%|▋         | 29/415 [00:07<01:45,  3.66it/s]  7%|▋         | 30/415 [00:08<01:45,  3.66it/s]  7%|▋         | 31/415 [00:08<01:44,  3.66it/s]  8%|▊         | 32/415 [00:08<01:44,  3.66it/s]  8%|▊         | 33/415 [00:09<01:44,  3.66it/s]  8%|▊         | 34/415 [00:09<01:43,  3.67it/s]  8%|▊         | 35/415 [00:09<01:43,  3.66it/s]  9%|▊         | 36/415 [00:09<01:43,  3.66it/s]  9%|▉         | 37/415 [00:10<01:43,  3.66it/s]  9%|▉         | 38/415 [00:10<01:43,  3.65it/s]  9%|▉         | 39/415 [00:10<01:42,  3.66it/s] 10%|▉         | 40/415 [00:10<01:42,  3.65it/s] 10%|▉         | 41/415 [00:11<01:42,  3.66it/s] 10%|█         | 42/415 [00:11<01:42,  3.66it/s] 10%|█         | 43/415 [00:11<01:41,  3.65it/s] 11%|█         | 44/415 [00:12<01:41,  3.65it/s] 11%|█         | 45/415 [00:12<01:41,  3.65it/s] 11%|█         | 46/415 [00:12<01:40,  3.65it/s] 11%|█▏        | 47/415 [00:12<01:40,  3.66it/s] 12%|█▏        | 48/415 [00:13<01:40,  3.65it/s] 12%|█▏        | 49/415 [00:13<01:40,  3.65it/s] 12%|█▏        | 50/415 [00:13<01:39,  3.65it/s] 12%|█▏        | 51/415 [00:13<01:39,  3.66it/s] 13%|█▎        | 52/415 [00:14<01:39,  3.65it/s] 13%|█▎        | 53/415 [00:14<01:39,  3.65it/s] 13%|█▎        | 54/415 [00:14<01:38,  3.65it/s] 13%|█▎        | 55/415 [00:15<01:38,  3.65it/s] 13%|█▎        | 56/415 [00:15<01:38,  3.65it/s] 14%|█▎        | 57/415 [00:15<01:38,  3.64it/s] 14%|█▍        | 58/415 [00:15<01:37,  3.64it/s] 14%|█▍        | 59/415 [00:16<01:37,  3.65it/s] 14%|█▍        | 60/415 [00:16<01:37,  3.65it/s] 15%|█▍        | 61/415 [00:16<01:36,  3.65it/s] 15%|█▍        | 62/415 [00:16<01:36,  3.65it/s] 15%|█▌        | 63/415 [00:17<01:36,  3.65it/s] 15%|█▌        | 64/415 [00:17<01:36,  3.65it/s] 16%|█▌        | 65/415 [00:17<01:36,  3.64it/s] 16%|█▌        | 66/415 [00:18<01:35,  3.64it/s] 16%|█▌        | 67/415 [00:18<01:35,  3.65it/s] 16%|█▋        | 68/415 [00:18<01:35,  3.65it/s] 17%|█▋        | 69/415 [00:18<01:34,  3.65it/s] 17%|█▋        | 70/415 [00:19<01:34,  3.65it/s] 17%|█▋        | 71/415 [00:19<01:34,  3.65it/s] 17%|█▋        | 72/415 [00:19<01:33,  3.65it/s] 18%|█▊        | 73/415 [00:19<01:33,  3.65it/s] 18%|█▊        | 74/415 [00:20<01:33,  3.65it/s] 18%|█▊        | 75/415 [00:20<01:32,  3.66it/s] 18%|█▊        | 76/415 [00:20<01:33,  3.64it/s] 19%|█▊        | 77/415 [00:21<01:32,  3.65it/s] 19%|█▉        | 78/415 [00:21<01:32,  3.65it/s] 19%|█▉        | 79/415 [00:21<01:32,  3.64it/s] 19%|█▉        | 80/415 [00:21<01:31,  3.65it/s] 20%|█▉        | 81/415 [00:22<01:31,  3.65it/s] 20%|█▉        | 82/415 [00:22<01:31,  3.65it/s] 20%|██        | 83/415 [00:22<01:26,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 03:53:10,888 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:53:10,888 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 03:53:10,888 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.00it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.31it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.77it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.13it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.92it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.54it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.47it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.40it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.32it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.35it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.38it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.45it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.45it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.49it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.29it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.46it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.44it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.27it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.23it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.33it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.35it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.37it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.42it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.42it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.41it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.35it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.31it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.22it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.21it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.32it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.37it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.40it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.42it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.44it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.46it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.50it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.47it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.37it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.34it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.44it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.38it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.30it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.33it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.34it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.34it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.17it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.25it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.35it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.33it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.36it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.33it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.33it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.34it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.25it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.23it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.19it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.30it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.32it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.32it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.30it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.32it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.27it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.21it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.20it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.24it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.27it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.29it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.38it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.31it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.28it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.29it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.28it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.20it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.20it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.18it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.20it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.32it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.33it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.29it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.31it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.28it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.18it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.18it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.18it/s][A 20%|██        | 83/415 [00:31<01:26,  3.83it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:53:20,143 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 03:53:20,167 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:53:22,392 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:53:22,411 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:53:22,427 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:34<21:02,  3.81s/it] 20%|██        | 85/415 [00:35<15:07,  2.75s/it] 21%|██        | 86/415 [00:35<11:00,  2.01s/it] 21%|██        | 87/415 [00:35<08:08,  1.49s/it] 21%|██        | 88/415 [00:35<06:07,  1.12s/it] 21%|██▏       | 89/415 [00:36<04:43,  1.15it/s] 22%|██▏       | 90/415 [00:36<03:44,  1.45it/s] 22%|██▏       | 91/415 [00:36<03:03,  1.77it/s] 22%|██▏       | 92/415 [00:36<02:34,  2.09it/s] 22%|██▏       | 93/415 [00:37<02:14,  2.40it/s] 23%|██▎       | 94/415 [00:37<02:00,  2.67it/s] 23%|██▎       | 95/415 [00:37<01:50,  2.90it/s] 23%|██▎       | 96/415 [00:38<01:43,  3.09it/s] 23%|██▎       | 97/415 [00:38<01:38,  3.24it/s] 24%|██▎       | 98/415 [00:38<01:34,  3.35it/s] 24%|██▍       | 99/415 [00:38<01:31,  3.44it/s] 24%|██▍       | 100/415 [00:39<01:29,  3.50it/s] 24%|██▍       | 101/415 [00:39<01:28,  3.54it/s] 25%|██▍       | 102/415 [00:39<01:27,  3.57it/s] 25%|██▍       | 103/415 [00:39<01:26,  3.60it/s] 25%|██▌       | 104/415 [00:40<01:26,  3.61it/s] 25%|██▌       | 105/415 [00:40<01:25,  3.62it/s] 26%|██▌       | 106/415 [00:40<01:25,  3.63it/s] 26%|██▌       | 107/415 [00:41<01:24,  3.63it/s] 26%|██▌       | 108/415 [00:41<01:24,  3.64it/s] 26%|██▋       | 109/415 [00:41<01:24,  3.64it/s] 27%|██▋       | 110/415 [00:41<01:23,  3.64it/s] 27%|██▋       | 111/415 [00:42<01:23,  3.64it/s] 27%|██▋       | 112/415 [00:42<01:23,  3.64it/s] 27%|██▋       | 113/415 [00:42<01:22,  3.64it/s] 27%|██▋       | 114/415 [00:43<01:22,  3.65it/s] 28%|██▊       | 115/415 [00:43<01:22,  3.65it/s] 28%|██▊       | 116/415 [00:43<01:22,  3.62it/s] 28%|██▊       | 117/415 [00:43<01:22,  3.62it/s] 28%|██▊       | 118/415 [00:44<01:21,  3.63it/s] 29%|██▊       | 119/415 [00:44<01:21,  3.63it/s] 29%|██▉       | 120/415 [00:44<01:21,  3.64it/s] 29%|██▉       | 121/415 [00:44<01:20,  3.64it/s] 29%|██▉       | 122/415 [00:45<01:20,  3.64it/s] 30%|██▉       | 123/415 [00:45<01:20,  3.64it/s] 30%|██▉       | 124/415 [00:45<01:19,  3.64it/s] 30%|███       | 125/415 [00:46<01:19,  3.64it/s] 30%|███       | 126/415 [00:46<01:19,  3.65it/s] 31%|███       | 127/415 [00:46<01:19,  3.63it/s] 31%|███       | 128/415 [00:46<01:18,  3.64it/s] 31%|███       | 129/415 [00:47<01:18,  3.64it/s] 31%|███▏      | 130/415 [00:47<01:18,  3.64it/s] 32%|███▏      | 131/415 [00:47<01:17,  3.64it/s] 32%|███▏      | 132/415 [00:47<01:17,  3.64it/s] 32%|███▏      | 133/415 [00:48<01:17,  3.64it/s] 32%|███▏      | 134/415 [00:48<01:17,  3.64it/s] 33%|███▎      | 135/415 [00:48<01:16,  3.64it/s] 33%|███▎      | 136/415 [00:49<01:16,  3.64it/s] 33%|███▎      | 137/415 [00:49<01:16,  3.64it/s] 33%|███▎      | 138/415 [00:49<01:16,  3.64it/s] 33%|███▎      | 139/415 [00:49<01:15,  3.64it/s] 34%|███▎      | 140/415 [00:50<01:15,  3.65it/s] 34%|███▍      | 141/415 [00:50<01:15,  3.65it/s] 34%|███▍      | 142/415 [00:50<01:14,  3.65it/s] 34%|███▍      | 143/415 [00:51<01:28,  3.09it/s] 35%|███▍      | 144/415 [00:51<01:24,  3.20it/s] 35%|███▍      | 145/415 [00:51<01:21,  3.32it/s] 35%|███▌      | 146/415 [00:51<01:20,  3.33it/s] 35%|███▌      | 147/415 [00:52<01:18,  3.40it/s] 36%|███▌      | 148/415 [00:52<01:17,  3.47it/s] 36%|███▌      | 149/415 [00:52<01:15,  3.51it/s] 36%|███▌      | 150/415 [00:53<01:14,  3.55it/s] 36%|███▋      | 151/415 [00:53<01:13,  3.58it/s] 37%|███▋      | 152/415 [00:53<01:13,  3.60it/s] 37%|███▋      | 153/415 [00:53<01:12,  3.62it/s] 37%|███▋      | 154/415 [00:54<01:11,  3.63it/s] 37%|███▋      | 155/415 [00:54<01:11,  3.63it/s] 38%|███▊      | 156/415 [00:54<01:11,  3.63it/s] 38%|███▊      | 157/415 [00:55<01:10,  3.64it/s] 38%|███▊      | 158/415 [00:55<01:10,  3.64it/s] 38%|███▊      | 159/415 [00:55<01:10,  3.64it/s] 39%|███▊      | 160/415 [00:55<01:10,  3.63it/s] 39%|███▉      | 161/415 [00:56<01:09,  3.63it/s] 39%|███▉      | 162/415 [00:56<01:09,  3.64it/s] 39%|███▉      | 163/415 [00:56<01:09,  3.64it/s] 40%|███▉      | 164/415 [00:56<01:08,  3.64it/s] 40%|███▉      | 165/415 [00:57<01:08,  3.64it/s] 40%|████      | 166/415 [00:57<01:04,  3.84it/s][INFO|trainer.py:2140] 2023-08-28 03:53:45,656 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:53:45,656 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 03:53:45,656 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2375, 'eval_samples_per_second': 378.456, 'eval_steps_per_second': 47.307, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.68it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.16it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.34it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.70it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.30it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.00it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.69it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.36it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.30it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.27it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.31it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.38it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.32it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.42it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.43it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.26it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.16it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.14it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.11it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.15it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.25it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.31it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.31it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.38it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.27it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.20it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.13it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.16it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.16it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.14it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.24it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.26it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.30it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.31it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.25it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.19it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.14it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.18it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.12it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.14it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.28it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.25it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.28it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.20it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.20it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.15it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.14it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.20it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.17it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.13it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.26it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.28it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.23it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.14it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.22it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.18it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.14it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.19it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.18it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.17it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.24it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.29it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.21it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.23it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.18it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.14it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.15it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.17it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.17it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.21it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.20it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.21it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.21it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.21it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.21it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.20it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.08it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.16it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.22it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.17it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.24it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.30it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.13it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.16it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.20it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.14it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.14it/s][A 40%|████      | 166/415 [01:06<01:04,  3.84it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:53:54,930 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 03:53:54,942 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:53:57,044 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:53:57,058 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:53:57,066 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:09<15:35,  3.77s/it] 40%|████      | 168/415 [01:09<11:12,  2.72s/it] 41%|████      | 169/415 [01:09<08:09,  1.99s/it] 41%|████      | 170/415 [01:10<06:01,  1.47s/it] 41%|████      | 171/415 [01:10<04:31,  1.11s/it] 41%|████▏     | 172/415 [01:10<03:29,  1.16it/s] 42%|████▏     | 173/415 [01:11<02:46,  1.46it/s] 42%|████▏     | 174/415 [01:11<02:15,  1.78it/s] 42%|████▏     | 175/415 [01:11<01:54,  2.10it/s] 42%|████▏     | 176/415 [01:11<01:39,  2.41it/s] 43%|████▎     | 177/415 [01:12<01:28,  2.68it/s] 43%|████▎     | 178/415 [01:12<01:21,  2.91it/s] 43%|████▎     | 179/415 [01:12<01:16,  3.10it/s] 43%|████▎     | 180/415 [01:12<01:12,  3.23it/s] 44%|████▎     | 181/415 [01:13<01:09,  3.35it/s] 44%|████▍     | 182/415 [01:13<01:07,  3.43it/s] 44%|████▍     | 183/415 [01:13<01:06,  3.49it/s] 44%|████▍     | 184/415 [01:14<01:05,  3.53it/s] 45%|████▍     | 185/415 [01:14<01:04,  3.56it/s] 45%|████▍     | 186/415 [01:14<01:03,  3.59it/s] 45%|████▌     | 187/415 [01:14<01:03,  3.60it/s] 45%|████▌     | 188/415 [01:15<01:02,  3.61it/s] 46%|████▌     | 189/415 [01:15<01:02,  3.62it/s] 46%|████▌     | 190/415 [01:15<01:02,  3.63it/s] 46%|████▌     | 191/415 [01:16<01:01,  3.62it/s] 46%|████▋     | 192/415 [01:16<01:01,  3.62it/s] 47%|████▋     | 193/415 [01:16<01:01,  3.63it/s] 47%|████▋     | 194/415 [01:16<01:00,  3.63it/s] 47%|████▋     | 195/415 [01:17<01:00,  3.64it/s] 47%|████▋     | 196/415 [01:17<01:00,  3.64it/s] 47%|████▋     | 197/415 [01:17<00:59,  3.64it/s] 48%|████▊     | 198/415 [01:17<00:59,  3.64it/s] 48%|████▊     | 199/415 [01:18<00:59,  3.64it/s] 48%|████▊     | 200/415 [01:18<00:59,  3.64it/s] 48%|████▊     | 201/415 [01:18<00:58,  3.64it/s] 49%|████▊     | 202/415 [01:19<00:58,  3.63it/s] 49%|████▉     | 203/415 [01:19<00:58,  3.63it/s] 49%|████▉     | 204/415 [01:19<00:58,  3.63it/s] 49%|████▉     | 205/415 [01:19<00:57,  3.63it/s] 50%|████▉     | 206/415 [01:20<00:57,  3.64it/s] 50%|████▉     | 207/415 [01:20<00:57,  3.64it/s] 50%|█████     | 208/415 [01:20<00:56,  3.64it/s] 50%|█████     | 209/415 [01:20<00:56,  3.63it/s] 51%|█████     | 210/415 [01:21<00:56,  3.64it/s] 51%|█████     | 211/415 [01:21<00:56,  3.64it/s] 51%|█████     | 212/415 [01:21<00:55,  3.64it/s] 51%|█████▏    | 213/415 [01:22<00:55,  3.62it/s] 52%|█████▏    | 214/415 [01:22<00:55,  3.63it/s] 52%|█████▏    | 215/415 [01:22<00:55,  3.63it/s] 52%|█████▏    | 216/415 [01:22<00:54,  3.63it/s] 52%|█████▏    | 217/415 [01:23<00:54,  3.63it/s] 53%|█████▎    | 218/415 [01:23<00:54,  3.64it/s] 53%|█████▎    | 219/415 [01:23<00:53,  3.64it/s] 53%|█████▎    | 220/415 [01:23<00:53,  3.64it/s] 53%|█████▎    | 221/415 [01:24<00:53,  3.64it/s] 53%|█████▎    | 222/415 [01:24<00:53,  3.64it/s] 54%|█████▎    | 223/415 [01:24<00:52,  3.64it/s] 54%|█████▍    | 224/415 [01:25<00:52,  3.62it/s] 54%|█████▍    | 225/415 [01:25<00:52,  3.63it/s] 54%|█████▍    | 226/415 [01:25<00:52,  3.63it/s] 55%|█████▍    | 227/415 [01:25<00:51,  3.63it/s] 55%|█████▍    | 228/415 [01:26<00:51,  3.63it/s] 55%|█████▌    | 229/415 [01:26<00:51,  3.63it/s] 55%|█████▌    | 230/415 [01:26<00:50,  3.63it/s] 56%|█████▌    | 231/415 [01:27<00:50,  3.64it/s] 56%|█████▌    | 232/415 [01:27<00:50,  3.64it/s] 56%|█████▌    | 233/415 [01:27<00:50,  3.64it/s] 56%|█████▋    | 234/415 [01:27<00:49,  3.63it/s] 57%|█████▋    | 235/415 [01:28<00:49,  3.63it/s] 57%|█████▋    | 236/415 [01:28<00:49,  3.63it/s] 57%|█████▋    | 237/415 [01:28<00:48,  3.63it/s] 57%|█████▋    | 238/415 [01:28<00:48,  3.63it/s] 58%|█████▊    | 239/415 [01:29<00:48,  3.64it/s] 58%|█████▊    | 240/415 [01:29<00:48,  3.64it/s] 58%|█████▊    | 241/415 [01:29<00:47,  3.64it/s] 58%|█████▊    | 242/415 [01:30<00:47,  3.64it/s] 59%|█████▊    | 243/415 [01:30<00:47,  3.64it/s] 59%|█████▉    | 244/415 [01:30<00:47,  3.64it/s] 59%|█████▉    | 245/415 [01:30<00:46,  3.64it/s] 59%|█████▉    | 246/415 [01:31<00:46,  3.63it/s] 60%|█████▉    | 247/415 [01:31<00:46,  3.63it/s] 60%|█████▉    | 248/415 [01:31<00:46,  3.62it/s] 60%|██████    | 249/415 [01:31<00:43,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 03:54:20,134 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:54:20,134 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 03:54:20,134 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.258, 'eval_samples_per_second': 377.62, 'eval_steps_per_second': 47.203, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.51it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.97it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.21it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.57it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.13it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.83it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.57it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.14it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.17it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.19it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.07it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.18it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.19it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.26it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.32it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.24it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.99it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.00it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.02it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.98it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.04it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.06it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.16it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.23it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.25it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.11it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.00it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.08it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.10it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.09it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.11it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.19it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.19it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.15it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.07it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.98it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.00it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.09it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.09it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.01it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.13it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.22it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.18it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.03it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.06it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.99it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.06it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.02it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.00it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.15it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.11it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.02it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.00it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.01it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.95it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.08it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.12it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.02it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.08it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.11it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.98it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.02it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.98it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.95it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.97it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.10it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.09it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.08it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.07it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.00it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.98it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.01it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.92it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.83it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.01it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.12it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.10it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.13it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.07it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.92it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.03it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.93it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.04it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.07it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.07it/s][A 60%|██████    | 249/415 [01:41<00:43,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:54:29,436 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 03:54:29,459 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:54:31,659 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:54:31,676 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:54:31,686 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [01:44<10:30,  3.82s/it] 60%|██████    | 251/415 [01:44<07:32,  2.76s/it] 61%|██████    | 252/415 [01:44<05:27,  2.01s/it] 61%|██████    | 253/415 [01:44<04:01,  1.49s/it] 61%|██████    | 254/415 [01:45<03:01,  1.13s/it] 61%|██████▏   | 255/415 [01:45<02:19,  1.15it/s] 62%|██████▏   | 256/415 [01:45<01:50,  1.44it/s] 62%|██████▏   | 257/415 [01:45<01:29,  1.76it/s] 62%|██████▏   | 258/415 [01:46<01:15,  2.09it/s] 62%|██████▏   | 259/415 [01:46<01:05,  2.39it/s] 63%|██████▎   | 260/415 [01:46<00:58,  2.66it/s] 63%|██████▎   | 261/415 [01:47<00:53,  2.89it/s] 63%|██████▎   | 262/415 [01:47<00:49,  3.08it/s] 63%|██████▎   | 263/415 [01:47<00:47,  3.23it/s] 64%|██████▎   | 264/415 [01:47<00:45,  3.34it/s] 64%|██████▍   | 265/415 [01:48<00:43,  3.42it/s] 64%|██████▍   | 266/415 [01:48<00:42,  3.48it/s] 64%|██████▍   | 267/415 [01:48<00:41,  3.53it/s] 65%|██████▍   | 268/415 [01:49<00:41,  3.56it/s] 65%|██████▍   | 269/415 [01:49<00:40,  3.58it/s] 65%|██████▌   | 270/415 [01:49<00:40,  3.60it/s] 65%|██████▌   | 271/415 [01:49<00:40,  3.60it/s] 66%|██████▌   | 272/415 [01:50<00:39,  3.61it/s] 66%|██████▌   | 273/415 [01:50<00:39,  3.62it/s] 66%|██████▌   | 274/415 [01:50<00:38,  3.62it/s] 66%|██████▋   | 275/415 [01:50<00:38,  3.63it/s] 67%|██████▋   | 276/415 [01:51<00:38,  3.62it/s] 67%|██████▋   | 277/415 [01:51<00:38,  3.63it/s] 67%|██████▋   | 278/415 [01:51<00:37,  3.63it/s] 67%|██████▋   | 279/415 [01:52<00:38,  3.51it/s] 67%|██████▋   | 280/415 [01:52<00:38,  3.52it/s] 68%|██████▊   | 281/415 [01:52<00:37,  3.55it/s] 68%|██████▊   | 282/415 [01:52<00:37,  3.57it/s] 68%|██████▊   | 283/415 [01:53<00:36,  3.59it/s] 68%|██████▊   | 284/415 [01:53<00:36,  3.60it/s] 69%|██████▊   | 285/415 [01:53<00:35,  3.61it/s] 69%|██████▉   | 286/415 [01:54<00:35,  3.62it/s] 69%|██████▉   | 287/415 [01:54<00:35,  3.63it/s] 69%|██████▉   | 288/415 [01:54<00:35,  3.63it/s] 70%|██████▉   | 289/415 [01:54<00:34,  3.63it/s] 70%|██████▉   | 290/415 [01:55<00:34,  3.63it/s] 70%|███████   | 291/415 [01:55<00:34,  3.63it/s] 70%|███████   | 292/415 [01:55<00:33,  3.63it/s] 71%|███████   | 293/415 [01:55<00:33,  3.62it/s] 71%|███████   | 294/415 [01:56<00:33,  3.63it/s] 71%|███████   | 295/415 [01:56<00:33,  3.63it/s] 71%|███████▏  | 296/415 [01:56<00:32,  3.63it/s] 72%|███████▏  | 297/415 [01:57<00:32,  3.63it/s] 72%|███████▏  | 298/415 [01:57<00:32,  3.64it/s] 72%|███████▏  | 299/415 [01:57<00:31,  3.64it/s] 72%|███████▏  | 300/415 [01:57<00:31,  3.64it/s] 73%|███████▎  | 301/415 [01:58<00:31,  3.63it/s] 73%|███████▎  | 302/415 [01:58<00:31,  3.64it/s] 73%|███████▎  | 303/415 [01:58<00:30,  3.64it/s] 73%|███████▎  | 304/415 [01:58<00:30,  3.63it/s] 73%|███████▎  | 305/415 [01:59<00:30,  3.63it/s] 74%|███████▎  | 306/415 [01:59<00:29,  3.63it/s] 74%|███████▍  | 307/415 [01:59<00:29,  3.63it/s] 74%|███████▍  | 308/415 [02:00<00:29,  3.63it/s] 74%|███████▍  | 309/415 [02:00<00:29,  3.63it/s] 75%|███████▍  | 310/415 [02:00<00:28,  3.63it/s] 75%|███████▍  | 311/415 [02:00<00:28,  3.64it/s] 75%|███████▌  | 312/415 [02:01<00:28,  3.63it/s] 75%|███████▌  | 313/415 [02:01<00:28,  3.64it/s] 76%|███████▌  | 314/415 [02:01<00:27,  3.64it/s] 76%|███████▌  | 315/415 [02:01<00:27,  3.63it/s] 76%|███████▌  | 316/415 [02:02<00:27,  3.63it/s] 76%|███████▋  | 317/415 [02:02<00:26,  3.63it/s] 77%|███████▋  | 318/415 [02:02<00:26,  3.63it/s] 77%|███████▋  | 319/415 [02:03<00:26,  3.63it/s] 77%|███████▋  | 320/415 [02:03<00:26,  3.63it/s] 77%|███████▋  | 321/415 [02:03<00:25,  3.63it/s] 78%|███████▊  | 322/415 [02:03<00:25,  3.63it/s] 78%|███████▊  | 323/415 [02:04<00:25,  3.64it/s] 78%|███████▊  | 324/415 [02:04<00:25,  3.64it/s] 78%|███████▊  | 325/415 [02:04<00:24,  3.64it/s] 79%|███████▊  | 326/415 [02:05<00:24,  3.60it/s] 79%|███████▉  | 327/415 [02:05<00:24,  3.61it/s] 79%|███████▉  | 328/415 [02:05<00:24,  3.62it/s] 79%|███████▉  | 329/415 [02:05<00:23,  3.63it/s] 80%|███████▉  | 330/415 [02:06<00:23,  3.63it/s] 80%|███████▉  | 331/415 [02:06<00:23,  3.63it/s] 80%|████████  | 332/415 [02:06<00:21,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 03:54:54,828 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:54:54,828 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 03:54:54,829 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2857, 'eval_samples_per_second': 376.492, 'eval_steps_per_second': 47.061, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.38it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.86it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.15it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.45it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.07it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.73it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.59it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.40it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.22it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.17it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.20it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.22it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.21it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.19it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.09it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.08it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.99it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.01it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.00it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.02it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.14it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.21it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.09it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.15it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.03it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.98it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.99it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.92it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.98it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.05it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.16it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.13it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.11it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.03it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.97it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.90it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.96it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.01it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.96it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.11it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.17it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.10it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.08it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.94it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.91it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.97it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.99it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.01it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.06it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.18it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.09it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.11it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.08it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.92it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.96it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.02it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.04it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.04it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.10it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.18it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.12it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.11it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.82it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.88it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.05it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.07it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.00it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.08it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.13it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.19it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.14it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.93it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.91it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.94it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.06it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.06it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.99it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.05it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.18it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.14it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.05it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.99it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.85it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.01it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.04it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.92it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.92it/s][A 80%|████████  | 332/415 [02:15<00:21,  3.83it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:55:04,143 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 03:55:04,166 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:55:06,673 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:55:06,687 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:55:06,698 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [02:19<05:21,  3.91s/it] 80%|████████  | 334/415 [02:19<03:48,  2.82s/it] 81%|████████  | 335/415 [02:19<02:44,  2.06s/it] 81%|████████  | 336/415 [02:19<02:00,  1.52s/it] 81%|████████  | 337/415 [02:20<01:29,  1.15s/it] 81%|████████▏ | 338/415 [02:20<01:08,  1.13it/s] 82%|████████▏ | 339/415 [02:20<00:53,  1.42it/s] 82%|████████▏ | 340/415 [02:20<00:43,  1.74it/s] 82%|████████▏ | 341/415 [02:21<00:35,  2.06it/s] 82%|████████▏ | 342/415 [02:21<00:30,  2.37it/s] 83%|████████▎ | 343/415 [02:21<00:27,  2.63it/s] 83%|████████▎ | 344/415 [02:22<00:24,  2.87it/s] 83%|████████▎ | 345/415 [02:22<00:22,  3.06it/s] 83%|████████▎ | 346/415 [02:22<00:21,  3.22it/s] 84%|████████▎ | 347/415 [02:22<00:20,  3.33it/s] 84%|████████▍ | 348/415 [02:23<00:19,  3.42it/s] 84%|████████▍ | 349/415 [02:23<00:18,  3.48it/s] 84%|████████▍ | 350/415 [02:23<00:18,  3.53it/s] 85%|████████▍ | 351/415 [02:24<00:17,  3.56it/s] 85%|████████▍ | 352/415 [02:24<00:17,  3.58it/s] 85%|████████▌ | 353/415 [02:24<00:17,  3.60it/s] 85%|████████▌ | 354/415 [02:24<00:16,  3.60it/s] 86%|████████▌ | 355/415 [02:25<00:16,  3.62it/s] 86%|████████▌ | 356/415 [02:25<00:16,  3.62it/s] 86%|████████▌ | 357/415 [02:25<00:15,  3.63it/s] 86%|████████▋ | 358/415 [02:25<00:15,  3.63it/s] 87%|████████▋ | 359/415 [02:26<00:15,  3.63it/s] 87%|████████▋ | 360/415 [02:26<00:15,  3.64it/s] 87%|████████▋ | 361/415 [02:26<00:14,  3.64it/s] 87%|████████▋ | 362/415 [02:27<00:14,  3.64it/s] 87%|████████▋ | 363/415 [02:27<00:14,  3.64it/s] 88%|████████▊ | 364/415 [02:27<00:14,  3.64it/s] 88%|████████▊ | 365/415 [02:27<00:13,  3.61it/s] 88%|████████▊ | 366/415 [02:28<00:13,  3.62it/s] 88%|████████▊ | 367/415 [02:28<00:13,  3.63it/s] 89%|████████▊ | 368/415 [02:28<00:12,  3.64it/s] 89%|████████▉ | 369/415 [02:28<00:12,  3.64it/s] 89%|████████▉ | 370/415 [02:29<00:12,  3.63it/s] 89%|████████▉ | 371/415 [02:29<00:12,  3.64it/s] 90%|████████▉ | 372/415 [02:29<00:11,  3.64it/s] 90%|████████▉ | 373/415 [02:30<00:11,  3.64it/s] 90%|█████████ | 374/415 [02:30<00:11,  3.64it/s] 90%|█████████ | 375/415 [02:30<00:10,  3.64it/s] 91%|█████████ | 376/415 [02:30<00:10,  3.62it/s] 91%|█████████ | 377/415 [02:31<00:10,  3.62it/s] 91%|█████████ | 378/415 [02:31<00:10,  3.63it/s] 91%|█████████▏| 379/415 [02:31<00:09,  3.63it/s] 92%|█████████▏| 380/415 [02:31<00:09,  3.63it/s] 92%|█████████▏| 381/415 [02:32<00:09,  3.63it/s] 92%|█████████▏| 382/415 [02:32<00:09,  3.63it/s] 92%|█████████▏| 383/415 [02:32<00:08,  3.64it/s] 93%|█████████▎| 384/415 [02:33<00:08,  3.64it/s] 93%|█████████▎| 385/415 [02:33<00:08,  3.64it/s] 93%|█████████▎| 386/415 [02:33<00:07,  3.64it/s] 93%|█████████▎| 387/415 [02:33<00:07,  3.63it/s] 93%|█████████▎| 388/415 [02:34<00:07,  3.63it/s] 94%|█████████▎| 389/415 [02:34<00:07,  3.63it/s] 94%|█████████▍| 390/415 [02:34<00:06,  3.63it/s] 94%|█████████▍| 391/415 [02:35<00:06,  3.63it/s] 94%|█████████▍| 392/415 [02:35<00:06,  3.63it/s] 95%|█████████▍| 393/415 [02:35<00:06,  3.64it/s] 95%|█████████▍| 394/415 [02:35<00:05,  3.64it/s] 95%|█████████▌| 395/415 [02:36<00:05,  3.64it/s] 95%|█████████▌| 396/415 [02:36<00:05,  3.64it/s] 96%|█████████▌| 397/415 [02:36<00:04,  3.64it/s] 96%|█████████▌| 398/415 [02:36<00:04,  3.63it/s] 96%|█████████▌| 399/415 [02:37<00:04,  3.63it/s] 96%|█████████▋| 400/415 [02:37<00:04,  3.63it/s] 97%|█████████▋| 401/415 [02:37<00:03,  3.63it/s] 97%|█████████▋| 402/415 [02:38<00:03,  3.64it/s] 97%|█████████▋| 403/415 [02:38<00:03,  3.64it/s] 97%|█████████▋| 404/415 [02:38<00:03,  3.64it/s] 98%|█████████▊| 405/415 [02:38<00:02,  3.64it/s] 98%|█████████▊| 406/415 [02:39<00:02,  3.64it/s] 98%|█████████▊| 407/415 [02:39<00:02,  3.64it/s] 98%|█████████▊| 408/415 [02:39<00:01,  3.64it/s] 99%|█████████▊| 409/415 [02:39<00:01,  3.63it/s] 99%|█████████▉| 410/415 [02:40<00:01,  3.63it/s] 99%|█████████▉| 411/415 [02:40<00:01,  3.63it/s] 99%|█████████▉| 412/415 [02:40<00:00,  3.63it/s]100%|█████████▉| 413/415 [02:41<00:00,  3.64it/s]100%|█████████▉| 414/415 [02:41<00:00,  3.64it/s]100%|██████████| 415/415 [02:41<00:00,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 03:55:29,785 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:55:29,786 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 03:55:29,786 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.29, 'eval_samples_per_second': 376.318, 'eval_steps_per_second': 47.04, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.16it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.30it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.59it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.17it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.85it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.46it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.19it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.12it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.17it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.14it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.21it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.22it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.19it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.20it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.04it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.00it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.95it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.00it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.10it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.09it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.04it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.15it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.15it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.05it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.95it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.88it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.89it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.03it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.14it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.04it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.10it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.16it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.07it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.00it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.89it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.87it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.97it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.10it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.16it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.03it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.09it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.97it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.89it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.86it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.82it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.88it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.02it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.14it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.11it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.07it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.03it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.95it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.94it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.83it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.83it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.98it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.10it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.14it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.07it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.01it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.95it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.92it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.90it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.86it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.95it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.08it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.08it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.08it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.98it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.92it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.91it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.94it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.89it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.87it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.02it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.11it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.11it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.08it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.96it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.86it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.90it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.88it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.77it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.96it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.08it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.08it/s][A100%|██████████| 415/415 [02:50<00:00,  3.83it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:55:39,099 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 03:55:39,117 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:55:41,213 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:55:41,231 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:55:41,238 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:55:41,498 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:55:41,498 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83 (score: 1.1523417234420776).
                                                 100%|██████████| 415/415 [02:54<00:00,  3.83it/s]100%|██████████| 415/415 [02:54<00:00,  2.37it/s]
[INFO|trainer.py:1894] 2023-08-28 03:55:43,103 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 03:55:43,117 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:55:45,021 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:55:45,043 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:55:45,058 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:55:45,234 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:45,234 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:45,234 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:45,234 >>   train_runtime            = 0:02:54.89
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:45,234 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:45,234 >>   train_samples_per_second =    151.518
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:45,234 >>   train_steps_per_second   =      2.373
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2954, 'eval_samples_per_second': 376.099, 'eval_steps_per_second': 47.012, 'epoch': 5.0}
{'train_runtime': 174.8972, 'train_samples_per_second': 151.518, 'train_steps_per_second': 2.373, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:55:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:55:45,274 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:55:45,274 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 03:55:45,274 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.98it/s]  3%|▎         | 12/437 [00:00<00:08, 51.62it/s]  4%|▍         | 18/437 [00:00<00:08, 49.66it/s]  5%|▌         | 24/437 [00:00<00:08, 48.81it/s]  7%|▋         | 29/437 [00:00<00:08, 48.41it/s]  8%|▊         | 34/437 [00:00<00:08, 48.15it/s]  9%|▉         | 39/437 [00:00<00:08, 47.83it/s] 10%|█         | 44/437 [00:00<00:08, 47.76it/s] 11%|█         | 49/437 [00:01<00:08, 47.63it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.53it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.46it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.39it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.31it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.35it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.41it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.39it/s] 20%|██        | 89/437 [00:01<00:07, 47.27it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.28it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.29it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.20it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.28it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.25it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.24it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.28it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.34it/s] 31%|███       | 134/437 [00:02<00:06, 47.31it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.28it/s] 33%|███▎      | 144/437 [00:03<00:06, 47.24it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.25it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.24it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.25it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.26it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.21it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.23it/s] 41%|████      | 179/437 [00:03<00:05, 47.26it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.19it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.23it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.24it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.28it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.26it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.20it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.27it/s] 50%|█████     | 219/437 [00:04<00:04, 47.23it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.25it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.21it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.21it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.26it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.20it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.30it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.24it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.14it/s] 60%|██████    | 264/437 [00:05<00:03, 47.27it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.29it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.15it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.15it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.03it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.08it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.06it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.11it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.08it/s] 71%|███████   | 309/437 [00:06<00:02, 47.11it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.20it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.16it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.17it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.10it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.09it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.08it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.03it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.06it/s] 81%|████████  | 354/437 [00:07<00:01, 47.13it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.18it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.25it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.24it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.19it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.19it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.18it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.07it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.08it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.14it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.15it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.13it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.22it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.21it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.17it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.11it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.09it/s]100%|██████████| 437/437 [00:09<00:00, 47.29it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:55:54,537 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,537 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,537 >>   eval_loss               =     1.1523
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,537 >>   eval_runtime            = 0:00:09.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,537 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,537 >>   eval_samples_per_second =      377.4
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,538 >>   eval_steps_per_second   =     47.175
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:55:54,538 >>   perplexity              =     3.1656
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:00,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:00,908 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:00,908 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:00,908 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:00,909 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:56:01,535 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:56:01,536 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:56:02,098 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:56:03,148 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:56:03,148 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:06,019 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:06,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:06,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:06,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:56:06,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:56:06,652 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:56:06,654 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:56:07,219 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:56:07,381 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:56:07,381 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-83
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-249
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-415
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/checkpoint-332
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:14,  1.72it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:15,  1.72it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.68it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:20,  1.52it/s]Extractor Predicting: 34it [00:20,  1.55it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:23,  1.66it/s]Extractor Predicting: 40it [00:24,  1.69it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.70it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.72it/s]Extractor Predicting: 48it [00:29,  1.73it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.72it/s]Extractor Predicting: 54it [00:32,  1.71it/s]Extractor Predicting: 55it [00:33,  1.72it/s]Extractor Predicting: 56it [00:33,  1.69it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:36,  1.64it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:38,  1.63it/s]Extractor Predicting: 64it [00:38,  1.64it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:39,  1.61it/s]Extractor Predicting: 67it [00:40,  1.59it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:41,  1.62it/s]Extractor Predicting: 70it [00:42,  1.68it/s]Extractor Predicting: 71it [00:42,  1.66it/s]Extractor Predicting: 72it [00:43,  1.64it/s]Extractor Predicting: 73it [00:44,  1.67it/s]Extractor Predicting: 74it [00:44,  1.65it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.62it/s]Extractor Predicting: 79it [00:47,  1.60it/s]Extractor Predicting: 80it [00:48,  1.61it/s]Extractor Predicting: 81it [00:49,  1.63it/s]Extractor Predicting: 82it [00:49,  1.64it/s]Extractor Predicting: 83it [00:50,  1.67it/s]Extractor Predicting: 84it [00:51,  1.60it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:52,  1.62it/s]Extractor Predicting: 87it [00:52,  1.60it/s]Extractor Predicting: 88it [00:53,  1.65it/s]Extractor Predicting: 89it [00:54,  1.64it/s]Extractor Predicting: 90it [00:54,  1.63it/s]Extractor Predicting: 91it [00:55,  1.68it/s]Extractor Predicting: 92it [00:55,  1.68it/s]Extractor Predicting: 93it [00:56,  1.68it/s]Extractor Predicting: 94it [00:57,  1.66it/s]Extractor Predicting: 95it [00:57,  1.67it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:58,  1.64it/s]Extractor Predicting: 98it [00:59,  1.63it/s]Extractor Predicting: 99it [01:00,  1.65it/s]Extractor Predicting: 100it [01:00,  1.63it/s]Extractor Predicting: 101it [01:01,  1.63it/s]Extractor Predicting: 102it [01:02,  1.59it/s]Extractor Predicting: 103it [01:02,  1.61it/s]Extractor Predicting: 104it [01:03,  1.61it/s]Extractor Predicting: 105it [01:03,  1.62it/s]Extractor Predicting: 106it [01:04,  1.66it/s]Extractor Predicting: 107it [01:05,  1.64it/s]Extractor Predicting: 108it [01:05,  1.66it/s]Extractor Predicting: 109it [01:06,  1.65it/s]Extractor Predicting: 110it [01:06,  1.63it/s]Extractor Predicting: 111it [01:07,  1.63it/s]Extractor Predicting: 112it [01:08,  1.66it/s]Extractor Predicting: 113it [01:08,  1.66it/s]Extractor Predicting: 114it [01:09,  1.51it/s]Extractor Predicting: 115it [01:10,  1.55it/s]Extractor Predicting: 116it [01:10,  1.63it/s]Extractor Predicting: 117it [01:11,  1.62it/s]Extractor Predicting: 118it [01:11,  1.61it/s]Extractor Predicting: 119it [01:12,  1.58it/s]Extractor Predicting: 120it [01:13,  1.61it/s]Extractor Predicting: 121it [01:13,  1.63it/s]Extractor Predicting: 122it [01:14,  1.61it/s]Extractor Predicting: 123it [01:14,  1.61it/s]Extractor Predicting: 124it [01:15,  1.59it/s]Extractor Predicting: 125it [01:16,  1.55it/s]Extractor Predicting: 126it [01:16,  1.60it/s]Extractor Predicting: 127it [01:17,  1.61it/s]Extractor Predicting: 128it [01:18,  1.63it/s]Extractor Predicting: 129it [01:18,  1.64it/s]Extractor Predicting: 130it [01:19,  1.68it/s]Extractor Predicting: 131it [01:19,  1.69it/s]Extractor Predicting: 132it [01:20,  1.68it/s]Extractor Predicting: 133it [01:21,  1.67it/s]Extractor Predicting: 134it [01:21,  1.67it/s]Extractor Predicting: 135it [01:22,  1.67it/s]Extractor Predicting: 136it [01:22,  1.66it/s]Extractor Predicting: 137it [01:23,  1.64it/s]Extractor Predicting: 138it [01:24,  1.63it/s]Extractor Predicting: 139it [01:24,  1.64it/s]Extractor Predicting: 140it [01:25,  1.63it/s]Extractor Predicting: 141it [01:25,  1.63it/s]Extractor Predicting: 142it [01:26,  1.61it/s]Extractor Predicting: 143it [01:27,  1.59it/s]Extractor Predicting: 144it [01:27,  1.77it/s]Extractor Predicting: 144it [01:27,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:43,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:43,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:43,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:43,106 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:43,106 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:57:43,817 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:57:43,818 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:57:44,495 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:57:45,514 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:57:45,514 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:48,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:48,062 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:48,062 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:48,062 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:57:48,062 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:57:48,392 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:57:48,393 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:57:48,657 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:57:48,818 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:57:48,818 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.74it/s]Extractor Predicting: 3it [00:01,  1.81it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.83it/s]Extractor Predicting: 6it [00:03,  1.84it/s]Extractor Predicting: 7it [00:03,  1.86it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:04,  1.88it/s]Extractor Predicting: 10it [00:05,  1.90it/s]Extractor Predicting: 11it [00:05,  1.94it/s]Extractor Predicting: 12it [00:06,  1.89it/s]Extractor Predicting: 13it [00:06,  1.89it/s]Extractor Predicting: 14it [00:07,  1.91it/s]Extractor Predicting: 15it [00:08,  1.91it/s]Extractor Predicting: 16it [00:08,  1.87it/s]Extractor Predicting: 17it [00:09,  1.86it/s]Extractor Predicting: 18it [00:09,  1.90it/s]Extractor Predicting: 19it [00:10,  1.91it/s]Extractor Predicting: 20it [00:10,  1.89it/s]Extractor Predicting: 21it [00:11,  1.93it/s]Extractor Predicting: 22it [00:11,  1.95it/s]Extractor Predicting: 23it [00:12,  1.91it/s]Extractor Predicting: 24it [00:12,  1.87it/s]Extractor Predicting: 25it [00:13,  1.83it/s]Extractor Predicting: 26it [00:13,  1.86it/s]Extractor Predicting: 27it [00:14,  1.88it/s]Extractor Predicting: 28it [00:15,  1.79it/s]Extractor Predicting: 29it [00:15,  1.75it/s]Extractor Predicting: 30it [00:16,  1.68it/s]Extractor Predicting: 31it [00:16,  1.64it/s]Extractor Predicting: 32it [00:17,  1.59it/s]Extractor Predicting: 33it [00:18,  1.57it/s]Extractor Predicting: 34it [00:18,  1.55it/s]Extractor Predicting: 35it [00:19,  1.58it/s]Extractor Predicting: 36it [00:20,  1.59it/s]Extractor Predicting: 37it [00:20,  1.59it/s]Extractor Predicting: 38it [00:21,  1.54it/s]Extractor Predicting: 39it [00:22,  1.55it/s]Extractor Predicting: 40it [00:22,  1.53it/s]Extractor Predicting: 41it [00:23,  1.52it/s]Extractor Predicting: 42it [00:24,  1.53it/s]Extractor Predicting: 43it [00:24,  1.54it/s]Extractor Predicting: 44it [00:25,  1.57it/s]Extractor Predicting: 45it [00:25,  1.56it/s]Extractor Predicting: 46it [00:26,  1.59it/s]Extractor Predicting: 47it [00:27,  1.56it/s]Extractor Predicting: 48it [00:27,  1.54it/s]Extractor Predicting: 49it [00:28,  1.54it/s]Extractor Predicting: 50it [00:29,  1.54it/s]Extractor Predicting: 51it [00:29,  1.53it/s]Extractor Predicting: 52it [00:30,  1.55it/s]Extractor Predicting: 53it [00:31,  1.55it/s]Extractor Predicting: 54it [00:31,  1.56it/s]Extractor Predicting: 55it [00:32,  1.56it/s]Extractor Predicting: 56it [00:33,  1.53it/s]Extractor Predicting: 57it [00:33,  1.56it/s]Extractor Predicting: 58it [00:34,  1.45it/s]Extractor Predicting: 59it [00:35,  1.50it/s]Extractor Predicting: 60it [00:35,  1.54it/s]Extractor Predicting: 61it [00:36,  1.52it/s]Extractor Predicting: 62it [00:37,  1.53it/s]Extractor Predicting: 63it [00:37,  1.57it/s]Extractor Predicting: 64it [00:38,  1.60it/s]Extractor Predicting: 65it [00:38,  1.61it/s]Extractor Predicting: 66it [00:39,  1.64it/s]Extractor Predicting: 67it [00:40,  1.66it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:41,  1.66it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:42,  1.73it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.65it/s]Extractor Predicting: 74it [00:44,  1.62it/s]Extractor Predicting: 75it [00:44,  1.62it/s]Extractor Predicting: 76it [00:45,  1.60it/s]Extractor Predicting: 77it [00:46,  1.60it/s]Extractor Predicting: 78it [00:46,  1.61it/s]Extractor Predicting: 79it [00:47,  1.61it/s]Extractor Predicting: 80it [00:48,  1.59it/s]Extractor Predicting: 81it [00:48,  1.59it/s]Extractor Predicting: 82it [00:49,  1.61it/s]Extractor Predicting: 83it [00:49,  1.61it/s]Extractor Predicting: 84it [00:50,  1.59it/s]Extractor Predicting: 85it [00:51,  1.60it/s]Extractor Predicting: 86it [00:51,  1.63it/s]Extractor Predicting: 87it [00:52,  1.67it/s]Extractor Predicting: 88it [00:52,  1.72it/s]Extractor Predicting: 89it [00:53,  1.75it/s]Extractor Predicting: 90it [00:53,  1.74it/s]Extractor Predicting: 91it [00:54,  1.80it/s]Extractor Predicting: 92it [00:55,  1.77it/s]Extractor Predicting: 93it [00:55,  1.83it/s]Extractor Predicting: 94it [00:56,  1.80it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:57,  1.78it/s]Extractor Predicting: 97it [00:57,  1.82it/s]Extractor Predicting: 98it [00:58,  1.84it/s]Extractor Predicting: 99it [00:58,  1.85it/s]Extractor Predicting: 100it [00:59,  1.90it/s]Extractor Predicting: 101it [00:59,  1.89it/s]Extractor Predicting: 102it [01:00,  1.84it/s]Extractor Predicting: 103it [01:01,  1.81it/s]Extractor Predicting: 104it [01:01,  1.83it/s]Extractor Predicting: 105it [01:02,  1.85it/s]Extractor Predicting: 106it [01:02,  1.84it/s]Extractor Predicting: 107it [01:03,  1.90it/s]Extractor Predicting: 108it [01:03,  1.85it/s]Extractor Predicting: 109it [01:04,  1.87it/s]Extractor Predicting: 110it [01:04,  1.87it/s]Extractor Predicting: 111it [01:05,  1.87it/s]Extractor Predicting: 112it [01:05,  1.89it/s]Extractor Predicting: 113it [01:06,  1.87it/s]Extractor Predicting: 114it [01:06,  1.85it/s]Extractor Predicting: 115it [01:07,  1.81it/s]Extractor Predicting: 116it [01:08,  1.77it/s]Extractor Predicting: 117it [01:08,  1.74it/s]Extractor Predicting: 118it [01:09,  1.74it/s]Extractor Predicting: 119it [01:09,  1.73it/s]Extractor Predicting: 120it [01:10,  1.69it/s]Extractor Predicting: 121it [01:11,  1.72it/s]Extractor Predicting: 122it [01:11,  1.71it/s]Extractor Predicting: 123it [01:12,  1.67it/s]Extractor Predicting: 124it [01:12,  1.73it/s]Extractor Predicting: 125it [01:13,  1.69it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:14,  1.69it/s]Extractor Predicting: 128it [01:15,  1.66it/s]Extractor Predicting: 129it [01:15,  1.68it/s]Extractor Predicting: 130it [01:16,  1.65it/s]Extractor Predicting: 131it [01:17,  1.64it/s]Extractor Predicting: 132it [01:17,  1.64it/s]Extractor Predicting: 133it [01:18,  1.67it/s]Extractor Predicting: 134it [01:18,  1.67it/s]Extractor Predicting: 135it [01:19,  1.65it/s]Extractor Predicting: 136it [01:20,  1.47it/s]Extractor Predicting: 137it [01:20,  1.54it/s]Extractor Predicting: 138it [01:21,  1.55it/s]Extractor Predicting: 139it [01:22,  1.58it/s]Extractor Predicting: 140it [01:22,  1.59it/s]Extractor Predicting: 141it [01:23,  1.62it/s]Extractor Predicting: 142it [01:23,  1.61it/s]Extractor Predicting: 143it [01:24,  2.05it/s]Extractor Predicting: 143it [01:24,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:19,362 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:19,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:19,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:19,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:19,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:59:19,981 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:59:19,982 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:59:20,831 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:59:21,823 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:59:21,823 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:24,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:24,648 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:24,648 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:24,648 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:59:24,648 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:59:25,313 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:59:25,314 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:59:25,896 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:59:26,045 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:59:26,045 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 2it [00:01,  1.34it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:59:28,484 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:59:28,485 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:59:28,547 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:59:28,548 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:59:28,571 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:59:31,840 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:59:31,852 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:59:31,866 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:59:31,867 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:59:31,877 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:59:31,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:59:31,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:59:31,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:59:31,883 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:59:31,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:59:31,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:59:32,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:32,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:33,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:34,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:35,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:36,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:36,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:37,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:38,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:39,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:40,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:40,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:41,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:42,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:43,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:44,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:45,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:45,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:46,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:47,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:48,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:49,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:49,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:50,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:53, 19.30s/it][WARNING|generation_utils.py:914] 2023-08-28 03:59:51,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:52,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:53,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:54,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:55,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:56,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:56,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:57,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:58,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:59:59,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:00,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:00,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:01,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:02,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:03,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:05,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:05,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:06,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:07,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:08,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:09,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:10,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:10,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:39<02:39, 19.94s/it][WARNING|generation_utils.py:914] 2023-08-28 04:00:11,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:12,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:13,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:14,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:15,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:15,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:16,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:17,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:18,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:19,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:20,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:20,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:21,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:22,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:23,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:24,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:24,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:25,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:26,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:27,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:28,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:28,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:57<02:12, 18.90s/it][WARNING|generation_utils.py:914] 2023-08-28 04:00:29,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:30,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:31,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:32,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:33,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:33,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:34,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:35,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:35,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:36,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:37,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:38,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:38,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:39,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:40,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:41,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:41,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:42,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:43,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:44,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:45,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:46,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:47,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:47,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:48,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:49,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:50,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:50,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:51,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:52,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:52,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:53,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:54,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:55,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:55,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:56,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:57,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:57,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:58,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:59,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:00:59,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:00,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:01,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:29<02:25, 24.23s/it][WARNING|generation_utils.py:914] 2023-08-28 04:01:01,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:02,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:03,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:04,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:05,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:05,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:06,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:07,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:08,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:09,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:09,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:10,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:11,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:12,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:12,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:13,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:14,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:15,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:16,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:16,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:17,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:18,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:19,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:20,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:20,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:49<01:52, 22.60s/it][WARNING|generation_utils.py:914] 2023-08-28 04:01:21,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:22,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:23,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:24,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:24,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:25,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:26,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:27,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:28,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:28,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:29,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:30,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:31,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:32,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:32,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:33,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:34,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:35,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:36,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:36,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:37,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:38,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:39,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:40,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:08<01:25, 21.47s/it][WARNING|generation_utils.py:914] 2023-08-28 04:01:40,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:41,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:42,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:43,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:44,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:45,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:45,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:46,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:47,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:48,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:49,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:50,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:50,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:51,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:52,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:53,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:54,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:55,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:56,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:57,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:57,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:58,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:01:59,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:00,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:01,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:01,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:30<01:05, 21.70s/it][WARNING|generation_utils.py:914] 2023-08-28 04:02:03,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:03,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:04,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:05,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:06,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:06,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:07,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:08,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:09,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:09,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:11,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:11,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:12,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:13,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:14,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:14,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:15,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:16,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:16,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:17,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:18,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:19,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:47<00:40, 20.16s/it][WARNING|generation_utils.py:914] 2023-08-28 04:02:19,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:20,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:21,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:22,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:22,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:23,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:24,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:25,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:26,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:27,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:27,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:28,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:29,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:30,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:30,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:31,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:32,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:33,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:34,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:34,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:36,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:37,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:37,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:38,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:07<00:19, 19.95s/it][WARNING|generation_utils.py:914] 2023-08-28 04:02:39,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:40,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:41,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:41,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:42,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:43,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:43,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:44,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:45,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:46,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:46,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:47,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:48,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:49,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:50,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:51,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:52,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:52,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:53,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:54,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:55,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:56,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:56,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:57,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:58,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:02:59,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:27<00:00, 20.18s/it]Generating: 100%|██████████| 10/10 [03:27<00:00, 20.79s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:05,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:05,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:05,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:05,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:05,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:03:06,577 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:03:06,578 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:03:07,149 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:03:08,213 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:03:08,213 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:11,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:11,065 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:11,066 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:11,066 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:03:11,066 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:03:11,777 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:03:11,778 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:03:12,352 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:03:12,508 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:03:12,509 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
{'target': 600, 'success': 9, 'raw': 32}
{'target': 600, 'success': 26, 'raw': 64}
{'target': 600, 'success': 37, 'raw': 96}
{'target': 600, 'success': 54, 'raw': 128}
{'target': 600, 'success': 67, 'raw': 160}
{'target': 600, 'success': 82, 'raw': 192}
{'target': 600, 'success': 97, 'raw': 224}
{'target': 600, 'success': 111, 'raw': 256}
{'target': 600, 'success': 125, 'raw': 288}
{'target': 600, 'success': 137, 'raw': 320}
{'target': 600, 'success': 152, 'raw': 352}
{'target': 600, 'success': 166, 'raw': 384}
{'target': 600, 'success': 180, 'raw': 416}
{'target': 600, 'success': 196, 'raw': 448}
{'target': 600, 'success': 214, 'raw': 480}
{'target': 600, 'success': 229, 'raw': 512}
{'target': 600, 'success': 246, 'raw': 544}
{'target': 600, 'success': 260, 'raw': 576}
{'target': 600, 'success': 273, 'raw': 608}
{'target': 600, 'success': 285, 'raw': 640}
{'target': 600, 'success': 301, 'raw': 672}
{'target': 600, 'success': 315, 'raw': 704}
{'target': 600, 'success': 327, 'raw': 736}
{'target': 600, 'success': 334, 'raw': 768}
{'target': 600, 'success': 355, 'raw': 800}
{'target': 600, 'success': 369, 'raw': 832}
{'target': 600, 'success': 386, 'raw': 864}
{'target': 600, 'success': 399, 'raw': 896}
{'target': 600, 'success': 412, 'raw': 928}
{'target': 600, 'success': 425, 'raw': 960}
{'target': 600, 'success': 433, 'raw': 992}
{'target': 600, 'success': 443, 'raw': 1024}
{'target': 600, 'success': 460, 'raw': 1056}
{'target': 600, 'success': 475, 'raw': 1088}
{'target': 600, 'success': 485, 'raw': 1120}
{'target': 600, 'success': 497, 'raw': 1152}
{'target': 600, 'success': 511, 'raw': 1184}
{'target': 600, 'success': 525, 'raw': 1216}
{'target': 600, 'success': 542, 'raw': 1248}
{'target': 600, 'success': 556, 'raw': 1280}
{'target': 600, 'success': 573, 'raw': 1312}
{'target': 600, 'success': 586, 'raw': 1344}
{'target': 600, 'success': 596, 'raw': 1376}
{'target': 600, 'success': 605, 'raw': 1408}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4296875, 'errors': {'', '(\'Love Is One of the Most Important Facts You Can Learn About Yourself\', \'original language of film or TV show\', \'\', \'On 3 November 2001 , he made the film " Love Is One of the Most Important Facts You Can Learn About Yourself " , about an alcoholic father .\')', '(\'The Hound of the Baskervilles\', \'original language of film or TV show\', \'\', \'His best work came in the 1983 horror movie " The Hound of the Baskervilles " directed by Gene Luen Yang .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 496, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8033854166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 254, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 450, 'raw': 608}
{'target': 600, 'success': 469, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.7271634615384616, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 526, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7525, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 428, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 595, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 11540
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11640, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.39it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:02,  1.33it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.48it/s]Extractor Estimating: 9it [00:06,  1.46it/s]Extractor Estimating: 10it [00:06,  1.48it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:08,  1.47it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.40it/s]Extractor Estimating: 15it [00:10,  1.39it/s]Extractor Estimating: 16it [00:11,  1.43it/s]Extractor Estimating: 17it [00:11,  1.42it/s]Extractor Estimating: 18it [00:12,  1.39it/s]Extractor Estimating: 19it [00:13,  1.43it/s]Extractor Estimating: 20it [00:13,  1.48it/s]Extractor Estimating: 21it [00:14,  1.49it/s]Extractor Estimating: 22it [00:15,  1.51it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.49it/s]Extractor Estimating: 25it [00:17,  1.52it/s]Extractor Estimating: 26it [00:17,  1.49it/s]Extractor Estimating: 27it [00:18,  1.36it/s]Extractor Estimating: 28it [00:19,  1.39it/s]Extractor Estimating: 29it [00:20,  1.42it/s]Extractor Estimating: 30it [00:20,  1.44it/s]Extractor Estimating: 31it [00:21,  1.45it/s]Extractor Estimating: 32it [00:22,  1.50it/s]Extractor Estimating: 33it [00:22,  1.52it/s]Extractor Estimating: 34it [00:23,  1.52it/s]Extractor Estimating: 35it [00:23,  1.52it/s]Extractor Estimating: 36it [00:24,  1.49it/s]Extractor Estimating: 37it [00:25,  1.51it/s]Extractor Estimating: 38it [00:26,  1.44it/s]Extractor Estimating: 39it [00:26,  1.41it/s]Extractor Estimating: 40it [00:27,  1.42it/s]Extractor Estimating: 41it [00:28,  1.41it/s]Extractor Estimating: 42it [00:29,  1.34it/s]Extractor Estimating: 43it [00:29,  1.30it/s]Extractor Estimating: 44it [00:30,  1.33it/s]Extractor Estimating: 45it [00:31,  1.39it/s]Extractor Estimating: 46it [00:32,  1.38it/s]Extractor Estimating: 47it [00:32,  1.42it/s]Extractor Estimating: 48it [00:33,  1.38it/s]Extractor Estimating: 49it [00:34,  1.45it/s]Extractor Estimating: 50it [00:34,  1.45it/s]Extractor Estimating: 51it [00:35,  1.46it/s]Extractor Estimating: 52it [00:36,  1.49it/s]Extractor Estimating: 53it [00:36,  1.50it/s]Extractor Estimating: 54it [00:37,  1.52it/s]Extractor Estimating: 55it [00:38,  1.50it/s]Extractor Estimating: 56it [00:38,  1.52it/s]Extractor Estimating: 57it [00:39,  1.51it/s]Extractor Estimating: 58it [00:40,  1.50it/s]Extractor Estimating: 59it [00:40,  1.52it/s]Extractor Estimating: 60it [00:41,  1.53it/s]Extractor Estimating: 61it [00:41,  1.54it/s]Extractor Estimating: 62it [00:42,  1.52it/s]Extractor Estimating: 63it [00:43,  1.54it/s]Extractor Estimating: 64it [00:43,  1.53it/s]Extractor Estimating: 65it [00:44,  1.51it/s]Extractor Estimating: 66it [00:45,  1.54it/s]Extractor Estimating: 67it [00:45,  1.51it/s]Extractor Estimating: 68it [00:46,  1.46it/s]Extractor Estimating: 69it [00:47,  1.51it/s]Extractor Estimating: 70it [00:47,  1.54it/s]Extractor Estimating: 71it [00:48,  1.54it/s]Extractor Estimating: 72it [00:49,  1.53it/s]Extractor Estimating: 73it [00:49,  1.54it/s]Extractor Estimating: 74it [00:50,  1.50it/s]Extractor Estimating: 75it [00:51,  1.53it/s]Extractor Estimating: 76it [00:51,  1.51it/s]Extractor Estimating: 77it [00:52,  1.52it/s]Extractor Estimating: 78it [00:53,  1.57it/s]Extractor Estimating: 79it [00:53,  1.52it/s]Extractor Estimating: 80it [00:54,  1.53it/s]Extractor Estimating: 81it [00:55,  1.51it/s]Extractor Estimating: 82it [00:55,  1.53it/s]Extractor Estimating: 83it [00:56,  1.56it/s]Extractor Estimating: 84it [00:56,  1.59it/s]Extractor Estimating: 85it [00:57,  1.61it/s]Extractor Estimating: 86it [00:58,  1.50it/s]Extractor Estimating: 87it [00:59,  1.41it/s]Extractor Estimating: 88it [00:59,  1.39it/s]Extractor Estimating: 89it [01:00,  1.31it/s]Extractor Estimating: 90it [01:01,  1.39it/s]Extractor Estimating: 91it [01:02,  1.41it/s]Extractor Estimating: 92it [01:02,  1.49it/s]Extractor Estimating: 93it [01:03,  1.52it/s]Extractor Estimating: 94it [01:03,  1.54it/s]Extractor Estimating: 95it [01:04,  1.44it/s]Extractor Estimating: 96it [01:05,  1.48it/s]Extractor Estimating: 97it [01:05,  1.51it/s]Extractor Estimating: 98it [01:06,  1.53it/s]Extractor Estimating: 99it [01:07,  1.52it/s]Extractor Estimating: 100it [01:07,  1.53it/s]Extractor Estimating: 101it [01:08,  1.50it/s]Extractor Estimating: 102it [01:09,  1.49it/s]Extractor Estimating: 103it [01:09,  1.51it/s]Extractor Estimating: 104it [01:10,  1.47it/s]Extractor Estimating: 105it [01:11,  1.49it/s]Extractor Estimating: 106it [01:11,  1.47it/s]Extractor Estimating: 107it [01:12,  1.51it/s]Extractor Estimating: 108it [01:13,  1.52it/s]Extractor Estimating: 109it [01:13,  1.47it/s]Extractor Estimating: 110it [01:14,  1.48it/s]Extractor Estimating: 111it [01:15,  1.48it/s]Extractor Estimating: 112it [01:16,  1.47it/s]Extractor Estimating: 113it [01:16,  1.48it/s]Extractor Estimating: 114it [01:17,  1.53it/s]Extractor Estimating: 115it [01:17,  1.49it/s]Extractor Estimating: 116it [01:18,  1.51it/s]Extractor Estimating: 117it [01:19,  1.53it/s]Extractor Estimating: 118it [01:19,  1.51it/s]Extractor Estimating: 119it [01:20,  1.53it/s]Extractor Estimating: 120it [01:21,  1.47it/s]Extractor Estimating: 121it [01:22,  1.47it/s]Extractor Estimating: 122it [01:22,  1.50it/s]Extractor Estimating: 123it [01:23,  1.46it/s]Extractor Estimating: 124it [01:24,  1.48it/s]Extractor Estimating: 125it [01:24,  1.47it/s]Extractor Estimating: 126it [01:25,  1.40it/s]Extractor Estimating: 127it [01:26,  1.42it/s]Extractor Estimating: 128it [01:26,  1.44it/s]Extractor Estimating: 129it [01:27,  1.44it/s]Extractor Estimating: 130it [01:28,  1.50it/s]Extractor Estimating: 131it [01:28,  1.51it/s]Extractor Estimating: 132it [01:29,  1.53it/s]Extractor Estimating: 133it [01:30,  1.50it/s]Extractor Estimating: 134it [01:30,  1.51it/s]Extractor Estimating: 135it [01:31,  1.53it/s]Extractor Estimating: 136it [01:32,  1.57it/s]Extractor Estimating: 137it [01:32,  1.54it/s]Extractor Estimating: 138it [01:33,  1.58it/s]Extractor Estimating: 139it [01:33,  1.58it/s]Extractor Estimating: 140it [01:34,  1.57it/s]Extractor Estimating: 141it [01:35,  1.53it/s]Extractor Estimating: 142it [01:35,  1.50it/s]Extractor Estimating: 143it [01:36,  1.52it/s]Extractor Estimating: 144it [01:37,  1.57it/s]Extractor Estimating: 145it [01:37,  1.56it/s]Extractor Estimating: 146it [01:38,  1.56it/s]Extractor Estimating: 147it [01:39,  1.56it/s]Extractor Estimating: 148it [01:39,  1.58it/s]Extractor Estimating: 149it [01:40,  1.57it/s]Extractor Estimating: 150it [01:41,  1.58it/s]Extractor Estimating: 151it [01:41,  1.56it/s]Extractor Estimating: 152it [01:42,  1.57it/s]Extractor Estimating: 153it [01:42,  1.55it/s]Extractor Estimating: 154it [01:43,  1.45it/s]Extractor Estimating: 155it [01:44,  1.43it/s]Extractor Estimating: 156it [01:45,  1.48it/s]Extractor Estimating: 157it [01:45,  1.54it/s]Extractor Estimating: 158it [01:46,  1.54it/s]Extractor Estimating: 159it [01:47,  1.50it/s]Extractor Estimating: 160it [01:47,  1.49it/s]Extractor Estimating: 161it [01:48,  1.46it/s]Extractor Estimating: 162it [01:49,  1.44it/s]Extractor Estimating: 163it [01:49,  1.43it/s]Extractor Estimating: 164it [01:50,  1.43it/s]Extractor Estimating: 165it [01:51,  1.42it/s]Extractor Estimating: 166it [01:52,  1.27it/s]Extractor Estimating: 167it [01:53,  1.28it/s]Extractor Estimating: 168it [01:53,  1.31it/s]Extractor Estimating: 169it [01:54,  1.37it/s]Extractor Estimating: 170it [01:55,  1.39it/s]Extractor Estimating: 171it [01:55,  1.42it/s]Extractor Estimating: 172it [01:56,  1.40it/s]Extractor Estimating: 173it [01:57,  1.41it/s]Extractor Estimating: 174it [01:57,  1.44it/s]Extractor Estimating: 175it [01:58,  1.35it/s]Extractor Estimating: 176it [01:59,  1.42it/s]Extractor Estimating: 177it [02:00,  1.44it/s]Extractor Estimating: 178it [02:00,  1.41it/s]Extractor Estimating: 179it [02:01,  1.44it/s]Extractor Estimating: 180it [02:02,  1.44it/s]Extractor Estimating: 181it [02:02,  1.44it/s]Extractor Estimating: 182it [02:03,  1.45it/s]Extractor Estimating: 183it [02:04,  1.49it/s]Extractor Estimating: 184it [02:04,  1.50it/s]Extractor Estimating: 185it [02:05,  1.58it/s]Extractor Estimating: 186it [02:05,  1.56it/s]Extractor Estimating: 187it [02:06,  1.50it/s]Extractor Estimating: 188it [02:07,  1.52it/s]Extractor Estimating: 189it [02:07,  1.54it/s]Extractor Estimating: 190it [02:08,  1.55it/s]Extractor Estimating: 191it [02:09,  1.50it/s]Extractor Estimating: 192it [02:09,  1.53it/s]Extractor Estimating: 193it [02:10,  1.51it/s]Extractor Estimating: 194it [02:11,  1.50it/s]Extractor Estimating: 195it [02:11,  1.52it/s]Extractor Estimating: 196it [02:12,  1.56it/s]Extractor Estimating: 197it [02:13,  1.54it/s]Extractor Estimating: 198it [02:13,  1.53it/s]Extractor Estimating: 199it [02:14,  1.53it/s]Extractor Estimating: 200it [02:15,  1.52it/s]Extractor Estimating: 201it [02:15,  1.45it/s]Extractor Estimating: 202it [02:16,  1.47it/s]Extractor Estimating: 203it [02:17,  1.50it/s]Extractor Estimating: 204it [02:17,  1.49it/s]Extractor Estimating: 205it [02:18,  1.53it/s]Extractor Estimating: 206it [02:19,  1.50it/s]Extractor Estimating: 207it [02:19,  1.49it/s]Extractor Estimating: 208it [02:20,  1.49it/s]Extractor Estimating: 209it [02:21,  1.47it/s]Extractor Estimating: 210it [02:21,  1.49it/s]Extractor Estimating: 211it [02:22,  1.45it/s]Extractor Estimating: 212it [02:23,  1.46it/s]Extractor Estimating: 213it [02:23,  1.50it/s]Extractor Estimating: 214it [02:24,  1.48it/s]Extractor Estimating: 215it [02:25,  1.47it/s]Extractor Estimating: 216it [02:26,  1.44it/s]Extractor Estimating: 217it [02:26,  1.45it/s]Extractor Estimating: 218it [02:27,  1.47it/s]Extractor Estimating: 219it [02:28,  1.49it/s]Extractor Estimating: 220it [02:28,  1.46it/s]Extractor Estimating: 221it [02:29,  1.47it/s]Extractor Estimating: 222it [02:30,  1.55it/s]Extractor Estimating: 223it [02:30,  1.50it/s]Extractor Estimating: 224it [02:31,  1.49it/s]Extractor Estimating: 225it [02:32,  1.49it/s]Extractor Estimating: 226it [02:32,  1.48it/s]Extractor Estimating: 227it [02:33,  1.35it/s]Extractor Estimating: 228it [02:34,  1.35it/s]Extractor Estimating: 229it [02:35,  1.41it/s]Extractor Estimating: 230it [02:35,  1.44it/s]Extractor Estimating: 231it [02:36,  1.49it/s]Extractor Estimating: 232it [02:37,  1.47it/s]Extractor Estimating: 233it [02:37,  1.46it/s]Extractor Estimating: 234it [02:38,  1.46it/s]Extractor Estimating: 235it [02:39,  1.50it/s]Extractor Estimating: 236it [02:39,  1.45it/s]Extractor Estimating: 237it [02:40,  1.42it/s]Extractor Estimating: 238it [02:41,  1.47it/s]Extractor Estimating: 239it [02:41,  1.45it/s]Extractor Estimating: 240it [02:42,  1.45it/s]Extractor Estimating: 241it [02:43,  1.39it/s]Extractor Estimating: 242it [02:44,  1.39it/s]Extractor Estimating: 243it [02:44,  1.42it/s]Extractor Estimating: 244it [02:45,  1.49it/s]Extractor Estimating: 245it [02:46,  1.43it/s]Extractor Estimating: 246it [02:46,  1.45it/s]Extractor Estimating: 247it [02:47,  1.48it/s]Extractor Estimating: 248it [02:48,  1.46it/s]Extractor Estimating: 249it [02:48,  1.48it/s]Extractor Estimating: 250it [02:49,  1.43it/s]Extractor Estimating: 250it [02:49,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:13,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:13,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:13,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:13,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:13,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:06:14,321 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:06:14,322 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:06:14,955 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:06:16,002 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:06:16,002 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:18,983 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:18,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:18,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:18,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:06:18,992 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:06:19,650 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:06:19,651 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:06:20,227 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:06:20,391 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:06:20,391 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 05:48:30,109 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 05:48:30,139 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5236 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 20687
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20787, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20787, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.201, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.190, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.194, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.187, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.222, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 2.315, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.186, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.209, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.186, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.205, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 2.301, loss:nan
g_step 1200, step 105, avg_time 1.201, loss:nan
g_step 1300, step 205, avg_time 1.210, loss:nan
g_step 1400, step 86, avg_time 1.203, loss:nan
g_step 1500, step 186, avg_time 1.184, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 2.335, loss:nan
g_step 1700, step 167, avg_time 1.174, loss:nan
g_step 1800, step 48, avg_time 1.206, loss:nan
g_step 1900, step 148, avg_time 1.188, loss:nan
g_step 2000, step 29, avg_time 1.184, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 2.340, loss:nan
g_step 2200, step 10, avg_time 1.192, loss:nan
g_step 2300, step 110, avg_time 1.182, loss:nan
g_step 2400, step 210, avg_time 1.204, loss:nan
g_step 2500, step 91, avg_time 1.174, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 2.334, loss:nan
g_step 2700, step 72, avg_time 1.175, loss:nan
g_step 2800, step 172, avg_time 1.214, loss:nan
g_step 2900, step 53, avg_time 1.197, loss:nan
g_step 3000, step 153, avg_time 1.192, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 2.306, loss:nan
g_step 3200, step 134, avg_time 1.179, loss:nan
g_step 3300, step 15, avg_time 1.198, loss:nan
g_step 3400, step 115, avg_time 1.205, loss:nan
g_step 3500, step 215, avg_time 1.181, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 2.320, loss:nan
g_step 3700, step 196, avg_time 1.195, loss:nan
g_step 3800, step 77, avg_time 1.176, loss:nan
g_step 3900, step 177, avg_time 1.198, loss:nan
g_step 4000, step 58, avg_time 1.182, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 2.307, loss:nan
g_step 4200, step 39, avg_time 1.204, loss:nan
g_step 4300, step 139, avg_time 1.184, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 05:48:30 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 05:48:30 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_05-48-30_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 05:48:31 - WARNING - datasets.builder -   Using custom data configuration default-7f4380b1d29f7a10
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7f4380b1d29f7a10/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 05:48:31,408 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:48:31,410 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:48:31,410 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:48:31,411 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:48:31,420 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:48:31,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:48:31,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:48:31,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:48:31,436 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:48:31,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:48:31,436 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 05:48:31,575 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:48:34,748 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 05:48:34,751 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7f4380b1d29f7a10/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.26ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.01ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.52ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.87ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.12ba/s]100%|██████████| 6/6 [00:01<00:00,  4.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.21ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.32ba/s]100%|██████████| 4/4 [00:00<00:00,  5.38ba/s]100%|██████████| 4/4 [00:00<00:00,  4.88ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.73ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.77ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  9.77ba/s]100%|██████████| 6/6 [00:00<00:00, 12.13ba/s]100%|██████████| 6/6 [00:00<00:00, 11.16ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.43ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.57ba/s]100%|██████████| 4/4 [00:00<00:00, 10.89ba/s]
[INFO|trainer.py:414] 2023-08-28 05:48:38,222 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 05:48:38,236 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 05:48:38,236 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 05:48:38,236 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 05:48:38,236 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 05:48:38,236 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 05:48:38,236 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 05:48:38,236 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.47it/s]  0%|          | 2/410 [00:00<01:53,  3.59it/s]  1%|          | 3/410 [00:00<01:51,  3.64it/s]  1%|          | 4/410 [00:01<01:51,  3.66it/s]  1%|          | 5/410 [00:01<01:50,  3.67it/s]  1%|▏         | 6/410 [00:01<01:49,  3.68it/s]  2%|▏         | 7/410 [00:01<01:49,  3.67it/s]  2%|▏         | 8/410 [00:02<01:49,  3.68it/s]  2%|▏         | 9/410 [00:02<01:49,  3.67it/s]  2%|▏         | 10/410 [00:02<01:48,  3.67it/s]  3%|▎         | 11/410 [00:03<01:48,  3.67it/s]  3%|▎         | 12/410 [00:03<01:48,  3.67it/s]  3%|▎         | 13/410 [00:03<01:48,  3.67it/s]  3%|▎         | 14/410 [00:03<01:47,  3.68it/s]  4%|▎         | 15/410 [00:04<01:47,  3.67it/s]  4%|▍         | 16/410 [00:04<01:47,  3.68it/s]  4%|▍         | 17/410 [00:04<01:46,  3.68it/s]  4%|▍         | 18/410 [00:04<01:46,  3.68it/s]  5%|▍         | 19/410 [00:05<01:46,  3.68it/s]  5%|▍         | 20/410 [00:05<01:46,  3.66it/s]  5%|▌         | 21/410 [00:05<01:46,  3.67it/s]  5%|▌         | 22/410 [00:06<01:45,  3.67it/s]  6%|▌         | 23/410 [00:06<01:45,  3.67it/s]  6%|▌         | 24/410 [00:06<01:45,  3.67it/s]  6%|▌         | 25/410 [00:06<01:44,  3.67it/s]  6%|▋         | 26/410 [00:07<01:44,  3.67it/s]  7%|▋         | 27/410 [00:07<01:44,  3.68it/s]  7%|▋         | 28/410 [00:07<01:43,  3.68it/s]  7%|▋         | 29/410 [00:07<01:43,  3.67it/s]  7%|▋         | 30/410 [00:08<01:43,  3.68it/s]  8%|▊         | 31/410 [00:08<01:43,  3.68it/s]  8%|▊         | 32/410 [00:08<01:42,  3.67it/s]  8%|▊         | 33/410 [00:08<01:42,  3.67it/s]  8%|▊         | 34/410 [00:09<01:42,  3.67it/s]  9%|▊         | 35/410 [00:09<01:42,  3.67it/s]  9%|▉         | 36/410 [00:09<01:41,  3.67it/s]  9%|▉         | 37/410 [00:10<01:41,  3.67it/s]  9%|▉         | 38/410 [00:10<01:41,  3.67it/s] 10%|▉         | 39/410 [00:10<01:41,  3.66it/s] 10%|▉         | 40/410 [00:10<01:40,  3.67it/s] 10%|█         | 41/410 [00:11<01:40,  3.67it/s] 10%|█         | 42/410 [00:11<01:40,  3.67it/s] 10%|█         | 43/410 [00:11<01:40,  3.67it/s] 11%|█         | 44/410 [00:11<01:39,  3.67it/s] 11%|█         | 45/410 [00:12<01:39,  3.67it/s] 11%|█         | 46/410 [00:12<01:39,  3.67it/s] 11%|█▏        | 47/410 [00:12<01:38,  3.67it/s] 12%|█▏        | 48/410 [00:13<01:38,  3.67it/s] 12%|█▏        | 49/410 [00:13<01:38,  3.67it/s] 12%|█▏        | 50/410 [00:13<01:38,  3.67it/s] 12%|█▏        | 51/410 [00:13<01:37,  3.67it/s] 13%|█▎        | 52/410 [00:14<01:37,  3.67it/s] 13%|█▎        | 53/410 [00:14<01:37,  3.67it/s] 13%|█▎        | 54/410 [00:14<01:36,  3.67it/s] 13%|█▎        | 55/410 [00:14<01:36,  3.67it/s] 14%|█▎        | 56/410 [00:15<01:36,  3.68it/s] 14%|█▍        | 57/410 [00:15<01:35,  3.68it/s] 14%|█▍        | 58/410 [00:15<01:35,  3.67it/s] 14%|█▍        | 59/410 [00:16<01:35,  3.67it/s] 15%|█▍        | 60/410 [00:16<01:35,  3.67it/s] 15%|█▍        | 61/410 [00:16<01:34,  3.68it/s] 15%|█▌        | 62/410 [00:16<01:34,  3.68it/s] 15%|█▌        | 63/410 [00:17<01:34,  3.68it/s] 16%|█▌        | 64/410 [00:17<01:34,  3.68it/s] 16%|█▌        | 65/410 [00:17<01:33,  3.68it/s] 16%|█▌        | 66/410 [00:17<01:33,  3.68it/s] 16%|█▋        | 67/410 [00:18<01:33,  3.68it/s] 17%|█▋        | 68/410 [00:18<01:33,  3.68it/s] 17%|█▋        | 69/410 [00:18<01:32,  3.68it/s] 17%|█▋        | 70/410 [00:19<01:32,  3.67it/s] 17%|█▋        | 71/410 [00:19<01:32,  3.67it/s] 18%|█▊        | 72/410 [00:19<01:32,  3.66it/s] 18%|█▊        | 73/410 [00:19<01:31,  3.67it/s] 18%|█▊        | 74/410 [00:20<01:31,  3.67it/s] 18%|█▊        | 75/410 [00:20<01:31,  3.67it/s] 19%|█▊        | 76/410 [00:20<01:30,  3.68it/s] 19%|█▉        | 77/410 [00:20<01:30,  3.67it/s] 19%|█▉        | 78/410 [00:21<01:30,  3.68it/s] 19%|█▉        | 79/410 [00:21<01:30,  3.68it/s] 20%|█▉        | 80/410 [00:21<01:29,  3.68it/s] 20%|█▉        | 81/410 [00:22<01:29,  3.68it/s] 20%|██        | 82/410 [00:22<01:26,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 05:49:00,545 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:49:00,545 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 05:49:00,545 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.57it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.67it/s][A
  5%|▌         | 24/437 [00:00<00:08, 48.86it/s][A
  7%|▋         | 29/437 [00:00<00:08, 48.50it/s][A
  8%|▊         | 34/437 [00:00<00:08, 48.27it/s][A
  9%|▉         | 39/437 [00:00<00:08, 48.12it/s][A
 10%|█         | 44/437 [00:00<00:08, 47.80it/s][A
 11%|█         | 49/437 [00:01<00:08, 47.61it/s][A
 12%|█▏        | 54/437 [00:01<00:08, 47.67it/s][A
 14%|█▎        | 59/437 [00:01<00:07, 47.67it/s][A
 15%|█▍        | 64/437 [00:01<00:07, 47.67it/s][A
 16%|█▌        | 69/437 [00:01<00:07, 47.65it/s][A
 17%|█▋        | 74/437 [00:01<00:07, 47.67it/s][A
 18%|█▊        | 79/437 [00:01<00:07, 47.61it/s][A
 19%|█▉        | 84/437 [00:01<00:07, 47.65it/s][A
 20%|██        | 89/437 [00:01<00:07, 47.61it/s][A
 22%|██▏       | 94/437 [00:01<00:07, 47.59it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 47.61it/s][A
 24%|██▍       | 104/437 [00:02<00:06, 47.59it/s][A
 25%|██▍       | 109/437 [00:02<00:06, 47.61it/s][A
 26%|██▌       | 114/437 [00:02<00:06, 47.65it/s][A
 27%|██▋       | 119/437 [00:02<00:06, 47.62it/s][A
 28%|██▊       | 124/437 [00:02<00:06, 47.35it/s][A
 30%|██▉       | 129/437 [00:02<00:06, 47.28it/s][A
 31%|███       | 134/437 [00:02<00:06, 47.45it/s][A
 32%|███▏      | 139/437 [00:02<00:06, 47.46it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 47.46it/s][A
 34%|███▍      | 149/437 [00:03<00:06, 47.46it/s][A
 35%|███▌      | 154/437 [00:03<00:05, 47.55it/s][A
 36%|███▋      | 159/437 [00:03<00:05, 47.56it/s][A
 38%|███▊      | 164/437 [00:03<00:05, 47.58it/s][A
 39%|███▊      | 169/437 [00:03<00:05, 47.60it/s][A
 40%|███▉      | 174/437 [00:03<00:05, 47.47it/s][A
 41%|████      | 179/437 [00:03<00:05, 47.47it/s][A
 42%|████▏     | 184/437 [00:03<00:05, 47.49it/s][A
 43%|████▎     | 189/437 [00:03<00:05, 47.60it/s][A
 44%|████▍     | 194/437 [00:04<00:05, 47.58it/s][A
 46%|████▌     | 199/437 [00:04<00:05, 47.52it/s][A
 47%|████▋     | 204/437 [00:04<00:04, 47.53it/s][A
 48%|████▊     | 209/437 [00:04<00:04, 47.61it/s][A
 49%|████▉     | 214/437 [00:04<00:04, 47.61it/s][A
 50%|█████     | 219/437 [00:04<00:04, 47.57it/s][A
 51%|█████▏    | 224/437 [00:04<00:04, 47.46it/s][A
 52%|█████▏    | 229/437 [00:04<00:04, 47.47it/s][A
 54%|█████▎    | 234/437 [00:04<00:04, 47.47it/s][A
 55%|█████▍    | 239/437 [00:05<00:04, 47.48it/s][A
 56%|█████▌    | 244/437 [00:05<00:04, 47.53it/s][A
 57%|█████▋    | 249/437 [00:05<00:03, 47.45it/s][A
 58%|█████▊    | 254/437 [00:05<00:03, 47.53it/s][A
 59%|█████▉    | 259/437 [00:05<00:03, 47.51it/s][A
 60%|██████    | 264/437 [00:05<00:03, 47.45it/s][A
 62%|██████▏   | 269/437 [00:05<00:03, 47.36it/s][A
 63%|██████▎   | 274/437 [00:05<00:03, 47.37it/s][A
 64%|██████▍   | 279/437 [00:05<00:03, 47.45it/s][A
 65%|██████▍   | 284/437 [00:05<00:03, 47.52it/s][A
 66%|██████▌   | 289/437 [00:06<00:03, 47.42it/s][A
 67%|██████▋   | 294/437 [00:06<00:03, 47.42it/s][A
 68%|██████▊   | 299/437 [00:06<00:02, 47.52it/s][A
 70%|██████▉   | 304/437 [00:06<00:02, 47.54it/s][A
 71%|███████   | 309/437 [00:06<00:02, 47.51it/s][A
 72%|███████▏  | 314/437 [00:06<00:02, 47.46it/s][A
 73%|███████▎  | 319/437 [00:06<00:02, 47.49it/s][A
 74%|███████▍  | 324/437 [00:06<00:02, 47.44it/s][A
 75%|███████▌  | 329/437 [00:06<00:02, 47.51it/s][A
 76%|███████▋  | 334/437 [00:07<00:02, 47.48it/s][A
 78%|███████▊  | 339/437 [00:07<00:02, 47.50it/s][A
 79%|███████▊  | 344/437 [00:07<00:01, 47.44it/s][A
 80%|███████▉  | 349/437 [00:07<00:01, 47.36it/s][A
 81%|████████  | 354/437 [00:07<00:01, 47.49it/s][A
 82%|████████▏ | 359/437 [00:07<00:01, 47.55it/s][A
 83%|████████▎ | 364/437 [00:07<00:01, 47.48it/s][A
 84%|████████▍ | 369/437 [00:07<00:01, 47.39it/s][A
 86%|████████▌ | 374/437 [00:07<00:01, 47.53it/s][A
 87%|████████▋ | 379/437 [00:07<00:01, 47.52it/s][A
 88%|████████▊ | 384/437 [00:08<00:01, 47.46it/s][A
 89%|████████▉ | 389/437 [00:08<00:01, 47.45it/s][A
 90%|█████████ | 394/437 [00:08<00:00, 47.50it/s][A
 91%|█████████▏| 399/437 [00:08<00:00, 47.41it/s][A
 92%|█████████▏| 404/437 [00:08<00:00, 47.42it/s][A
 94%|█████████▎| 409/437 [00:08<00:00, 47.43it/s][A
 95%|█████████▍| 414/437 [00:08<00:00, 47.49it/s][A
 96%|█████████▌| 419/437 [00:08<00:00, 47.56it/s][A
 97%|█████████▋| 424/437 [00:08<00:00, 47.53it/s][A
 98%|█████████▊| 429/437 [00:09<00:00, 47.52it/s][A
 99%|█████████▉| 434/437 [00:09<00:00, 47.51it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.51it/s][A 20%|██        | 82/410 [00:31<01:26,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:49:09,767 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 05:49:09,786 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:49:12,297 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:49:12,313 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:49:12,326 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:34<21:14,  3.90s/it] 20%|██        | 84/410 [00:34<15:16,  2.81s/it] 21%|██        | 85/410 [00:35<11:05,  2.05s/it] 21%|██        | 86/410 [00:35<08:11,  1.52s/it] 21%|██        | 87/410 [00:35<06:09,  1.14s/it] 21%|██▏       | 88/410 [00:36<04:44,  1.13it/s] 22%|██▏       | 89/410 [00:36<03:44,  1.43it/s] 22%|██▏       | 90/410 [00:36<03:02,  1.75it/s] 22%|██▏       | 91/410 [00:36<02:33,  2.07it/s] 22%|██▏       | 92/410 [00:37<02:13,  2.39it/s] 23%|██▎       | 93/410 [00:37<01:58,  2.67it/s] 23%|██▎       | 94/410 [00:37<01:48,  2.90it/s] 23%|██▎       | 95/410 [00:37<01:42,  3.07it/s] 23%|██▎       | 96/410 [00:38<01:37,  3.23it/s] 24%|██▎       | 97/410 [00:38<01:33,  3.34it/s] 24%|██▍       | 98/410 [00:38<01:30,  3.44it/s] 24%|██▍       | 99/410 [00:39<01:28,  3.50it/s] 24%|██▍       | 100/410 [00:39<01:27,  3.55it/s] 25%|██▍       | 101/410 [00:39<01:26,  3.58it/s] 25%|██▍       | 102/410 [00:39<01:25,  3.61it/s] 25%|██▌       | 103/410 [00:40<01:24,  3.62it/s] 25%|██▌       | 104/410 [00:40<01:24,  3.63it/s] 26%|██▌       | 105/410 [00:40<01:23,  3.65it/s] 26%|██▌       | 106/410 [00:40<01:23,  3.63it/s] 26%|██▌       | 107/410 [00:41<01:23,  3.64it/s] 26%|██▋       | 108/410 [00:41<01:22,  3.65it/s] 27%|██▋       | 109/410 [00:41<01:22,  3.65it/s] 27%|██▋       | 110/410 [00:42<01:21,  3.66it/s] 27%|██▋       | 111/410 [00:42<01:21,  3.66it/s] 27%|██▋       | 112/410 [00:42<01:21,  3.66it/s] 28%|██▊       | 113/410 [00:42<01:21,  3.66it/s] 28%|██▊       | 114/410 [00:43<01:20,  3.66it/s] 28%|██▊       | 115/410 [00:43<01:20,  3.66it/s] 28%|██▊       | 116/410 [00:43<01:20,  3.66it/s] 29%|██▊       | 117/410 [00:43<01:19,  3.67it/s] 29%|██▉       | 118/410 [00:44<01:19,  3.66it/s] 29%|██▉       | 119/410 [00:44<01:19,  3.66it/s] 29%|██▉       | 120/410 [00:44<01:19,  3.66it/s] 30%|██▉       | 121/410 [00:45<01:18,  3.66it/s] 30%|██▉       | 122/410 [00:45<01:18,  3.67it/s] 30%|███       | 123/410 [00:45<01:18,  3.67it/s] 30%|███       | 124/410 [00:45<01:18,  3.67it/s] 30%|███       | 125/410 [00:46<01:17,  3.67it/s] 31%|███       | 126/410 [00:46<01:17,  3.66it/s] 31%|███       | 127/410 [00:46<01:17,  3.66it/s] 31%|███       | 128/410 [00:46<01:17,  3.66it/s] 31%|███▏      | 129/410 [00:47<01:16,  3.66it/s] 32%|███▏      | 130/410 [00:47<01:16,  3.66it/s] 32%|███▏      | 131/410 [00:47<01:16,  3.66it/s] 32%|███▏      | 132/410 [00:48<01:15,  3.66it/s] 32%|███▏      | 133/410 [00:48<01:15,  3.66it/s] 33%|███▎      | 134/410 [00:48<01:15,  3.66it/s] 33%|███▎      | 135/410 [00:48<01:15,  3.66it/s] 33%|███▎      | 136/410 [00:49<01:14,  3.66it/s] 33%|███▎      | 137/410 [00:49<01:14,  3.66it/s] 34%|███▎      | 138/410 [00:49<01:14,  3.66it/s] 34%|███▍      | 139/410 [00:49<01:14,  3.66it/s] 34%|███▍      | 140/410 [00:50<01:13,  3.66it/s] 34%|███▍      | 141/410 [00:50<01:13,  3.66it/s] 35%|███▍      | 142/410 [00:50<01:13,  3.66it/s] 35%|███▍      | 143/410 [00:51<01:13,  3.65it/s] 35%|███▌      | 144/410 [00:51<01:12,  3.65it/s] 35%|███▌      | 145/410 [00:51<01:12,  3.65it/s] 36%|███▌      | 146/410 [00:51<01:15,  3.52it/s] 36%|███▌      | 147/410 [00:52<01:14,  3.53it/s] 36%|███▌      | 148/410 [00:52<01:13,  3.55it/s] 36%|███▋      | 149/410 [00:52<01:12,  3.59it/s] 37%|███▋      | 150/410 [00:53<01:12,  3.61it/s] 37%|███▋      | 151/410 [00:53<01:11,  3.62it/s] 37%|███▋      | 152/410 [00:53<01:11,  3.63it/s] 37%|███▋      | 153/410 [00:53<01:10,  3.65it/s] 38%|███▊      | 154/410 [00:54<01:10,  3.65it/s] 38%|███▊      | 155/410 [00:54<01:09,  3.65it/s] 38%|███▊      | 156/410 [00:54<01:09,  3.65it/s] 38%|███▊      | 157/410 [00:54<01:09,  3.65it/s] 39%|███▊      | 158/410 [00:55<01:08,  3.66it/s] 39%|███▉      | 159/410 [00:55<01:09,  3.64it/s] 39%|███▉      | 160/410 [00:55<01:08,  3.64it/s] 39%|███▉      | 161/410 [00:56<01:08,  3.65it/s] 40%|███▉      | 162/410 [00:56<01:07,  3.65it/s] 40%|███▉      | 163/410 [00:56<01:07,  3.65it/s] 40%|████      | 164/410 [00:56<01:04,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 05:49:35,073 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:49:35,073 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 05:49:35,073 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.1993, 'eval_samples_per_second': 380.028, 'eval_steps_per_second': 47.503, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.75it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.19it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.52it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.84it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.33it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.10it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.79it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.60it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.44it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.50it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.47it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.42it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.47it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.41it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.39it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.39it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.37it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.29it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.22it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.25it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.30it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.31it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.35it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.45it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.44it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.29it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.30it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.30it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.34it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.27it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.37it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.33it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.34it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.40it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.42it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.35it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.25it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.30it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.33it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.36it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.31it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.37it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.28it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.31it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.26it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.23it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.32it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.37it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.36it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.30it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.33it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.35it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.36it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.30it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.21it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.17it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.23it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.34it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.38it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.37it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.34it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.30it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.28it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.27it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.25it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.27it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.32it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.36it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.35it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.36it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.29it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.16it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.20it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.28it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.32it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.33it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.40it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.26it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.33it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.27it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.27it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.24it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.23it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.23it/s][A 40%|████      | 164/410 [01:06<01:04,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:49:44,336 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 05:49:44,359 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:49:46,706 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:49:46,723 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:49:46,732 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:09<15:42,  3.85s/it] 40%|████      | 166/410 [01:09<11:17,  2.78s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:09<08:12,  2.03s/it] 41%|████      | 168/410 [01:09<06:02,  1.50s/it] 41%|████      | 169/410 [01:10<04:32,  1.13s/it] 41%|████▏     | 170/410 [01:10<03:29,  1.14it/s] 42%|████▏     | 171/410 [01:10<02:45,  1.44it/s] 42%|████▏     | 172/410 [01:10<02:15,  1.76it/s] 42%|████▏     | 173/410 [01:11<01:53,  2.09it/s] 42%|████▏     | 174/410 [01:11<01:38,  2.39it/s] 43%|████▎     | 175/410 [01:11<01:28,  2.67it/s] 43%|████▎     | 176/410 [01:12<01:20,  2.90it/s] 43%|████▎     | 177/410 [01:12<01:15,  3.10it/s] 43%|████▎     | 178/410 [01:12<01:11,  3.25it/s] 44%|████▎     | 179/410 [01:12<01:08,  3.36it/s] 44%|████▍     | 180/410 [01:13<01:06,  3.45it/s] 44%|████▍     | 181/410 [01:13<01:05,  3.51it/s] 44%|████▍     | 182/410 [01:13<01:04,  3.56it/s] 45%|████▍     | 183/410 [01:13<01:03,  3.59it/s] 45%|████▍     | 184/410 [01:14<01:02,  3.61it/s] 45%|████▌     | 185/410 [01:14<01:02,  3.62it/s] 45%|████▌     | 186/410 [01:14<01:01,  3.63it/s] 46%|████▌     | 187/410 [01:15<01:01,  3.64it/s] 46%|████▌     | 188/410 [01:15<01:00,  3.65it/s] 46%|████▌     | 189/410 [01:15<01:00,  3.65it/s] 46%|████▋     | 190/410 [01:15<01:00,  3.65it/s] 47%|████▋     | 191/410 [01:16<00:59,  3.66it/s] 47%|████▋     | 192/410 [01:16<00:59,  3.66it/s] 47%|████▋     | 193/410 [01:16<00:59,  3.66it/s] 47%|████▋     | 194/410 [01:16<00:58,  3.67it/s] 48%|████▊     | 195/410 [01:17<00:58,  3.66it/s] 48%|████▊     | 196/410 [01:17<00:58,  3.67it/s] 48%|████▊     | 197/410 [01:17<00:58,  3.66it/s] 48%|████▊     | 198/410 [01:18<00:57,  3.66it/s] 49%|████▊     | 199/410 [01:18<00:57,  3.66it/s] 49%|████▉     | 200/410 [01:18<00:57,  3.66it/s] 49%|████▉     | 201/410 [01:18<00:57,  3.66it/s] 49%|████▉     | 202/410 [01:19<00:56,  3.66it/s] 50%|████▉     | 203/410 [01:19<00:56,  3.66it/s] 50%|████▉     | 204/410 [01:19<00:56,  3.66it/s] 50%|█████     | 205/410 [01:19<00:56,  3.66it/s] 50%|█████     | 206/410 [01:20<00:55,  3.66it/s] 50%|█████     | 207/410 [01:20<00:55,  3.66it/s] 51%|█████     | 208/410 [01:20<00:55,  3.66it/s] 51%|█████     | 209/410 [01:21<00:54,  3.66it/s] 51%|█████     | 210/410 [01:21<00:54,  3.66it/s] 51%|█████▏    | 211/410 [01:21<00:54,  3.66it/s] 52%|█████▏    | 212/410 [01:21<00:54,  3.66it/s] 52%|█████▏    | 213/410 [01:22<00:53,  3.66it/s] 52%|█████▏    | 214/410 [01:22<00:53,  3.66it/s] 52%|█████▏    | 215/410 [01:22<00:53,  3.65it/s] 53%|█████▎    | 216/410 [01:22<00:53,  3.65it/s] 53%|█████▎    | 217/410 [01:23<00:52,  3.66it/s] 53%|█████▎    | 218/410 [01:23<00:52,  3.66it/s] 53%|█████▎    | 219/410 [01:23<00:52,  3.64it/s] 54%|█████▎    | 220/410 [01:24<00:52,  3.64it/s] 54%|█████▍    | 221/410 [01:24<00:51,  3.64it/s] 54%|█████▍    | 222/410 [01:24<00:51,  3.65it/s] 54%|█████▍    | 223/410 [01:24<00:51,  3.65it/s] 55%|█████▍    | 224/410 [01:25<00:50,  3.65it/s] 55%|█████▍    | 225/410 [01:25<00:50,  3.66it/s] 55%|█████▌    | 226/410 [01:25<00:50,  3.66it/s] 55%|█████▌    | 227/410 [01:25<00:50,  3.66it/s] 56%|█████▌    | 228/410 [01:26<00:49,  3.66it/s] 56%|█████▌    | 229/410 [01:26<00:49,  3.65it/s] 56%|█████▌    | 230/410 [01:26<00:49,  3.64it/s] 56%|█████▋    | 231/410 [01:27<00:49,  3.65it/s] 57%|█████▋    | 232/410 [01:27<00:48,  3.65it/s] 57%|█████▋    | 233/410 [01:27<00:48,  3.66it/s] 57%|█████▋    | 234/410 [01:27<00:48,  3.65it/s] 57%|█████▋    | 235/410 [01:28<00:47,  3.66it/s] 58%|█████▊    | 236/410 [01:28<00:47,  3.66it/s] 58%|█████▊    | 237/410 [01:28<00:47,  3.66it/s] 58%|█████▊    | 238/410 [01:29<00:47,  3.66it/s] 58%|█████▊    | 239/410 [01:29<00:46,  3.66it/s] 59%|█████▊    | 240/410 [01:29<00:46,  3.66it/s] 59%|█████▉    | 241/410 [01:29<00:46,  3.64it/s] 59%|█████▉    | 242/410 [01:30<00:46,  3.65it/s] 59%|█████▉    | 243/410 [01:30<00:45,  3.65it/s] 60%|█████▉    | 244/410 [01:30<00:45,  3.65it/s] 60%|█████▉    | 245/410 [01:30<00:45,  3.64it/s] 60%|██████    | 246/410 [01:31<00:43,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 05:50:09,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:50:09,403 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 05:50:09,403 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2371, 'eval_samples_per_second': 378.472, 'eval_steps_per_second': 47.309, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.79it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.05it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.37it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.56it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.13it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.90it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.54it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.24it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.18it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.00it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.17it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.18it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.27it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.31it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.26it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.10it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.01it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.94it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.02it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.06it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.10it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.07it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.20it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.20it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.22it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.11it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.99it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.95it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.01it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.88it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.03it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.23it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.19it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.96it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.89it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.94it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.03it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.07it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.09it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.17it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.15it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.12it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.12it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.04it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.98it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.01it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.95it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.95it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.09it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.21it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.20it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.15it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.06it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.98it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.93it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.95it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.01it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.03it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.99it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.09it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.12it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.13it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.08it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.01it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.88it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.96it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.96it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.06it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.18it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.19it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.08it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.12it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.00it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.94it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.00it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.03it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.92it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.15it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.22it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.13it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.14it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.10it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.01it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.96it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.91it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.91it/s][A 60%|██████    | 246/410 [01:40<00:43,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:50:18,702 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 05:50:18,715 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:50:20,885 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:50:20,896 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:50:20,907 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:43<10:18,  3.80s/it] 60%|██████    | 248/410 [01:43<07:23,  2.74s/it] 61%|██████    | 249/410 [01:43<05:21,  2.00s/it] 61%|██████    | 250/410 [01:44<03:57,  1.48s/it] 61%|██████    | 251/410 [01:44<02:57,  1.12s/it] 61%|██████▏   | 252/410 [01:44<02:16,  1.16it/s] 62%|██████▏   | 253/410 [01:44<01:48,  1.45it/s] 62%|██████▏   | 254/410 [01:45<01:27,  1.77it/s] 62%|██████▏   | 255/410 [01:45<01:13,  2.10it/s] 62%|██████▏   | 256/410 [01:45<01:04,  2.40it/s] 63%|██████▎   | 257/410 [01:45<00:57,  2.67it/s] 63%|██████▎   | 258/410 [01:46<00:52,  2.91it/s] 63%|██████▎   | 259/410 [01:46<00:48,  3.09it/s] 63%|██████▎   | 260/410 [01:46<00:46,  3.24it/s] 64%|██████▎   | 261/410 [01:47<00:44,  3.36it/s] 64%|██████▍   | 262/410 [01:47<00:43,  3.44it/s] 64%|██████▍   | 263/410 [01:47<00:41,  3.50it/s] 64%|██████▍   | 264/410 [01:47<00:41,  3.54it/s] 65%|██████▍   | 265/410 [01:48<00:40,  3.57it/s] 65%|██████▍   | 266/410 [01:48<00:40,  3.60it/s] 65%|██████▌   | 267/410 [01:48<00:39,  3.61it/s] 65%|██████▌   | 268/410 [01:48<00:39,  3.62it/s] 66%|██████▌   | 269/410 [01:49<00:38,  3.63it/s] 66%|██████▌   | 270/410 [01:49<00:38,  3.63it/s] 66%|██████▌   | 271/410 [01:49<00:38,  3.64it/s] 66%|██████▋   | 272/410 [01:50<00:37,  3.64it/s] 67%|██████▋   | 273/410 [01:50<00:37,  3.64it/s] 67%|██████▋   | 274/410 [01:50<00:37,  3.65it/s] 67%|██████▋   | 275/410 [01:50<00:37,  3.64it/s] 67%|██████▋   | 276/410 [01:51<00:36,  3.64it/s] 68%|██████▊   | 277/410 [01:51<00:36,  3.64it/s] 68%|██████▊   | 278/410 [01:51<00:36,  3.64it/s] 68%|██████▊   | 279/410 [01:51<00:36,  3.55it/s] 68%|██████▊   | 280/410 [01:52<00:36,  3.56it/s] 69%|██████▊   | 281/410 [01:52<00:35,  3.59it/s] 69%|██████▉   | 282/410 [01:52<00:35,  3.61it/s] 69%|██████▉   | 283/410 [01:53<00:35,  3.62it/s] 69%|██████▉   | 284/410 [01:53<00:34,  3.63it/s] 70%|██████▉   | 285/410 [01:53<00:34,  3.63it/s] 70%|██████▉   | 286/410 [01:53<00:34,  3.64it/s] 70%|███████   | 287/410 [01:54<00:33,  3.64it/s] 70%|███████   | 288/410 [01:54<00:33,  3.65it/s] 70%|███████   | 289/410 [01:54<00:33,  3.65it/s] 71%|███████   | 290/410 [01:55<00:32,  3.64it/s] 71%|███████   | 291/410 [01:55<00:32,  3.64it/s] 71%|███████   | 292/410 [01:55<00:32,  3.64it/s] 71%|███████▏  | 293/410 [01:55<00:32,  3.64it/s] 72%|███████▏  | 294/410 [01:56<00:31,  3.65it/s] 72%|███████▏  | 295/410 [01:56<00:31,  3.64it/s] 72%|███████▏  | 296/410 [01:56<00:31,  3.65it/s] 72%|███████▏  | 297/410 [01:56<00:30,  3.65it/s] 73%|███████▎  | 298/410 [01:57<00:30,  3.65it/s] 73%|███████▎  | 299/410 [01:57<00:30,  3.65it/s] 73%|███████▎  | 300/410 [01:57<00:30,  3.65it/s] 73%|███████▎  | 301/410 [01:58<00:29,  3.64it/s] 74%|███████▎  | 302/410 [01:58<00:29,  3.64it/s] 74%|███████▍  | 303/410 [01:58<00:29,  3.64it/s] 74%|███████▍  | 304/410 [01:58<00:29,  3.65it/s] 74%|███████▍  | 305/410 [01:59<00:28,  3.65it/s] 75%|███████▍  | 306/410 [01:59<00:28,  3.65it/s] 75%|███████▍  | 307/410 [01:59<00:28,  3.65it/s] 75%|███████▌  | 308/410 [01:59<00:27,  3.65it/s] 75%|███████▌  | 309/410 [02:00<00:27,  3.65it/s] 76%|███████▌  | 310/410 [02:00<00:27,  3.65it/s] 76%|███████▌  | 311/410 [02:00<00:27,  3.65it/s] 76%|███████▌  | 312/410 [02:01<00:27,  3.62it/s] 76%|███████▋  | 313/410 [02:01<00:26,  3.63it/s] 77%|███████▋  | 314/410 [02:01<00:26,  3.64it/s] 77%|███████▋  | 315/410 [02:01<00:26,  3.64it/s] 77%|███████▋  | 316/410 [02:02<00:25,  3.64it/s] 77%|███████▋  | 317/410 [02:02<00:25,  3.64it/s] 78%|███████▊  | 318/410 [02:02<00:25,  3.64it/s] 78%|███████▊  | 319/410 [02:02<00:24,  3.64it/s] 78%|███████▊  | 320/410 [02:03<00:24,  3.64it/s] 78%|███████▊  | 321/410 [02:03<00:24,  3.64it/s] 79%|███████▊  | 322/410 [02:03<00:24,  3.65it/s] 79%|███████▉  | 323/410 [02:04<00:23,  3.64it/s] 79%|███████▉  | 324/410 [02:04<00:23,  3.64it/s] 79%|███████▉  | 325/410 [02:04<00:23,  3.64it/s] 80%|███████▉  | 326/410 [02:04<00:23,  3.65it/s] 80%|███████▉  | 327/410 [02:05<00:22,  3.65it/s] 80%|████████  | 328/410 [02:05<00:21,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 05:50:43,649 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:50:43,649 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 05:50:43,649 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2868, 'eval_samples_per_second': 376.45, 'eval_steps_per_second': 47.056, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.80it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.82it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.22it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.54it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.09it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.82it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.52it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.98it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.20it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.20it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.09it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.09it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.22it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.26it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.16it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.17it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.86it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.89it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.99it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.04it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.08it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.18it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.10it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.14it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.11it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.01it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.96it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.01it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.06it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.93it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.06it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.18it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.19it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.09it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.05it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.88it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.87it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.01it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.04it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.08it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.17it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.06it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.05it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.03it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.97it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.95it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.01it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.05it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.00it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.01it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.14it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.10it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.05it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.06it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.88it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.89it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.02it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.06it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.09it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.16it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.10it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.04it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.01it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.00it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.94it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.03it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.06it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.02it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.00it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.09it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.06it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.01it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.02it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.86it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.90it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.02it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.04it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.05it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.12it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.10it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.95it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.93it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.90it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.01it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.05it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.05it/s][A 80%|████████  | 328/410 [02:14<00:21,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:50:52,963 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 05:50:52,979 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:50:55,117 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:50:55,136 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:50:55,145 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:17<05:08,  3.81s/it] 80%|████████  | 330/410 [02:17<03:39,  2.75s/it] 81%|████████  | 331/410 [02:18<02:38,  2.01s/it] 81%|████████  | 332/410 [02:18<01:55,  1.49s/it] 81%|████████  | 333/410 [02:18<01:26,  1.12s/it] 81%|████████▏ | 334/410 [02:18<01:06,  1.15it/s] 82%|████████▏ | 335/410 [02:19<00:51,  1.45it/s] 82%|████████▏ | 336/410 [02:19<00:41,  1.77it/s] 82%|████████▏ | 337/410 [02:19<00:34,  2.09it/s] 82%|████████▏ | 338/410 [02:19<00:30,  2.40it/s] 83%|████████▎ | 339/410 [02:20<00:26,  2.67it/s] 83%|████████▎ | 340/410 [02:20<00:24,  2.90it/s] 83%|████████▎ | 341/410 [02:20<00:22,  3.09it/s] 83%|████████▎ | 342/410 [02:21<00:20,  3.24it/s] 84%|████████▎ | 343/410 [02:21<00:19,  3.36it/s] 84%|████████▍ | 344/410 [02:21<00:19,  3.44it/s] 84%|████████▍ | 345/410 [02:21<00:18,  3.50it/s] 84%|████████▍ | 346/410 [02:22<00:18,  3.55it/s] 85%|████████▍ | 347/410 [02:22<00:17,  3.58it/s] 85%|████████▍ | 348/410 [02:22<00:17,  3.60it/s] 85%|████████▌ | 349/410 [02:22<00:16,  3.62it/s] 85%|████████▌ | 350/410 [02:23<00:16,  3.63it/s] 86%|████████▌ | 351/410 [02:23<00:16,  3.64it/s] 86%|████████▌ | 352/410 [02:23<00:15,  3.64it/s] 86%|████████▌ | 353/410 [02:24<00:15,  3.65it/s] 86%|████████▋ | 354/410 [02:24<00:15,  3.65it/s] 87%|████████▋ | 355/410 [02:24<00:15,  3.63it/s] 87%|████████▋ | 356/410 [02:24<00:14,  3.64it/s] 87%|████████▋ | 357/410 [02:25<00:14,  3.64it/s] 87%|████████▋ | 358/410 [02:25<00:14,  3.65it/s] 88%|████████▊ | 359/410 [02:25<00:13,  3.65it/s] 88%|████████▊ | 360/410 [02:25<00:13,  3.65it/s] 88%|████████▊ | 361/410 [02:26<00:13,  3.65it/s] 88%|████████▊ | 362/410 [02:26<00:13,  3.64it/s] 89%|████████▊ | 363/410 [02:26<00:12,  3.64it/s] 89%|████████▉ | 364/410 [02:27<00:12,  3.65it/s] 89%|████████▉ | 365/410 [02:27<00:12,  3.64it/s] 89%|████████▉ | 366/410 [02:27<00:12,  3.64it/s] 90%|████████▉ | 367/410 [02:27<00:11,  3.65it/s] 90%|████████▉ | 368/410 [02:28<00:11,  3.65it/s] 90%|█████████ | 369/410 [02:28<00:11,  3.65it/s] 90%|█████████ | 370/410 [02:28<00:10,  3.65it/s] 90%|█████████ | 371/410 [02:29<00:10,  3.66it/s] 91%|█████████ | 372/410 [02:29<00:10,  3.66it/s] 91%|█████████ | 373/410 [02:29<00:10,  3.66it/s] 91%|█████████ | 374/410 [02:29<00:09,  3.65it/s] 91%|█████████▏| 375/410 [02:30<00:09,  3.65it/s] 92%|█████████▏| 376/410 [02:30<00:09,  3.65it/s] 92%|█████████▏| 377/410 [02:30<00:09,  3.65it/s] 92%|█████████▏| 378/410 [02:30<00:08,  3.65it/s] 92%|█████████▏| 379/410 [02:31<00:08,  3.65it/s] 93%|█████████▎| 380/410 [02:31<00:08,  3.65it/s] 93%|█████████▎| 381/410 [02:31<00:07,  3.65it/s] 93%|█████████▎| 382/410 [02:32<00:07,  3.65it/s] 93%|█████████▎| 383/410 [02:32<00:07,  3.65it/s] 94%|█████████▎| 384/410 [02:32<00:07,  3.64it/s] 94%|█████████▍| 385/410 [02:32<00:06,  3.64it/s] 94%|█████████▍| 386/410 [02:33<00:06,  3.64it/s] 94%|█████████▍| 387/410 [02:33<00:06,  3.65it/s] 95%|█████████▍| 388/410 [02:33<00:06,  3.64it/s] 95%|█████████▍| 389/410 [02:33<00:05,  3.65it/s] 95%|█████████▌| 390/410 [02:34<00:05,  3.64it/s] 95%|█████████▌| 391/410 [02:34<00:05,  3.65it/s] 96%|█████████▌| 392/410 [02:34<00:04,  3.64it/s] 96%|█████████▌| 393/410 [02:35<00:04,  3.64it/s] 96%|█████████▌| 394/410 [02:35<00:04,  3.65it/s] 96%|█████████▋| 395/410 [02:35<00:04,  3.65it/s] 97%|█████████▋| 396/410 [02:35<00:03,  3.65it/s] 97%|█████████▋| 397/410 [02:36<00:03,  3.65it/s] 97%|█████████▋| 398/410 [02:36<00:03,  3.65it/s] 97%|█████████▋| 399/410 [02:36<00:03,  3.64it/s] 98%|█████████▊| 400/410 [02:36<00:02,  3.64it/s] 98%|█████████▊| 401/410 [02:37<00:02,  3.65it/s] 98%|█████████▊| 402/410 [02:37<00:02,  3.65it/s] 98%|█████████▊| 403/410 [02:37<00:01,  3.65it/s] 99%|█████████▊| 404/410 [02:38<00:01,  3.65it/s] 99%|█████████▉| 405/410 [02:38<00:01,  3.65it/s] 99%|█████████▉| 406/410 [02:38<00:01,  3.65it/s] 99%|█████████▉| 407/410 [02:38<00:00,  3.64it/s]100%|█████████▉| 408/410 [02:39<00:00,  3.64it/s]100%|█████████▉| 409/410 [02:39<00:00,  3.65it/s]100%|██████████| 410/410 [02:39<00:00,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 05:51:17,912 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:51:17,912 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 05:51:17,912 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.292, 'eval_samples_per_second': 376.239, 'eval_steps_per_second': 47.03, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.82it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.16it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.35it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.57it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.12it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.71it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.51it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.25it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.14it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.16it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.22it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.08it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.05it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.19it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.17it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.06it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.99it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.83it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.94it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.09it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.12it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.12it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.13it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.08it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.04it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.97it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.95it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.91it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.98it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.04it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.07it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.06it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.11it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.09it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.00it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.94it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.94it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.90it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.03it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.04it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.07it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.04it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.11it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.01it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.98it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.78it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.93it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.99it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.03it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.07it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.08it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.13it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.07it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.96it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.93it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.87it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.93it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.01it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.05it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.02it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.04it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.06it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.94it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.01it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.97it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.89it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.00it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.02it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.03it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.12it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.12it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.98it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.98it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.93it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.88it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.99it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.03it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.98it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.04it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.06it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.03it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.95it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.97it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.92it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.93it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.07it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.07it/s][A100%|██████████| 410/410 [02:48<00:00,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:51:27,227 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 05:51:27,244 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:51:29,442 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:51:29,452 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:51:29,465 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 05:51:29,707 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 05:51:29,708 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82 (score: 1.1523417234420776).
                                                 100%|██████████| 410/410 [02:53<00:00,  3.78it/s]100%|██████████| 410/410 [02:53<00:00,  2.37it/s]
[INFO|trainer.py:1894] 2023-08-28 05:51:31,283 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 05:51:31,308 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:51:33,753 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:51:33,768 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:51:33,774 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:51:33,955 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:33,955 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:33,955 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:33,955 >>   train_runtime            = 0:02:53.00
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:33,955 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:33,955 >>   train_samples_per_second =    151.409
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:33,956 >>   train_steps_per_second   =       2.37
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2942, 'eval_samples_per_second': 376.148, 'eval_steps_per_second': 47.019, 'epoch': 5.0}
{'train_runtime': 173.0082, 'train_samples_per_second': 151.409, 'train_steps_per_second': 2.37, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 05:51:34 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 05:51:34,007 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:51:34,008 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 05:51:34,008 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.32it/s]  3%|▎         | 12/437 [00:00<00:08, 51.48it/s]  4%|▍         | 18/437 [00:00<00:08, 49.51it/s]  5%|▌         | 23/437 [00:00<00:08, 48.86it/s]  6%|▋         | 28/437 [00:00<00:08, 48.45it/s]  8%|▊         | 33/437 [00:00<00:08, 48.04it/s]  9%|▊         | 38/437 [00:00<00:08, 47.90it/s] 10%|▉         | 43/437 [00:00<00:08, 47.75it/s] 11%|█         | 48/437 [00:00<00:08, 47.54it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.46it/s] 13%|█▎        | 58/437 [00:01<00:07, 47.46it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.40it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.39it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.45it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.48it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.41it/s] 20%|██        | 88/437 [00:01<00:07, 47.38it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.31it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.21it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.19it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.31it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.35it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.33it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.36it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.43it/s] 30%|███       | 133/437 [00:02<00:06, 47.44it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.32it/s] 33%|███▎      | 143/437 [00:02<00:06, 47.30it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.21it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.19it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.26it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.28it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.35it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.38it/s] 41%|████      | 178/437 [00:03<00:05, 47.43it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.36it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.23it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.17it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.25it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.31it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.34it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.36it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.23it/s] 51%|█████     | 223/437 [00:04<00:04, 47.26it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.31it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.34it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.29it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.25it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.24it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.32it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.33it/s] 60%|██████    | 263/437 [00:05<00:03, 47.32it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.35it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.33it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.35it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.30it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.27it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.18it/s] 68%|██████▊   | 298/437 [00:06<00:02, 47.25it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.31it/s] 70%|███████   | 308/437 [00:06<00:02, 47.31it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.23it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.33it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.34it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.30it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.30it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.28it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.24it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.13it/s] 81%|████████  | 353/437 [00:07<00:01, 47.26it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.31it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.33it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.35it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.37it/s] 86%|████████▋ | 378/437 [00:07<00:01, 47.27it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.18it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.23it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.21it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.20it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.28it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.32it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.21it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.30it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.32it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.28it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.16it/s]100%|██████████| 437/437 [00:09<00:00, 47.38it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:51:43,254 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   eval_loss               =     1.1523
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   eval_runtime            = 0:00:09.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   eval_samples_per_second =    378.101
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   eval_steps_per_second   =     47.263
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:51:43,254 >>   perplexity              =     3.1656
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:48,432 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:48,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:48,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:48,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:48,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:51:49,155 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:51:49,156 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:51:49,423 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:51:50,445 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:51:50,445 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:52,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:52,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:52,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:52,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:51:52,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:51:53,553 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:51:53,554 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:51:54,182 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:51:54,323 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:51:54,323 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:14,  1.72it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:15,  1.72it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.62it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:26,  1.71it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.72it/s]Extractor Predicting: 48it [00:29,  1.74it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:31,  1.76it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:32,  1.73it/s]Extractor Predicting: 55it [00:33,  1.74it/s]Extractor Predicting: 56it [00:33,  1.71it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:36,  1.65it/s]Extractor Predicting: 61it [00:36,  1.66it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:38,  1.64it/s]Extractor Predicting: 64it [00:38,  1.65it/s]Extractor Predicting: 65it [00:39,  1.66it/s]Extractor Predicting: 66it [00:39,  1.63it/s]Extractor Predicting: 67it [00:40,  1.60it/s]Extractor Predicting: 68it [00:41,  1.63it/s]Extractor Predicting: 69it [00:41,  1.64it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:42,  1.67it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:45,  1.65it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:46,  1.65it/s]Extractor Predicting: 78it [00:47,  1.63it/s]Extractor Predicting: 79it [00:47,  1.62it/s]Extractor Predicting: 80it [00:48,  1.62it/s]Extractor Predicting: 81it [00:48,  1.64it/s]Extractor Predicting: 82it [00:49,  1.65it/s]Extractor Predicting: 83it [00:50,  1.68it/s]Extractor Predicting: 84it [00:50,  1.61it/s]Extractor Predicting: 85it [00:51,  1.61it/s]Extractor Predicting: 86it [00:51,  1.64it/s]Extractor Predicting: 87it [00:52,  1.61it/s]Extractor Predicting: 88it [00:53,  1.66it/s]Extractor Predicting: 89it [00:53,  1.66it/s]Extractor Predicting: 90it [00:54,  1.65it/s]Extractor Predicting: 91it [00:54,  1.70it/s]Extractor Predicting: 92it [00:55,  1.69it/s]Extractor Predicting: 93it [00:56,  1.72it/s]Extractor Predicting: 94it [00:56,  1.68it/s]Extractor Predicting: 95it [00:57,  1.70it/s]Extractor Predicting: 96it [00:57,  1.68it/s]Extractor Predicting: 97it [00:58,  1.65it/s]Extractor Predicting: 98it [00:59,  1.63it/s]Extractor Predicting: 99it [00:59,  1.65it/s]Extractor Predicting: 100it [01:00,  1.64it/s]Extractor Predicting: 101it [01:01,  1.64it/s]Extractor Predicting: 102it [01:01,  1.59it/s]Extractor Predicting: 103it [01:02,  1.48it/s]Extractor Predicting: 104it [01:03,  1.52it/s]Extractor Predicting: 105it [01:03,  1.55it/s]Extractor Predicting: 106it [01:04,  1.61it/s]Extractor Predicting: 107it [01:04,  1.59it/s]Extractor Predicting: 108it [01:05,  1.62it/s]Extractor Predicting: 109it [01:06,  1.62it/s]Extractor Predicting: 110it [01:06,  1.62it/s]Extractor Predicting: 111it [01:07,  1.62it/s]Extractor Predicting: 112it [01:07,  1.64it/s]Extractor Predicting: 113it [01:08,  1.65it/s]Extractor Predicting: 114it [01:09,  1.64it/s]Extractor Predicting: 115it [01:09,  1.63it/s]Extractor Predicting: 116it [01:10,  1.70it/s]Extractor Predicting: 117it [01:10,  1.67it/s]Extractor Predicting: 118it [01:11,  1.65it/s]Extractor Predicting: 119it [01:12,  1.61it/s]Extractor Predicting: 120it [01:12,  1.63it/s]Extractor Predicting: 121it [01:13,  1.65it/s]Extractor Predicting: 122it [01:14,  1.63it/s]Extractor Predicting: 123it [01:14,  1.62it/s]Extractor Predicting: 124it [01:15,  1.60it/s]Extractor Predicting: 125it [01:15,  1.57it/s]Extractor Predicting: 126it [01:16,  1.61it/s]Extractor Predicting: 127it [01:17,  1.62it/s]Extractor Predicting: 128it [01:17,  1.63it/s]Extractor Predicting: 129it [01:18,  1.64it/s]Extractor Predicting: 130it [01:18,  1.68it/s]Extractor Predicting: 131it [01:19,  1.69it/s]Extractor Predicting: 132it [01:20,  1.68it/s]Extractor Predicting: 133it [01:20,  1.66it/s]Extractor Predicting: 134it [01:21,  1.67it/s]Extractor Predicting: 135it [01:21,  1.67it/s]Extractor Predicting: 136it [01:22,  1.65it/s]Extractor Predicting: 137it [01:23,  1.64it/s]Extractor Predicting: 138it [01:23,  1.63it/s]Extractor Predicting: 139it [01:24,  1.64it/s]Extractor Predicting: 140it [01:25,  1.63it/s]Extractor Predicting: 141it [01:25,  1.62it/s]Extractor Predicting: 142it [01:26,  1.60it/s]Extractor Predicting: 143it [01:26,  1.60it/s]Extractor Predicting: 144it [01:27,  1.77it/s]Extractor Predicting: 144it [01:27,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:29,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:29,354 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:29,354 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:29,355 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:29,355 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:53:29,662 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:53:29,663 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:53:30,346 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:53:31,369 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:53:31,370 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:34,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:34,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:34,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:34,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:53:34,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:53:34,878 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:53:34,880 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:53:35,476 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:53:35,642 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:53:35,643 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.74it/s]Extractor Predicting: 3it [00:01,  1.81it/s]Extractor Predicting: 4it [00:02,  1.83it/s]Extractor Predicting: 5it [00:02,  1.84it/s]Extractor Predicting: 6it [00:03,  1.85it/s]Extractor Predicting: 7it [00:03,  1.86it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:04,  1.88it/s]Extractor Predicting: 10it [00:05,  1.90it/s]Extractor Predicting: 11it [00:05,  1.94it/s]Extractor Predicting: 12it [00:06,  1.88it/s]Extractor Predicting: 13it [00:06,  1.88it/s]Extractor Predicting: 14it [00:07,  1.90it/s]Extractor Predicting: 15it [00:08,  1.90it/s]Extractor Predicting: 16it [00:08,  1.86it/s]Extractor Predicting: 17it [00:09,  1.86it/s]Extractor Predicting: 18it [00:09,  1.90it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.89it/s]Extractor Predicting: 21it [00:11,  1.92it/s]Extractor Predicting: 22it [00:11,  1.95it/s]Extractor Predicting: 23it [00:12,  1.90it/s]Extractor Predicting: 24it [00:12,  1.87it/s]Extractor Predicting: 25it [00:13,  1.82it/s]Extractor Predicting: 26it [00:13,  1.86it/s]Extractor Predicting: 27it [00:14,  1.88it/s]Extractor Predicting: 28it [00:15,  1.78it/s]Extractor Predicting: 29it [00:15,  1.75it/s]Extractor Predicting: 30it [00:16,  1.67it/s]Extractor Predicting: 31it [00:16,  1.63it/s]Extractor Predicting: 32it [00:17,  1.58it/s]Extractor Predicting: 33it [00:18,  1.57it/s]Extractor Predicting: 34it [00:18,  1.55it/s]Extractor Predicting: 35it [00:19,  1.58it/s]Extractor Predicting: 36it [00:20,  1.58it/s]Extractor Predicting: 37it [00:20,  1.59it/s]Extractor Predicting: 38it [00:21,  1.53it/s]Extractor Predicting: 39it [00:22,  1.54it/s]Extractor Predicting: 40it [00:22,  1.52it/s]Extractor Predicting: 41it [00:23,  1.51it/s]Extractor Predicting: 42it [00:24,  1.52it/s]Extractor Predicting: 43it [00:24,  1.54it/s]Extractor Predicting: 44it [00:25,  1.57it/s]Extractor Predicting: 45it [00:26,  1.56it/s]Extractor Predicting: 46it [00:26,  1.59it/s]Extractor Predicting: 47it [00:27,  1.46it/s]Extractor Predicting: 48it [00:28,  1.47it/s]Extractor Predicting: 49it [00:28,  1.49it/s]Extractor Predicting: 50it [00:29,  1.51it/s]Extractor Predicting: 51it [00:30,  1.51it/s]Extractor Predicting: 52it [00:30,  1.53it/s]Extractor Predicting: 53it [00:31,  1.53it/s]Extractor Predicting: 54it [00:31,  1.55it/s]Extractor Predicting: 55it [00:32,  1.54it/s]Extractor Predicting: 56it [00:33,  1.51it/s]Extractor Predicting: 57it [00:33,  1.54it/s]Extractor Predicting: 58it [00:34,  1.54it/s]Extractor Predicting: 59it [00:35,  1.57it/s]Extractor Predicting: 60it [00:35,  1.60it/s]Extractor Predicting: 61it [00:36,  1.55it/s]Extractor Predicting: 62it [00:37,  1.55it/s]Extractor Predicting: 63it [00:37,  1.58it/s]Extractor Predicting: 64it [00:38,  1.61it/s]Extractor Predicting: 65it [00:38,  1.62it/s]Extractor Predicting: 66it [00:39,  1.65it/s]Extractor Predicting: 67it [00:40,  1.67it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:42,  1.74it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.66it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:44,  1.63it/s]Extractor Predicting: 76it [00:45,  1.61it/s]Extractor Predicting: 77it [00:46,  1.61it/s]Extractor Predicting: 78it [00:46,  1.61it/s]Extractor Predicting: 79it [00:47,  1.62it/s]Extractor Predicting: 80it [00:48,  1.61it/s]Extractor Predicting: 81it [00:48,  1.60it/s]Extractor Predicting: 82it [00:49,  1.62it/s]Extractor Predicting: 83it [00:49,  1.63it/s]Extractor Predicting: 84it [00:50,  1.60it/s]Extractor Predicting: 85it [00:51,  1.62it/s]Extractor Predicting: 86it [00:51,  1.64it/s]Extractor Predicting: 87it [00:52,  1.69it/s]Extractor Predicting: 88it [00:52,  1.74it/s]Extractor Predicting: 89it [00:53,  1.76it/s]Extractor Predicting: 90it [00:53,  1.77it/s]Extractor Predicting: 91it [00:54,  1.82it/s]Extractor Predicting: 92it [00:55,  1.79it/s]Extractor Predicting: 93it [00:55,  1.84it/s]Extractor Predicting: 94it [00:56,  1.81it/s]Extractor Predicting: 95it [00:56,  1.80it/s]Extractor Predicting: 96it [00:57,  1.80it/s]Extractor Predicting: 97it [00:57,  1.84it/s]Extractor Predicting: 98it [00:58,  1.86it/s]Extractor Predicting: 99it [00:58,  1.86it/s]Extractor Predicting: 100it [00:59,  1.92it/s]Extractor Predicting: 101it [00:59,  1.90it/s]Extractor Predicting: 102it [01:00,  1.85it/s]Extractor Predicting: 103it [01:00,  1.81it/s]Extractor Predicting: 104it [01:01,  1.82it/s]Extractor Predicting: 105it [01:02,  1.85it/s]Extractor Predicting: 106it [01:02,  1.84it/s]Extractor Predicting: 107it [01:03,  1.92it/s]Extractor Predicting: 108it [01:03,  1.85it/s]Extractor Predicting: 109it [01:04,  1.87it/s]Extractor Predicting: 110it [01:04,  1.87it/s]Extractor Predicting: 111it [01:05,  1.88it/s]Extractor Predicting: 112it [01:05,  1.90it/s]Extractor Predicting: 113it [01:06,  1.88it/s]Extractor Predicting: 114it [01:06,  1.85it/s]Extractor Predicting: 115it [01:07,  1.81it/s]Extractor Predicting: 116it [01:08,  1.77it/s]Extractor Predicting: 117it [01:08,  1.73it/s]Extractor Predicting: 118it [01:09,  1.74it/s]Extractor Predicting: 119it [01:09,  1.72it/s]Extractor Predicting: 120it [01:10,  1.68it/s]Extractor Predicting: 121it [01:10,  1.71it/s]Extractor Predicting: 122it [01:11,  1.71it/s]Extractor Predicting: 123it [01:12,  1.67it/s]Extractor Predicting: 124it [01:12,  1.73it/s]Extractor Predicting: 125it [01:13,  1.68it/s]Extractor Predicting: 126it [01:13,  1.70it/s]Extractor Predicting: 127it [01:14,  1.67it/s]Extractor Predicting: 128it [01:15,  1.65it/s]Extractor Predicting: 129it [01:15,  1.67it/s]Extractor Predicting: 130it [01:16,  1.64it/s]Extractor Predicting: 131it [01:17,  1.63it/s]Extractor Predicting: 132it [01:17,  1.63it/s]Extractor Predicting: 133it [01:18,  1.66it/s]Extractor Predicting: 134it [01:18,  1.66it/s]Extractor Predicting: 135it [01:19,  1.50it/s]Extractor Predicting: 136it [01:20,  1.50it/s]Extractor Predicting: 137it [01:20,  1.57it/s]Extractor Predicting: 138it [01:21,  1.57it/s]Extractor Predicting: 139it [01:22,  1.60it/s]Extractor Predicting: 140it [01:22,  1.60it/s]Extractor Predicting: 141it [01:23,  1.63it/s]Extractor Predicting: 142it [01:23,  1.62it/s]Extractor Predicting: 143it [01:24,  2.05it/s]Extractor Predicting: 143it [01:24,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:06,406 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:06,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:06,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:06,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:06,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:55:07,016 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:55:07,017 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:55:07,586 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:55:08,576 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:55:08,576 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:11,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:11,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:11,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:11,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:55:11,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:55:12,082 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:55:12,083 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:55:12,677 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:55:12,842 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:55:12,842 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 2it [00:01,  1.37it/s]
[INFO|configuration_utils.py:515] 2023-08-28 05:55:14,617 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:55:14,618 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:55:14,620 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:55:14,621 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 05:55:14,624 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:55:17,586 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 05:55:17,588 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 05:55:17,599 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:55:17,599 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:55:17,604 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:55:17,607 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:55:17,607 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:55:17,607 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:55:17,607 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:55:17,607 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:55:17,607 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 05:55:17,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:18,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:19,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:20,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:21,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:21,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:22,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:23,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:24,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:24,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:25,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:26,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:27,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:28,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:29,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:29,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:30,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:31,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:32,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:33,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:33,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:34,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:35,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:36,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:53, 19.29s/it][WARNING|generation_utils.py:914] 2023-08-28 05:55:37,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:37,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:38,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:39,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:40,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:41,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:41,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:42,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:43,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:44,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:44,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:45,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:46,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:47,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:48,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:49,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:50,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:51,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:52,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:52,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:54,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:54,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:55,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:56,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:39<02:39, 19.88s/it][WARNING|generation_utils.py:914] 2023-08-28 05:55:57,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:58,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:59,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:55:59,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:00,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:01,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:02,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:03,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:03,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:05,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:06,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:07,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:07,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:08,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:09,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:10,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:11,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:11,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:12,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:13,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:14,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:57<02:12, 18.86s/it][WARNING|generation_utils.py:914] 2023-08-28 05:56:15,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:15,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:16,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:17,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:18,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:18,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:19,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:20,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:20,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:21,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:22,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:23,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:23,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:24,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:25,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:26,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:26,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:27,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:28,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:29,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:29,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:30,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:31,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:32,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:33,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:34,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:34,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:35,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:36,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:37,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:37,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:38,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:39,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:39,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:40,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:41,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:41,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:42,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:43,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:43,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:44,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:45,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:45,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:46,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:29<02:25, 24.17s/it][WARNING|generation_utils.py:914] 2023-08-28 05:56:47,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:48,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:49,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:49,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:50,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:51,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:52,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:52,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:53,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:54,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:55,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:56,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:56,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:57,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:58,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:56:59,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:00,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:00,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:01,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:02,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:03,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:03,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:04,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:05,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:06,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:49<01:52, 22.50s/it][WARNING|generation_utils.py:914] 2023-08-28 05:57:06,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:07,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:08,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:09,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:10,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:11,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:11,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:12,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:13,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:14,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:15,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:15,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:16,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:17,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:18,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:19,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:19,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:20,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:21,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:22,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:23,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:23,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:24,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:25,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:08<01:25, 21.40s/it][WARNING|generation_utils.py:914] 2023-08-28 05:57:26,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:26,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:27,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:28,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:29,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:30,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:31,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:31,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:32,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:33,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:34,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:35,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:36,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:37,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:38,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:39,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:39,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:41,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:42,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:43,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:43,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:44,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:45,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:46,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:47,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:30<01:04, 21.62s/it][WARNING|generation_utils.py:914] 2023-08-28 05:57:48,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:49,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:49,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:50,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:51,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:52,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:52,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:53,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:54,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:54,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:55,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:56,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:57,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:57,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:58,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:59,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:57:59,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:00,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:01,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:02,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:02,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:03,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:04,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:47<00:40, 20.13s/it][WARNING|generation_utils.py:914] 2023-08-28 05:58:05,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:05,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:06,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:07,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:08,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:09,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:09,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:10,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:11,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:12,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:13,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:13,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:14,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:15,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:16,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:17,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:17,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:18,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:19,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:20,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:20,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:21,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:22,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:23,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:23,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:06<00:19, 19.93s/it][WARNING|generation_utils.py:914] 2023-08-28 05:58:24,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:25,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:26,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:27,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:27,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:28,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:29,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:30,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:31,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:32,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:33,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:33,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:34,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:35,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:36,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:37,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:38,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:38,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:39,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:40,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:41,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:41,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:42,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:43,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:44,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:58:45,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:28<00:00, 20.34s/it]Generating: 100%|██████████| 10/10 [03:28<00:00, 20.81s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:58:56,464 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:58:56,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:58:56,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:58:56,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:58:56,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:58:57,188 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:58:57,189 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:58:57,445 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:58:58,529 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:58:58,529 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:59:00,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:59:00,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:59:00,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:59:00,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:59:00,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:59:01,398 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:59:01,399 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:59:01,669 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:59:01,824 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:59:01,825 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
{'target': 600, 'success': 9, 'raw': 32}
{'target': 600, 'success': 26, 'raw': 64}
{'target': 600, 'success': 37, 'raw': 96}
{'target': 600, 'success': 54, 'raw': 128}
{'target': 600, 'success': 67, 'raw': 160}
{'target': 600, 'success': 82, 'raw': 192}
{'target': 600, 'success': 97, 'raw': 224}
{'target': 600, 'success': 111, 'raw': 256}
{'target': 600, 'success': 125, 'raw': 288}
{'target': 600, 'success': 137, 'raw': 320}
{'target': 600, 'success': 152, 'raw': 352}
{'target': 600, 'success': 166, 'raw': 384}
{'target': 600, 'success': 180, 'raw': 416}
{'target': 600, 'success': 196, 'raw': 448}
{'target': 600, 'success': 214, 'raw': 480}
{'target': 600, 'success': 229, 'raw': 512}
{'target': 600, 'success': 246, 'raw': 544}
{'target': 600, 'success': 260, 'raw': 576}
{'target': 600, 'success': 273, 'raw': 608}
{'target': 600, 'success': 285, 'raw': 640}
{'target': 600, 'success': 301, 'raw': 672}
{'target': 600, 'success': 315, 'raw': 704}
{'target': 600, 'success': 327, 'raw': 736}
{'target': 600, 'success': 334, 'raw': 768}
{'target': 600, 'success': 355, 'raw': 800}
{'target': 600, 'success': 369, 'raw': 832}
{'target': 600, 'success': 386, 'raw': 864}
{'target': 600, 'success': 399, 'raw': 896}
{'target': 600, 'success': 412, 'raw': 928}
{'target': 600, 'success': 425, 'raw': 960}
{'target': 600, 'success': 433, 'raw': 992}
{'target': 600, 'success': 443, 'raw': 1024}
{'target': 600, 'success': 460, 'raw': 1056}
{'target': 600, 'success': 475, 'raw': 1088}
{'target': 600, 'success': 485, 'raw': 1120}
{'target': 600, 'success': 497, 'raw': 1152}
{'target': 600, 'success': 511, 'raw': 1184}
{'target': 600, 'success': 525, 'raw': 1216}
{'target': 600, 'success': 542, 'raw': 1248}
{'target': 600, 'success': 556, 'raw': 1280}
{'target': 600, 'success': 573, 'raw': 1312}
{'target': 600, 'success': 586, 'raw': 1344}
{'target': 600, 'success': 596, 'raw': 1376}
{'target': 600, 'success': 605, 'raw': 1408}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.4296875, 'errors': {'', '(\'Love Is One of the Most Important Facts You Can Learn About Yourself\', \'original language of film or TV show\', \'\', \'On 3 November 2001 , he made the film " Love Is One of the Most Important Facts You Can Learn About Yourself " , about an alcoholic father .\')', '(\'The Hound of the Baskervilles\', \'original language of film or TV show\', \'\', \'His best work came in the 1983 horror movie " The Hound of the Baskervilles " directed by Gene Luen Yang .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 496, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8033854166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 254, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 450, 'raw': 608}
{'target': 600, 'success': 469, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 582, 'raw': 800}
{'target': 600, 'success': 605, 'raw': 832}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.7271634615384616, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 526, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7525, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 428, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 595, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 11540
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11640, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.23it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.30it/s]Extractor Estimating: 4it [00:02,  1.43it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:06,  1.49it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:08,  1.49it/s]Extractor Estimating: 13it [00:08,  1.48it/s]Extractor Estimating: 14it [00:09,  1.41it/s]Extractor Estimating: 15it [00:10,  1.40it/s]Extractor Estimating: 16it [00:11,  1.44it/s]Extractor Estimating: 17it [00:11,  1.43it/s]Extractor Estimating: 18it [00:12,  1.40it/s]Extractor Estimating: 19it [00:13,  1.44it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:15,  1.51it/s]Extractor Estimating: 23it [00:15,  1.50it/s]Extractor Estimating: 24it [00:16,  1.45it/s]Extractor Estimating: 25it [00:17,  1.49it/s]Extractor Estimating: 26it [00:17,  1.47it/s]Extractor Estimating: 27it [00:18,  1.42it/s]Extractor Estimating: 28it [00:19,  1.44it/s]Extractor Estimating: 29it [00:19,  1.45it/s]Extractor Estimating: 30it [00:20,  1.47it/s]Extractor Estimating: 31it [00:21,  1.47it/s]Extractor Estimating: 32it [00:21,  1.52it/s]Extractor Estimating: 33it [00:22,  1.54it/s]Extractor Estimating: 34it [00:23,  1.52it/s]Extractor Estimating: 35it [00:23,  1.53it/s]Extractor Estimating: 36it [00:24,  1.49it/s]Extractor Estimating: 37it [00:25,  1.52it/s]Extractor Estimating: 38it [00:25,  1.45it/s]Extractor Estimating: 39it [00:26,  1.41it/s]Extractor Estimating: 40it [00:27,  1.42it/s]Extractor Estimating: 41it [00:28,  1.41it/s]Extractor Estimating: 42it [00:28,  1.35it/s]Extractor Estimating: 43it [00:29,  1.31it/s]Extractor Estimating: 44it [00:30,  1.33it/s]Extractor Estimating: 45it [00:31,  1.40it/s]Extractor Estimating: 46it [00:31,  1.39it/s]Extractor Estimating: 47it [00:32,  1.43it/s]Extractor Estimating: 48it [00:33,  1.38it/s]Extractor Estimating: 49it [00:33,  1.46it/s]Extractor Estimating: 50it [00:34,  1.46it/s]Extractor Estimating: 51it [00:35,  1.47it/s]Extractor Estimating: 52it [00:35,  1.51it/s]Extractor Estimating: 53it [00:36,  1.51it/s]Extractor Estimating: 54it [00:37,  1.53it/s]Extractor Estimating: 55it [00:37,  1.51it/s]Extractor Estimating: 56it [00:38,  1.53it/s]Extractor Estimating: 57it [00:39,  1.52it/s]Extractor Estimating: 58it [00:39,  1.51it/s]Extractor Estimating: 59it [00:40,  1.53it/s]Extractor Estimating: 60it [00:41,  1.54it/s]Extractor Estimating: 61it [00:41,  1.55it/s]Extractor Estimating: 62it [00:42,  1.52it/s]Extractor Estimating: 63it [00:43,  1.54it/s]Extractor Estimating: 64it [00:43,  1.53it/s]Extractor Estimating: 65it [00:44,  1.51it/s]Extractor Estimating: 66it [00:45,  1.54it/s]Extractor Estimating: 67it [00:45,  1.51it/s]Extractor Estimating: 68it [00:46,  1.47it/s]Extractor Estimating: 69it [00:47,  1.52it/s]Extractor Estimating: 70it [00:47,  1.55it/s]Extractor Estimating: 71it [00:48,  1.55it/s]Extractor Estimating: 72it [00:48,  1.53it/s]Extractor Estimating: 73it [00:49,  1.55it/s]Extractor Estimating: 74it [00:50,  1.50it/s]Extractor Estimating: 75it [00:50,  1.53it/s]Extractor Estimating: 76it [00:51,  1.52it/s]Extractor Estimating: 77it [00:52,  1.53it/s]Extractor Estimating: 78it [00:52,  1.58it/s]Extractor Estimating: 79it [00:53,  1.53it/s]Extractor Estimating: 80it [00:54,  1.54it/s]Extractor Estimating: 81it [00:54,  1.52it/s]Extractor Estimating: 82it [00:55,  1.52it/s]Extractor Estimating: 83it [00:56,  1.57it/s]Extractor Estimating: 84it [00:56,  1.59it/s]Extractor Estimating: 85it [00:57,  1.61it/s]Extractor Estimating: 86it [00:58,  1.50it/s]Extractor Estimating: 87it [00:58,  1.43it/s]Extractor Estimating: 88it [00:59,  1.40it/s]Extractor Estimating: 89it [01:00,  1.32it/s]Extractor Estimating: 90it [01:01,  1.40it/s]Extractor Estimating: 91it [01:01,  1.41it/s]Extractor Estimating: 92it [01:02,  1.50it/s]Extractor Estimating: 93it [01:02,  1.53it/s]Extractor Estimating: 94it [01:03,  1.54it/s]Extractor Estimating: 95it [01:04,  1.58it/s]Extractor Estimating: 96it [01:04,  1.58it/s]Extractor Estimating: 97it [01:05,  1.59it/s]Extractor Estimating: 98it [01:06,  1.60it/s]Extractor Estimating: 99it [01:06,  1.58it/s]Extractor Estimating: 100it [01:07,  1.57it/s]Extractor Estimating: 101it [01:08,  1.43it/s]Extractor Estimating: 102it [01:08,  1.43it/s]Extractor Estimating: 103it [01:09,  1.48it/s]Extractor Estimating: 104it [01:10,  1.46it/s]Extractor Estimating: 105it [01:10,  1.48it/s]Extractor Estimating: 106it [01:11,  1.46it/s]Extractor Estimating: 107it [01:12,  1.51it/s]Extractor Estimating: 108it [01:12,  1.52it/s]Extractor Estimating: 109it [01:13,  1.47it/s]Extractor Estimating: 110it [01:14,  1.49it/s]Extractor Estimating: 111it [01:14,  1.49it/s]Extractor Estimating: 112it [01:15,  1.47it/s]Extractor Estimating: 113it [01:16,  1.49it/s]Extractor Estimating: 114it [01:16,  1.53it/s]Extractor Estimating: 115it [01:17,  1.50it/s]Extractor Estimating: 116it [01:18,  1.52it/s]Extractor Estimating: 117it [01:18,  1.54it/s]Extractor Estimating: 118it [01:19,  1.52it/s]Extractor Estimating: 119it [01:20,  1.54it/s]Extractor Estimating: 120it [01:20,  1.48it/s]Extractor Estimating: 121it [01:21,  1.48it/s]Extractor Estimating: 122it [01:22,  1.52it/s]Extractor Estimating: 123it [01:22,  1.47it/s]Extractor Estimating: 124it [01:23,  1.49it/s]Extractor Estimating: 125it [01:24,  1.47it/s]Extractor Estimating: 126it [01:25,  1.41it/s]Extractor Estimating: 127it [01:25,  1.42it/s]Extractor Estimating: 128it [01:26,  1.45it/s]Extractor Estimating: 129it [01:27,  1.45it/s]Extractor Estimating: 130it [01:27,  1.51it/s]Extractor Estimating: 131it [01:28,  1.52it/s]Extractor Estimating: 132it [01:28,  1.54it/s]Extractor Estimating: 133it [01:29,  1.51it/s]Extractor Estimating: 134it [01:30,  1.52it/s]Extractor Estimating: 135it [01:30,  1.54it/s]Extractor Estimating: 136it [01:31,  1.58it/s]Extractor Estimating: 137it [01:32,  1.55it/s]Extractor Estimating: 138it [01:32,  1.59it/s]Extractor Estimating: 139it [01:33,  1.58it/s]Extractor Estimating: 140it [01:34,  1.57it/s]Extractor Estimating: 141it [01:34,  1.53it/s]Extractor Estimating: 142it [01:35,  1.50it/s]Extractor Estimating: 143it [01:36,  1.53it/s]Extractor Estimating: 144it [01:36,  1.58it/s]Extractor Estimating: 145it [01:37,  1.57it/s]Extractor Estimating: 146it [01:37,  1.57it/s]Extractor Estimating: 147it [01:38,  1.56it/s]Extractor Estimating: 148it [01:39,  1.58it/s]Extractor Estimating: 149it [01:39,  1.57it/s]Extractor Estimating: 150it [01:40,  1.59it/s]Extractor Estimating: 151it [01:41,  1.56it/s]Extractor Estimating: 152it [01:41,  1.57it/s]Extractor Estimating: 153it [01:42,  1.56it/s]Extractor Estimating: 154it [01:43,  1.44it/s]Extractor Estimating: 155it [01:43,  1.42it/s]Extractor Estimating: 156it [01:44,  1.47it/s]Extractor Estimating: 157it [01:45,  1.54it/s]Extractor Estimating: 158it [01:45,  1.54it/s]Extractor Estimating: 159it [01:46,  1.51it/s]Extractor Estimating: 160it [01:47,  1.50it/s]Extractor Estimating: 161it [01:47,  1.47it/s]Extractor Estimating: 162it [01:48,  1.44it/s]Extractor Estimating: 163it [01:49,  1.44it/s]Extractor Estimating: 164it [01:50,  1.44it/s]Extractor Estimating: 165it [01:50,  1.43it/s]Extractor Estimating: 166it [01:51,  1.28it/s]Extractor Estimating: 167it [01:52,  1.30it/s]Extractor Estimating: 168it [01:53,  1.33it/s]Extractor Estimating: 169it [01:53,  1.39it/s]Extractor Estimating: 170it [01:54,  1.40it/s]Extractor Estimating: 171it [01:55,  1.44it/s]Extractor Estimating: 172it [01:55,  1.42it/s]Extractor Estimating: 173it [01:56,  1.42it/s]Extractor Estimating: 174it [01:57,  1.45it/s]Extractor Estimating: 175it [01:58,  1.37it/s]Extractor Estimating: 176it [01:58,  1.44it/s]Extractor Estimating: 177it [01:59,  1.45it/s]Extractor Estimating: 178it [02:00,  1.42it/s]Extractor Estimating: 179it [02:00,  1.45it/s]Extractor Estimating: 180it [02:01,  1.45it/s]Extractor Estimating: 181it [02:02,  1.45it/s]Extractor Estimating: 182it [02:02,  1.47it/s]Extractor Estimating: 183it [02:03,  1.51it/s]Extractor Estimating: 184it [02:04,  1.52it/s]Extractor Estimating: 185it [02:04,  1.60it/s]Extractor Estimating: 186it [02:05,  1.57it/s]Extractor Estimating: 187it [02:05,  1.51it/s]Extractor Estimating: 188it [02:06,  1.53it/s]Extractor Estimating: 189it [02:07,  1.55it/s]Extractor Estimating: 190it [02:07,  1.56it/s]Extractor Estimating: 191it [02:08,  1.51it/s]Extractor Estimating: 192it [02:09,  1.54it/s]Extractor Estimating: 193it [02:09,  1.53it/s]Extractor Estimating: 194it [02:10,  1.51it/s]Extractor Estimating: 195it [02:11,  1.53it/s]Extractor Estimating: 196it [02:11,  1.57it/s]Extractor Estimating: 197it [02:12,  1.55it/s]Extractor Estimating: 198it [02:13,  1.55it/s]Extractor Estimating: 199it [02:13,  1.55it/s]Extractor Estimating: 200it [02:14,  1.53it/s]Extractor Estimating: 201it [02:15,  1.46it/s]Extractor Estimating: 202it [02:15,  1.48it/s]Extractor Estimating: 203it [02:16,  1.51it/s]Extractor Estimating: 204it [02:17,  1.50it/s]Extractor Estimating: 205it [02:17,  1.54it/s]Extractor Estimating: 206it [02:18,  1.50it/s]Extractor Estimating: 207it [02:19,  1.50it/s]Extractor Estimating: 208it [02:19,  1.50it/s]Extractor Estimating: 209it [02:20,  1.48it/s]Extractor Estimating: 210it [02:21,  1.48it/s]Extractor Estimating: 211it [02:21,  1.44it/s]Extractor Estimating: 212it [02:22,  1.46it/s]Extractor Estimating: 213it [02:23,  1.50it/s]Extractor Estimating: 214it [02:23,  1.48it/s]Extractor Estimating: 215it [02:24,  1.46it/s]Extractor Estimating: 216it [02:25,  1.44it/s]Extractor Estimating: 217it [02:25,  1.46it/s]Extractor Estimating: 218it [02:26,  1.48it/s]Extractor Estimating: 219it [02:27,  1.49it/s]Extractor Estimating: 220it [02:28,  1.46it/s]Extractor Estimating: 221it [02:28,  1.47it/s]Extractor Estimating: 222it [02:29,  1.54it/s]Extractor Estimating: 223it [02:29,  1.50it/s]Extractor Estimating: 224it [02:30,  1.49it/s]Extractor Estimating: 225it [02:31,  1.49it/s]Extractor Estimating: 226it [02:31,  1.48it/s]Extractor Estimating: 227it [02:32,  1.37it/s]Extractor Estimating: 228it [02:33,  1.35it/s]Extractor Estimating: 229it [02:34,  1.41it/s]Extractor Estimating: 230it [02:34,  1.44it/s]Extractor Estimating: 231it [02:35,  1.49it/s]Extractor Estimating: 232it [02:36,  1.47it/s]Extractor Estimating: 233it [02:36,  1.46it/s]Extractor Estimating: 234it [02:37,  1.45it/s]Extractor Estimating: 235it [02:38,  1.50it/s]Extractor Estimating: 236it [02:38,  1.45it/s]Extractor Estimating: 237it [02:39,  1.42it/s]Extractor Estimating: 238it [02:40,  1.46it/s]Extractor Estimating: 239it [02:41,  1.45it/s]Extractor Estimating: 240it [02:41,  1.46it/s]Extractor Estimating: 241it [02:42,  1.39it/s]Extractor Estimating: 242it [02:43,  1.40it/s]Extractor Estimating: 243it [02:43,  1.43it/s]Extractor Estimating: 244it [02:44,  1.49it/s]Extractor Estimating: 245it [02:45,  1.44it/s]Extractor Estimating: 246it [02:45,  1.47it/s]Extractor Estimating: 247it [02:46,  1.50it/s]Extractor Estimating: 248it [02:47,  1.48it/s]Extractor Estimating: 249it [02:47,  1.49it/s]Extractor Estimating: 250it [02:48,  1.44it/s]Extractor Estimating: 250it [02:48,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:02,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:02,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:02,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:02,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:02,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:02:03,598 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:02:03,599 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:02:04,172 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:02:05,211 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:02:05,211 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:08,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:08,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:08,059 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:08,060 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:02:08,060 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:02:08,681 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:02:08,682 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:02:09,439 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:02:09,581 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:02:09,581 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 07:43:06,325 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 07:43:06,350 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5097 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 19052
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19152, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19152, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.212, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.209, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.218, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.203, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.207, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.341, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.207, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.215, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.203, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.200, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.362, loss:nan
g_step 1200, step 135, avg_time 1.196, loss:nan
g_step 1300, step 22, avg_time 1.198, loss:nan
g_step 1400, step 122, avg_time 1.214, loss:nan
g_step 1500, step 9, avg_time 1.212, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.331, loss:nan
g_step 1700, step 209, avg_time 1.228, loss:nan
g_step 1800, step 96, avg_time 1.209, loss:nan
g_step 1900, step 196, avg_time 1.209, loss:nan
g_step 2000, step 83, avg_time 1.211, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.343, loss:nan
g_step 2200, step 70, avg_time 1.200, loss:nan
g_step 2300, step 170, avg_time 1.221, loss:nan
g_step 2400, step 57, avg_time 1.210, loss:nan
g_step 2500, step 157, avg_time 1.211, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.318, loss:nan
g_step 2700, step 144, avg_time 1.202, loss:nan
g_step 2800, step 31, avg_time 1.223, loss:nan
g_step 2900, step 131, avg_time 1.214, loss:nan
g_step 3000, step 18, avg_time 1.206, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.347, loss:nan
g_step 3200, step 5, avg_time 1.203, loss:nan
g_step 3300, step 105, avg_time 1.204, loss:nan
g_step 3400, step 205, avg_time 1.213, loss:nan
g_step 3500, step 92, avg_time 1.207, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.336, loss:nan
g_step 3700, step 79, avg_time 1.206, loss:nan
g_step 3800, step 179, avg_time 1.198, loss:nan
g_step 3900, step 66, avg_time 1.215, loss:nan
g_step 4000, step 166, avg_time 1.213, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.334, loss:nan
g_step 4200, step 153, avg_time 1.198, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 07:43:06 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 07:43:06 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_07-43-06_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 07:43:07 - WARNING - datasets.builder -   Using custom data configuration default-a9bee86aeeb44f89
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a9bee86aeeb44f89/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 07:43:07,685 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:43:07,686 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:43:07,686 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:43:07,687 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:43:07,693 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:43:07,696 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:43:07,697 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:43:07,697 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:43:07,697 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:43:07,697 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:43:07,697 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 07:43:07,848 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:43:11,026 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 07:43:11,029 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a9bee86aeeb44f89/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.96ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.77ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.15ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.36ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.48ba/s]100%|██████████| 6/6 [00:01<00:00,  4.94ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.35ba/s]100%|██████████| 4/4 [00:00<00:00,  5.42ba/s]100%|██████████| 4/4 [00:00<00:00,  4.91ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  6.74ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.91ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.56ba/s]100%|██████████| 6/6 [00:00<00:00, 10.83ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.35ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.19ba/s]100%|██████████| 4/4 [00:00<00:00, 10.20ba/s]
[INFO|trainer.py:414] 2023-08-28 07:43:14,407 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 07:43:14,420 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 07:43:14,420 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 07:43:14,420 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 07:43:14,420 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 07:43:14,421 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 07:43:14,421 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 07:43:14,421 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<02:05,  3.19it/s]  0%|          | 2/400 [00:00<01:55,  3.46it/s]  1%|          | 3/400 [00:00<01:51,  3.56it/s]  1%|          | 4/400 [00:01<01:49,  3.60it/s]  1%|▏         | 5/400 [00:01<01:48,  3.63it/s]  2%|▏         | 6/400 [00:01<01:48,  3.64it/s]  2%|▏         | 7/400 [00:01<01:47,  3.66it/s]  2%|▏         | 8/400 [00:02<01:46,  3.67it/s]  2%|▏         | 9/400 [00:02<01:46,  3.67it/s]  2%|▎         | 10/400 [00:02<01:46,  3.67it/s]  3%|▎         | 11/400 [00:03<01:46,  3.66it/s]  3%|▎         | 12/400 [00:03<01:45,  3.67it/s]  3%|▎         | 13/400 [00:03<01:45,  3.67it/s]  4%|▎         | 14/400 [00:03<01:45,  3.67it/s]  4%|▍         | 15/400 [00:04<01:44,  3.67it/s]  4%|▍         | 16/400 [00:04<01:44,  3.67it/s]  4%|▍         | 17/400 [00:04<01:44,  3.67it/s]  4%|▍         | 18/400 [00:04<01:43,  3.68it/s]  5%|▍         | 19/400 [00:05<01:43,  3.67it/s]  5%|▌         | 20/400 [00:05<01:43,  3.68it/s]  5%|▌         | 21/400 [00:05<01:43,  3.68it/s]  6%|▌         | 22/400 [00:06<01:42,  3.67it/s]  6%|▌         | 23/400 [00:06<01:43,  3.66it/s]  6%|▌         | 24/400 [00:06<01:42,  3.66it/s]  6%|▋         | 25/400 [00:06<01:42,  3.66it/s]  6%|▋         | 26/400 [00:07<01:42,  3.67it/s]  7%|▋         | 27/400 [00:07<01:41,  3.67it/s]  7%|▋         | 28/400 [00:07<01:41,  3.67it/s]  7%|▋         | 29/400 [00:07<01:41,  3.67it/s]  8%|▊         | 30/400 [00:08<01:40,  3.67it/s]  8%|▊         | 31/400 [00:08<01:40,  3.67it/s]  8%|▊         | 32/400 [00:08<01:40,  3.67it/s]  8%|▊         | 33/400 [00:09<01:40,  3.66it/s]  8%|▊         | 34/400 [00:09<01:40,  3.65it/s]  9%|▉         | 35/400 [00:09<01:39,  3.66it/s]  9%|▉         | 36/400 [00:09<01:39,  3.66it/s]  9%|▉         | 37/400 [00:10<01:39,  3.66it/s] 10%|▉         | 38/400 [00:10<01:38,  3.67it/s] 10%|▉         | 39/400 [00:10<01:38,  3.67it/s] 10%|█         | 40/400 [00:10<01:38,  3.67it/s] 10%|█         | 41/400 [00:11<01:37,  3.67it/s] 10%|█         | 42/400 [00:11<01:37,  3.67it/s] 11%|█         | 43/400 [00:11<01:37,  3.67it/s] 11%|█         | 44/400 [00:12<01:37,  3.67it/s] 11%|█▏        | 45/400 [00:12<01:37,  3.66it/s] 12%|█▏        | 46/400 [00:12<01:36,  3.65it/s] 12%|█▏        | 47/400 [00:12<01:36,  3.66it/s] 12%|█▏        | 48/400 [00:13<01:36,  3.66it/s] 12%|█▏        | 49/400 [00:13<01:35,  3.66it/s] 12%|█▎        | 50/400 [00:13<01:35,  3.66it/s] 13%|█▎        | 51/400 [00:13<01:35,  3.66it/s] 13%|█▎        | 52/400 [00:14<01:34,  3.67it/s] 13%|█▎        | 53/400 [00:14<01:34,  3.66it/s] 14%|█▎        | 54/400 [00:14<01:34,  3.67it/s] 14%|█▍        | 55/400 [00:15<01:34,  3.67it/s] 14%|█▍        | 56/400 [00:15<01:34,  3.66it/s] 14%|█▍        | 57/400 [00:15<01:33,  3.66it/s] 14%|█▍        | 58/400 [00:15<01:33,  3.66it/s] 15%|█▍        | 59/400 [00:16<01:32,  3.67it/s] 15%|█▌        | 60/400 [00:16<01:32,  3.67it/s] 15%|█▌        | 61/400 [00:16<01:32,  3.67it/s] 16%|█▌        | 62/400 [00:16<01:32,  3.67it/s] 16%|█▌        | 63/400 [00:17<01:31,  3.67it/s] 16%|█▌        | 64/400 [00:17<01:31,  3.67it/s] 16%|█▋        | 65/400 [00:17<01:31,  3.67it/s] 16%|█▋        | 66/400 [00:18<01:31,  3.67it/s] 17%|█▋        | 67/400 [00:18<01:30,  3.67it/s] 17%|█▋        | 68/400 [00:18<01:30,  3.67it/s] 17%|█▋        | 69/400 [00:18<01:30,  3.67it/s] 18%|█▊        | 70/400 [00:19<01:29,  3.67it/s] 18%|█▊        | 71/400 [00:19<01:29,  3.67it/s] 18%|█▊        | 72/400 [00:19<01:29,  3.67it/s] 18%|█▊        | 73/400 [00:19<01:29,  3.67it/s] 18%|█▊        | 74/400 [00:20<01:28,  3.67it/s] 19%|█▉        | 75/400 [00:20<01:28,  3.67it/s] 19%|█▉        | 76/400 [00:20<01:28,  3.65it/s] 19%|█▉        | 77/400 [00:21<01:28,  3.66it/s] 20%|█▉        | 78/400 [00:21<01:28,  3.66it/s] 20%|█▉        | 79/400 [00:21<01:27,  3.66it/s] 20%|██        | 80/400 [00:21<01:20,  4.00it/s][INFO|trainer.py:2140] 2023-08-28 07:43:36,204 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:43:36,204 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 07:43:36,204 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.23it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.54it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.63it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.93it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.44it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.18it/s][A
  9%|▊         | 38/437 [00:00<00:08, 48.01it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.76it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.72it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.72it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.69it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.62it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.60it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.52it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.49it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.50it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.46it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.41it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.39it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.48it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.49it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.55it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.45it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.55it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.52it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.46it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.39it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.43it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.38it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.41it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.43it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.54it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.53it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.45it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.50it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.41it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.32it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.35it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.37it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.16it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.39it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.46it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.46it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.41it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.42it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.42it/s][A
 54%|█████▍    | 238/437 [00:04<00:04, 47.35it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.38it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.41it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.43it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.43it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.34it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.46it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.45it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.44it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.35it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.39it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.34it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.34it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.40it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.44it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.40it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.43it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.52it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.42it/s][A
 76%|███████▌  | 333/437 [00:06<00:02, 47.44it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.34it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.38it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.31it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.26it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.40it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.37it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.42it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.43it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.47it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.40it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.24it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.32it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.33it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.34it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.39it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.49it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.47it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.46it/s][A
 98%|█████████▊| 428/437 [00:08<00:00, 47.45it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.45it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.45it/s][A 20%|██        | 80/400 [00:30<01:20,  4.00it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:43:45,451 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 07:43:45,484 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:43:47,924 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:43:47,944 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:43:47,952 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:34<20:32,  3.86s/it] 20%|██        | 82/400 [00:34<14:46,  2.79s/it] 21%|██        | 83/400 [00:34<10:44,  2.03s/it] 21%|██        | 84/400 [00:34<07:55,  1.50s/it] 21%|██▏       | 85/400 [00:35<05:57,  1.14s/it] 22%|██▏       | 86/400 [00:35<04:35,  1.14it/s] 22%|██▏       | 87/400 [00:35<03:37,  1.44it/s] 22%|██▏       | 88/400 [00:35<02:57,  1.76it/s] 22%|██▏       | 89/400 [00:36<02:29,  2.08it/s] 22%|██▎       | 90/400 [00:36<02:09,  2.39it/s] 23%|██▎       | 91/400 [00:36<01:55,  2.67it/s] 23%|██▎       | 92/400 [00:37<01:45,  2.91it/s] 23%|██▎       | 93/400 [00:37<01:39,  3.10it/s] 24%|██▎       | 94/400 [00:37<01:34,  3.25it/s] 24%|██▍       | 95/400 [00:37<01:30,  3.37it/s] 24%|██▍       | 96/400 [00:38<01:28,  3.45it/s] 24%|██▍       | 97/400 [00:38<01:26,  3.51it/s] 24%|██▍       | 98/400 [00:38<01:24,  3.56it/s] 25%|██▍       | 99/400 [00:38<01:23,  3.59it/s] 25%|██▌       | 100/400 [00:39<01:23,  3.61it/s] 25%|██▌       | 101/400 [00:39<01:22,  3.63it/s] 26%|██▌       | 102/400 [00:39<01:21,  3.64it/s] 26%|██▌       | 103/400 [00:40<01:21,  3.64it/s] 26%|██▌       | 104/400 [00:40<01:21,  3.63it/s] 26%|██▋       | 105/400 [00:40<01:20,  3.64it/s] 26%|██▋       | 106/400 [00:40<01:20,  3.65it/s] 27%|██▋       | 107/400 [00:41<01:20,  3.66it/s] 27%|██▋       | 108/400 [00:41<01:19,  3.65it/s] 27%|██▋       | 109/400 [00:41<01:19,  3.66it/s] 28%|██▊       | 110/400 [00:41<01:19,  3.66it/s] 28%|██▊       | 111/400 [00:42<01:18,  3.66it/s] 28%|██▊       | 112/400 [00:42<01:18,  3.66it/s] 28%|██▊       | 113/400 [00:42<01:18,  3.66it/s] 28%|██▊       | 114/400 [00:43<01:18,  3.66it/s] 29%|██▉       | 115/400 [00:43<01:18,  3.65it/s] 29%|██▉       | 116/400 [00:43<01:17,  3.65it/s] 29%|██▉       | 117/400 [00:43<01:17,  3.66it/s] 30%|██▉       | 118/400 [00:44<01:16,  3.66it/s] 30%|██▉       | 119/400 [00:44<01:16,  3.66it/s] 30%|███       | 120/400 [00:44<01:16,  3.66it/s] 30%|███       | 121/400 [00:45<01:16,  3.66it/s] 30%|███       | 122/400 [00:45<01:15,  3.66it/s] 31%|███       | 123/400 [00:45<01:15,  3.66it/s] 31%|███       | 124/400 [00:45<01:15,  3.66it/s] 31%|███▏      | 125/400 [00:46<01:15,  3.66it/s] 32%|███▏      | 126/400 [00:46<01:15,  3.65it/s] 32%|███▏      | 127/400 [00:46<01:14,  3.65it/s] 32%|███▏      | 128/400 [00:46<01:14,  3.65it/s] 32%|███▏      | 129/400 [00:47<01:14,  3.65it/s] 32%|███▎      | 130/400 [00:47<01:13,  3.65it/s] 33%|███▎      | 131/400 [00:47<01:13,  3.65it/s] 33%|███▎      | 132/400 [00:48<01:13,  3.66it/s] 33%|███▎      | 133/400 [00:48<01:13,  3.66it/s] 34%|███▎      | 134/400 [00:48<01:12,  3.66it/s] 34%|███▍      | 135/400 [00:48<01:12,  3.66it/s] 34%|███▍      | 136/400 [00:49<01:12,  3.65it/s] 34%|███▍      | 137/400 [00:49<01:12,  3.64it/s] 34%|███▍      | 138/400 [00:49<01:11,  3.64it/s] 35%|███▍      | 139/400 [00:49<01:11,  3.65it/s] 35%|███▌      | 140/400 [00:50<01:11,  3.65it/s] 35%|███▌      | 141/400 [00:50<01:10,  3.66it/s] 36%|███▌      | 142/400 [00:50<01:10,  3.65it/s] 36%|███▌      | 143/400 [00:51<01:10,  3.64it/s] 36%|███▌      | 144/400 [00:51<01:10,  3.65it/s] 36%|███▋      | 145/400 [00:51<01:09,  3.65it/s] 36%|███▋      | 146/400 [00:51<01:09,  3.65it/s] 37%|███▋      | 147/400 [00:52<01:10,  3.57it/s] 37%|███▋      | 148/400 [00:52<01:10,  3.56it/s] 37%|███▋      | 149/400 [00:52<01:09,  3.59it/s] 38%|███▊      | 150/400 [00:52<01:09,  3.61it/s] 38%|███▊      | 151/400 [00:53<01:08,  3.62it/s] 38%|███▊      | 152/400 [00:53<01:08,  3.63it/s] 38%|███▊      | 153/400 [00:53<01:07,  3.64it/s] 38%|███▊      | 154/400 [00:54<01:07,  3.64it/s] 39%|███▉      | 155/400 [00:54<01:07,  3.65it/s] 39%|███▉      | 156/400 [00:54<01:06,  3.65it/s] 39%|███▉      | 157/400 [00:54<01:06,  3.66it/s] 40%|███▉      | 158/400 [00:55<01:06,  3.65it/s] 40%|███▉      | 159/400 [00:55<01:06,  3.63it/s] 40%|████      | 160/400 [00:55<01:00,  3.97it/s][INFO|trainer.py:2140] 2023-08-28 07:44:10,061 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:44:10,061 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 07:44:10,061 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2148, 'eval_samples_per_second': 379.388, 'eval_steps_per_second': 47.424, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.17it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.46it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.84it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.35it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.05it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.72it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.36it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.36it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.35it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.41it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.39it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.44it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.50it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.40it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.34it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.23it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.27it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.24it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.28it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.36it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.37it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.37it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.40it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.40it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.32it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.24it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.28it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.25it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.32it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.32it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.40it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.34it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.28it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.30it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.21it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.20it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.29it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.37it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.34it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.35it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.33it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.33it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.24it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.26it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.30it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.26it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.31it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.34it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.34it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.22it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.29it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.33it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.26it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.26it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.30it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.34it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.35it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.35it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.34it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.28it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.25it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.27it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.27it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.26it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.33it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.36it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.30it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.23it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.29it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.29it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.23it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.30it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.31it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.30it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.28it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.27it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.20it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.26it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.26it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.21it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.26it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.29it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.31it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.27it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.27it/s][A 40%|████      | 160/400 [01:04<01:00,  3.97it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:44:19,327 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 07:44:19,340 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:44:21,526 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:44:21,539 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:44:21,552 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:07<15:05,  3.79s/it] 40%|████      | 162/400 [01:07<10:50,  2.73s/it] 41%|████      | 163/400 [01:08<07:53,  2.00s/it] 41%|████      | 164/400 [01:08<05:49,  1.48s/it] 41%|████▏     | 165/400 [01:08<04:22,  1.12s/it] 42%|████▏     | 166/400 [01:09<03:22,  1.16it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:09<02:40,  1.46it/s] 42%|████▏     | 168/400 [01:09<02:10,  1.78it/s] 42%|████▏     | 169/400 [01:09<01:49,  2.10it/s] 42%|████▎     | 170/400 [01:10<01:35,  2.41it/s] 43%|████▎     | 171/400 [01:10<01:25,  2.68it/s] 43%|████▎     | 172/400 [01:10<01:18,  2.91it/s] 43%|████▎     | 173/400 [01:10<01:13,  3.11it/s] 44%|████▎     | 174/400 [01:11<01:09,  3.25it/s] 44%|████▍     | 175/400 [01:11<01:06,  3.36it/s] 44%|████▍     | 176/400 [01:11<01:05,  3.45it/s] 44%|████▍     | 177/400 [01:12<01:03,  3.50it/s] 44%|████▍     | 178/400 [01:12<01:02,  3.55it/s] 45%|████▍     | 179/400 [01:12<01:01,  3.58it/s] 45%|████▌     | 180/400 [01:12<01:01,  3.60it/s] 45%|████▌     | 181/400 [01:13<01:00,  3.62it/s] 46%|████▌     | 182/400 [01:13<01:14,  2.92it/s] 46%|████▌     | 183/400 [01:13<01:09,  3.11it/s] 46%|████▌     | 184/400 [01:14<01:06,  3.25it/s] 46%|████▋     | 185/400 [01:14<01:03,  3.36it/s] 46%|████▋     | 186/400 [01:14<01:02,  3.44it/s] 47%|████▋     | 187/400 [01:15<01:00,  3.50it/s] 47%|████▋     | 188/400 [01:15<00:59,  3.55it/s] 47%|████▋     | 189/400 [01:15<00:58,  3.58it/s] 48%|████▊     | 190/400 [01:15<00:58,  3.60it/s] 48%|████▊     | 191/400 [01:16<00:57,  3.61it/s] 48%|████▊     | 192/400 [01:16<00:57,  3.63it/s] 48%|████▊     | 193/400 [01:16<00:57,  3.61it/s] 48%|████▊     | 194/400 [01:16<00:56,  3.62it/s] 49%|████▉     | 195/400 [01:17<00:56,  3.63it/s] 49%|████▉     | 196/400 [01:17<00:56,  3.64it/s] 49%|████▉     | 197/400 [01:17<00:55,  3.63it/s] 50%|████▉     | 198/400 [01:18<00:55,  3.64it/s] 50%|████▉     | 199/400 [01:18<00:55,  3.64it/s] 50%|█████     | 200/400 [01:18<00:54,  3.64it/s] 50%|█████     | 201/400 [01:18<00:54,  3.65it/s] 50%|█████     | 202/400 [01:19<00:54,  3.65it/s] 51%|█████     | 203/400 [01:19<00:53,  3.65it/s] 51%|█████     | 204/400 [01:19<00:53,  3.63it/s] 51%|█████▏    | 205/400 [01:19<00:53,  3.64it/s] 52%|█████▏    | 206/400 [01:20<00:53,  3.64it/s] 52%|█████▏    | 207/400 [01:20<00:52,  3.65it/s] 52%|█████▏    | 208/400 [01:20<00:52,  3.64it/s] 52%|█████▏    | 209/400 [01:21<00:52,  3.64it/s] 52%|█████▎    | 210/400 [01:21<00:52,  3.65it/s] 53%|█████▎    | 211/400 [01:21<00:51,  3.65it/s] 53%|█████▎    | 212/400 [01:21<00:51,  3.65it/s] 53%|█████▎    | 213/400 [01:22<00:51,  3.64it/s] 54%|█████▎    | 214/400 [01:22<00:51,  3.65it/s] 54%|█████▍    | 215/400 [01:22<00:50,  3.64it/s] 54%|█████▍    | 216/400 [01:22<00:50,  3.64it/s] 54%|█████▍    | 217/400 [01:23<00:50,  3.65it/s] 55%|█████▍    | 218/400 [01:23<00:49,  3.65it/s] 55%|█████▍    | 219/400 [01:23<00:49,  3.65it/s] 55%|█████▌    | 220/400 [01:24<00:49,  3.65it/s] 55%|█████▌    | 221/400 [01:24<00:49,  3.65it/s] 56%|█████▌    | 222/400 [01:24<00:48,  3.65it/s] 56%|█████▌    | 223/400 [01:24<00:48,  3.65it/s] 56%|█████▌    | 224/400 [01:25<00:48,  3.65it/s] 56%|█████▋    | 225/400 [01:25<00:47,  3.65it/s] 56%|█████▋    | 226/400 [01:25<00:47,  3.64it/s] 57%|█████▋    | 227/400 [01:25<00:47,  3.64it/s] 57%|█████▋    | 228/400 [01:26<00:47,  3.64it/s] 57%|█████▋    | 229/400 [01:26<00:46,  3.64it/s] 57%|█████▊    | 230/400 [01:26<00:46,  3.65it/s] 58%|█████▊    | 231/400 [01:27<00:46,  3.65it/s] 58%|█████▊    | 232/400 [01:27<00:46,  3.64it/s] 58%|█████▊    | 233/400 [01:27<00:45,  3.65it/s] 58%|█████▊    | 234/400 [01:27<00:45,  3.65it/s] 59%|█████▉    | 235/400 [01:28<00:45,  3.65it/s] 59%|█████▉    | 236/400 [01:28<00:44,  3.65it/s] 59%|█████▉    | 237/400 [01:28<00:44,  3.64it/s] 60%|█████▉    | 238/400 [01:29<00:44,  3.64it/s] 60%|█████▉    | 239/400 [01:29<00:44,  3.64it/s] 60%|██████    | 240/400 [01:29<00:40,  3.98it/s][INFO|trainer.py:2140] 2023-08-28 07:44:43,905 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:44:43,905 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 07:44:43,905 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2396, 'eval_samples_per_second': 378.372, 'eval_steps_per_second': 47.296, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.09it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.26it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.43it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.67it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.28it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.00it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.70it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.34it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.16it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.13it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.23it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.28it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.34it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.31it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.41it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.35it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.15it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.08it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.07it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.10it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.21it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.29it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.35it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.34it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.21it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.11it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.11it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.12it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.67it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.10it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.19it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.28it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.28it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.28it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.21it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.08it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.10it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.07it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.13it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.17it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.27it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.32it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.33it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.31it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.18it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.06it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.13it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.14it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.13it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.22it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.28it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.31it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.28it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.24it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.10it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.03it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.12it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.13it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.13it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.20it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.29it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.27it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.20it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.19it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.10it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.04it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.13it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.13it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.14it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.19it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.28it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.27it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.22it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.18it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.12it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.10it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.15it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.16it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.16it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.21it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.29it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.24it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.20it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.20it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.16it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.16it/s][A 60%|██████    | 240/400 [01:38<00:40,  3.98it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:44:53,183 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 07:44:53,204 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:44:55,413 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:44:55,445 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:44:55,456 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [01:41<10:04,  3.80s/it] 60%|██████    | 242/400 [01:41<07:13,  2.74s/it] 61%|██████    | 243/400 [01:42<05:14,  2.00s/it] 61%|██████    | 244/400 [01:42<03:51,  1.48s/it] 61%|██████▏   | 245/400 [01:42<02:53,  1.12s/it] 62%|██████▏   | 246/400 [01:42<02:13,  1.15it/s] 62%|██████▏   | 247/400 [01:43<01:45,  1.45it/s] 62%|██████▏   | 248/400 [01:43<01:25,  1.77it/s] 62%|██████▏   | 249/400 [01:43<01:11,  2.10it/s] 62%|██████▎   | 250/400 [01:44<01:02,  2.40it/s] 63%|██████▎   | 251/400 [01:44<00:55,  2.67it/s] 63%|██████▎   | 252/400 [01:44<00:50,  2.91it/s] 63%|██████▎   | 253/400 [01:44<00:47,  3.09it/s] 64%|██████▎   | 254/400 [01:45<00:45,  3.24it/s] 64%|██████▍   | 255/400 [01:45<00:43,  3.35it/s] 64%|██████▍   | 256/400 [01:45<00:41,  3.44it/s] 64%|██████▍   | 257/400 [01:45<00:40,  3.50it/s] 64%|██████▍   | 258/400 [01:46<00:40,  3.54it/s] 65%|██████▍   | 259/400 [01:46<00:39,  3.58it/s] 65%|██████▌   | 260/400 [01:46<00:38,  3.60it/s] 65%|██████▌   | 261/400 [01:47<00:38,  3.61it/s] 66%|██████▌   | 262/400 [01:47<00:38,  3.62it/s] 66%|██████▌   | 263/400 [01:47<00:37,  3.62it/s] 66%|██████▌   | 264/400 [01:47<00:37,  3.64it/s] 66%|██████▋   | 265/400 [01:48<00:37,  3.64it/s] 66%|██████▋   | 266/400 [01:48<00:36,  3.64it/s] 67%|██████▋   | 267/400 [01:48<00:36,  3.65it/s] 67%|██████▋   | 268/400 [01:48<00:36,  3.65it/s] 67%|██████▋   | 269/400 [01:49<00:35,  3.65it/s] 68%|██████▊   | 270/400 [01:49<00:35,  3.65it/s] 68%|██████▊   | 271/400 [01:49<00:35,  3.65it/s] 68%|██████▊   | 272/400 [01:50<00:35,  3.65it/s] 68%|██████▊   | 273/400 [01:50<00:35,  3.63it/s] 68%|██████▊   | 274/400 [01:50<00:34,  3.64it/s] 69%|██████▉   | 275/400 [01:50<00:34,  3.64it/s] 69%|██████▉   | 276/400 [01:51<00:34,  3.56it/s] 69%|██████▉   | 277/400 [01:51<00:34,  3.57it/s] 70%|██████▉   | 278/400 [01:51<00:33,  3.60it/s] 70%|██████▉   | 279/400 [01:52<00:33,  3.61it/s] 70%|███████   | 280/400 [01:52<00:33,  3.53it/s] 70%|███████   | 281/400 [01:52<00:33,  3.54it/s] 70%|███████   | 282/400 [01:52<00:32,  3.58it/s] 71%|███████   | 283/400 [01:53<00:32,  3.60it/s] 71%|███████   | 284/400 [01:53<00:32,  3.61it/s] 71%|███████▏  | 285/400 [01:53<00:31,  3.62it/s] 72%|███████▏  | 286/400 [01:53<00:31,  3.63it/s] 72%|███████▏  | 287/400 [01:54<00:31,  3.64it/s] 72%|███████▏  | 288/400 [01:54<00:30,  3.64it/s] 72%|███████▏  | 289/400 [01:54<00:30,  3.65it/s] 72%|███████▎  | 290/400 [01:55<00:30,  3.65it/s] 73%|███████▎  | 291/400 [01:55<00:29,  3.64it/s] 73%|███████▎  | 292/400 [01:55<00:29,  3.65it/s] 73%|███████▎  | 293/400 [01:55<00:29,  3.65it/s] 74%|███████▎  | 294/400 [01:56<00:29,  3.65it/s] 74%|███████▍  | 295/400 [01:56<00:28,  3.62it/s] 74%|███████▍  | 296/400 [01:56<00:28,  3.63it/s] 74%|███████▍  | 297/400 [01:56<00:28,  3.64it/s] 74%|███████▍  | 298/400 [01:57<00:27,  3.64it/s] 75%|███████▍  | 299/400 [01:57<00:27,  3.64it/s] 75%|███████▌  | 300/400 [01:57<00:27,  3.65it/s] 75%|███████▌  | 301/400 [01:58<00:27,  3.65it/s] 76%|███████▌  | 302/400 [01:58<00:26,  3.65it/s] 76%|███████▌  | 303/400 [01:58<00:26,  3.65it/s] 76%|███████▌  | 304/400 [01:58<00:26,  3.65it/s] 76%|███████▋  | 305/400 [01:59<00:26,  3.65it/s] 76%|███████▋  | 306/400 [01:59<00:25,  3.62it/s] 77%|███████▋  | 307/400 [01:59<00:25,  3.63it/s] 77%|███████▋  | 308/400 [01:59<00:25,  3.63it/s] 77%|███████▋  | 309/400 [02:00<00:24,  3.64it/s] 78%|███████▊  | 310/400 [02:00<00:24,  3.65it/s] 78%|███████▊  | 311/400 [02:00<00:24,  3.64it/s] 78%|███████▊  | 312/400 [02:01<00:24,  3.64it/s] 78%|███████▊  | 313/400 [02:01<00:23,  3.65it/s] 78%|███████▊  | 314/400 [02:01<00:23,  3.65it/s] 79%|███████▉  | 315/400 [02:01<00:23,  3.65it/s] 79%|███████▉  | 316/400 [02:02<00:23,  3.65it/s] 79%|███████▉  | 317/400 [02:02<00:22,  3.64it/s] 80%|███████▉  | 318/400 [02:02<00:22,  3.64it/s] 80%|███████▉  | 319/400 [02:03<00:22,  3.65it/s] 80%|████████  | 320/400 [02:03<00:20,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 07:45:17,631 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:45:17,631 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 07:45:17,631 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2618, 'eval_samples_per_second': 377.463, 'eval_steps_per_second': 47.183, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.34it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.08it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.31it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.64it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.22it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.95it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.64it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.29it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.18it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.17it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.17it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.23it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.31it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.29it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.34it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.28it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.16it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.07it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.07it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.09it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.13it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.22it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.29it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.30it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.23it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.11it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.08it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.04it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.08it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.13it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.22it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.30it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.27it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.22it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.18it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.05it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.08it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.12it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.07it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.14it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.19it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.23it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.20it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.19it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.06it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.00it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.10it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.11it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.11it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.19it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.26it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.24it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.17it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.17it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.16it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.10it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.15it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.16it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.15it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.20it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.23it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.20it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.18it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.20it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.15it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.12it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.18it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.18it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.17it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.22it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.21it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.09it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.05it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.06it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.01it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.97it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.03it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.96it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.06it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.15it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.14it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.06it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.10it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.06it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.02it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.02it/s][A 80%|████████  | 320/400 [02:12<00:20,  3.99it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:45:26,922 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 07:45:26,942 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:45:29,212 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:45:29,229 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:45:29,237 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [02:15<05:03,  3.84s/it] 80%|████████  | 322/400 [02:15<03:35,  2.77s/it] 81%|████████  | 323/400 [02:15<02:35,  2.02s/it] 81%|████████  | 324/400 [02:16<01:53,  1.50s/it] 81%|████████▏ | 325/400 [02:16<01:24,  1.13s/it] 82%|████████▏ | 326/400 [02:16<01:04,  1.15it/s] 82%|████████▏ | 327/400 [02:17<00:50,  1.44it/s] 82%|████████▏ | 328/400 [02:17<00:40,  1.76it/s] 82%|████████▏ | 329/400 [02:17<00:34,  2.09it/s] 82%|████████▎ | 330/400 [02:17<00:29,  2.39it/s] 83%|████████▎ | 331/400 [02:18<00:25,  2.66it/s] 83%|████████▎ | 332/400 [02:18<00:23,  2.90it/s] 83%|████████▎ | 333/400 [02:18<00:21,  3.09it/s] 84%|████████▎ | 334/400 [02:18<00:20,  3.24it/s] 84%|████████▍ | 335/400 [02:19<00:19,  3.36it/s] 84%|████████▍ | 336/400 [02:19<00:18,  3.44it/s] 84%|████████▍ | 337/400 [02:19<00:17,  3.50it/s] 84%|████████▍ | 338/400 [02:20<00:17,  3.55it/s] 85%|████████▍ | 339/400 [02:20<00:17,  3.58it/s] 85%|████████▌ | 340/400 [02:20<00:16,  3.60it/s] 85%|████████▌ | 341/400 [02:20<00:16,  3.62it/s] 86%|████████▌ | 342/400 [02:21<00:16,  3.62it/s] 86%|████████▌ | 343/400 [02:21<00:15,  3.63it/s] 86%|████████▌ | 344/400 [02:21<00:15,  3.63it/s] 86%|████████▋ | 345/400 [02:21<00:15,  3.64it/s] 86%|████████▋ | 346/400 [02:22<00:14,  3.64it/s] 87%|████████▋ | 347/400 [02:22<00:14,  3.65it/s] 87%|████████▋ | 348/400 [02:22<00:14,  3.65it/s] 87%|████████▋ | 349/400 [02:23<00:13,  3.65it/s] 88%|████████▊ | 350/400 [02:23<00:13,  3.65it/s] 88%|████████▊ | 351/400 [02:23<00:13,  3.65it/s] 88%|████████▊ | 352/400 [02:23<00:13,  3.65it/s] 88%|████████▊ | 353/400 [02:24<00:14,  3.16it/s] 88%|████████▊ | 354/400 [02:24<00:13,  3.30it/s] 89%|████████▉ | 355/400 [02:24<00:13,  3.40it/s] 89%|████████▉ | 356/400 [02:25<00:12,  3.47it/s] 89%|████████▉ | 357/400 [02:25<00:12,  3.52it/s] 90%|████████▉ | 358/400 [02:25<00:11,  3.56it/s] 90%|████████▉ | 359/400 [02:25<00:11,  3.59it/s] 90%|█████████ | 360/400 [02:26<00:11,  3.61it/s] 90%|█████████ | 361/400 [02:26<00:10,  3.62it/s] 90%|█████████ | 362/400 [02:26<00:10,  3.63it/s] 91%|█████████ | 363/400 [02:27<00:10,  3.64it/s] 91%|█████████ | 364/400 [02:27<00:09,  3.63it/s] 91%|█████████▏| 365/400 [02:27<00:09,  3.63it/s] 92%|█████████▏| 366/400 [02:27<00:09,  3.64it/s] 92%|█████████▏| 367/400 [02:28<00:09,  3.65it/s] 92%|█████████▏| 368/400 [02:28<00:08,  3.64it/s] 92%|█████████▏| 369/400 [02:28<00:08,  3.65it/s] 92%|█████████▎| 370/400 [02:28<00:08,  3.65it/s] 93%|█████████▎| 371/400 [02:29<00:07,  3.65it/s] 93%|█████████▎| 372/400 [02:29<00:07,  3.65it/s] 93%|█████████▎| 373/400 [02:29<00:07,  3.65it/s] 94%|█████████▎| 374/400 [02:30<00:07,  3.64it/s] 94%|█████████▍| 375/400 [02:30<00:06,  3.61it/s] 94%|█████████▍| 376/400 [02:30<00:06,  3.62it/s] 94%|█████████▍| 377/400 [02:30<00:06,  3.63it/s] 94%|█████████▍| 378/400 [02:31<00:06,  3.63it/s] 95%|█████████▍| 379/400 [02:31<00:05,  3.64it/s] 95%|█████████▌| 380/400 [02:31<00:05,  3.65it/s] 95%|█████████▌| 381/400 [02:31<00:05,  3.65it/s] 96%|█████████▌| 382/400 [02:32<00:04,  3.65it/s] 96%|█████████▌| 383/400 [02:32<00:04,  3.65it/s] 96%|█████████▌| 384/400 [02:32<00:04,  3.66it/s] 96%|█████████▋| 385/400 [02:33<00:04,  3.66it/s] 96%|█████████▋| 386/400 [02:33<00:03,  3.63it/s] 97%|█████████▋| 387/400 [02:33<00:03,  3.64it/s] 97%|█████████▋| 388/400 [02:33<00:03,  3.64it/s] 97%|█████████▋| 389/400 [02:34<00:03,  3.64it/s] 98%|█████████▊| 390/400 [02:34<00:02,  3.64it/s] 98%|█████████▊| 391/400 [02:34<00:02,  3.64it/s] 98%|█████████▊| 392/400 [02:35<00:02,  3.65it/s] 98%|█████████▊| 393/400 [02:35<00:01,  3.65it/s] 98%|█████████▊| 394/400 [02:35<00:01,  3.65it/s] 99%|█████████▉| 395/400 [02:35<00:01,  3.65it/s] 99%|█████████▉| 396/400 [02:36<00:01,  3.65it/s] 99%|█████████▉| 397/400 [02:36<00:00,  3.65it/s]100%|█████████▉| 398/400 [02:36<00:00,  3.65it/s]100%|█████████▉| 399/400 [02:36<00:00,  3.65it/s]100%|██████████| 400/400 [02:37<00:00,  3.98it/s][INFO|trainer.py:2140] 2023-08-28 07:45:51,555 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:45:51,556 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 07:45:51,556 >>   Batch size = 8
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2705, 'eval_samples_per_second': 377.11, 'eval_steps_per_second': 47.139, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.66it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.85it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.25it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.57it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.14it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.82it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.63it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.27it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.23it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.26it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.18it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.21it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.22it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.21it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.17it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.10it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.95it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.88it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.05it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.89it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.94it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.06it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.07it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.07it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.06it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.95it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.81it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.00it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.05it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.04it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.10it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.13it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.12it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.09it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.00it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.87it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.90it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.02it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.08it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.09it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.19it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.20it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.08it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.99it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.92it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.86it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.01it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.06it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.07it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.00it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.06it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.03it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.98it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.94it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.91it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.97it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.03it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.04it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.99it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.03it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.06it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.97it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.95it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.93it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.93it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.07it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.09it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.02it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.12it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.13it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.00it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.97it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.91it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.89it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.04it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.09it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.06it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.07it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.12it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.89it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.91it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.79it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.81it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.89it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.99it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.04it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.04it/s][A100%|██████████| 400/400 [02:46<00:00,  3.98it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:46:00,861 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 07:46:00,885 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:46:04,037 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:46:04,059 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:46:04,068 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 07:46:04,303 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 07:46:04,304 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80 (score: 1.1523417234420776).
                                                 100%|██████████| 400/400 [02:51<00:00,  3.98it/s]100%|██████████| 400/400 [02:51<00:00,  2.33it/s]
[INFO|trainer.py:1894] 2023-08-28 07:46:05,898 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 07:46:05,914 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:46:08,295 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:46:08,312 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:46:08,322 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:46:08,515 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:08,515 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:08,515 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:08,515 >>   train_runtime            = 0:02:51.47
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:08,515 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:08,516 >>   train_samples_per_second =    148.711
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:08,516 >>   train_steps_per_second   =      2.333
{'eval_loss': 1.1523417234420776, 'eval_runtime': 9.2929, 'eval_samples_per_second': 376.202, 'eval_steps_per_second': 47.025, 'epoch': 5.0}
{'train_runtime': 171.4736, 'train_samples_per_second': 148.711, 'train_steps_per_second': 2.333, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 07:46:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 07:46:08,550 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:46:08,550 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 07:46:08,550 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.89it/s]  3%|▎         | 12/437 [00:00<00:08, 51.68it/s]  4%|▍         | 18/437 [00:00<00:08, 49.76it/s]  5%|▌         | 24/437 [00:00<00:08, 48.90it/s]  7%|▋         | 29/437 [00:00<00:08, 48.45it/s]  8%|▊         | 34/437 [00:00<00:08, 48.16it/s]  9%|▉         | 39/437 [00:00<00:08, 47.87it/s] 10%|█         | 44/437 [00:00<00:08, 47.65it/s] 11%|█         | 49/437 [00:01<00:08, 47.53it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.42it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.38it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.40it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.45it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.43it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.35it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.41it/s] 20%|██        | 89/437 [00:01<00:07, 47.33it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.28it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.23it/s] 24%|██▍       | 104/437 [00:02<00:07, 47.25it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.29it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.32it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.39it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.38it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.36it/s] 31%|███       | 134/437 [00:02<00:06, 47.37it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.28it/s] 33%|███▎      | 144/437 [00:03<00:06, 47.22it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.25it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.35it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.25it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.33it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.36it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.32it/s] 41%|████      | 179/437 [00:03<00:05, 47.26it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.24it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.22it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.23it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.22it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.34it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.36it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.28it/s] 50%|█████     | 219/437 [00:04<00:04, 47.32it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.32it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.26it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.23it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.28it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.27it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.23it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.19it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.33it/s] 60%|██████    | 264/437 [00:05<00:03, 47.37it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.31it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.32it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.25it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.19it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.22it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.21it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.19it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.19it/s] 71%|███████   | 309/437 [00:06<00:02, 47.31it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.31it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.33it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.33it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.27it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.24it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.23it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.23it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.17it/s] 81%|████████  | 354/437 [00:07<00:01, 47.25it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.37it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.37it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.25it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.26it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.27it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.23it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.20it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.28it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.25it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.22it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.28it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.33it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.23it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.21it/s] 98%|█████████▊| 429/437 [00:09<00:00, 47.26it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.22it/s]100%|██████████| 437/437 [00:09<00:00, 47.37it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:46:17,799 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   eval_loss               =     1.1523
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   eval_runtime            = 0:00:09.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   eval_samples_per_second =    378.005
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   eval_steps_per_second   =     47.251
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:46:17,799 >>   perplexity              =     3.1656
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:24,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:24,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:24,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:24,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:24,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:46:24,766 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:46:24,767 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:46:25,331 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:46:26,342 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:46:26,342 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:29,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:29,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:29,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:29,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:46:29,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:46:29,865 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:46:29,867 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:46:30,463 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:46:30,609 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:46:30,610 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-160
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-400
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-80
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/generator/iter5/model/checkpoint-320
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:14,  1.72it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:15,  1.72it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:18,  1.62it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:24,  1.67it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:26,  1.72it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:28,  1.72it/s]Extractor Predicting: 48it [00:28,  1.74it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:31,  1.77it/s]Extractor Predicting: 53it [00:31,  1.74it/s]Extractor Predicting: 54it [00:32,  1.73it/s]Extractor Predicting: 55it [00:33,  1.74it/s]Extractor Predicting: 56it [00:33,  1.61it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:36,  1.64it/s]Extractor Predicting: 62it [00:37,  1.64it/s]Extractor Predicting: 63it [00:38,  1.63it/s]Extractor Predicting: 64it [00:38,  1.64it/s]Extractor Predicting: 65it [00:39,  1.64it/s]Extractor Predicting: 66it [00:39,  1.61it/s]Extractor Predicting: 67it [00:40,  1.59it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:41,  1.63it/s]Extractor Predicting: 70it [00:42,  1.68it/s]Extractor Predicting: 71it [00:42,  1.66it/s]Extractor Predicting: 72it [00:43,  1.64it/s]Extractor Predicting: 73it [00:44,  1.66it/s]Extractor Predicting: 74it [00:44,  1.65it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.62it/s]Extractor Predicting: 79it [00:47,  1.60it/s]Extractor Predicting: 80it [00:48,  1.60it/s]Extractor Predicting: 81it [00:49,  1.41it/s]Extractor Predicting: 82it [00:50,  1.48it/s]Extractor Predicting: 83it [00:50,  1.55it/s]Extractor Predicting: 84it [00:51,  1.51it/s]Extractor Predicting: 85it [00:51,  1.54it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:53,  1.57it/s]Extractor Predicting: 88it [00:53,  1.62it/s]Extractor Predicting: 89it [00:54,  1.62it/s]Extractor Predicting: 90it [00:54,  1.62it/s]Extractor Predicting: 91it [00:55,  1.67it/s]Extractor Predicting: 92it [00:56,  1.67it/s]Extractor Predicting: 93it [00:56,  1.70it/s]Extractor Predicting: 94it [00:57,  1.66it/s]Extractor Predicting: 95it [00:57,  1.68it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:59,  1.63it/s]Extractor Predicting: 98it [00:59,  1.62it/s]Extractor Predicting: 99it [01:00,  1.64it/s]Extractor Predicting: 100it [01:00,  1.63it/s]Extractor Predicting: 101it [01:01,  1.63it/s]Extractor Predicting: 102it [01:02,  1.58it/s]Extractor Predicting: 103it [01:02,  1.61it/s]Extractor Predicting: 104it [01:03,  1.61it/s]Extractor Predicting: 105it [01:04,  1.62it/s]Extractor Predicting: 106it [01:04,  1.66it/s]Extractor Predicting: 107it [01:05,  1.64it/s]Extractor Predicting: 108it [01:05,  1.65it/s]Extractor Predicting: 109it [01:06,  1.65it/s]Extractor Predicting: 110it [01:07,  1.64it/s]Extractor Predicting: 111it [01:07,  1.64it/s]Extractor Predicting: 112it [01:08,  1.66it/s]Extractor Predicting: 113it [01:08,  1.67it/s]Extractor Predicting: 114it [01:09,  1.66it/s]Extractor Predicting: 115it [01:10,  1.65it/s]Extractor Predicting: 116it [01:10,  1.72it/s]Extractor Predicting: 117it [01:11,  1.68it/s]Extractor Predicting: 118it [01:11,  1.66it/s]Extractor Predicting: 119it [01:12,  1.61it/s]Extractor Predicting: 120it [01:13,  1.64it/s]Extractor Predicting: 121it [01:13,  1.65it/s]Extractor Predicting: 122it [01:14,  1.63it/s]Extractor Predicting: 123it [01:14,  1.62it/s]Extractor Predicting: 124it [01:15,  1.60it/s]Extractor Predicting: 125it [01:16,  1.57it/s]Extractor Predicting: 126it [01:16,  1.61it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:18,  1.64it/s]Extractor Predicting: 129it [01:18,  1.66it/s]Extractor Predicting: 130it [01:19,  1.70it/s]Extractor Predicting: 131it [01:19,  1.71it/s]Extractor Predicting: 132it [01:20,  1.70it/s]Extractor Predicting: 133it [01:20,  1.69it/s]Extractor Predicting: 134it [01:21,  1.69it/s]Extractor Predicting: 135it [01:22,  1.69it/s]Extractor Predicting: 136it [01:22,  1.68it/s]Extractor Predicting: 137it [01:23,  1.66it/s]Extractor Predicting: 138it [01:24,  1.65it/s]Extractor Predicting: 139it [01:24,  1.66it/s]Extractor Predicting: 140it [01:25,  1.64it/s]Extractor Predicting: 141it [01:25,  1.65it/s]Extractor Predicting: 142it [01:26,  1.63it/s]Extractor Predicting: 143it [01:27,  1.62it/s]Extractor Predicting: 144it [01:27,  1.62it/s]Extractor Predicting: 144it [01:27,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:05,722 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:05,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:05,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:05,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:05,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:48:06,419 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:48:06,420 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:48:06,980 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:48:08,015 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:48:08,015 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:10,881 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:10,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:10,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:10,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:48:10,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:48:11,547 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:48:11,548 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:48:12,183 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:48:12,329 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:48:12,329 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.74it/s]Extractor Predicting: 3it [00:01,  1.81it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.83it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.84it/s]Extractor Predicting: 9it [00:04,  1.86it/s]Extractor Predicting: 10it [00:05,  1.89it/s]Extractor Predicting: 11it [00:05,  1.93it/s]Extractor Predicting: 12it [00:06,  1.88it/s]Extractor Predicting: 13it [00:07,  1.88it/s]Extractor Predicting: 14it [00:07,  1.90it/s]Extractor Predicting: 15it [00:08,  1.90it/s]Extractor Predicting: 16it [00:08,  1.86it/s]Extractor Predicting: 17it [00:09,  1.85it/s]Extractor Predicting: 18it [00:09,  1.89it/s]Extractor Predicting: 19it [00:10,  1.88it/s]Extractor Predicting: 20it [00:10,  1.87it/s]Extractor Predicting: 21it [00:11,  1.90it/s]Extractor Predicting: 22it [00:11,  1.93it/s]Extractor Predicting: 23it [00:12,  1.89it/s]Extractor Predicting: 24it [00:12,  1.85it/s]Extractor Predicting: 25it [00:13,  1.80it/s]Extractor Predicting: 26it [00:13,  1.84it/s]Extractor Predicting: 27it [00:14,  1.85it/s]Extractor Predicting: 28it [00:15,  1.77it/s]Extractor Predicting: 29it [00:15,  1.74it/s]Extractor Predicting: 30it [00:16,  1.67it/s]Extractor Predicting: 31it [00:17,  1.63it/s]Extractor Predicting: 32it [00:17,  1.58it/s]Extractor Predicting: 33it [00:18,  1.58it/s]Extractor Predicting: 34it [00:19,  1.56it/s]Extractor Predicting: 35it [00:19,  1.59it/s]Extractor Predicting: 36it [00:20,  1.60it/s]Extractor Predicting: 37it [00:20,  1.60it/s]Extractor Predicting: 38it [00:21,  1.55it/s]Extractor Predicting: 39it [00:22,  1.56it/s]Extractor Predicting: 40it [00:22,  1.54it/s]Extractor Predicting: 41it [00:23,  1.52it/s]Extractor Predicting: 42it [00:24,  1.54it/s]Extractor Predicting: 43it [00:24,  1.55it/s]Extractor Predicting: 44it [00:25,  1.59it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:26,  1.61it/s]Extractor Predicting: 47it [00:27,  1.47it/s]Extractor Predicting: 48it [00:28,  1.48it/s]Extractor Predicting: 49it [00:28,  1.49it/s]Extractor Predicting: 50it [00:29,  1.51it/s]Extractor Predicting: 51it [00:30,  1.51it/s]Extractor Predicting: 52it [00:30,  1.53it/s]Extractor Predicting: 53it [00:31,  1.53it/s]Extractor Predicting: 54it [00:31,  1.54it/s]Extractor Predicting: 55it [00:32,  1.54it/s]Extractor Predicting: 56it [00:33,  1.52it/s]Extractor Predicting: 57it [00:33,  1.54it/s]Extractor Predicting: 58it [00:34,  1.55it/s]Extractor Predicting: 59it [00:35,  1.57it/s]Extractor Predicting: 60it [00:35,  1.60it/s]Extractor Predicting: 61it [00:36,  1.55it/s]Extractor Predicting: 62it [00:37,  1.56it/s]Extractor Predicting: 63it [00:37,  1.58it/s]Extractor Predicting: 64it [00:38,  1.61it/s]Extractor Predicting: 65it [00:38,  1.62it/s]Extractor Predicting: 66it [00:39,  1.65it/s]Extractor Predicting: 67it [00:40,  1.66it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:42,  1.75it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.66it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:44,  1.63it/s]Extractor Predicting: 76it [00:45,  1.60it/s]Extractor Predicting: 77it [00:46,  1.61it/s]Extractor Predicting: 78it [00:46,  1.60it/s]Extractor Predicting: 79it [00:47,  1.61it/s]Extractor Predicting: 80it [00:48,  1.59it/s]Extractor Predicting: 81it [00:48,  1.59it/s]Extractor Predicting: 82it [00:49,  1.60it/s]Extractor Predicting: 83it [00:49,  1.61it/s]Extractor Predicting: 84it [00:50,  1.56it/s]Extractor Predicting: 85it [00:51,  1.58it/s]Extractor Predicting: 86it [00:51,  1.61it/s]Extractor Predicting: 87it [00:52,  1.67it/s]Extractor Predicting: 88it [00:52,  1.72it/s]Extractor Predicting: 89it [00:53,  1.74it/s]Extractor Predicting: 90it [00:54,  1.75it/s]Extractor Predicting: 91it [00:54,  1.80it/s]Extractor Predicting: 92it [00:55,  1.78it/s]Extractor Predicting: 93it [00:55,  1.83it/s]Extractor Predicting: 94it [00:56,  1.81it/s]Extractor Predicting: 95it [00:56,  1.79it/s]Extractor Predicting: 96it [00:57,  1.79it/s]Extractor Predicting: 97it [00:57,  1.84it/s]Extractor Predicting: 98it [00:58,  1.86it/s]Extractor Predicting: 99it [00:58,  1.86it/s]Extractor Predicting: 100it [00:59,  1.92it/s]Extractor Predicting: 101it [00:59,  1.90it/s]Extractor Predicting: 102it [01:00,  1.84it/s]Extractor Predicting: 103it [01:01,  1.81it/s]Extractor Predicting: 104it [01:01,  1.83it/s]Extractor Predicting: 105it [01:02,  1.85it/s]Extractor Predicting: 106it [01:02,  1.85it/s]Extractor Predicting: 107it [01:03,  1.92it/s]Extractor Predicting: 108it [01:03,  1.85it/s]Extractor Predicting: 109it [01:04,  1.88it/s]Extractor Predicting: 110it [01:04,  1.87it/s]Extractor Predicting: 111it [01:05,  1.88it/s]Extractor Predicting: 112it [01:05,  1.90it/s]Extractor Predicting: 113it [01:06,  1.88it/s]Extractor Predicting: 114it [01:06,  1.85it/s]Extractor Predicting: 115it [01:07,  1.82it/s]Extractor Predicting: 116it [01:08,  1.78it/s]Extractor Predicting: 117it [01:08,  1.74it/s]Extractor Predicting: 118it [01:09,  1.75it/s]Extractor Predicting: 119it [01:09,  1.73it/s]Extractor Predicting: 120it [01:10,  1.64it/s]Extractor Predicting: 121it [01:11,  1.67it/s]Extractor Predicting: 122it [01:11,  1.68it/s]Extractor Predicting: 123it [01:12,  1.66it/s]Extractor Predicting: 124it [01:12,  1.72it/s]Extractor Predicting: 125it [01:13,  1.68it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:14,  1.69it/s]Extractor Predicting: 128it [01:15,  1.67it/s]Extractor Predicting: 129it [01:15,  1.68it/s]Extractor Predicting: 130it [01:16,  1.66it/s]Extractor Predicting: 131it [01:17,  1.65it/s]Extractor Predicting: 132it [01:17,  1.51it/s]Extractor Predicting: 133it [01:18,  1.57it/s]Extractor Predicting: 134it [01:19,  1.59it/s]Extractor Predicting: 135it [01:19,  1.60it/s]Extractor Predicting: 136it [01:20,  1.57it/s]Extractor Predicting: 137it [01:20,  1.62it/s]Extractor Predicting: 138it [01:21,  1.61it/s]Extractor Predicting: 139it [01:22,  1.63it/s]Extractor Predicting: 140it [01:22,  1.61it/s]Extractor Predicting: 141it [01:23,  1.64it/s]Extractor Predicting: 142it [01:24,  1.63it/s]Extractor Predicting: 143it [01:24,  2.05it/s]Extractor Predicting: 143it [01:24,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:43,258 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:43,266 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:43,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:43,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:43,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:49:43,596 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:49:43,597 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:49:44,180 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:49:45,185 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:49:45,185 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:48,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:48,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:48,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:48,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:49:48,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:49:48,685 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:49:48,686 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:49:49,252 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:49:49,397 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:49:49,397 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 2it [00:01,  1.36it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_3', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 11540
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11640, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_5_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:19, 19.32s/it]Extractor Estimating: 2it [00:20,  8.68s/it]Extractor Estimating: 3it [00:22,  5.57s/it]Extractor Estimating: 4it [00:23,  3.60s/it]Extractor Estimating: 5it [00:23,  2.55s/it]Extractor Estimating: 6it [00:24,  1.91s/it]Extractor Estimating: 7it [00:24,  1.49s/it]Extractor Estimating: 8it [00:25,  1.23s/it]Extractor Estimating: 9it [00:26,  1.06s/it]Extractor Estimating: 10it [00:26,  1.07it/s]Extractor Estimating: 11it [00:27,  1.17it/s]Extractor Estimating: 12it [00:28,  1.26it/s]Extractor Estimating: 13it [00:28,  1.32it/s]Extractor Estimating: 14it [00:30,  1.08it/s]Extractor Estimating: 15it [00:31,  1.16it/s]Extractor Estimating: 16it [00:31,  1.26it/s]Extractor Estimating: 17it [00:32,  1.30it/s]Extractor Estimating: 18it [00:33,  1.09it/s]Extractor Estimating: 19it [00:34,  1.20it/s]Extractor Estimating: 20it [00:34,  1.31it/s]Extractor Estimating: 21it [00:35,  1.37it/s]Extractor Estimating: 22it [00:36,  1.42it/s]Extractor Estimating: 23it [00:36,  1.44it/s]Extractor Estimating: 24it [00:37,  1.46it/s]Extractor Estimating: 25it [00:38,  1.50it/s]Extractor Estimating: 26it [00:38,  1.48it/s]Extractor Estimating: 27it [00:39,  1.43it/s]Extractor Estimating: 28it [00:40,  1.45it/s]Extractor Estimating: 29it [00:40,  1.46it/s]Extractor Estimating: 30it [00:41,  1.48it/s]Extractor Estimating: 31it [00:42,  1.48it/s]Extractor Estimating: 32it [00:42,  1.53it/s]Extractor Estimating: 33it [00:43,  1.55it/s]Extractor Estimating: 34it [00:44,  1.55it/s]Extractor Estimating: 35it [00:44,  1.55it/s]Extractor Estimating: 36it [00:45,  1.51it/s]Extractor Estimating: 37it [00:46,  1.54it/s]Extractor Estimating: 38it [00:46,  1.46it/s]Extractor Estimating: 39it [00:47,  1.42it/s]Extractor Estimating: 40it [00:48,  1.43it/s]Extractor Estimating: 41it [00:48,  1.43it/s]Extractor Estimating: 42it [00:50,  1.12it/s]Extractor Estimating: 43it [00:51,  1.15it/s]Extractor Estimating: 44it [00:51,  1.22it/s]Extractor Estimating: 45it [00:52,  1.31it/s]Extractor Estimating: 46it [00:53,  1.33it/s]Extractor Estimating: 47it [00:53,  1.39it/s]Extractor Estimating: 48it [00:54,  1.36it/s]Extractor Estimating: 49it [00:55,  1.43it/s]Extractor Estimating: 50it [00:55,  1.45it/s]Extractor Estimating: 51it [00:56,  1.46it/s]Extractor Estimating: 52it [00:57,  1.50it/s]Extractor Estimating: 53it [00:57,  1.46it/s]Extractor Estimating: 54it [00:58,  1.49it/s]Extractor Estimating: 55it [00:59,  1.48it/s]Extractor Estimating: 56it [00:59,  1.52it/s]Extractor Estimating: 57it [01:00,  1.50it/s]Extractor Estimating: 58it [01:01,  1.50it/s]Extractor Estimating: 59it [01:01,  1.52it/s]Extractor Estimating: 60it [01:02,  1.53it/s]Extractor Estimating: 61it [01:03,  1.55it/s]Extractor Estimating: 62it [01:03,  1.53it/s]Extractor Estimating: 63it [01:04,  1.44it/s]Extractor Estimating: 64it [01:05,  1.46it/s]Extractor Estimating: 65it [01:05,  1.47it/s]Extractor Estimating: 66it [01:06,  1.51it/s]Extractor Estimating: 67it [01:07,  1.49it/s]Extractor Estimating: 68it [01:07,  1.46it/s]Extractor Estimating: 69it [01:08,  1.52it/s]Extractor Estimating: 70it [01:09,  1.55it/s]Extractor Estimating: 71it [01:09,  1.55it/s]Extractor Estimating: 72it [01:10,  1.54it/s]Extractor Estimating: 73it [01:11,  1.55it/s]Extractor Estimating: 74it [01:11,  1.50it/s]Extractor Estimating: 75it [01:12,  1.50it/s]Extractor Estimating: 76it [01:13,  1.50it/s]Extractor Estimating: 77it [01:13,  1.52it/s]Extractor Estimating: 78it [01:14,  1.57it/s]Extractor Estimating: 79it [01:15,  1.53it/s]Extractor Estimating: 80it [01:15,  1.53it/s]Extractor Estimating: 81it [01:16,  1.52it/s]Extractor Estimating: 82it [01:17,  1.53it/s]Extractor Estimating: 83it [01:17,  1.57it/s]Extractor Estimating: 84it [01:18,  1.60it/s]Extractor Estimating: 85it [01:18,  1.62it/s]Extractor Estimating: 86it [01:19,  1.51it/s]Extractor Estimating: 87it [01:20,  1.17it/s]Extractor Estimating: 88it [01:21,  1.22it/s]Extractor Estimating: 89it [01:24,  1.31s/it]Extractor Estimating: 90it [01:24,  1.10s/it]Extractor Estimating: 91it [01:25,  1.03it/s]Extractor Estimating: 92it [01:25,  1.17it/s]Extractor Estimating: 93it [01:26,  1.28it/s]Extractor Estimating: 94it [01:27,  1.36it/s]Extractor Estimating: 95it [01:27,  1.44it/s]Extractor Estimating: 96it [01:28,  1.48it/s]Extractor Estimating: 97it [01:29,  1.52it/s]Extractor Estimating: 98it [01:29,  1.55it/s]Extractor Estimating: 99it [01:30,  1.54it/s]Extractor Estimating: 100it [01:30,  1.56it/s]Extractor Estimating: 101it [01:31,  1.53it/s]Extractor Estimating: 102it [01:32,  1.52it/s]Extractor Estimating: 103it [01:32,  1.54it/s]Extractor Estimating: 104it [01:33,  1.51it/s]Extractor Estimating: 105it [01:34,  1.52it/s]Extractor Estimating: 106it [01:34,  1.50it/s]Extractor Estimating: 107it [01:35,  1.54it/s]Extractor Estimating: 108it [01:36,  1.55it/s]Extractor Estimating: 109it [01:36,  1.50it/s]Extractor Estimating: 110it [01:37,  1.51it/s]Extractor Estimating: 111it [01:38,  1.51it/s]Extractor Estimating: 112it [01:38,  1.50it/s]Extractor Estimating: 113it [01:39,  1.51it/s]Extractor Estimating: 114it [01:40,  1.56it/s]Extractor Estimating: 115it [01:40,  1.52it/s]Extractor Estimating: 116it [01:41,  1.55it/s]Extractor Estimating: 117it [01:42,  1.57it/s]Extractor Estimating: 118it [01:42,  1.55it/s]Extractor Estimating: 119it [01:43,  1.56it/s]Extractor Estimating: 120it [01:44,  1.50it/s]Extractor Estimating: 121it [01:44,  1.50it/s]Extractor Estimating: 122it [01:45,  1.54it/s]Extractor Estimating: 123it [01:46,  1.49it/s]Extractor Estimating: 124it [01:46,  1.51it/s]Extractor Estimating: 125it [01:47,  1.50it/s]Extractor Estimating: 126it [01:48,  1.40it/s]Extractor Estimating: 127it [01:48,  1.42it/s]Extractor Estimating: 128it [01:49,  1.45it/s]Extractor Estimating: 129it [01:50,  1.46it/s]Extractor Estimating: 130it [01:50,  1.51it/s]Extractor Estimating: 131it [01:51,  1.49it/s]Extractor Estimating: 132it [01:52,  1.52it/s]Extractor Estimating: 133it [01:52,  1.50it/s]Extractor Estimating: 134it [01:53,  1.51it/s]Extractor Estimating: 135it [01:54,  1.54it/s]Extractor Estimating: 136it [01:54,  1.58it/s]Extractor Estimating: 137it [01:55,  1.55it/s]Extractor Estimating: 138it [01:55,  1.59it/s]Extractor Estimating: 139it [01:56,  1.59it/s]Extractor Estimating: 140it [01:57,  1.58it/s]Extractor Estimating: 141it [01:57,  1.53it/s]Extractor Estimating: 142it [01:58,  1.50it/s]Extractor Estimating: 143it [01:59,  1.53it/s]Extractor Estimating: 144it [02:00,  1.47it/s]Extractor Estimating: 145it [02:00,  1.49it/s]Extractor Estimating: 146it [02:01,  1.51it/s]Extractor Estimating: 147it [02:01,  1.53it/s]Extractor Estimating: 148it [02:02,  1.56it/s]Extractor Estimating: 149it [02:03,  1.56it/s]Extractor Estimating: 150it [02:03,  1.58it/s]Extractor Estimating: 151it [02:04,  1.55it/s]Extractor Estimating: 152it [02:05,  1.56it/s]Extractor Estimating: 153it [02:05,  1.55it/s]Extractor Estimating: 154it [02:06,  1.45it/s]Extractor Estimating: 155it [02:07,  1.43it/s]Extractor Estimating: 156it [02:07,  1.47it/s]Extractor Estimating: 157it [02:08,  1.55it/s]Extractor Estimating: 158it [02:09,  1.55it/s]Extractor Estimating: 159it [02:09,  1.51it/s]Extractor Estimating: 160it [02:10,  1.50it/s]Extractor Estimating: 161it [02:11,  1.45it/s]Extractor Estimating: 162it [02:11,  1.44it/s]Extractor Estimating: 163it [02:12,  1.44it/s]Extractor Estimating: 164it [02:13,  1.44it/s]Extractor Estimating: 165it [02:14,  1.43it/s]Extractor Estimating: 166it [02:14,  1.37it/s]Extractor Estimating: 167it [02:15,  1.37it/s]Extractor Estimating: 168it [02:16,  1.38it/s]Extractor Estimating: 169it [02:16,  1.44it/s]Extractor Estimating: 170it [02:17,  1.44it/s]Extractor Estimating: 171it [02:18,  1.47it/s]Extractor Estimating: 172it [02:19,  1.44it/s]Extractor Estimating: 173it [02:19,  1.44it/s]Extractor Estimating: 174it [02:20,  1.47it/s]Extractor Estimating: 175it [02:21,  1.38it/s]Extractor Estimating: 176it [02:21,  1.46it/s]Extractor Estimating: 177it [02:22,  1.48it/s]Extractor Estimating: 178it [02:23,  1.44it/s]Extractor Estimating: 179it [02:23,  1.47it/s]Extractor Estimating: 180it [02:24,  1.47it/s]Extractor Estimating: 181it [02:25,  1.47it/s]Extractor Estimating: 182it [02:25,  1.48it/s]Extractor Estimating: 183it [02:26,  1.52it/s]Extractor Estimating: 184it [02:27,  1.53it/s]Extractor Estimating: 185it [02:27,  1.61it/s]Extractor Estimating: 186it [02:28,  1.58it/s]Extractor Estimating: 187it [02:29,  1.52it/s]Extractor Estimating: 188it [02:29,  1.54it/s]Extractor Estimating: 189it [02:30,  1.56it/s]Extractor Estimating: 190it [02:30,  1.57it/s]Extractor Estimating: 191it [02:31,  1.52it/s]Extractor Estimating: 192it [02:32,  1.54it/s]Extractor Estimating: 193it [02:32,  1.53it/s]Extractor Estimating: 194it [02:33,  1.51it/s]Extractor Estimating: 195it [02:34,  1.53it/s]Extractor Estimating: 196it [02:34,  1.57it/s]Extractor Estimating: 197it [02:35,  1.56it/s]Extractor Estimating: 198it [02:36,  1.55it/s]Extractor Estimating: 199it [02:36,  1.55it/s]Extractor Estimating: 200it [02:37,  1.54it/s]Extractor Estimating: 201it [02:38,  1.46it/s]Extractor Estimating: 202it [02:38,  1.48it/s]Extractor Estimating: 203it [02:39,  1.51it/s]Extractor Estimating: 204it [02:40,  1.40it/s]Extractor Estimating: 205it [02:40,  1.47it/s]Extractor Estimating: 206it [02:41,  1.45it/s]Extractor Estimating: 207it [02:42,  1.46it/s]Extractor Estimating: 208it [02:42,  1.47it/s]Extractor Estimating: 209it [02:43,  1.46it/s]Extractor Estimating: 210it [02:44,  1.48it/s]Extractor Estimating: 211it [02:45,  1.43it/s]Extractor Estimating: 212it [02:45,  1.45it/s]Extractor Estimating: 213it [02:46,  1.50it/s]Extractor Estimating: 214it [02:47,  1.48it/s]Extractor Estimating: 215it [02:47,  1.47it/s]Extractor Estimating: 216it [02:48,  1.44it/s]Extractor Estimating: 217it [02:49,  1.46it/s]Extractor Estimating: 218it [02:49,  1.48it/s]Extractor Estimating: 219it [02:50,  1.49it/s]Extractor Estimating: 220it [02:51,  1.46it/s]Extractor Estimating: 221it [02:51,  1.47it/s]Extractor Estimating: 222it [02:52,  1.55it/s]Extractor Estimating: 223it [02:53,  1.51it/s]Extractor Estimating: 224it [02:53,  1.50it/s]Extractor Estimating: 225it [02:54,  1.49it/s]Extractor Estimating: 226it [02:55,  1.48it/s]Extractor Estimating: 227it [02:55,  1.46it/s]Extractor Estimating: 228it [02:56,  1.43it/s]Extractor Estimating: 229it [02:57,  1.47it/s]Extractor Estimating: 230it [02:57,  1.50it/s]Extractor Estimating: 231it [02:58,  1.53it/s]Extractor Estimating: 232it [02:59,  1.51it/s]Extractor Estimating: 233it [02:59,  1.50it/s]Extractor Estimating: 234it [03:00,  1.49it/s]Extractor Estimating: 235it [03:01,  1.53it/s]Extractor Estimating: 236it [03:01,  1.47it/s]Extractor Estimating: 237it [03:02,  1.44it/s]Extractor Estimating: 238it [03:03,  1.49it/s]Extractor Estimating: 239it [03:03,  1.46it/s]Extractor Estimating: 240it [03:04,  1.47it/s]Extractor Estimating: 241it [03:05,  1.41it/s]Extractor Estimating: 242it [03:06,  1.41it/s]Extractor Estimating: 243it [03:06,  1.45it/s]Extractor Estimating: 244it [03:07,  1.51it/s]Extractor Estimating: 245it [03:08,  1.46it/s]Extractor Estimating: 246it [03:08,  1.48it/s]Extractor Estimating: 247it [03:09,  1.51it/s]Extractor Estimating: 248it [03:10,  1.48it/s]Extractor Estimating: 249it [03:10,  1.50it/s]Extractor Estimating: 250it [03:11,  1.48it/s]Extractor Estimating: 250it [03:11,  1.31it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4991 mean pseudo reward: 0.9234451798696066
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 23867
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23967, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_5_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23967, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.397, loss:1286.5738
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.058, loss:1272.5831
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.053, loss:1187.6969
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.056, loss:1211.8602
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.060, loss:1138.8193
>> valid entity prec:0.5242, rec:0.5117, f1:0.5179
>> valid relation prec:0.2727, rec:0.0266, f1:0.0485
>> valid relation with NER prec:0.2727, rec:0.0266, f1:0.0485
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.421, loss:1154.7723
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.048, loss:1108.3565
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.062, loss:1103.8514
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.068, loss:1092.8844
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.052, loss:1031.7433
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6027, rec:0.5478, f1:0.5739
>> valid relation prec:0.2818, rec:0.0438, f1:0.0758
>> valid relation with NER prec:0.2818, rec:0.0438, f1:0.0758
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.401, loss:1026.9962
g_step 1200, step 160, avg_time 1.063, loss:1011.1807
g_step 1300, step 52, avg_time 1.051, loss:986.9606
g_step 1400, step 152, avg_time 1.069, loss:981.9966
g_step 1500, step 44, avg_time 1.053, loss:922.5941
>> valid entity prec:0.5740, rec:0.6182, f1:0.5952
>> valid relation prec:0.3697, rec:0.0572, f1:0.0991
>> valid relation with NER prec:0.3697, rec:0.0572, f1:0.0991
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 144, avg_time 2.409, loss:939.6291
g_step 1700, step 36, avg_time 1.065, loss:896.6076
g_step 1800, step 136, avg_time 1.056, loss:877.9010
g_step 1900, step 28, avg_time 1.058, loss:868.9313
g_step 2000, step 128, avg_time 1.067, loss:835.0104
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5498, rec:0.6407, f1:0.5918
>> valid relation prec:0.2756, rec:0.0655, f1:0.1058
>> valid relation with NER prec:0.2756, rec:0.0655, f1:0.1058
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.411, loss:840.1222
g_step 2200, step 120, avg_time 1.056, loss:781.9181
g_step 2300, step 12, avg_time 1.057, loss:795.8570
g_step 2400, step 112, avg_time 1.054, loss:750.2517
g_step 2500, step 4, avg_time 1.064, loss:784.9826
>> valid entity prec:0.5906, rec:0.6299, f1:0.6096
>> valid relation prec:0.1910, rec:0.0635, f1:0.0953
>> valid relation with NER prec:0.1910, rec:0.0635, f1:0.0953
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.406, loss:721.7209
g_step 2700, step 204, avg_time 1.061, loss:731.0002
g_step 2800, step 96, avg_time 1.061, loss:667.7359
g_step 2900, step 196, avg_time 1.060, loss:708.0154
g_step 3000, step 88, avg_time 1.060, loss:645.9813
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5966, rec:0.5334, f1:0.5632
>> valid relation prec:0.1899, rec:0.0604, f1:0.0916
>> valid relation with NER prec:0.1899, rec:0.0604, f1:0.0916
g_step 3100, step 188, avg_time 2.404, loss:677.6955
g_step 3200, step 80, avg_time 1.050, loss:620.9305
g_step 3300, step 180, avg_time 1.060, loss:654.1445
g_step 3400, step 72, avg_time 1.065, loss:597.6684
g_step 3500, step 172, avg_time 1.062, loss:627.6657
>> valid entity prec:0.5803, rec:0.5849, f1:0.5826
>> valid relation prec:0.1357, rec:0.0403, f1:0.0622
>> valid relation with NER prec:0.1357, rec:0.0403, f1:0.0622
g_step 3600, step 64, avg_time 2.407, loss:579.3299
g_step 3700, step 164, avg_time 1.058, loss:574.6642
g_step 3800, step 56, avg_time 1.056, loss:567.2422
g_step 3900, step 156, avg_time 1.065, loss:556.3831
g_step 4000, step 48, avg_time 1.060, loss:545.4857
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5775, rec:0.6104, f1:0.5935
>> valid relation prec:0.2069, rec:0.0707, f1:0.1053
>> valid relation with NER prec:0.2069, rec:0.0707, f1:0.1053
g_step 4100, step 148, avg_time 2.391, loss:540.3265
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 09:26:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 09:26:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_09-26-44_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 09:26:45 - WARNING - datasets.builder -   Using custom data configuration default-96fded4d1d9a20b4
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-96fded4d1d9a20b4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  8.04 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 09:26:45,471 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:26:45,472 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:26:45,473 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:26:45,474 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:26:45,483 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:26:45,487 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:26:45,487 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:26:45,487 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:26:45,487 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:26:45,487 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:26:45,487 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 09:26:45,597 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:26:48,639 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 09:26:48,642 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-96fded4d1d9a20b4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 09:26:48 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1531c2f26290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.02ba/s] 40%|████      | 2/5 [00:00<00:00,  3.85ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.15ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.31ba/s]100%|██████████| 5/5 [00:01<00:00,  4.40ba/s]100%|██████████| 5/5 [00:01<00:00,  4.18ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.01ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.23ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.32ba/s]100%|██████████| 4/4 [00:00<00:00,  5.36ba/s]100%|██████████| 4/4 [00:00<00:00,  4.88ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.10ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.43ba/s]100%|██████████| 5/5 [00:00<00:00, 10.38ba/s]100%|██████████| 5/5 [00:00<00:00, 10.30ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.12ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.61ba/s]100%|██████████| 4/4 [00:00<00:00, 11.97ba/s]
[INFO|trainer.py:414] 2023-08-28 09:26:51,866 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 09:26:51,881 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 09:26:51,881 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 09:26:51,881 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 09:26:51,881 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 09:26:51,881 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 09:26:51,882 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 09:26:51,882 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.35it/s]  1%|          | 2/390 [00:00<01:52,  3.46it/s]  1%|          | 3/390 [00:00<01:50,  3.49it/s]  1%|          | 4/390 [00:01<01:49,  3.51it/s]  1%|▏         | 5/390 [00:01<01:49,  3.52it/s]  2%|▏         | 6/390 [00:01<01:49,  3.52it/s]  2%|▏         | 7/390 [00:01<01:48,  3.53it/s]  2%|▏         | 8/390 [00:02<01:48,  3.53it/s]  2%|▏         | 9/390 [00:02<01:47,  3.53it/s]  3%|▎         | 10/390 [00:02<01:47,  3.53it/s]  3%|▎         | 11/390 [00:03<01:47,  3.52it/s]  3%|▎         | 12/390 [00:03<01:47,  3.52it/s]  3%|▎         | 13/390 [00:03<01:47,  3.52it/s]  4%|▎         | 14/390 [00:03<01:46,  3.53it/s]  4%|▍         | 15/390 [00:04<01:46,  3.52it/s]  4%|▍         | 16/390 [00:04<01:46,  3.53it/s]  4%|▍         | 17/390 [00:04<01:45,  3.53it/s]  5%|▍         | 18/390 [00:05<01:45,  3.53it/s]  5%|▍         | 19/390 [00:05<01:45,  3.53it/s]  5%|▌         | 20/390 [00:05<01:44,  3.53it/s]  5%|▌         | 21/390 [00:05<01:44,  3.52it/s]  6%|▌         | 22/390 [00:06<01:44,  3.51it/s]  6%|▌         | 23/390 [00:06<01:44,  3.52it/s]  6%|▌         | 24/390 [00:06<01:43,  3.52it/s]  6%|▋         | 25/390 [00:07<01:43,  3.52it/s]  7%|▋         | 26/390 [00:07<01:43,  3.52it/s]  7%|▋         | 27/390 [00:07<01:42,  3.53it/s]  7%|▋         | 28/390 [00:07<01:42,  3.52it/s]  7%|▋         | 29/390 [00:08<01:42,  3.52it/s]  8%|▊         | 30/390 [00:08<01:42,  3.52it/s]  8%|▊         | 31/390 [00:08<01:41,  3.52it/s]  8%|▊         | 32/390 [00:09<01:41,  3.52it/s]  8%|▊         | 33/390 [00:09<01:41,  3.51it/s]  9%|▊         | 34/390 [00:09<01:41,  3.52it/s]  9%|▉         | 35/390 [00:09<01:40,  3.52it/s]  9%|▉         | 36/390 [00:10<01:40,  3.52it/s]  9%|▉         | 37/390 [00:10<01:40,  3.52it/s] 10%|▉         | 38/390 [00:10<01:39,  3.52it/s] 10%|█         | 39/390 [00:11<01:39,  3.53it/s] 10%|█         | 40/390 [00:11<01:39,  3.52it/s] 11%|█         | 41/390 [00:11<01:38,  3.53it/s] 11%|█         | 42/390 [00:11<01:38,  3.52it/s] 11%|█         | 43/390 [00:12<01:38,  3.53it/s] 11%|█▏        | 44/390 [00:12<01:38,  3.52it/s] 12%|█▏        | 45/390 [00:12<01:38,  3.52it/s] 12%|█▏        | 46/390 [00:13<01:37,  3.52it/s] 12%|█▏        | 47/390 [00:13<01:37,  3.52it/s] 12%|█▏        | 48/390 [00:13<01:37,  3.52it/s] 13%|█▎        | 49/390 [00:13<01:36,  3.52it/s] 13%|█▎        | 50/390 [00:14<01:36,  3.53it/s] 13%|█▎        | 51/390 [00:14<01:36,  3.52it/s] 13%|█▎        | 52/390 [00:14<01:36,  3.52it/s] 14%|█▎        | 53/390 [00:15<01:35,  3.52it/s] 14%|█▍        | 54/390 [00:15<01:35,  3.52it/s] 14%|█▍        | 55/390 [00:15<01:35,  3.51it/s] 14%|█▍        | 56/390 [00:15<01:35,  3.51it/s] 15%|█▍        | 57/390 [00:16<01:34,  3.51it/s] 15%|█▍        | 58/390 [00:16<01:34,  3.52it/s] 15%|█▌        | 59/390 [00:16<01:34,  3.52it/s] 15%|█▌        | 60/390 [00:17<01:33,  3.52it/s] 16%|█▌        | 61/390 [00:17<01:33,  3.53it/s] 16%|█▌        | 62/390 [00:17<01:33,  3.52it/s] 16%|█▌        | 63/390 [00:17<01:32,  3.52it/s] 16%|█▋        | 64/390 [00:18<01:32,  3.53it/s] 17%|█▋        | 65/390 [00:18<01:32,  3.53it/s] 17%|█▋        | 66/390 [00:18<01:32,  3.51it/s] 17%|█▋        | 67/390 [00:19<01:31,  3.51it/s] 17%|█▋        | 68/390 [00:19<01:31,  3.52it/s] 18%|█▊        | 69/390 [00:19<01:31,  3.52it/s] 18%|█▊        | 70/390 [00:19<01:30,  3.52it/s] 18%|█▊        | 71/390 [00:20<01:30,  3.52it/s] 18%|█▊        | 72/390 [00:20<01:30,  3.52it/s] 19%|█▊        | 73/390 [00:20<01:30,  3.52it/s] 19%|█▉        | 74/390 [00:21<01:29,  3.52it/s] 19%|█▉        | 75/390 [00:21<01:29,  3.52it/s] 19%|█▉        | 76/390 [00:21<01:29,  3.52it/s] 20%|█▉        | 77/390 [00:21<01:29,  3.48it/s] 20%|██        | 78/390 [00:22<01:29,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 09:27:14,093 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:27:14,093 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 09:27:14,093 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.39it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.46it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.68it/s][A
  5%|▌         | 24/437 [00:00<00:08, 48.91it/s][A
  7%|▋         | 29/437 [00:00<00:08, 48.48it/s][A
  8%|▊         | 34/437 [00:00<00:08, 48.12it/s][A
  9%|▉         | 39/437 [00:00<00:08, 47.81it/s][A
 10%|█         | 44/437 [00:00<00:08, 47.50it/s][A
 11%|█         | 49/437 [00:01<00:08, 47.50it/s][A
 12%|█▏        | 54/437 [00:01<00:08, 47.46it/s][A
 14%|█▎        | 59/437 [00:01<00:07, 47.56it/s][A
 15%|█▍        | 64/437 [00:01<00:07, 47.58it/s][A
 16%|█▌        | 69/437 [00:01<00:07, 47.59it/s][A
 17%|█▋        | 74/437 [00:01<00:07, 47.63it/s][A
 18%|█▊        | 79/437 [00:01<00:07, 47.59it/s][A
 19%|█▉        | 84/437 [00:01<00:07, 47.50it/s][A
 20%|██        | 89/437 [00:01<00:07, 47.39it/s][A
 22%|██▏       | 94/437 [00:01<00:07, 47.41it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 47.39it/s][A
 24%|██▍       | 104/437 [00:02<00:07, 47.43it/s][A
 25%|██▍       | 109/437 [00:02<00:06, 47.47it/s][A
 26%|██▌       | 114/437 [00:02<00:06, 47.59it/s][A
 27%|██▋       | 119/437 [00:02<00:06, 47.61it/s][A
 28%|██▊       | 124/437 [00:02<00:06, 47.59it/s][A
 30%|██▉       | 129/437 [00:02<00:06, 47.47it/s][A
 31%|███       | 134/437 [00:02<00:06, 47.45it/s][A
 32%|███▏      | 139/437 [00:02<00:06, 47.35it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 47.36it/s][A
 34%|███▍      | 149/437 [00:03<00:06, 47.43it/s][A
 35%|███▌      | 154/437 [00:03<00:05, 47.48it/s][A
 36%|███▋      | 159/437 [00:03<00:05, 47.53it/s][A
 38%|███▊      | 164/437 [00:03<00:05, 47.55it/s][A
 39%|███▊      | 169/437 [00:03<00:05, 47.56it/s][A
 40%|███▉      | 174/437 [00:03<00:05, 47.50it/s][A
 41%|████      | 179/437 [00:03<00:05, 47.39it/s][A
 42%|████▏     | 184/437 [00:03<00:05, 47.35it/s][A
 43%|████▎     | 189/437 [00:03<00:05, 47.39it/s][A
 44%|████▍     | 194/437 [00:04<00:05, 47.37it/s][A
 46%|████▌     | 199/437 [00:04<00:05, 47.42it/s][A
 47%|████▋     | 204/437 [00:04<00:04, 47.46it/s][A
 48%|████▊     | 209/437 [00:04<00:04, 47.40it/s][A
 49%|████▉     | 214/437 [00:04<00:04, 47.52it/s][A
 50%|█████     | 219/437 [00:04<00:04, 47.47it/s][A
 51%|█████▏    | 224/437 [00:04<00:04, 47.43it/s][A
 52%|█████▏    | 229/437 [00:04<00:04, 47.40it/s][A
 54%|█████▎    | 234/437 [00:04<00:04, 47.38it/s][A
 55%|█████▍    | 239/437 [00:05<00:04, 47.38it/s][A
 56%|█████▌    | 244/437 [00:05<00:04, 47.40it/s][A
 57%|█████▋    | 249/437 [00:05<00:03, 47.37it/s][A
 58%|█████▊    | 254/437 [00:05<00:03, 47.39it/s][A
 59%|█████▉    | 259/437 [00:05<00:03, 47.46it/s][A
 60%|██████    | 264/437 [00:05<00:03, 47.53it/s][A
 62%|██████▏   | 269/437 [00:05<00:03, 47.52it/s][A
 63%|██████▎   | 274/437 [00:05<00:03, 47.37it/s][A
 64%|██████▍   | 279/437 [00:05<00:03, 47.37it/s][A
 65%|██████▍   | 284/437 [00:05<00:03, 47.37it/s][A
 66%|██████▌   | 289/437 [00:06<00:03, 47.35it/s][A
 67%|██████▋   | 294/437 [00:06<00:03, 47.37it/s][A
 68%|██████▊   | 299/437 [00:06<00:02, 47.41it/s][A
 70%|██████▉   | 304/437 [00:06<00:02, 47.38it/s][A
 71%|███████   | 309/437 [00:06<00:02, 47.35it/s][A
 72%|███████▏  | 314/437 [00:06<00:02, 47.45it/s][A
 73%|███████▎  | 319/437 [00:06<00:02, 47.50it/s][A
 74%|███████▍  | 324/437 [00:06<00:02, 47.42it/s][A
 75%|███████▌  | 329/437 [00:06<00:02, 47.40it/s][A
 76%|███████▋  | 334/437 [00:07<00:02, 47.45it/s][A
 78%|███████▊  | 339/437 [00:07<00:02, 47.41it/s][A
 79%|███████▊  | 344/437 [00:07<00:01, 47.32it/s][A
 80%|███████▉  | 349/437 [00:07<00:01, 47.35it/s][A
 81%|████████  | 354/437 [00:07<00:01, 47.37it/s][A
 82%|████████▏ | 359/437 [00:07<00:01, 47.36it/s][A
 83%|████████▎ | 364/437 [00:07<00:01, 47.35it/s][A
 84%|████████▍ | 369/437 [00:07<00:01, 47.48it/s][A
 86%|████████▌ | 374/437 [00:07<00:01, 47.44it/s][A
 87%|████████▋ | 379/437 [00:07<00:01, 47.42it/s][A
 88%|████████▊ | 384/437 [00:08<00:01, 47.40it/s][A
 89%|████████▉ | 389/437 [00:08<00:01, 47.43it/s][A
 90%|█████████ | 394/437 [00:08<00:00, 47.41it/s][A
 91%|█████████▏| 399/437 [00:08<00:00, 47.38it/s][A
 92%|█████████▏| 404/437 [00:08<00:00, 47.38it/s][A
 94%|█████████▎| 409/437 [00:08<00:00, 47.31it/s][A
 95%|█████████▍| 414/437 [00:08<00:00, 47.32it/s][A
 96%|█████████▌| 419/437 [00:08<00:00, 47.32it/s][A
 97%|█████████▋| 424/437 [00:08<00:00, 47.45it/s][A
 98%|█████████▊| 429/437 [00:09<00:00, 47.45it/s][A
 99%|█████████▉| 434/437 [00:09<00:00, 47.42it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:31<01:29,  3.49it/s]
100%|██████████| 437/437 [00:09<00:00, 47.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:27:23,332 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 09:27:23,350 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:27:25,501 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:27:25,518 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:27:25,526 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:38<26:09,  5.05s/it] 21%|██        | 80/390 [00:38<18:41,  3.62s/it] 21%|██        | 81/390 [00:38<13:28,  2.62s/it] 21%|██        | 82/390 [00:39<09:50,  1.92s/it] 21%|██▏       | 83/390 [00:39<07:18,  1.43s/it] 22%|██▏       | 84/390 [00:39<05:31,  1.08s/it] 22%|██▏       | 85/390 [00:40<04:17,  1.18it/s] 22%|██▏       | 86/390 [00:40<03:25,  1.48it/s] 22%|██▏       | 87/390 [00:40<02:49,  1.79it/s] 23%|██▎       | 88/390 [00:40<02:23,  2.10it/s] 23%|██▎       | 89/390 [00:41<02:06,  2.39it/s] 23%|██▎       | 90/390 [00:41<01:53,  2.64it/s] 23%|██▎       | 91/390 [00:41<01:44,  2.85it/s] 24%|██▎       | 92/390 [00:42<01:38,  3.02it/s] 24%|██▍       | 93/390 [00:42<01:34,  3.16it/s] 24%|██▍       | 94/390 [00:42<01:30,  3.26it/s] 24%|██▍       | 95/390 [00:42<01:28,  3.33it/s] 25%|██▍       | 96/390 [00:43<01:26,  3.39it/s] 25%|██▍       | 97/390 [00:43<01:25,  3.42it/s] 25%|██▌       | 98/390 [00:43<01:24,  3.45it/s] 25%|██▌       | 99/390 [00:44<01:23,  3.47it/s] 26%|██▌       | 100/390 [00:44<01:23,  3.49it/s] 26%|██▌       | 101/390 [00:44<01:22,  3.50it/s] 26%|██▌       | 102/390 [00:44<01:22,  3.49it/s] 26%|██▋       | 103/390 [00:45<01:21,  3.50it/s] 27%|██▋       | 104/390 [00:45<01:21,  3.50it/s] 27%|██▋       | 105/390 [00:45<01:21,  3.51it/s] 27%|██▋       | 106/390 [00:46<01:20,  3.51it/s] 27%|██▋       | 107/390 [00:46<01:20,  3.51it/s] 28%|██▊       | 108/390 [00:46<01:20,  3.51it/s] 28%|██▊       | 109/390 [00:46<01:19,  3.51it/s] 28%|██▊       | 110/390 [00:47<01:19,  3.51it/s] 28%|██▊       | 111/390 [00:47<01:19,  3.52it/s] 29%|██▊       | 112/390 [00:47<01:19,  3.52it/s] 29%|██▉       | 113/390 [00:47<01:19,  3.49it/s] 29%|██▉       | 114/390 [00:48<01:18,  3.50it/s] 29%|██▉       | 115/390 [00:48<01:18,  3.50it/s] 30%|██▉       | 116/390 [00:48<01:18,  3.51it/s] 30%|███       | 117/390 [00:49<01:17,  3.51it/s] 30%|███       | 118/390 [00:49<01:17,  3.51it/s] 31%|███       | 119/390 [00:49<01:17,  3.51it/s] 31%|███       | 120/390 [00:49<01:16,  3.51it/s] 31%|███       | 121/390 [00:50<01:16,  3.51it/s] 31%|███▏      | 122/390 [00:50<01:16,  3.51it/s] 32%|███▏      | 123/390 [00:50<01:15,  3.51it/s] 32%|███▏      | 124/390 [00:51<01:15,  3.51it/s] 32%|███▏      | 125/390 [00:51<01:15,  3.50it/s] 32%|███▏      | 126/390 [00:51<01:15,  3.51it/s] 33%|███▎      | 127/390 [00:51<01:14,  3.51it/s] 33%|███▎      | 128/390 [00:52<01:17,  3.40it/s] 33%|███▎      | 129/390 [00:52<01:16,  3.42it/s] 33%|███▎      | 130/390 [00:52<01:15,  3.45it/s] 34%|███▎      | 131/390 [00:53<01:14,  3.47it/s] 34%|███▍      | 132/390 [00:53<01:14,  3.48it/s] 34%|███▍      | 133/390 [00:53<01:13,  3.49it/s] 34%|███▍      | 134/390 [00:54<01:13,  3.50it/s] 35%|███▍      | 135/390 [00:54<01:12,  3.50it/s] 35%|███▍      | 136/390 [00:54<01:12,  3.50it/s] 35%|███▌      | 137/390 [00:54<01:12,  3.50it/s] 35%|███▌      | 138/390 [00:55<01:11,  3.51it/s] 36%|███▌      | 139/390 [00:55<01:11,  3.51it/s] 36%|███▌      | 140/390 [00:55<01:11,  3.51it/s] 36%|███▌      | 141/390 [00:56<01:10,  3.51it/s] 36%|███▋      | 142/390 [00:56<01:10,  3.51it/s] 37%|███▋      | 143/390 [00:56<01:10,  3.51it/s] 37%|███▋      | 144/390 [00:56<01:10,  3.50it/s] 37%|███▋      | 145/390 [00:57<01:09,  3.50it/s] 37%|███▋      | 146/390 [00:57<01:09,  3.51it/s] 38%|███▊      | 147/390 [00:57<01:09,  3.51it/s] 38%|███▊      | 148/390 [00:58<01:09,  3.51it/s] 38%|███▊      | 149/390 [00:58<01:08,  3.51it/s] 38%|███▊      | 150/390 [00:58<01:08,  3.51it/s] 39%|███▊      | 151/390 [00:58<01:08,  3.51it/s] 39%|███▉      | 152/390 [00:59<01:07,  3.51it/s] 39%|███▉      | 153/390 [00:59<01:07,  3.51it/s] 39%|███▉      | 154/390 [00:59<01:07,  3.51it/s] 40%|███▉      | 155/390 [00:59<01:07,  3.50it/s] 40%|████      | 156/390 [01:00<01:06,  3.50it/s][INFO|trainer.py:2140] 2023-08-28 09:27:52,208 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:27:52,208 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 09:27:52,208 >>   Batch size = 8
{'eval_loss': 1.0357869863510132, 'eval_runtime': 9.2175, 'eval_samples_per_second': 379.278, 'eval_steps_per_second': 47.41, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.82it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.09it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.44it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.80it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.37it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.00it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.63it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.35it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.32it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.29it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.30it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.29it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.38it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.46it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.51it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.46it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.30it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.22it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.09it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.22it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.25it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.25it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.35it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.43it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.44it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.31it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.29it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.30it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.29it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.27it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.27it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.28it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.23it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.36it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.39it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.33it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.30it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.28it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.29it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.26it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.27it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.29it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.30it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.38it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.38it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.28it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.28it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.32it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.32it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.30it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.27it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.29it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.28it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.32it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.36it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.23it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.24it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.31it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.32it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.25it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.16it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.24it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.27it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.29it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.33it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.27it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.02it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.41it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.38it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.27it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.25it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.32it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.33it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.31it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.33it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.36it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.29it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.29it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.28it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.20it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.22it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.24it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.26it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.30it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.37it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:09<01:06,  3.50it/s]
100%|██████████| 437/437 [00:09<00:00, 47.37it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:28:01,483 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 09:28:01,519 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:28:03,926 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:28:03,947 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:28:03,958 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:16<20:12,  5.20s/it] 41%|████      | 158/390 [01:17<14:24,  3.73s/it] 41%|████      | 159/390 [01:17<10:22,  2.69s/it] 41%|████      | 160/390 [01:17<07:33,  1.97s/it] 41%|████▏     | 161/390 [01:18<05:35,  1.47s/it] 42%|████▏     | 162/390 [01:18<04:13,  1.11s/it] 42%|████▏     | 163/390 [01:18<03:15,  1.16it/s] 42%|████▏     | 164/390 [01:18<02:35,  1.45it/s] 42%|████▏     | 165/390 [01:19<02:07,  1.76it/s] 43%|████▎     | 166/390 [01:19<01:48,  2.07it/s] 43%|████▎     | 167/390 [01:19<01:34,  2.36it/s] 43%|████▎     | 168/390 [01:20<01:24,  2.62it/s] 43%|████▎     | 169/390 [01:20<01:18,  2.83it/s] 44%|████▎     | 170/390 [01:20<01:13,  3.00it/s] 44%|████▍     | 171/390 [01:20<01:09,  3.14it/s] 44%|████▍     | 172/390 [01:21<01:07,  3.25it/s] 44%|████▍     | 173/390 [01:21<01:05,  3.32it/s] 45%|████▍     | 174/390 [01:21<01:03,  3.38it/s] 45%|████▍     | 175/390 [01:22<01:02,  3.42it/s] 45%|████▌     | 176/390 [01:22<01:02,  3.45it/s] 45%|████▌     | 177/390 [01:22<01:01,  3.47it/s] 46%|████▌     | 178/390 [01:22<01:00,  3.48it/s] 46%|████▌     | 179/390 [01:23<01:00,  3.49it/s] 46%|████▌     | 180/390 [01:23<01:00,  3.47it/s] 46%|████▋     | 181/390 [01:23<01:00,  3.48it/s] 47%|████▋     | 182/390 [01:24<00:59,  3.49it/s] 47%|████▋     | 183/390 [01:24<00:59,  3.50it/s] 47%|████▋     | 184/390 [01:24<00:58,  3.50it/s] 47%|████▋     | 185/390 [01:24<00:58,  3.50it/s] 48%|████▊     | 186/390 [01:25<00:58,  3.51it/s] 48%|████▊     | 187/390 [01:25<00:57,  3.51it/s] 48%|████▊     | 188/390 [01:25<00:57,  3.51it/s] 48%|████▊     | 189/390 [01:26<00:57,  3.51it/s] 49%|████▊     | 190/390 [01:26<00:57,  3.51it/s] 49%|████▉     | 191/390 [01:26<00:56,  3.51it/s] 49%|████▉     | 192/390 [01:26<00:56,  3.51it/s] 49%|████▉     | 193/390 [01:27<00:56,  3.51it/s] 50%|████▉     | 194/390 [01:27<00:55,  3.51it/s] 50%|█████     | 195/390 [01:27<00:55,  3.48it/s] 50%|█████     | 196/390 [01:28<00:55,  3.48it/s] 51%|█████     | 197/390 [01:28<00:55,  3.49it/s] 51%|█████     | 198/390 [01:28<00:54,  3.49it/s] 51%|█████     | 199/390 [01:28<00:54,  3.50it/s] 51%|█████▏    | 200/390 [01:29<00:54,  3.50it/s] 52%|█████▏    | 201/390 [01:29<00:53,  3.50it/s] 52%|█████▏    | 202/390 [01:29<00:53,  3.51it/s] 52%|█████▏    | 203/390 [01:30<00:53,  3.51it/s] 52%|█████▏    | 204/390 [01:30<00:52,  3.51it/s] 53%|█████▎    | 205/390 [01:30<00:52,  3.51it/s] 53%|█████▎    | 206/390 [01:30<00:52,  3.50it/s] 53%|█████▎    | 207/390 [01:31<00:52,  3.50it/s] 53%|█████▎    | 208/390 [01:31<00:51,  3.50it/s] 54%|█████▎    | 209/390 [01:31<00:51,  3.51it/s] 54%|█████▍    | 210/390 [01:32<00:51,  3.51it/s] 54%|█████▍    | 211/390 [01:32<00:51,  3.51it/s] 54%|█████▍    | 212/390 [01:32<00:50,  3.51it/s] 55%|█████▍    | 213/390 [01:32<00:50,  3.51it/s] 55%|█████▍    | 214/390 [01:33<00:50,  3.51it/s] 55%|█████▌    | 215/390 [01:33<00:49,  3.51it/s] 55%|█████▌    | 216/390 [01:33<00:49,  3.50it/s] 56%|█████▌    | 217/390 [01:34<00:49,  3.49it/s] 56%|█████▌    | 218/390 [01:34<00:49,  3.50it/s] 56%|█████▌    | 219/390 [01:34<00:48,  3.50it/s] 56%|█████▋    | 220/390 [01:34<00:48,  3.50it/s] 57%|█████▋    | 221/390 [01:35<00:48,  3.50it/s] 57%|█████▋    | 222/390 [01:35<00:47,  3.50it/s] 57%|█████▋    | 223/390 [01:35<00:47,  3.51it/s] 57%|█████▋    | 224/390 [01:36<00:47,  3.50it/s] 58%|█████▊    | 225/390 [01:36<00:47,  3.51it/s] 58%|█████▊    | 226/390 [01:36<00:46,  3.51it/s] 58%|█████▊    | 227/390 [01:36<00:46,  3.50it/s] 58%|█████▊    | 228/390 [01:37<00:46,  3.49it/s] 59%|█████▊    | 229/390 [01:37<00:46,  3.50it/s] 59%|█████▉    | 230/390 [01:37<00:45,  3.50it/s] 59%|█████▉    | 231/390 [01:38<00:45,  3.50it/s] 59%|█████▉    | 232/390 [01:38<00:45,  3.50it/s] 60%|█████▉    | 233/390 [01:38<00:44,  3.51it/s] 60%|██████    | 234/390 [01:38<00:44,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 09:28:30,848 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:28:30,848 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 09:28:30,848 >>   Batch size = 8
{'eval_loss': 1.036555528640747, 'eval_runtime': 9.2437, 'eval_samples_per_second': 378.203, 'eval_steps_per_second': 47.275, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.09it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.16it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.35it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.71it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.16it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.94it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.60it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.25it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.17it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.26it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.20it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.10it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.20it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.27it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.26it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.17it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.03it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.04it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.98it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.11it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.12it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.07it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.19it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.25it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.15it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.07it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.09it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.04it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.06it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.12it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.10it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.03it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.16it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.16it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.13it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.08it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.07it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.00it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.07it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.06it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.02it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.09it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.20it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.17it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.11it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.00it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.05it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.04it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.04it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.09it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.07it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.16it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.14it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.12it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.04it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.06it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.06it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.02it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.05it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.06it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.08it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.20it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.15it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.11it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.10it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.09it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.06it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.02it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.12it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.10it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.11it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.21it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.09it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.06it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.09it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.06it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.96it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.12it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.10it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.04it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.99it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.24it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.16it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.11it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.08it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.99it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:48<00:44,  3.51it/s]
100%|██████████| 437/437 [00:09<00:00, 46.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:28:40,143 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 09:28:40,165 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:28:42,676 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:28:42,693 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:28:42,699 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:55<13:37,  5.27s/it] 61%|██████    | 236/390 [01:56<09:41,  3.78s/it] 61%|██████    | 237/390 [01:56<06:57,  2.73s/it] 61%|██████    | 238/390 [01:56<05:03,  2.00s/it] 61%|██████▏   | 239/390 [01:56<03:43,  1.48s/it] 62%|██████▏   | 240/390 [01:57<02:48,  1.12s/it] 62%|██████▏   | 241/390 [01:57<02:09,  1.15it/s] 62%|██████▏   | 242/390 [01:57<01:42,  1.44it/s] 62%|██████▏   | 243/390 [01:58<01:24,  1.75it/s] 63%|██████▎   | 244/390 [01:58<01:11,  2.05it/s] 63%|██████▎   | 245/390 [01:58<01:01,  2.35it/s] 63%|██████▎   | 246/390 [01:58<00:55,  2.61it/s] 63%|██████▎   | 247/390 [01:59<00:50,  2.83it/s] 64%|██████▎   | 248/390 [01:59<00:47,  3.00it/s] 64%|██████▍   | 249/390 [01:59<00:44,  3.14it/s] 64%|██████▍   | 250/390 [02:00<00:43,  3.24it/s] 64%|██████▍   | 251/390 [02:00<00:41,  3.31it/s] 65%|██████▍   | 252/390 [02:00<00:40,  3.37it/s] 65%|██████▍   | 253/390 [02:00<00:40,  3.41it/s] 65%|██████▌   | 254/390 [02:01<00:39,  3.44it/s] 65%|██████▌   | 255/390 [02:01<00:39,  3.45it/s] 66%|██████▌   | 256/390 [02:01<00:38,  3.47it/s] 66%|██████▌   | 257/390 [02:02<00:38,  3.48it/s] 66%|██████▌   | 258/390 [02:02<00:37,  3.49it/s] 66%|██████▋   | 259/390 [02:02<00:37,  3.49it/s] 67%|██████▋   | 260/390 [02:02<00:37,  3.50it/s] 67%|██████▋   | 261/390 [02:03<00:36,  3.51it/s] 67%|██████▋   | 262/390 [02:03<00:36,  3.51it/s] 67%|██████▋   | 263/390 [02:03<00:36,  3.50it/s] 68%|██████▊   | 264/390 [02:04<00:35,  3.50it/s] 68%|██████▊   | 265/390 [02:04<00:35,  3.51it/s] 68%|██████▊   | 266/390 [02:04<00:35,  3.49it/s] 68%|██████▊   | 267/390 [02:04<00:35,  3.50it/s] 69%|██████▊   | 268/390 [02:05<00:34,  3.50it/s] 69%|██████▉   | 269/390 [02:05<00:34,  3.50it/s] 69%|██████▉   | 270/390 [02:05<00:34,  3.51it/s] 69%|██████▉   | 271/390 [02:06<00:33,  3.51it/s] 70%|██████▉   | 272/390 [02:06<00:33,  3.51it/s] 70%|███████   | 273/390 [02:06<00:33,  3.51it/s] 70%|███████   | 274/390 [02:06<00:33,  3.51it/s] 71%|███████   | 275/390 [02:07<00:32,  3.51it/s] 71%|███████   | 276/390 [02:07<00:32,  3.51it/s] 71%|███████   | 277/390 [02:07<00:32,  3.49it/s] 71%|███████▏  | 278/390 [02:08<00:31,  3.50it/s] 72%|███████▏  | 279/390 [02:08<00:31,  3.50it/s] 72%|███████▏  | 280/390 [02:08<00:31,  3.50it/s] 72%|███████▏  | 281/390 [02:08<00:31,  3.51it/s] 72%|███████▏  | 282/390 [02:09<00:30,  3.50it/s] 73%|███████▎  | 283/390 [02:09<00:30,  3.51it/s] 73%|███████▎  | 284/390 [02:09<00:30,  3.51it/s] 73%|███████▎  | 285/390 [02:10<00:29,  3.50it/s] 73%|███████▎  | 286/390 [02:10<00:29,  3.50it/s] 74%|███████▎  | 287/390 [02:10<00:29,  3.51it/s] 74%|███████▍  | 288/390 [02:10<00:29,  3.50it/s] 74%|███████▍  | 289/390 [02:11<00:28,  3.50it/s] 74%|███████▍  | 290/390 [02:11<00:28,  3.50it/s] 75%|███████▍  | 291/390 [02:11<00:28,  3.51it/s] 75%|███████▍  | 292/390 [02:12<00:27,  3.51it/s] 75%|███████▌  | 293/390 [02:12<00:27,  3.50it/s] 75%|███████▌  | 294/390 [02:12<00:27,  3.51it/s] 76%|███████▌  | 295/390 [02:12<00:27,  3.50it/s] 76%|███████▌  | 296/390 [02:13<00:26,  3.50it/s] 76%|███████▌  | 297/390 [02:13<00:26,  3.51it/s] 76%|███████▋  | 298/390 [02:13<00:26,  3.51it/s] 77%|███████▋  | 299/390 [02:14<00:26,  3.49it/s] 77%|███████▋  | 300/390 [02:14<00:25,  3.49it/s] 77%|███████▋  | 301/390 [02:14<00:25,  3.50it/s] 77%|███████▋  | 302/390 [02:14<00:25,  3.50it/s] 78%|███████▊  | 303/390 [02:15<00:24,  3.50it/s] 78%|███████▊  | 304/390 [02:15<00:24,  3.50it/s] 78%|███████▊  | 305/390 [02:15<00:24,  3.50it/s] 78%|███████▊  | 306/390 [02:16<00:24,  3.50it/s] 79%|███████▊  | 307/390 [02:16<00:23,  3.50it/s] 79%|███████▉  | 308/390 [02:16<00:23,  3.50it/s] 79%|███████▉  | 309/390 [02:16<00:23,  3.50it/s] 79%|███████▉  | 310/390 [02:17<00:22,  3.49it/s] 80%|███████▉  | 311/390 [02:17<00:22,  3.49it/s] 80%|████████  | 312/390 [02:17<00:22,  3.50it/s][INFO|trainer.py:2140] 2023-08-28 09:29:09,737 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:29:09,737 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 09:29:09,737 >>   Batch size = 8
{'eval_loss': 1.0387511253356934, 'eval_runtime': 9.282, 'eval_samples_per_second': 376.642, 'eval_steps_per_second': 47.08, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.86it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.00it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.25it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.60it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.12it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.81it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.55it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.03it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.10it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.21it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.19it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.17it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.24it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.20it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.22it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.13it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.00it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.96it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.04it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.10it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.15it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.04it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.17it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.17it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.11it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.00it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.93it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.03it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.12it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.17it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.18it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.16it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.25it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.15it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.94it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.99it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.01it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.00it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.10it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.16it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.12it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.15it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.13it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.00it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.96it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.99it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.02it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.02it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.14it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.00it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.02it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.09it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.00it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.92it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.04it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.05it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.97it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.09it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.11it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.05it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.11it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.05it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.86it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.01it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.04it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.96it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.07it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.08it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.01it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.08it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.08it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.93it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.98it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.09it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.03it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.04it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.16it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.18it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.09it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.13it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.04it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.95it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.02it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.11it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.06it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.11it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:27<00:22,  3.50it/s]
100%|██████████| 437/437 [00:09<00:00, 47.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:29:19,046 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 09:29:19,066 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:29:21,406 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:29:21,427 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:29:21,437 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:34<06:34,  5.12s/it] 81%|████████  | 314/390 [02:34<04:38,  3.67s/it] 81%|████████  | 315/390 [02:34<03:19,  2.65s/it] 81%|████████  | 316/390 [02:35<02:23,  1.94s/it] 81%|████████▏ | 317/390 [02:35<01:45,  1.45s/it] 82%|████████▏ | 318/390 [02:35<01:18,  1.10s/it] 82%|████████▏ | 319/390 [02:35<01:00,  1.17it/s] 82%|████████▏ | 320/390 [02:36<00:47,  1.46it/s] 82%|████████▏ | 321/390 [02:36<00:38,  1.78it/s] 83%|████████▎ | 322/390 [02:36<00:32,  2.08it/s] 83%|████████▎ | 323/390 [02:37<00:28,  2.37it/s] 83%|████████▎ | 324/390 [02:37<00:25,  2.63it/s] 83%|████████▎ | 325/390 [02:37<00:22,  2.83it/s] 84%|████████▎ | 326/390 [02:37<00:21,  3.01it/s] 84%|████████▍ | 327/390 [02:38<00:20,  3.14it/s] 84%|████████▍ | 328/390 [02:38<00:19,  3.24it/s] 84%|████████▍ | 329/390 [02:38<00:18,  3.32it/s] 85%|████████▍ | 330/390 [02:39<00:17,  3.37it/s] 85%|████████▍ | 331/390 [02:39<00:17,  3.41it/s] 85%|████████▌ | 332/390 [02:39<00:16,  3.44it/s] 85%|████████▌ | 333/390 [02:39<00:16,  3.46it/s] 86%|████████▌ | 334/390 [02:40<00:16,  3.48it/s] 86%|████████▌ | 335/390 [02:40<00:15,  3.49it/s] 86%|████████▌ | 336/390 [02:40<00:15,  3.49it/s] 86%|████████▋ | 337/390 [02:41<00:15,  3.49it/s] 87%|████████▋ | 338/390 [02:41<00:14,  3.50it/s] 87%|████████▋ | 339/390 [02:41<00:14,  3.50it/s] 87%|████████▋ | 340/390 [02:41<00:14,  3.50it/s] 87%|████████▋ | 341/390 [02:42<00:13,  3.50it/s] 88%|████████▊ | 342/390 [02:42<00:13,  3.51it/s] 88%|████████▊ | 343/390 [02:42<00:13,  3.51it/s] 88%|████████▊ | 344/390 [02:43<00:13,  3.51it/s] 88%|████████▊ | 345/390 [02:43<00:12,  3.51it/s] 89%|████████▊ | 346/390 [02:43<00:12,  3.51it/s] 89%|████████▉ | 347/390 [02:43<00:12,  3.49it/s] 89%|████████▉ | 348/390 [02:44<00:12,  3.50it/s] 89%|████████▉ | 349/390 [02:44<00:11,  3.49it/s] 90%|████████▉ | 350/390 [02:44<00:11,  3.50it/s] 90%|█████████ | 351/390 [02:45<00:11,  3.50it/s] 90%|█████████ | 352/390 [02:45<00:10,  3.50it/s] 91%|█████████ | 353/390 [02:45<00:10,  3.50it/s] 91%|█████████ | 354/390 [02:45<00:10,  3.50it/s] 91%|█████████ | 355/390 [02:46<00:10,  3.49it/s] 91%|█████████▏| 356/390 [02:46<00:09,  3.49it/s] 92%|█████████▏| 357/390 [02:46<00:09,  3.50it/s] 92%|█████████▏| 358/390 [02:47<00:09,  3.47it/s] 92%|█████████▏| 359/390 [02:47<00:08,  3.48it/s] 92%|█████████▏| 360/390 [02:47<00:08,  3.48it/s] 93%|█████████▎| 361/390 [02:47<00:08,  3.49it/s] 93%|█████████▎| 362/390 [02:48<00:08,  3.50it/s] 93%|█████████▎| 363/390 [02:48<00:07,  3.50it/s] 93%|█████████▎| 364/390 [02:48<00:07,  3.50it/s] 94%|█████████▎| 365/390 [02:49<00:07,  3.50it/s] 94%|█████████▍| 366/390 [02:49<00:06,  3.50it/s] 94%|█████████▍| 367/390 [02:49<00:06,  3.50it/s] 94%|█████████▍| 368/390 [02:49<00:06,  3.50it/s] 95%|█████████▍| 369/390 [02:50<00:06,  3.48it/s] 95%|█████████▍| 370/390 [02:50<00:05,  3.48it/s] 95%|█████████▌| 371/390 [02:50<00:05,  3.49it/s] 95%|█████████▌| 372/390 [02:51<00:05,  3.49it/s] 96%|█████████▌| 373/390 [02:51<00:04,  3.50it/s] 96%|█████████▌| 374/390 [02:51<00:04,  3.50it/s] 96%|█████████▌| 375/390 [02:51<00:04,  3.50it/s] 96%|█████████▋| 376/390 [02:52<00:04,  3.50it/s] 97%|█████████▋| 377/390 [02:52<00:03,  3.49it/s] 97%|█████████▋| 378/390 [02:52<00:03,  3.50it/s] 97%|█████████▋| 379/390 [02:53<00:03,  3.50it/s] 97%|█████████▋| 380/390 [02:53<00:02,  3.49it/s] 98%|█████████▊| 381/390 [02:53<00:02,  3.49it/s] 98%|█████████▊| 382/390 [02:53<00:02,  3.49it/s] 98%|█████████▊| 383/390 [02:54<00:02,  3.50it/s] 98%|█████████▊| 384/390 [02:54<00:01,  3.50it/s] 99%|█████████▊| 385/390 [02:54<00:01,  3.50it/s] 99%|█████████▉| 386/390 [02:55<00:01,  3.50it/s] 99%|█████████▉| 387/390 [02:55<00:00,  3.50it/s] 99%|█████████▉| 388/390 [02:55<00:00,  3.38it/s]100%|█████████▉| 389/390 [02:55<00:00,  3.40it/s]100%|██████████| 390/390 [02:56<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 09:29:48,130 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:29:48,130 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 09:29:48,130 >>   Batch size = 8
{'eval_loss': 1.0411278009414673, 'eval_runtime': 9.2878, 'eval_samples_per_second': 376.409, 'eval_steps_per_second': 47.051, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.09it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.32it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.54it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.12it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.84it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.57it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.26it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.26it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.24it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.19it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.13it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.09it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.15it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.17it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.08it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.04it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.02it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.10it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.19it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.15it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.08it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.15it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.99it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.97it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.04it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.97it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.02it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.12it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.06it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.08it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.11it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.06it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.96it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.01it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.01it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.00it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.04it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.10it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.01it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.99it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.95it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.91it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.88it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.97it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.00it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.05it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.61it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.09it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.07it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.02it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.94it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.03it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.07it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.04it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.06it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.05it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.97it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.99it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.97it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.96it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.08it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.11it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.09it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.13it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.04it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.95it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.03it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.02it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.97it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.12it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.17it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.08it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.08it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.05it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.81it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.92it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.94it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.82it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.97it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.09it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.03it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.04it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.02it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:05<00:00,  3.43it/s]
100%|██████████| 437/437 [00:09<00:00, 47.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:29:57,434 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 09:29:57,466 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:29:59,718 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:29:59,735 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:29:59,746 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 09:30:04,282 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 09:30:04,285 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78 (score: 1.0357869863510132).
                                                 100%|██████████| 390/390 [03:14<00:00,  3.43it/s]100%|██████████| 390/390 [03:14<00:00,  2.01it/s]
[INFO|trainer.py:1894] 2023-08-28 09:30:06,135 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 09:30:06,152 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:30:08,494 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:30:08,510 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:30:08,519 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:30:08,691 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:08,691 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:08,691 >>   train_loss               =     0.8141
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:08,691 >>   train_runtime            = 0:03:14.25
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:08,691 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:08,691 >>   train_samples_per_second =      128.7
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:08,691 >>   train_steps_per_second   =      2.008
{'eval_loss': 1.0442571640014648, 'eval_runtime': 9.2918, 'eval_samples_per_second': 376.248, 'eval_steps_per_second': 47.031, 'epoch': 4.99}
{'train_runtime': 194.2502, 'train_samples_per_second': 128.7, 'train_steps_per_second': 2.008, 'train_loss': 0.8140836275540866, 'epoch': 4.99}
08/28/2023 09:30:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 09:30:08,719 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:30:08,719 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 09:30:08,719 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.79it/s]  3%|▎         | 12/437 [00:00<00:08, 51.32it/s]  4%|▍         | 18/437 [00:00<00:08, 49.44it/s]  5%|▌         | 23/437 [00:00<00:08, 48.79it/s]  6%|▋         | 28/437 [00:00<00:08, 48.37it/s]  8%|▊         | 33/437 [00:00<00:08, 48.11it/s]  9%|▊         | 38/437 [00:00<00:08, 47.87it/s] 10%|▉         | 43/437 [00:00<00:08, 47.80it/s] 11%|█         | 48/437 [00:00<00:08, 47.53it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.43it/s] 13%|█▎        | 58/437 [00:01<00:07, 47.46it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.51it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.49it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.44it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.47it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.34it/s] 20%|██        | 88/437 [00:01<00:07, 47.36it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.30it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.23it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.26it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.31it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.35it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.43it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.47it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.39it/s] 30%|███       | 133/437 [00:02<00:06, 47.44it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.33it/s] 33%|███▎      | 143/437 [00:02<00:06, 47.20it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.22it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.18it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.18it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.26it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.33it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.43it/s] 41%|████      | 178/437 [00:03<00:05, 47.44it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.35it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.29it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.30it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.28it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.30it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.31it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.32it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.30it/s] 51%|█████     | 223/437 [00:04<00:04, 47.28it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.33it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.27it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.27it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.26it/s] 57%|█████▋    | 248/437 [00:05<00:03, 47.26it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.29it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.28it/s] 60%|██████    | 263/437 [00:05<00:03, 47.23it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.30it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.33it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.35it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.33it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.25it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.21it/s] 68%|██████▊   | 298/437 [00:06<00:02, 47.24it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.26it/s] 70%|███████   | 308/437 [00:06<00:02, 47.32it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.32it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.29it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.36it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.36it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.32it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.30it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.29it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.29it/s] 81%|████████  | 353/437 [00:07<00:01, 47.30it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.32it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.30it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.30it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.35it/s] 86%|████████▋ | 378/437 [00:07<00:01, 47.37it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.36it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.34it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.27it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.28it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.29it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.24it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.31it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.31it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.19it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.33it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.33it/s]100%|██████████| 437/437 [00:09<00:00, 47.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:30:17,956 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   eval_loss               =     1.0358
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   eval_runtime            = 0:00:09.23
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   eval_samples_per_second =    378.495
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   eval_steps_per_second   =     47.312
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:30:17,956 >>   perplexity              =     2.8173
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:24,239 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:24,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:24,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:24,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:24,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:30:24,860 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:30:24,861 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:30:25,457 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:30:26,509 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:30:26,509 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:29,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:29,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:29,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:29,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:30:29,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:30:30,146 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:30:30,148 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:30:30,730 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:30:30,871 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:30:30,872 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.48it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:15,  1.59it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.47it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:31,  1.59it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.58it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:34,  1.58it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.52it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:43,  1.48it/s]Extractor Predicting: 68it [00:44,  1.39it/s]Extractor Predicting: 69it [00:45,  1.43it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.50it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:55,  1.48it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:57,  1.48it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:58,  1.52it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:00,  1.57it/s]Extractor Predicting: 94it [01:01,  1.55it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:02,  1.54it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:05,  1.50it/s]Extractor Predicting: 101it [01:06,  1.50it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:10,  1.52it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.50it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.52it/s]Extractor Predicting: 113it [01:14,  1.51it/s]Extractor Predicting: 114it [01:14,  1.51it/s]Extractor Predicting: 115it [01:15,  1.50it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:18,  1.49it/s]Extractor Predicting: 120it [01:18,  1.51it/s]Extractor Predicting: 121it [01:19,  1.52it/s]Extractor Predicting: 122it [01:20,  1.50it/s]Extractor Predicting: 123it [01:20,  1.50it/s]Extractor Predicting: 124it [01:21,  1.48it/s]Extractor Predicting: 125it [01:22,  1.46it/s]Extractor Predicting: 126it [01:22,  1.49it/s]Extractor Predicting: 127it [01:23,  1.50it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:24,  1.53it/s]Extractor Predicting: 130it [01:25,  1.56it/s]Extractor Predicting: 131it [01:26,  1.57it/s]Extractor Predicting: 132it [01:26,  1.56it/s]Extractor Predicting: 133it [01:27,  1.55it/s]Extractor Predicting: 134it [01:28,  1.56it/s]Extractor Predicting: 135it [01:28,  1.56it/s]Extractor Predicting: 136it [01:29,  1.54it/s]Extractor Predicting: 137it [01:30,  1.52it/s]Extractor Predicting: 138it [01:30,  1.52it/s]Extractor Predicting: 139it [01:31,  1.52it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:32,  1.51it/s]Extractor Predicting: 142it [01:33,  1.49it/s]Extractor Predicting: 143it [01:34,  1.49it/s]Extractor Predicting: 144it [01:34,  1.66it/s]Extractor Predicting: 144it [01:34,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:12,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:12,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:12,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:12,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:12,873 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:32:13,513 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:32:13,514 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:32:14,075 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:32:15,117 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:32:15,117 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:17,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:17,967 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:17,968 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:17,968 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:17,968 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:32:18,617 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:32:18,618 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:32:19,198 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:32:19,352 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:32:19,352 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6624472573839663,
  "recall": 0.08981693363844394,
  "score": 0.15818639798488668,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.74it/s]Extractor Predicting: 13it [00:07,  1.74it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.75it/s]Extractor Predicting: 16it [00:09,  1.71it/s]Extractor Predicting: 17it [00:09,  1.71it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:10,  1.74it/s]Extractor Predicting: 20it [00:11,  1.73it/s]Extractor Predicting: 21it [00:12,  1.75it/s]Extractor Predicting: 22it [00:12,  1.78it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.72it/s]Extractor Predicting: 25it [00:14,  1.67it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:15,  1.72it/s]Extractor Predicting: 28it [00:16,  1.63it/s]Extractor Predicting: 29it [00:16,  1.61it/s]Extractor Predicting: 30it [00:17,  1.55it/s]Extractor Predicting: 31it [00:18,  1.51it/s]Extractor Predicting: 32it [00:19,  1.47it/s]Extractor Predicting: 33it [00:19,  1.46it/s]Extractor Predicting: 34it [00:20,  1.45it/s]Extractor Predicting: 35it [00:21,  1.47it/s]Extractor Predicting: 36it [00:21,  1.48it/s]Extractor Predicting: 37it [00:22,  1.47it/s]Extractor Predicting: 38it [00:23,  1.43it/s]Extractor Predicting: 39it [00:23,  1.44it/s]Extractor Predicting: 40it [00:24,  1.42it/s]Extractor Predicting: 41it [00:25,  1.41it/s]Extractor Predicting: 42it [00:26,  1.42it/s]Extractor Predicting: 43it [00:26,  1.43it/s]Extractor Predicting: 44it [00:27,  1.46it/s]Extractor Predicting: 45it [00:28,  1.45it/s]Extractor Predicting: 46it [00:28,  1.47it/s]Extractor Predicting: 47it [00:29,  1.45it/s]Extractor Predicting: 48it [00:30,  1.43it/s]Extractor Predicting: 49it [00:30,  1.43it/s]Extractor Predicting: 50it [00:31,  1.43it/s]Extractor Predicting: 51it [00:32,  1.42it/s]Extractor Predicting: 52it [00:32,  1.44it/s]Extractor Predicting: 53it [00:33,  1.43it/s]Extractor Predicting: 54it [00:34,  1.44it/s]Extractor Predicting: 55it [00:35,  1.43it/s]Extractor Predicting: 56it [00:35,  1.41it/s]Extractor Predicting: 57it [00:36,  1.43it/s]Extractor Predicting: 58it [00:37,  1.44it/s]Extractor Predicting: 59it [00:37,  1.46it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.44it/s]Extractor Predicting: 62it [00:39,  1.44it/s]Extractor Predicting: 63it [00:40,  1.47it/s]Extractor Predicting: 64it [00:41,  1.50it/s]Extractor Predicting: 65it [00:41,  1.51it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:43,  1.55it/s]Extractor Predicting: 69it [00:44,  1.54it/s]Extractor Predicting: 70it [00:45,  1.56it/s]Extractor Predicting: 71it [00:45,  1.61it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:48,  1.51it/s]Extractor Predicting: 76it [00:49,  1.39it/s]Extractor Predicting: 77it [00:49,  1.42it/s]Extractor Predicting: 78it [00:50,  1.44it/s]Extractor Predicting: 79it [00:51,  1.46it/s]Extractor Predicting: 80it [00:51,  1.46it/s]Extractor Predicting: 81it [00:52,  1.46it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:53,  1.49it/s]Extractor Predicting: 84it [00:54,  1.48it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:57,  1.59it/s]Extractor Predicting: 89it [00:57,  1.61it/s]Extractor Predicting: 90it [00:58,  1.61it/s]Extractor Predicting: 91it [00:58,  1.66it/s]Extractor Predicting: 92it [00:59,  1.64it/s]Extractor Predicting: 93it [00:59,  1.69it/s]Extractor Predicting: 94it [01:00,  1.66it/s]Extractor Predicting: 95it [01:01,  1.64it/s]Extractor Predicting: 96it [01:01,  1.65it/s]Extractor Predicting: 97it [01:02,  1.68it/s]Extractor Predicting: 98it [01:02,  1.70it/s]Extractor Predicting: 99it [01:03,  1.70it/s]Extractor Predicting: 100it [01:04,  1.75it/s]Extractor Predicting: 101it [01:04,  1.73it/s]Extractor Predicting: 102it [01:05,  1.68it/s]Extractor Predicting: 103it [01:05,  1.66it/s]Extractor Predicting: 104it [01:06,  1.67it/s]Extractor Predicting: 105it [01:07,  1.69it/s]Extractor Predicting: 106it [01:07,  1.68it/s]Extractor Predicting: 107it [01:08,  1.74it/s]Extractor Predicting: 108it [01:08,  1.69it/s]Extractor Predicting: 109it [01:09,  1.71it/s]Extractor Predicting: 110it [01:10,  1.70it/s]Extractor Predicting: 111it [01:10,  1.69it/s]Extractor Predicting: 112it [01:11,  1.71it/s]Extractor Predicting: 113it [01:11,  1.70it/s]Extractor Predicting: 114it [01:12,  1.68it/s]Extractor Predicting: 115it [01:13,  1.65it/s]Extractor Predicting: 116it [01:13,  1.61it/s]Extractor Predicting: 117it [01:14,  1.58it/s]Extractor Predicting: 118it [01:14,  1.58it/s]Extractor Predicting: 119it [01:15,  1.57it/s]Extractor Predicting: 120it [01:16,  1.54it/s]Extractor Predicting: 121it [01:16,  1.56it/s]Extractor Predicting: 122it [01:17,  1.55it/s]Extractor Predicting: 123it [01:18,  1.53it/s]Extractor Predicting: 124it [01:18,  1.57it/s]Extractor Predicting: 125it [01:19,  1.54it/s]Extractor Predicting: 126it [01:20,  1.55it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.51it/s]Extractor Predicting: 129it [01:22,  1.52it/s]Extractor Predicting: 130it [01:22,  1.50it/s]Extractor Predicting: 131it [01:23,  1.49it/s]Extractor Predicting: 132it [01:24,  1.49it/s]Extractor Predicting: 133it [01:24,  1.52it/s]Extractor Predicting: 134it [01:25,  1.51it/s]Extractor Predicting: 135it [01:26,  1.50it/s]Extractor Predicting: 136it [01:26,  1.47it/s]Extractor Predicting: 137it [01:27,  1.51it/s]Extractor Predicting: 138it [01:28,  1.49it/s]Extractor Predicting: 139it [01:28,  1.50it/s]Extractor Predicting: 140it [01:29,  1.49it/s]Extractor Predicting: 141it [01:30,  1.51it/s]Extractor Predicting: 142it [01:30,  1.50it/s]Extractor Predicting: 143it [01:31,  1.87it/s]Extractor Predicting: 143it [01:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:55,966 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:55,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:55,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:55,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:55,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:33:56,253 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:33:56,254 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:33:56,517 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:33:57,554 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:33:57,556 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:59,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:59,952 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:59,953 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:59,953 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:59,953 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:34:00,586 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:34:00,590 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:34:01,171 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:34:01,327 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:34:01,327 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5806451612903226,
  "recall": 0.07390029325513196,
  "score": 0.13111342351716962,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 2it [00:01,  1.39it/s]
[INFO|configuration_utils.py:515] 2023-08-28 09:34:03,098 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:34:03,098 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:34:03,104 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:34:03,105 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 09:34:03,106 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:34:06,134 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 09:34:06,139 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 09:34:06,150 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:34:06,150 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:34:06,157 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:06,162 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:06,162 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:06,163 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:06,163 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:06,163 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:34:06,163 >> loading file outputs/wrapper/fewrel/unseen_5_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.022222222222222223,
  "score": 0.0425531914893617,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 09:34:06,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:07,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:07,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:08,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:09,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:10,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:11,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:12,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:13,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:14,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:14,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:15,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:16,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:17,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:18,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:19,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:19,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:20,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:21,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:21,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:22,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:23,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:18<02:42, 18.07s/it][WARNING|generation_utils.py:914] 2023-08-28 09:34:24,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:25,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:26,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:27,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:27,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:28,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:29,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:30,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:31,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:32,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:32,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:33,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:34,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:35,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:35,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:36,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:37,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:37,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:38,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:39,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:40,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:41,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:35<02:21, 17.67s/it][WARNING|generation_utils.py:914] 2023-08-28 09:34:41,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:42,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:43,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:44,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:45,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:45,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:46,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:47,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:48,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:49,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:49,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:50,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:51,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:52,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:52,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:53,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:54,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:55,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:55,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:56,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:57,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:58,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:52<02:01, 17.39s/it][WARNING|generation_utils.py:914] 2023-08-28 09:34:58,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:34:59,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:00,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:01,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:01,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:02,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:03,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:04,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:04,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:05,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:06,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:07,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:07,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:08,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:09,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:09,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:10,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:11,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:12,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:12,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:13,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:14,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:14,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:15,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:16,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:16,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:17,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:18,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:19,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:19,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:20,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:21,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:21,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:22,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:23,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:23,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:18<02:04, 20.72s/it][WARNING|generation_utils.py:914] 2023-08-28 09:35:24,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:25,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:26,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:27,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:27,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:28,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:29,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:30,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:31,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:32,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:32,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:33,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:34,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:35,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:36,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:37,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:38,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:38,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:39,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:40,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:41,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:42,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:43,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:43,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:38<01:42, 20.57s/it][WARNING|generation_utils.py:914] 2023-08-28 09:35:45,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:45,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:46,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:47,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:48,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:49,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:49,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:50,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:51,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:52,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:52,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:53,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:54,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:55,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:56,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:57,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:57,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:58,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:35:59,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:00,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:01,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:02,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:02,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:03,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:58<01:21, 20.27s/it][WARNING|generation_utils.py:914] 2023-08-28 09:36:04,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:05,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:06,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:07,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:07,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:08,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:09,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:10,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:11,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:11,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:12,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:13,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:14,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:15,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:15,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:16,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:17,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:18,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:18,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:19,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:20,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:21,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:22,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:22,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:17<00:59, 19.81s/it][WARNING|generation_utils.py:914] 2023-08-28 09:36:23,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:24,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:24,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:25,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:26,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:27,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:28,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:28,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:29,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:30,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:30,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:31,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:32,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:32,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:33,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:34,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:34,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:35,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:36,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:36,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:37,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:38,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:32<00:36, 18.40s/it][WARNING|generation_utils.py:914] 2023-08-28 09:36:38,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:39,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:40,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:41,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:42,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:42,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:43,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:44,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:45,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:46,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:47,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:48,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:49,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:50,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:50,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:51,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:52,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:53,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:54,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:54,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:55,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:56,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:56,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:51<00:18, 18.53s/it][WARNING|generation_utils.py:914] 2023-08-28 09:36:57,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:58,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:59,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:36:59,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:00,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:01,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:02,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:02,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:03,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:04,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:05,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:05,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:06,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:07,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:08,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:08,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:09,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:10,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:11,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:11,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:12,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:13,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:14,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:37:15,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:09<00:00, 18.43s/it]Generating: 100%|██████████| 10/10 [03:09<00:00, 18.96s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:21,516 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:21,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:21,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:21,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:21,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:37:22,140 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:37:22,141 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:37:22,693 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:37:23,766 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:37:23,766 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:26,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:26,631 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:26,631 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:26,631 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:37:26,631 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:37:27,277 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:37:27,278 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:37:27,896 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:37:28,062 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:37:28,062 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8792613636363636, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original language of film or TV show . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : The Walking Dead , Tail Entity : the Spanish language .\n']
{'target': 600, 'success': 14, 'raw': 32}
{'target': 600, 'success': 31, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 67, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 98, 'raw': 192}
{'target': 600, 'success': 113, 'raw': 224}
{'target': 600, 'success': 130, 'raw': 256}
{'target': 600, 'success': 145, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 175, 'raw': 352}
{'target': 600, 'success': 194, 'raw': 384}
{'target': 600, 'success': 214, 'raw': 416}
{'target': 600, 'success': 227, 'raw': 448}
{'target': 600, 'success': 247, 'raw': 480}
{'target': 600, 'success': 260, 'raw': 512}
{'target': 600, 'success': 279, 'raw': 544}
{'target': 600, 'success': 296, 'raw': 576}
{'target': 600, 'success': 309, 'raw': 608}
{'target': 600, 'success': 327, 'raw': 640}
{'target': 600, 'success': 344, 'raw': 672}
{'target': 600, 'success': 360, 'raw': 704}
{'target': 600, 'success': 382, 'raw': 736}
{'target': 600, 'success': 401, 'raw': 768}
{'target': 600, 'success': 417, 'raw': 800}
{'target': 600, 'success': 435, 'raw': 832}
{'target': 600, 'success': 452, 'raw': 864}
{'target': 600, 'success': 467, 'raw': 896}
{'target': 600, 'success': 481, 'raw': 928}
{'target': 600, 'success': 499, 'raw': 960}
{'target': 600, 'success': 514, 'raw': 992}
{'target': 600, 'success': 530, 'raw': 1024}
{'target': 600, 'success': 547, 'raw': 1056}
{'target': 600, 'success': 566, 'raw': 1088}
{'target': 600, 'success': 582, 'raw': 1120}
{'target': 600, 'success': 601, 'raw': 1152}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.5217013888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : owned by .', 'success_rate': 0.7903645833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Wachowskis - Dies - Unser\', \'owned by\', \'\', \'A series of " Wachowskis - Dies - Unser " books was published by the author in 1987 , and the author continued to publish it until 1991 .\')'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8636363636363636, 'errors': {'', '(\'  \', \'licensed to broadcast to\', \'FX\', \'" Mika and Mika (    ) " was produced by FX , and is currently available in the United States under a theatrical release starting in October 2015 .\')'}}
['Relation : occupant . Context : Later in the year ( the year before ) , a fleet of warships entered the Strait of Gibraltar and began strafing the French islands . Head Entity : sea , Tail Entity : French .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 10559
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10659, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.52it/s]Extractor Estimating: 6it [00:04,  1.49it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.47it/s]Extractor Estimating: 9it [00:06,  1.44it/s]Extractor Estimating: 10it [00:06,  1.46it/s]Extractor Estimating: 11it [00:07,  1.46it/s]Extractor Estimating: 12it [00:08,  1.46it/s]Extractor Estimating: 13it [00:08,  1.45it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:10,  1.55it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.51it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.52it/s]Extractor Estimating: 22it [00:14,  1.51it/s]Extractor Estimating: 23it [00:15,  1.53it/s]Extractor Estimating: 24it [00:16,  1.46it/s]Extractor Estimating: 25it [00:16,  1.42it/s]Extractor Estimating: 26it [00:17,  1.43it/s]Extractor Estimating: 27it [00:18,  1.42it/s]Extractor Estimating: 28it [00:18,  1.44it/s]Extractor Estimating: 29it [00:19,  1.49it/s]Extractor Estimating: 30it [00:20,  1.45it/s]Extractor Estimating: 31it [00:21,  1.42it/s]Extractor Estimating: 32it [00:21,  1.46it/s]Extractor Estimating: 33it [00:22,  1.46it/s]Extractor Estimating: 34it [00:23,  1.48it/s]Extractor Estimating: 35it [00:23,  1.50it/s]Extractor Estimating: 36it [00:24,  1.49it/s]Extractor Estimating: 37it [00:24,  1.51it/s]Extractor Estimating: 38it [00:25,  1.46it/s]Extractor Estimating: 39it [00:26,  1.43it/s]Extractor Estimating: 40it [00:27,  1.48it/s]Extractor Estimating: 41it [00:27,  1.48it/s]Extractor Estimating: 42it [00:28,  1.45it/s]Extractor Estimating: 43it [00:29,  1.49it/s]Extractor Estimating: 44it [00:29,  1.49it/s]Extractor Estimating: 45it [00:30,  1.49it/s]Extractor Estimating: 46it [00:31,  1.46it/s]Extractor Estimating: 47it [00:31,  1.45it/s]Extractor Estimating: 48it [00:32,  1.43it/s]Extractor Estimating: 49it [00:33,  1.44it/s]Extractor Estimating: 50it [00:33,  1.45it/s]Extractor Estimating: 51it [00:34,  1.50it/s]Extractor Estimating: 52it [00:35,  1.50it/s]Extractor Estimating: 53it [00:35,  1.52it/s]Extractor Estimating: 54it [00:36,  1.52it/s]Extractor Estimating: 55it [00:37,  1.51it/s]Extractor Estimating: 56it [00:37,  1.59it/s]Extractor Estimating: 57it [00:38,  1.57it/s]Extractor Estimating: 58it [00:39,  1.57it/s]Extractor Estimating: 59it [00:39,  1.54it/s]Extractor Estimating: 60it [00:40,  1.54it/s]Extractor Estimating: 61it [00:41,  1.52it/s]Extractor Estimating: 62it [00:41,  1.53it/s]Extractor Estimating: 63it [00:42,  1.55it/s]Extractor Estimating: 64it [00:43,  1.44it/s]Extractor Estimating: 65it [00:43,  1.45it/s]Extractor Estimating: 66it [00:44,  1.49it/s]Extractor Estimating: 67it [00:45,  1.54it/s]Extractor Estimating: 68it [00:45,  1.56it/s]Extractor Estimating: 69it [00:46,  1.59it/s]Extractor Estimating: 70it [00:46,  1.62it/s]Extractor Estimating: 71it [00:47,  1.59it/s]Extractor Estimating: 72it [00:48,  1.63it/s]Extractor Estimating: 73it [00:48,  1.60it/s]Extractor Estimating: 74it [00:49,  1.61it/s]Extractor Estimating: 75it [00:49,  1.59it/s]Extractor Estimating: 76it [00:50,  1.57it/s]Extractor Estimating: 77it [00:51,  1.54it/s]Extractor Estimating: 78it [00:51,  1.53it/s]Extractor Estimating: 79it [00:52,  1.54it/s]Extractor Estimating: 80it [00:53,  1.52it/s]Extractor Estimating: 81it [00:53,  1.61it/s]Extractor Estimating: 82it [00:54,  1.57it/s]Extractor Estimating: 83it [00:55,  1.55it/s]Extractor Estimating: 84it [00:55,  1.53it/s]Extractor Estimating: 85it [00:56,  1.48it/s]Extractor Estimating: 86it [00:57,  1.49it/s]Extractor Estimating: 87it [00:57,  1.51it/s]Extractor Estimating: 88it [00:58,  1.53it/s]Extractor Estimating: 89it [00:59,  1.60it/s]Extractor Estimating: 90it [00:59,  1.57it/s]Extractor Estimating: 91it [01:00,  1.57it/s]Extractor Estimating: 92it [01:00,  1.62it/s]Extractor Estimating: 93it [01:01,  1.57it/s]Extractor Estimating: 94it [01:02,  1.59it/s]Extractor Estimating: 95it [01:02,  1.59it/s]Extractor Estimating: 96it [01:03,  1.62it/s]Extractor Estimating: 97it [01:04,  1.63it/s]Extractor Estimating: 98it [01:04,  1.60it/s]Extractor Estimating: 99it [01:05,  1.55it/s]Extractor Estimating: 100it [01:05,  1.60it/s]Extractor Estimating: 101it [01:06,  1.58it/s]Extractor Estimating: 102it [01:07,  1.54it/s]Extractor Estimating: 103it [01:07,  1.55it/s]Extractor Estimating: 104it [01:08,  1.57it/s]Extractor Estimating: 105it [01:09,  1.53it/s]Extractor Estimating: 106it [01:09,  1.53it/s]Extractor Estimating: 107it [01:10,  1.54it/s]Extractor Estimating: 108it [01:11,  1.50it/s]Extractor Estimating: 109it [01:11,  1.52it/s]Extractor Estimating: 110it [01:12,  1.52it/s]Extractor Estimating: 111it [01:13,  1.41it/s]Extractor Estimating: 112it [01:14,  1.44it/s]Extractor Estimating: 113it [01:14,  1.37it/s]Extractor Estimating: 114it [01:15,  1.45it/s]Extractor Estimating: 115it [01:16,  1.46it/s]Extractor Estimating: 116it [01:16,  1.46it/s]Extractor Estimating: 117it [01:17,  1.44it/s]Extractor Estimating: 118it [01:18,  1.43it/s]Extractor Estimating: 119it [01:18,  1.48it/s]Extractor Estimating: 120it [01:19,  1.48it/s]Extractor Estimating: 121it [01:20,  1.48it/s]Extractor Estimating: 122it [01:20,  1.52it/s]Extractor Estimating: 123it [01:21,  1.57it/s]Extractor Estimating: 124it [01:22,  1.54it/s]Extractor Estimating: 125it [01:22,  1.54it/s]Extractor Estimating: 126it [01:23,  1.51it/s]Extractor Estimating: 127it [01:24,  1.47it/s]Extractor Estimating: 128it [01:24,  1.53it/s]Extractor Estimating: 129it [01:25,  1.51it/s]Extractor Estimating: 130it [01:26,  1.53it/s]Extractor Estimating: 131it [01:26,  1.54it/s]Extractor Estimating: 132it [01:27,  1.58it/s]Extractor Estimating: 133it [01:27,  1.57it/s]Extractor Estimating: 134it [01:28,  1.56it/s]Extractor Estimating: 135it [01:29,  1.54it/s]Extractor Estimating: 136it [01:29,  1.53it/s]Extractor Estimating: 137it [01:30,  1.41it/s]Extractor Estimating: 138it [01:31,  1.44it/s]Extractor Estimating: 139it [01:32,  1.48it/s]Extractor Estimating: 140it [01:32,  1.50it/s]Extractor Estimating: 141it [01:33,  1.49it/s]Extractor Estimating: 142it [01:33,  1.52it/s]Extractor Estimating: 143it [01:34,  1.51it/s]Extractor Estimating: 144it [01:35,  1.48it/s]Extractor Estimating: 145it [01:36,  1.44it/s]Extractor Estimating: 146it [01:36,  1.42it/s]Extractor Estimating: 147it [01:37,  1.47it/s]Extractor Estimating: 148it [01:38,  1.55it/s]Extractor Estimating: 149it [01:38,  1.54it/s]Extractor Estimating: 150it [01:39,  1.47it/s]Extractor Estimating: 151it [01:40,  1.47it/s]Extractor Estimating: 152it [01:40,  1.45it/s]Extractor Estimating: 153it [01:41,  1.49it/s]Extractor Estimating: 154it [01:42,  1.46it/s]Extractor Estimating: 155it [01:42,  1.47it/s]Extractor Estimating: 156it [01:43,  1.49it/s]Extractor Estimating: 157it [01:44,  1.49it/s]Extractor Estimating: 158it [01:44,  1.51it/s]Extractor Estimating: 159it [01:45,  1.51it/s]Extractor Estimating: 160it [01:46,  1.47it/s]Extractor Estimating: 161it [01:46,  1.50it/s]Extractor Estimating: 162it [01:47,  1.53it/s]Extractor Estimating: 163it [01:48,  1.54it/s]Extractor Estimating: 164it [01:48,  1.56it/s]Extractor Estimating: 165it [01:49,  1.56it/s]Extractor Estimating: 166it [01:50,  1.53it/s]Extractor Estimating: 167it [01:50,  1.48it/s]Extractor Estimating: 168it [01:51,  1.45it/s]Extractor Estimating: 169it [01:52,  1.39it/s]Extractor Estimating: 170it [01:52,  1.42it/s]Extractor Estimating: 171it [01:53,  1.44it/s]Extractor Estimating: 172it [01:54,  1.46it/s]Extractor Estimating: 173it [01:54,  1.48it/s]Extractor Estimating: 174it [01:55,  1.52it/s]Extractor Estimating: 175it [01:56,  1.50it/s]Extractor Estimating: 176it [01:56,  1.56it/s]Extractor Estimating: 177it [01:57,  1.59it/s]Extractor Estimating: 178it [01:58,  1.54it/s]Extractor Estimating: 179it [01:58,  1.54it/s]Extractor Estimating: 180it [01:59,  1.33it/s]Extractor Estimating: 181it [02:00,  1.44it/s]Extractor Estimating: 182it [02:00,  1.47it/s]Extractor Estimating: 183it [02:01,  1.50it/s]Extractor Estimating: 184it [02:02,  1.52it/s]Extractor Estimating: 185it [02:02,  1.51it/s]Extractor Estimating: 186it [02:03,  1.49it/s]Extractor Estimating: 187it [02:04,  1.50it/s]Extractor Estimating: 188it [02:04,  1.52it/s]Extractor Estimating: 189it [02:05,  1.53it/s]Extractor Estimating: 190it [02:06,  1.53it/s]Extractor Estimating: 191it [02:06,  1.52it/s]Extractor Estimating: 192it [02:07,  1.51it/s]Extractor Estimating: 193it [02:08,  1.50it/s]Extractor Estimating: 194it [02:08,  1.52it/s]Extractor Estimating: 195it [02:09,  1.57it/s]Extractor Estimating: 196it [02:10,  1.60it/s]Extractor Estimating: 197it [02:10,  1.58it/s]Extractor Estimating: 198it [02:11,  1.59it/s]Extractor Estimating: 199it [02:11,  1.55it/s]Extractor Estimating: 200it [02:12,  1.50it/s]Extractor Estimating: 201it [02:13,  1.43it/s]Extractor Estimating: 202it [02:14,  1.46it/s]Extractor Estimating: 203it [02:14,  1.51it/s]Extractor Estimating: 204it [02:15,  1.50it/s]Extractor Estimating: 205it [02:15,  1.54it/s]Extractor Estimating: 206it [02:16,  1.51it/s]Extractor Estimating: 207it [02:17,  1.56it/s]Extractor Estimating: 208it [02:17,  1.52it/s]Extractor Estimating: 209it [02:18,  1.49it/s]Extractor Estimating: 210it [02:19,  1.48it/s]Extractor Estimating: 211it [02:20,  1.44it/s]Extractor Estimating: 212it [02:20,  1.34it/s]Extractor Estimating: 213it [02:21,  1.37it/s]Extractor Estimating: 214it [02:22,  1.42it/s]Extractor Estimating: 215it [02:22,  1.48it/s]Extractor Estimating: 216it [02:23,  1.47it/s]Extractor Estimating: 217it [02:24,  1.45it/s]Extractor Estimating: 218it [02:24,  1.46it/s]Extractor Estimating: 219it [02:25,  1.46it/s]Extractor Estimating: 220it [02:26,  1.47it/s]Extractor Estimating: 221it [02:27,  1.45it/s]Extractor Estimating: 222it [02:27,  1.51it/s]Extractor Estimating: 223it [02:28,  1.47it/s]Extractor Estimating: 224it [02:29,  1.47it/s]Extractor Estimating: 225it [02:29,  1.41it/s]Extractor Estimating: 226it [02:30,  1.47it/s]Extractor Estimating: 227it [02:31,  1.47it/s]Extractor Estimating: 228it [02:31,  1.48it/s]Extractor Estimating: 229it [02:32,  1.52it/s]Extractor Estimating: 230it [02:32,  1.57it/s]Extractor Estimating: 231it [02:33,  1.53it/s]Extractor Estimating: 232it [02:34,  1.54it/s]Extractor Estimating: 233it [02:34,  1.57it/s]Extractor Estimating: 234it [02:35,  1.58it/s]Extractor Estimating: 235it [02:36,  1.55it/s]Extractor Estimating: 236it [02:36,  1.55it/s]Extractor Estimating: 237it [02:37,  1.57it/s]Extractor Estimating: 238it [02:38,  1.55it/s]Extractor Estimating: 239it [02:38,  1.53it/s]Extractor Estimating: 240it [02:39,  1.53it/s]Extractor Estimating: 241it [02:40,  1.51it/s]Extractor Estimating: 242it [02:40,  1.58it/s]Extractor Estimating: 243it [02:41,  1.64it/s]Extractor Estimating: 244it [02:41,  1.62it/s]Extractor Estimating: 245it [02:42,  1.60it/s]Extractor Estimating: 246it [02:43,  1.56it/s]Extractor Estimating: 247it [02:43,  1.54it/s]Extractor Estimating: 248it [02:44,  1.60it/s]Extractor Estimating: 249it [02:45,  1.56it/s]Extractor Estimating: 250it [02:45,  1.58it/s]Extractor Estimating: 250it [02:45,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:25,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:25,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:25,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:25,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:25,704 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:40:26,308 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:40:26,309 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:40:26,875 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:40:27,936 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:40:27,936 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:30,767 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:30,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:30,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:30,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:40:30,771 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:40:31,385 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:40:31,387 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:40:31,969 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:40:32,123 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:40:32,123 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 11:14:27,695 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 11:14:27,728 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4989 mean pseudo reward: 0.9356958387322187
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 22422
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22522, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22522, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.070, loss:1025.1449
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.123, loss:942.2156
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.100, loss:898.2857
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 1.076, loss:929.5886
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.103, loss:880.2151
>> valid entity prec:0.6171, rec:0.5690, f1:0.5920
>> valid relation prec:0.3390, rec:0.0626, f1:0.1057
>> valid relation with NER prec:0.3390, rec:0.0626, f1:0.1057
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.431, loss:870.8840
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.078, loss:836.0857
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.099, loss:852.8852
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.080, loss:846.9181
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.099, loss:820.7395
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6097, rec:0.6112, f1:0.6105
>> valid relation prec:0.2466, rec:0.0615, f1:0.0984
>> valid relation with NER prec:0.2466, rec:0.0615, f1:0.0984
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.439, loss:796.3091
g_step 1200, step 160, avg_time 1.090, loss:773.0503
g_step 1300, step 52, avg_time 1.093, loss:794.6928
g_step 1400, step 152, avg_time 1.097, loss:754.2629
g_step 1500, step 44, avg_time 1.092, loss:726.9652
>> valid entity prec:0.5724, rec:0.5724, f1:0.5724
>> valid relation prec:0.2304, rec:0.0621, f1:0.0978
>> valid relation with NER prec:0.2304, rec:0.0621, f1:0.0978
g_step 1600, step 144, avg_time 2.438, loss:706.3327
g_step 1700, step 36, avg_time 1.082, loss:682.6158
g_step 1800, step 136, avg_time 1.094, loss:665.7476
g_step 1900, step 28, avg_time 1.077, loss:664.3118
g_step 2000, step 128, avg_time 1.098, loss:641.4213
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5843, rec:0.5963, f1:0.5902
>> valid relation prec:0.2156, rec:0.0618, f1:0.0960
>> valid relation with NER prec:0.2156, rec:0.0618, f1:0.0960
g_step 2100, step 20, avg_time 2.441, loss:623.3115
g_step 2200, step 120, avg_time 1.093, loss:590.8607
g_step 2300, step 12, avg_time 1.094, loss:621.1047
g_step 2400, step 112, avg_time 1.095, loss:558.3030
g_step 2500, step 4, avg_time 1.082, loss:593.8686
>> valid entity prec:0.5776, rec:0.5682, f1:0.5729
>> valid relation prec:0.2034, rec:0.0644, f1:0.0978
>> valid relation with NER prec:0.2034, rec:0.0644, f1:0.0978
g_step 2600, step 104, avg_time 2.445, loss:538.7204
g_step 2700, step 204, avg_time 1.088, loss:561.9402
g_step 2800, step 96, avg_time 1.099, loss:493.5807
g_step 2900, step 196, avg_time 1.076, loss:544.0609
g_step 3000, step 88, avg_time 1.102, loss:492.2842
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5723, rec:0.6193, f1:0.5949
>> valid relation prec:0.1925, rec:0.0818, f1:0.1148
>> valid relation with NER prec:0.1925, rec:0.0818, f1:0.1148
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.463, loss:495.5182
g_step 3200, step 80, avg_time 1.100, loss:468.3884
g_step 3300, step 180, avg_time 1.091, loss:467.9282
g_step 3400, step 72, avg_time 1.133, loss:439.6498
g_step 3500, step 172, avg_time 1.085, loss:455.2146
>> valid entity prec:0.5691, rec:0.5868, f1:0.5778
>> valid relation prec:0.2132, rec:0.0769, f1:0.1131
>> valid relation with NER prec:0.2132, rec:0.0769, f1:0.1131
g_step 3600, step 64, avg_time 2.478, loss:445.4310
g_step 3700, step 164, avg_time 1.090, loss:426.1238
g_step 3800, step 56, avg_time 1.091, loss:434.9406
g_step 3900, step 156, avg_time 1.118, loss:411.4566
g_step 4000, step 48, avg_time 1.079, loss:401.0829
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5559, rec:0.5803, f1:0.5679
>> valid relation prec:0.1720, rec:0.0649, f1:0.0943
>> valid relation with NER prec:0.1720, rec:0.0649, f1:0.0943
g_step 4100, step 148, avg_time 2.464, loss:386.3455
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 11:14:27 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 11:14:27 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_11-14-27_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 11:14:28 - WARNING - datasets.builder -   Using custom data configuration default-9f0de73818aa25d5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9f0de73818aa25d5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 11:14:28,952 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:14:28,953 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:14:28,954 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:14:28,955 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:14:28,962 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:14:28,967 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:14:28,967 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:14:28,968 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:14:28,968 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:14:28,968 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:14:28,968 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 11:14:29,105 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:14:32,443 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 11:14:32,445 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9f0de73818aa25d5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.12ba/s] 40%|████      | 2/5 [00:00<00:00,  3.91ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.32ba/s]100%|██████████| 5/5 [00:01<00:00,  4.39ba/s]100%|██████████| 5/5 [00:01<00:00,  4.20ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.00ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.23ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.29ba/s]100%|██████████| 4/4 [00:00<00:00,  5.33ba/s]100%|██████████| 4/4 [00:00<00:00,  4.85ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.50ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.83ba/s]100%|██████████| 5/5 [00:00<00:00, 10.19ba/s]100%|██████████| 5/5 [00:00<00:00, 10.01ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.59ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.72ba/s]100%|██████████| 4/4 [00:00<00:00, 11.11ba/s]
[INFO|trainer.py:414] 2023-08-28 11:14:35,691 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 11:14:35,704 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 11:14:35,705 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 11:14:35,705 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 11:14:35,705 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 11:14:35,705 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 11:14:35,705 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 11:14:35,705 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:53,  3.42it/s]  1%|          | 2/390 [00:00<01:51,  3.47it/s]  1%|          | 3/390 [00:00<01:50,  3.49it/s]  1%|          | 4/390 [00:01<01:50,  3.50it/s]  1%|▏         | 5/390 [00:01<01:49,  3.50it/s]  2%|▏         | 6/390 [00:01<01:49,  3.51it/s]  2%|▏         | 7/390 [00:02<01:49,  3.51it/s]  2%|▏         | 8/390 [00:02<01:48,  3.51it/s]  2%|▏         | 9/390 [00:02<01:48,  3.51it/s]  3%|▎         | 10/390 [00:02<01:48,  3.51it/s]  3%|▎         | 11/390 [00:03<01:47,  3.51it/s]  3%|▎         | 12/390 [00:03<01:47,  3.51it/s]  3%|▎         | 13/390 [00:03<01:47,  3.51it/s]  4%|▎         | 14/390 [00:03<01:47,  3.50it/s]  4%|▍         | 15/390 [00:04<01:47,  3.50it/s]  4%|▍         | 16/390 [00:04<01:46,  3.50it/s]  4%|▍         | 17/390 [00:04<01:46,  3.50it/s]  5%|▍         | 18/390 [00:05<01:46,  3.50it/s]  5%|▍         | 19/390 [00:05<01:45,  3.50it/s]  5%|▌         | 20/390 [00:05<01:45,  3.50it/s]  5%|▌         | 21/390 [00:05<01:45,  3.50it/s]  6%|▌         | 22/390 [00:06<01:45,  3.50it/s]  6%|▌         | 23/390 [00:06<01:44,  3.50it/s]  6%|▌         | 24/390 [00:06<01:44,  3.50it/s]  6%|▋         | 25/390 [00:07<01:44,  3.50it/s]  7%|▋         | 26/390 [00:07<01:44,  3.50it/s]  7%|▋         | 27/390 [00:07<01:43,  3.50it/s]  7%|▋         | 28/390 [00:07<01:43,  3.50it/s]  7%|▋         | 29/390 [00:08<01:43,  3.50it/s]  8%|▊         | 30/390 [00:08<01:42,  3.50it/s]  8%|▊         | 31/390 [00:08<01:42,  3.50it/s]  8%|▊         | 32/390 [00:09<01:42,  3.50it/s]  8%|▊         | 33/390 [00:09<01:41,  3.50it/s]  9%|▊         | 34/390 [00:09<01:41,  3.50it/s]  9%|▉         | 35/390 [00:09<01:41,  3.50it/s]  9%|▉         | 36/390 [00:10<01:41,  3.50it/s]  9%|▉         | 37/390 [00:10<01:40,  3.50it/s] 10%|▉         | 38/390 [00:10<01:40,  3.50it/s] 10%|█         | 39/390 [00:11<01:40,  3.50it/s] 10%|█         | 40/390 [00:11<01:40,  3.50it/s] 11%|█         | 41/390 [00:11<01:39,  3.50it/s] 11%|█         | 42/390 [00:11<01:39,  3.50it/s] 11%|█         | 43/390 [00:12<01:39,  3.50it/s] 11%|█▏        | 44/390 [00:12<01:38,  3.50it/s] 12%|█▏        | 45/390 [00:12<01:38,  3.50it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.50it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.50it/s] 12%|█▏        | 48/390 [00:13<01:37,  3.50it/s] 13%|█▎        | 49/390 [00:13<01:37,  3.50it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.50it/s] 13%|█▎        | 51/390 [00:14<01:36,  3.50it/s] 13%|█▎        | 52/390 [00:14<01:36,  3.50it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.50it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.50it/s] 14%|█▍        | 55/390 [00:15<01:35,  3.50it/s] 14%|█▍        | 56/390 [00:15<01:35,  3.50it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 58/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 59/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 60/390 [00:17<01:34,  3.50it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.50it/s] 16%|█▌        | 62/390 [00:17<01:33,  3.50it/s] 16%|█▌        | 63/390 [00:17<01:33,  3.50it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.50it/s] 17%|█▋        | 65/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 66/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.50it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.50it/s] 18%|█▊        | 69/390 [00:19<01:31,  3.50it/s] 18%|█▊        | 70/390 [00:20<01:31,  3.50it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.50it/s] 18%|█▊        | 72/390 [00:20<01:30,  3.50it/s] 19%|█▊        | 73/390 [00:20<01:30,  3.50it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.49it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.49it/s] 19%|█▉        | 76/390 [00:21<01:29,  3.49it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.50it/s] 20%|██        | 78/390 [00:22<01:29,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 11:14:58,040 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:14:58,040 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 11:14:58,040 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 58.16it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.13it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.44it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.66it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.10it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.90it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.74it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.44it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.30it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.40it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.35it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.34it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.35it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.39it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.05it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.25it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.18it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.30it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.29it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.35it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.26it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.37it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.42it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.30it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.31it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.29it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.42it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.36it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.37it/s][A
 35%|███▌      | 153/437 [00:03<00:05, 47.41it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.25it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.40it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.20it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.32it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.29it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.33it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.39it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.40it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.46it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.32it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.31it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.32it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.39it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.18it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.24it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.25it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.28it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.31it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.32it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.16it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.14it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.17it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.16it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.19it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.22it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.24it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.34it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.32it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.31it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.17it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.19it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.14it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.20it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.21it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.25it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.23it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.30it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.26it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.14it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.29it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.15it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.12it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.22it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.21it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.19it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.22it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.28it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.14it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.23it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.12it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.03it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.11it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.16it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.13it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.20it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.20it/s][A 20%|██        | 78/390 [00:31<01:29,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:15:07,304 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 11:15:07,326 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:15:09,787 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:15:09,808 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:15:09,824 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<28:02,  5.41s/it] 21%|██        | 80/390 [00:39<20:01,  3.88s/it] 21%|██        | 81/390 [00:40<14:24,  2.80s/it] 21%|██        | 82/390 [00:40<10:29,  2.04s/it] 21%|██▏       | 83/390 [00:40<07:45,  1.52s/it] 22%|██▏       | 84/390 [00:41<05:51,  1.15s/it] 22%|██▏       | 85/390 [00:41<04:31,  1.12it/s] 22%|██▏       | 86/390 [00:41<03:35,  1.41it/s] 22%|██▏       | 87/390 [00:41<02:56,  1.72it/s] 23%|██▎       | 88/390 [00:42<02:28,  2.03it/s] 23%|██▎       | 89/390 [00:42<02:09,  2.32it/s] 23%|██▎       | 90/390 [00:42<01:56,  2.58it/s] 23%|██▎       | 91/390 [00:43<01:47,  2.79it/s] 24%|██▎       | 92/390 [00:43<01:40,  2.97it/s] 24%|██▍       | 93/390 [00:43<01:35,  3.11it/s] 24%|██▍       | 94/390 [00:43<01:32,  3.21it/s] 24%|██▍       | 95/390 [00:44<01:29,  3.29it/s] 25%|██▍       | 96/390 [00:44<01:27,  3.35it/s] 25%|██▍       | 97/390 [00:44<01:26,  3.39it/s] 25%|██▌       | 98/390 [00:45<01:25,  3.42it/s] 25%|██▌       | 99/390 [00:45<01:24,  3.44it/s] 26%|██▌       | 100/390 [00:45<01:23,  3.46it/s] 26%|██▌       | 101/390 [00:45<01:23,  3.47it/s] 26%|██▌       | 102/390 [00:46<01:23,  3.47it/s] 26%|██▋       | 103/390 [00:46<01:22,  3.47it/s] 27%|██▋       | 104/390 [00:46<01:22,  3.48it/s] 27%|██▋       | 105/390 [00:47<01:21,  3.48it/s] 27%|██▋       | 106/390 [00:47<01:21,  3.48it/s] 27%|██▋       | 107/390 [00:47<01:21,  3.49it/s] 28%|██▊       | 108/390 [00:47<01:20,  3.48it/s] 28%|██▊       | 109/390 [00:48<01:20,  3.48it/s] 28%|██▊       | 110/390 [00:48<01:20,  3.49it/s] 28%|██▊       | 111/390 [00:48<01:19,  3.49it/s] 29%|██▊       | 112/390 [00:49<01:19,  3.49it/s] 29%|██▉       | 113/390 [00:49<01:19,  3.48it/s] 29%|██▉       | 114/390 [00:49<01:19,  3.48it/s] 29%|██▉       | 115/390 [00:49<01:18,  3.49it/s] 30%|██▉       | 116/390 [00:50<01:18,  3.49it/s] 30%|███       | 117/390 [00:50<01:18,  3.49it/s] 30%|███       | 118/390 [00:50<01:17,  3.49it/s] 31%|███       | 119/390 [00:51<01:17,  3.47it/s] 31%|███       | 120/390 [00:51<01:17,  3.48it/s] 31%|███       | 121/390 [00:51<01:17,  3.48it/s] 31%|███▏      | 122/390 [00:51<01:16,  3.48it/s] 32%|███▏      | 123/390 [00:52<01:18,  3.42it/s] 32%|███▏      | 124/390 [00:52<01:17,  3.41it/s] 32%|███▏      | 125/390 [00:52<01:17,  3.44it/s] 32%|███▏      | 126/390 [00:53<01:16,  3.45it/s] 33%|███▎      | 127/390 [00:53<01:15,  3.46it/s] 33%|███▎      | 128/390 [00:53<01:15,  3.47it/s] 33%|███▎      | 129/390 [00:54<01:15,  3.47it/s] 33%|███▎      | 130/390 [00:54<01:14,  3.48it/s] 34%|███▎      | 131/390 [00:54<01:14,  3.48it/s] 34%|███▍      | 132/390 [00:54<01:14,  3.48it/s] 34%|███▍      | 133/390 [00:55<01:13,  3.48it/s] 34%|███▍      | 134/390 [00:55<01:13,  3.49it/s] 35%|███▍      | 135/390 [00:55<01:13,  3.48it/s] 35%|███▍      | 136/390 [00:56<01:12,  3.48it/s] 35%|███▌      | 137/390 [00:56<01:12,  3.48it/s] 35%|███▌      | 138/390 [00:56<01:12,  3.49it/s] 36%|███▌      | 139/390 [00:56<01:12,  3.48it/s] 36%|███▌      | 140/390 [00:57<01:11,  3.48it/s] 36%|███▌      | 141/390 [00:57<01:11,  3.49it/s] 36%|███▋      | 142/390 [00:57<01:11,  3.49it/s] 37%|███▋      | 143/390 [00:58<01:10,  3.49it/s] 37%|███▋      | 144/390 [00:58<01:10,  3.49it/s] 37%|███▋      | 145/390 [00:58<01:10,  3.49it/s] 37%|███▋      | 146/390 [00:58<01:09,  3.49it/s] 38%|███▊      | 147/390 [00:59<01:09,  3.49it/s] 38%|███▊      | 148/390 [00:59<01:09,  3.49it/s] 38%|███▊      | 149/390 [00:59<01:09,  3.49it/s] 38%|███▊      | 150/390 [01:00<01:08,  3.48it/s] 39%|███▊      | 151/390 [01:00<01:08,  3.49it/s] 39%|███▉      | 152/390 [01:00<01:08,  3.49it/s] 39%|███▉      | 153/390 [01:00<01:07,  3.49it/s] 39%|███▉      | 154/390 [01:01<01:07,  3.49it/s] 40%|███▉      | 155/390 [01:01<01:07,  3.49it/s] 40%|████      | 156/390 [01:01<01:07,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 11:15:37,522 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:15:37,522 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 11:15:37,523 >>   Batch size = 8
{'eval_loss': 1.0313676595687866, 'eval_runtime': 9.2504, 'eval_samples_per_second': 377.928, 'eval_steps_per_second': 47.241, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.59it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.97it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.21it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.50it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.12it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.84it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.67it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.27it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.32it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.36it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.21it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.41it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.25it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.26it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.27it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.22it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.14it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.15it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.23it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.27it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.17it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.18it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.19it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.13it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.11it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.08it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.04it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.09it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.21it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.23it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.09it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.11it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.00it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.08it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.06it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.07it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.20it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.24it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.10it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.12it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.09it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.05it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.01it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.11it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.15it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.12it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.20it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.08it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.14it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.14it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.03it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.05it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.08it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.18it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.19it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.19it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.16it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.04it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.06it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.11it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.09it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.96it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.12it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.21it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.12it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.17it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.09it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.97it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.04it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.03it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.07it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.16it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.24it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.06it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.07it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.12it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.01it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.91it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.09it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.07it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.10it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.17it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.08it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.02it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.06it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.06it/s][A 40%|████      | 156/390 [01:11<01:07,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:15:46,821 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 11:15:46,837 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:15:49,205 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:15:49,230 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:15:49,248 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:18<20:27,  5.27s/it] 41%|████      | 158/390 [01:18<14:35,  3.77s/it] 41%|████      | 159/390 [01:19<10:30,  2.73s/it] 41%|████      | 160/390 [01:19<07:38,  2.00s/it] 41%|████▏     | 161/390 [01:19<05:39,  1.48s/it] 42%|████▏     | 162/390 [01:20<04:16,  1.12s/it] 42%|████▏     | 163/390 [01:20<03:18,  1.15it/s] 42%|████▏     | 164/390 [01:20<02:37,  1.44it/s] 42%|████▏     | 165/390 [01:20<02:09,  1.74it/s] 43%|████▎     | 166/390 [01:21<01:49,  2.05it/s] 43%|████▎     | 167/390 [01:21<01:35,  2.34it/s] 43%|████▎     | 168/390 [01:21<01:25,  2.60it/s] 43%|████▎     | 169/390 [01:22<01:18,  2.81it/s] 44%|████▎     | 170/390 [01:22<01:13,  2.98it/s] 44%|████▍     | 171/390 [01:22<01:10,  3.12it/s] 44%|████▍     | 172/390 [01:22<01:07,  3.22it/s] 44%|████▍     | 173/390 [01:23<01:05,  3.30it/s] 45%|████▍     | 174/390 [01:23<01:04,  3.35it/s] 45%|████▍     | 175/390 [01:23<01:03,  3.40it/s] 45%|████▌     | 176/390 [01:24<01:02,  3.42it/s] 45%|████▌     | 177/390 [01:24<01:01,  3.44it/s] 46%|████▌     | 178/390 [01:24<01:01,  3.46it/s] 46%|████▌     | 179/390 [01:24<01:00,  3.47it/s] 46%|████▌     | 180/390 [01:25<01:00,  3.47it/s] 46%|████▋     | 181/390 [01:25<01:00,  3.47it/s] 47%|████▋     | 182/390 [01:25<00:59,  3.48it/s] 47%|████▋     | 183/390 [01:26<00:59,  3.48it/s] 47%|████▋     | 184/390 [01:26<00:59,  3.48it/s] 47%|████▋     | 185/390 [01:26<00:58,  3.49it/s] 48%|████▊     | 186/390 [01:26<00:58,  3.49it/s] 48%|████▊     | 187/390 [01:27<00:58,  3.48it/s] 48%|████▊     | 188/390 [01:27<00:57,  3.49it/s] 48%|████▊     | 189/390 [01:27<00:57,  3.49it/s] 49%|████▊     | 190/390 [01:28<00:57,  3.48it/s] 49%|████▉     | 191/390 [01:28<00:57,  3.47it/s] 49%|████▉     | 192/390 [01:28<00:56,  3.48it/s] 49%|████▉     | 193/390 [01:28<00:56,  3.48it/s] 50%|████▉     | 194/390 [01:29<00:56,  3.48it/s] 50%|█████     | 195/390 [01:29<00:56,  3.48it/s] 50%|█████     | 196/390 [01:29<00:55,  3.48it/s] 51%|█████     | 197/390 [01:30<00:55,  3.48it/s] 51%|█████     | 198/390 [01:30<00:55,  3.48it/s] 51%|█████     | 199/390 [01:30<00:54,  3.48it/s] 51%|█████▏    | 200/390 [01:30<00:54,  3.48it/s] 52%|█████▏    | 201/390 [01:31<00:54,  3.49it/s] 52%|█████▏    | 202/390 [01:31<00:54,  3.47it/s] 52%|█████▏    | 203/390 [01:31<00:53,  3.48it/s] 52%|█████▏    | 204/390 [01:32<00:53,  3.48it/s] 53%|█████▎    | 205/390 [01:32<00:53,  3.48it/s] 53%|█████▎    | 206/390 [01:32<00:52,  3.48it/s] 53%|█████▎    | 207/390 [01:33<00:52,  3.49it/s] 53%|█████▎    | 208/390 [01:33<00:52,  3.48it/s] 54%|█████▎    | 209/390 [01:33<00:51,  3.48it/s] 54%|█████▍    | 210/390 [01:33<00:51,  3.48it/s] 54%|█████▍    | 211/390 [01:34<00:51,  3.48it/s] 54%|█████▍    | 212/390 [01:34<00:51,  3.48it/s] 55%|█████▍    | 213/390 [01:34<00:50,  3.48it/s] 55%|█████▍    | 214/390 [01:35<00:50,  3.48it/s] 55%|█████▌    | 215/390 [01:35<00:50,  3.48it/s] 55%|█████▌    | 216/390 [01:35<00:49,  3.48it/s] 56%|█████▌    | 217/390 [01:35<00:49,  3.48it/s] 56%|█████▌    | 218/390 [01:36<00:49,  3.48it/s] 56%|█████▌    | 219/390 [01:36<00:49,  3.48it/s] 56%|█████▋    | 220/390 [01:36<00:48,  3.48it/s] 57%|█████▋    | 221/390 [01:37<00:48,  3.48it/s] 57%|█████▋    | 222/390 [01:37<00:48,  3.48it/s] 57%|█████▋    | 223/390 [01:37<00:47,  3.48it/s] 57%|█████▋    | 224/390 [01:37<00:48,  3.45it/s] 58%|█████▊    | 225/390 [01:38<00:47,  3.46it/s] 58%|█████▊    | 226/390 [01:38<00:47,  3.47it/s] 58%|█████▊    | 227/390 [01:38<00:46,  3.47it/s] 58%|█████▊    | 228/390 [01:39<00:46,  3.48it/s] 59%|█████▊    | 229/390 [01:39<00:46,  3.48it/s] 59%|█████▉    | 230/390 [01:39<00:45,  3.48it/s] 59%|█████▉    | 231/390 [01:39<00:45,  3.48it/s] 59%|█████▉    | 232/390 [01:40<00:45,  3.48it/s] 60%|█████▉    | 233/390 [01:40<00:45,  3.48it/s] 60%|██████    | 234/390 [01:40<00:44,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 11:16:16,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:16:16,519 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 11:16:16,519 >>   Batch size = 8
{'eval_loss': 1.0328925848007202, 'eval_runtime': 9.2761, 'eval_samples_per_second': 376.884, 'eval_steps_per_second': 47.11, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.77it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.01it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.24it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.55it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.03it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.72it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.53it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.26it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.17it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.30it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.32it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.15it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.25it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.18it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.17it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.11it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.03it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.94it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.05it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.13it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.22it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.16it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.25it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.08it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.99it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.10it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.04it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.99it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.06it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.18it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.15it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.18it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.01it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.01it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.96it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.94it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.99it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.07it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.16it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.13it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.13it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.99it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.99it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.91it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.00it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.09it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.01it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.12it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.10it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.07it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.09it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.03it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.94it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.06it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.10it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.08it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.15it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.16it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.95it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.98it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.00it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.00it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.03it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.04it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.06it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.01it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.16it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.07it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.96it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.11it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.04it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.03it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.06it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.06it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.05it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.07it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.08it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.01it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.03it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.01it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.99it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.10it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.08it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.99it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.03it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.03it/s][A 60%|██████    | 234/390 [01:50<00:44,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:16:25,823 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 11:16:25,847 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:16:28,258 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:16:28,276 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:16:28,285 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:57<13:40,  5.30s/it] 61%|██████    | 236/390 [01:58<09:44,  3.79s/it] 61%|██████    | 237/390 [01:58<06:59,  2.74s/it] 61%|██████    | 238/390 [01:58<05:04,  2.01s/it] 61%|██████▏   | 239/390 [01:58<03:44,  1.49s/it] 62%|██████▏   | 240/390 [01:59<02:49,  1.13s/it] 62%|██████▏   | 241/390 [01:59<02:10,  1.14it/s] 62%|██████▏   | 242/390 [01:59<01:43,  1.43it/s] 62%|██████▏   | 243/390 [02:00<01:24,  1.74it/s] 63%|██████▎   | 244/390 [02:00<01:11,  2.05it/s] 63%|██████▎   | 245/390 [02:00<01:02,  2.33it/s] 63%|██████▎   | 246/390 [02:00<00:55,  2.59it/s] 63%|██████▎   | 247/390 [02:01<00:51,  2.80it/s] 64%|██████▎   | 248/390 [02:01<00:47,  2.97it/s] 64%|██████▍   | 249/390 [02:01<00:45,  3.11it/s] 64%|██████▍   | 250/390 [02:02<00:43,  3.21it/s] 64%|██████▍   | 251/390 [02:02<00:42,  3.29it/s] 65%|██████▍   | 252/390 [02:02<00:41,  3.35it/s] 65%|██████▍   | 253/390 [02:02<00:40,  3.39it/s] 65%|██████▌   | 254/390 [02:03<00:39,  3.42it/s] 65%|██████▌   | 255/390 [02:03<00:39,  3.44it/s] 66%|██████▌   | 256/390 [02:03<00:38,  3.45it/s] 66%|██████▌   | 257/390 [02:04<00:38,  3.46it/s] 66%|██████▌   | 258/390 [02:04<00:38,  3.46it/s] 66%|██████▋   | 259/390 [02:04<00:37,  3.47it/s] 67%|██████▋   | 260/390 [02:04<00:37,  3.47it/s] 67%|██████▋   | 261/390 [02:05<00:37,  3.48it/s] 67%|██████▋   | 262/390 [02:05<00:36,  3.48it/s] 67%|██████▋   | 263/390 [02:05<00:36,  3.48it/s] 68%|██████▊   | 264/390 [02:06<00:36,  3.48it/s] 68%|██████▊   | 265/390 [02:06<00:35,  3.48it/s] 68%|██████▊   | 266/390 [02:06<00:35,  3.48it/s] 68%|██████▊   | 267/390 [02:06<00:35,  3.48it/s] 69%|██████▊   | 268/390 [02:07<00:35,  3.49it/s] 69%|██████▉   | 269/390 [02:07<00:34,  3.48it/s] 69%|██████▉   | 270/390 [02:07<00:34,  3.48it/s] 69%|██████▉   | 271/390 [02:08<00:34,  3.48it/s] 70%|██████▉   | 272/390 [02:08<00:33,  3.48it/s] 70%|███████   | 273/390 [02:08<00:33,  3.49it/s] 70%|███████   | 274/390 [02:08<00:33,  3.48it/s] 71%|███████   | 275/390 [02:09<00:33,  3.48it/s] 71%|███████   | 276/390 [02:09<00:32,  3.48it/s] 71%|███████   | 277/390 [02:09<00:32,  3.48it/s] 71%|███████▏  | 278/390 [02:10<00:32,  3.48it/s] 72%|███████▏  | 279/390 [02:10<00:31,  3.48it/s] 72%|███████▏  | 280/390 [02:10<00:31,  3.48it/s] 72%|███████▏  | 281/390 [02:10<00:31,  3.48it/s] 72%|███████▏  | 282/390 [02:11<00:31,  3.48it/s] 73%|███████▎  | 283/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 284/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 285/390 [02:12<00:30,  3.48it/s] 73%|███████▎  | 286/390 [02:12<00:29,  3.48it/s] 74%|███████▎  | 287/390 [02:12<00:29,  3.48it/s] 74%|███████▍  | 288/390 [02:12<00:29,  3.48it/s] 74%|███████▍  | 289/390 [02:13<00:28,  3.48it/s] 74%|███████▍  | 290/390 [02:13<00:28,  3.48it/s] 75%|███████▍  | 291/390 [02:13<00:28,  3.47it/s] 75%|███████▍  | 292/390 [02:14<00:28,  3.48it/s] 75%|███████▌  | 293/390 [02:14<00:27,  3.48it/s] 75%|███████▌  | 294/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 295/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 296/390 [02:15<00:26,  3.48it/s] 76%|███████▌  | 297/390 [02:15<00:26,  3.48it/s] 76%|███████▋  | 298/390 [02:15<00:26,  3.48it/s] 77%|███████▋  | 299/390 [02:16<00:26,  3.48it/s] 77%|███████▋  | 300/390 [02:16<00:25,  3.48it/s] 77%|███████▋  | 301/390 [02:16<00:25,  3.49it/s] 77%|███████▋  | 302/390 [02:17<00:25,  3.46it/s] 78%|███████▊  | 303/390 [02:17<00:25,  3.46it/s] 78%|███████▊  | 304/390 [02:17<00:24,  3.47it/s] 78%|███████▊  | 305/390 [02:17<00:24,  3.47it/s] 78%|███████▊  | 306/390 [02:18<00:24,  3.47it/s] 79%|███████▊  | 307/390 [02:18<00:23,  3.48it/s] 79%|███████▉  | 308/390 [02:18<00:23,  3.48it/s] 79%|███████▉  | 309/390 [02:19<00:23,  3.48it/s] 79%|███████▉  | 310/390 [02:19<00:23,  3.48it/s] 80%|███████▉  | 311/390 [02:19<00:22,  3.48it/s] 80%|████████  | 312/390 [02:19<00:22,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 11:16:55,626 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:16:55,626 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 11:16:55,626 >>   Batch size = 8
{'eval_loss': 1.0472326278686523, 'eval_runtime': 9.2871, 'eval_samples_per_second': 376.437, 'eval_steps_per_second': 47.055, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.90it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.93it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.06it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.45it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.09it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.68it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.52it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.15it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.03it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.13it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.14it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.10it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.02it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.16it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.10it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.17it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.20it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.98it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.00it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.12it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.17it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.10it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.13it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.12it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.10it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.10it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.03it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.00it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.92it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.07it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.14it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.96it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.09it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.04it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.98it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.88it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.85it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.75it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.93it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.01it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.94it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.01it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.97it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.82it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.86it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.82it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.78it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.85it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.94it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.97it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.06it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.96it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.84it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.84it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.79it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.76it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.90it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.88it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.90it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.94it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.96it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.92it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.96it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.88it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.65it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.85it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.91it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.89it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.90it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.01it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.97it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.94it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.94it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.81it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.89it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.94it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.82it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.99it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.00it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.96it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.03it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.87it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.81it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.92it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.96it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.91it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.91it/s][A 80%|████████  | 312/390 [02:29<00:22,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:17:04,957 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 11:17:04,972 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:17:07,426 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:17:07,444 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:17:07,455 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:37<06:54,  5.38s/it] 81%|████████  | 314/390 [02:37<04:52,  3.85s/it] 81%|████████  | 315/390 [02:37<03:28,  2.78s/it] 81%|████████  | 316/390 [02:37<02:30,  2.03s/it] 81%|████████▏ | 317/390 [02:38<01:50,  1.51s/it] 82%|████████▏ | 318/390 [02:38<01:22,  1.14s/it] 82%|████████▏ | 319/390 [02:38<01:02,  1.13it/s] 82%|████████▏ | 320/390 [02:39<00:49,  1.41it/s] 82%|████████▏ | 321/390 [02:39<00:40,  1.72it/s] 83%|████████▎ | 322/390 [02:39<00:33,  2.03it/s] 83%|████████▎ | 323/390 [02:40<00:28,  2.32it/s] 83%|████████▎ | 324/390 [02:40<00:25,  2.58it/s] 83%|████████▎ | 325/390 [02:40<00:23,  2.79it/s] 84%|████████▎ | 326/390 [02:40<00:21,  2.97it/s] 84%|████████▍ | 327/390 [02:41<00:20,  3.10it/s] 84%|████████▍ | 328/390 [02:41<00:19,  3.21it/s] 84%|████████▍ | 329/390 [02:41<00:18,  3.29it/s] 85%|████████▍ | 330/390 [02:42<00:17,  3.34it/s] 85%|████████▍ | 331/390 [02:42<00:17,  3.38it/s] 85%|████████▌ | 332/390 [02:42<00:16,  3.41it/s] 85%|████████▌ | 333/390 [02:42<00:16,  3.43it/s] 86%|████████▌ | 334/390 [02:43<00:16,  3.45it/s] 86%|████████▌ | 335/390 [02:43<00:15,  3.46it/s] 86%|████████▌ | 336/390 [02:43<00:15,  3.46it/s] 86%|████████▋ | 337/390 [02:44<00:15,  3.47it/s] 87%|████████▋ | 338/390 [02:44<00:14,  3.47it/s] 87%|████████▋ | 339/390 [02:44<00:14,  3.47it/s] 87%|████████▋ | 340/390 [02:44<00:14,  3.48it/s] 87%|████████▋ | 341/390 [02:45<00:14,  3.48it/s] 88%|████████▊ | 342/390 [02:45<00:13,  3.48it/s] 88%|████████▊ | 343/390 [02:45<00:13,  3.48it/s] 88%|████████▊ | 344/390 [02:46<00:13,  3.48it/s] 88%|████████▊ | 345/390 [02:46<00:12,  3.48it/s] 89%|████████▊ | 346/390 [02:46<00:12,  3.48it/s] 89%|████████▉ | 347/390 [02:46<00:12,  3.47it/s] 89%|████████▉ | 348/390 [02:47<00:12,  3.48it/s] 89%|████████▉ | 349/390 [02:47<00:11,  3.48it/s] 90%|████████▉ | 350/390 [02:47<00:11,  3.48it/s] 90%|█████████ | 351/390 [02:48<00:11,  3.48it/s] 90%|█████████ | 352/390 [02:48<00:10,  3.48it/s] 91%|█████████ | 353/390 [02:48<00:10,  3.48it/s] 91%|█████████ | 354/390 [02:48<00:10,  3.48it/s] 91%|█████████ | 355/390 [02:49<00:10,  3.48it/s] 91%|█████████▏| 356/390 [02:49<00:09,  3.48it/s] 92%|█████████▏| 357/390 [02:49<00:09,  3.48it/s] 92%|█████████▏| 358/390 [02:50<00:09,  3.47it/s] 92%|█████████▏| 359/390 [02:50<00:08,  3.47it/s] 92%|█████████▏| 360/390 [02:50<00:08,  3.48it/s] 93%|█████████▎| 361/390 [02:50<00:08,  3.48it/s] 93%|█████████▎| 362/390 [02:51<00:08,  3.48it/s] 93%|█████████▎| 363/390 [02:51<00:07,  3.48it/s] 93%|█████████▎| 364/390 [02:51<00:07,  3.48it/s] 94%|█████████▎| 365/390 [02:52<00:07,  3.48it/s] 94%|█████████▍| 366/390 [02:52<00:06,  3.48it/s] 94%|█████████▍| 367/390 [02:52<00:06,  3.46it/s] 94%|█████████▍| 368/390 [02:52<00:06,  3.38it/s] 95%|█████████▍| 369/390 [02:53<00:06,  3.40it/s] 95%|█████████▍| 370/390 [02:53<00:05,  3.42it/s] 95%|█████████▌| 371/390 [02:53<00:05,  3.44it/s] 95%|█████████▌| 372/390 [02:54<00:05,  3.45it/s] 96%|█████████▌| 373/390 [02:54<00:04,  3.45it/s] 96%|█████████▌| 374/390 [02:54<00:04,  3.46it/s] 96%|█████████▌| 375/390 [02:54<00:04,  3.46it/s] 96%|█████████▋| 376/390 [02:55<00:04,  3.47it/s] 97%|█████████▋| 377/390 [02:55<00:03,  3.47it/s] 97%|█████████▋| 378/390 [02:55<00:03,  3.48it/s] 97%|█████████▋| 379/390 [02:56<00:03,  3.48it/s] 97%|█████████▋| 380/390 [02:56<00:02,  3.48it/s] 98%|█████████▊| 381/390 [02:56<00:02,  3.48it/s] 98%|█████████▊| 382/390 [02:57<00:02,  3.48it/s] 98%|█████████▊| 383/390 [02:57<00:02,  3.48it/s] 98%|█████████▊| 384/390 [02:57<00:01,  3.48it/s] 99%|█████████▊| 385/390 [02:57<00:01,  3.48it/s] 99%|█████████▉| 386/390 [02:58<00:01,  3.48it/s] 99%|█████████▉| 387/390 [02:58<00:00,  3.47it/s] 99%|█████████▉| 388/390 [02:58<00:00,  3.47it/s]100%|█████████▉| 389/390 [02:59<00:00,  3.48it/s]100%|██████████| 390/390 [02:59<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 11:17:35,012 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:17:35,012 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 11:17:35,012 >>   Batch size = 8
{'eval_loss': 1.0493180751800537, 'eval_runtime': 9.307, 'eval_samples_per_second': 375.633, 'eval_steps_per_second': 46.954, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.49it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.94it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.15it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.28it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.00it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.73it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.40it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.28it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.10it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.17it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.11it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.18it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.11it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.17it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.12it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.18it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.19it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.89it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.99it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.00it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.10it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.08it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.11it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.13it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.14it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.99it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.99it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.95it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.90it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.06it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.05it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.03it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.15it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.05it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.06it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.04it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.04it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.99it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.98it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.97it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.94it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.06it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.10it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.99it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.98it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.04it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.03it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.99it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.99it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.91it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.98it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.01it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.03it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.92it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.01it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.98it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.99it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.07it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.90it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.89it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.01it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.02it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.94it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.02it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.91it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.91it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.03it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.03it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.94it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.99it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.96it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.98it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.03it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.01it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.93it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.98it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.09it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.07it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.99it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.95it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.93it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.82it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.96it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.96it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.94it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.06it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.06it/s][A100%|██████████| 390/390 [03:08<00:00,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:17:44,324 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 11:17:44,341 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:17:46,660 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:17:46,675 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:17:46,685 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 11:17:51,685 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 11:17:51,688 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78 (score: 1.0313676595687866).
                                                 100%|██████████| 390/390 [03:17<00:00,  3.47it/s]100%|██████████| 390/390 [03:17<00:00,  1.97it/s]
[INFO|trainer.py:1894] 2023-08-28 11:17:53,322 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 11:17:53,338 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:17:55,720 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:17:55,739 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:17:55,748 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:17:55,938 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:17:55,938 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:17:55,938 >>   train_loss               =     0.7444
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:17:55,939 >>   train_runtime            = 0:03:17.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:17:55,939 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:17:55,939 >>   train_samples_per_second =    126.509
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:17:55,939 >>   train_steps_per_second   =      1.974
{'eval_loss': 1.0545499324798584, 'eval_runtime': 9.2968, 'eval_samples_per_second': 376.043, 'eval_steps_per_second': 47.005, 'epoch': 4.99}
{'train_runtime': 197.6142, 'train_samples_per_second': 126.509, 'train_steps_per_second': 1.974, 'train_loss': 0.7444259252303685, 'epoch': 4.99}
08/28/2023 11:17:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 11:17:55,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:17:55,979 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 11:17:55,979 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.33it/s]  3%|▎         | 12/437 [00:00<00:08, 51.32it/s]  4%|▍         | 18/437 [00:00<00:08, 49.36it/s]  5%|▌         | 23/437 [00:00<00:08, 48.62it/s]  6%|▋         | 28/437 [00:00<00:08, 48.15it/s]  8%|▊         | 33/437 [00:00<00:08, 47.93it/s]  9%|▊         | 38/437 [00:00<00:08, 47.80it/s] 10%|▉         | 43/437 [00:00<00:08, 47.64it/s] 11%|█         | 48/437 [00:00<00:08, 47.59it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.45it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.27it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.36it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.40it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.34it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.27it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.24it/s] 20%|██        | 88/437 [00:01<00:07, 47.32it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.35it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.27it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.29it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.14it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.20it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.18it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.30it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.27it/s] 30%|███       | 133/437 [00:02<00:06, 47.28it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.31it/s] 33%|███▎      | 143/437 [00:02<00:06, 47.36it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.33it/s] 35%|███▌      | 153/437 [00:03<00:05, 47.34it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.18it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.23it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.27it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.27it/s] 41%|████      | 178/437 [00:03<00:05, 47.26it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.22it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.28it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.28it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.27it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.27it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.17it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.27it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.29it/s] 51%|█████     | 223/437 [00:04<00:04, 47.20it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.27it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.16it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.16it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.09it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.11it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.12it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.19it/s] 60%|██████    | 263/437 [00:05<00:03, 47.21it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.30it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.26it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.17it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.12it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.17it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.10it/s] 68%|██████▊   | 298/437 [00:06<00:02, 47.17it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.17it/s] 70%|███████   | 308/437 [00:06<00:02, 47.23it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.24it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.29it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.20it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.10it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.07it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.14it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.11it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.18it/s] 81%|████████  | 353/437 [00:07<00:01, 47.21it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.23it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.22it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.20it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.11it/s] 86%|████████▋ | 378/437 [00:07<00:01, 46.78it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.13it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.12it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.19it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.19it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.20it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.12it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.09it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.09it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.08it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.07it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.11it/s]100%|██████████| 437/437 [00:09<00:00, 47.28it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:18:05,243 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   eval_loss               =     1.0314
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   eval_runtime            = 0:00:09.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   eval_samples_per_second =     377.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   eval_steps_per_second   =     47.174
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:18:05,243 >>   perplexity              =     2.8049
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:11,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:11,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:11,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:11,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:11,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:18:12,448 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:18:12,449 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:18:13,013 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:18:14,049 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:18:14,049 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:16,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:16,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:16,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:16,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:18:16,885 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:18:17,526 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:18:17,527 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:18:18,096 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:18:18,251 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:18:18,251 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/checkpoint-390
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.53it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.55it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.49it/s]Extractor Predicting: 39it [00:26,  1.53it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:28,  1.49it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.52it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.59it/s]Extractor Predicting: 53it [00:35,  1.55it/s]Extractor Predicting: 54it [00:35,  1.55it/s]Extractor Predicting: 55it [00:36,  1.57it/s]Extractor Predicting: 56it [00:37,  1.55it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:45,  1.47it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:46,  1.52it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.45it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:51,  1.47it/s]Extractor Predicting: 79it [00:52,  1.46it/s]Extractor Predicting: 80it [00:53,  1.46it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:56,  1.45it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.47it/s]Extractor Predicting: 87it [00:58,  1.45it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.48it/s]Extractor Predicting: 90it [01:00,  1.48it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:01,  1.52it/s]Extractor Predicting: 93it [01:02,  1.54it/s]Extractor Predicting: 94it [01:02,  1.52it/s]Extractor Predicting: 95it [01:03,  1.52it/s]Extractor Predicting: 96it [01:04,  1.51it/s]Extractor Predicting: 97it [01:04,  1.49it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:06,  1.49it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:08,  1.46it/s]Extractor Predicting: 104it [01:09,  1.47it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:10,  1.52it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:12,  1.49it/s]Extractor Predicting: 109it [01:12,  1.50it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:14,  1.48it/s]Extractor Predicting: 112it [01:14,  1.51it/s]Extractor Predicting: 113it [01:15,  1.51it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.55it/s]Extractor Predicting: 117it [01:18,  1.53it/s]Extractor Predicting: 118it [01:18,  1.52it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:20,  1.50it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:21,  1.49it/s]Extractor Predicting: 123it [01:22,  1.48it/s]Extractor Predicting: 124it [01:22,  1.47it/s]Extractor Predicting: 125it [01:23,  1.44it/s]Extractor Predicting: 126it [01:24,  1.34it/s]Extractor Predicting: 127it [01:25,  1.39it/s]Extractor Predicting: 128it [01:25,  1.43it/s]Extractor Predicting: 129it [01:26,  1.45it/s]Extractor Predicting: 130it [01:27,  1.50it/s]Extractor Predicting: 131it [01:27,  1.52it/s]Extractor Predicting: 132it [01:28,  1.52it/s]Extractor Predicting: 133it [01:29,  1.51it/s]Extractor Predicting: 134it [01:29,  1.52it/s]Extractor Predicting: 135it [01:30,  1.52it/s]Extractor Predicting: 136it [01:31,  1.51it/s]Extractor Predicting: 137it [01:31,  1.50it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:33,  1.50it/s]Extractor Predicting: 140it [01:33,  1.48it/s]Extractor Predicting: 141it [01:34,  1.48it/s]Extractor Predicting: 142it [01:35,  1.46it/s]Extractor Predicting: 143it [01:35,  1.46it/s]Extractor Predicting: 144it [01:36,  1.63it/s]Extractor Predicting: 144it [01:36,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:01,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:01,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:01,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:01,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:01,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:20:02,490 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:20:02,491 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:20:03,074 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:20:04,080 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:20:04,080 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:06,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:06,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:06,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:06,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:20:06,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:20:07,622 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:20:07,623 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:20:09,255 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:20:09,400 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:20:09,400 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6129032258064516,
  "recall": 0.1358695652173913,
  "score": 0.22243034418169047,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.75it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.73it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:09,  1.69it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.73it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:14,  1.64it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:15,  1.69it/s]Extractor Predicting: 28it [00:16,  1.61it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:17,  1.52it/s]Extractor Predicting: 31it [00:18,  1.49it/s]Extractor Predicting: 32it [00:19,  1.44it/s]Extractor Predicting: 33it [00:20,  1.44it/s]Extractor Predicting: 34it [00:20,  1.41it/s]Extractor Predicting: 35it [00:21,  1.44it/s]Extractor Predicting: 36it [00:22,  1.44it/s]Extractor Predicting: 37it [00:22,  1.44it/s]Extractor Predicting: 38it [00:23,  1.40it/s]Extractor Predicting: 39it [00:24,  1.41it/s]Extractor Predicting: 40it [00:25,  1.39it/s]Extractor Predicting: 41it [00:25,  1.37it/s]Extractor Predicting: 42it [00:26,  1.38it/s]Extractor Predicting: 43it [00:27,  1.39it/s]Extractor Predicting: 44it [00:27,  1.42it/s]Extractor Predicting: 45it [00:28,  1.42it/s]Extractor Predicting: 46it [00:29,  1.44it/s]Extractor Predicting: 47it [00:30,  1.41it/s]Extractor Predicting: 48it [00:30,  1.40it/s]Extractor Predicting: 49it [00:31,  1.40it/s]Extractor Predicting: 50it [00:32,  1.41it/s]Extractor Predicting: 51it [00:32,  1.40it/s]Extractor Predicting: 52it [00:33,  1.41it/s]Extractor Predicting: 53it [00:34,  1.41it/s]Extractor Predicting: 54it [00:34,  1.42it/s]Extractor Predicting: 55it [00:35,  1.41it/s]Extractor Predicting: 56it [00:36,  1.39it/s]Extractor Predicting: 57it [00:37,  1.41it/s]Extractor Predicting: 58it [00:37,  1.32it/s]Extractor Predicting: 59it [00:38,  1.36it/s]Extractor Predicting: 60it [00:39,  1.40it/s]Extractor Predicting: 61it [00:40,  1.38it/s]Extractor Predicting: 62it [00:40,  1.39it/s]Extractor Predicting: 63it [00:41,  1.43it/s]Extractor Predicting: 64it [00:42,  1.45it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:43,  1.49it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:46,  1.52it/s]Extractor Predicting: 71it [00:46,  1.56it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:48,  1.47it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.46it/s]Extractor Predicting: 77it [00:50,  1.46it/s]Extractor Predicting: 78it [00:51,  1.46it/s]Extractor Predicting: 79it [00:52,  1.46it/s]Extractor Predicting: 80it [00:52,  1.45it/s]Extractor Predicting: 81it [00:53,  1.45it/s]Extractor Predicting: 82it [00:54,  1.47it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:55,  1.45it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:56,  1.48it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:58,  1.56it/s]Extractor Predicting: 89it [00:58,  1.58it/s]Extractor Predicting: 90it [00:59,  1.58it/s]Extractor Predicting: 91it [00:59,  1.62it/s]Extractor Predicting: 92it [01:00,  1.60it/s]Extractor Predicting: 93it [01:01,  1.64it/s]Extractor Predicting: 94it [01:01,  1.62it/s]Extractor Predicting: 95it [01:02,  1.61it/s]Extractor Predicting: 96it [01:03,  1.61it/s]Extractor Predicting: 97it [01:03,  1.65it/s]Extractor Predicting: 98it [01:04,  1.66it/s]Extractor Predicting: 99it [01:04,  1.66it/s]Extractor Predicting: 100it [01:05,  1.71it/s]Extractor Predicting: 101it [01:06,  1.70it/s]Extractor Predicting: 102it [01:06,  1.65it/s]Extractor Predicting: 103it [01:07,  1.63it/s]Extractor Predicting: 104it [01:07,  1.64it/s]Extractor Predicting: 105it [01:08,  1.66it/s]Extractor Predicting: 106it [01:09,  1.65it/s]Extractor Predicting: 107it [01:09,  1.72it/s]Extractor Predicting: 108it [01:10,  1.67it/s]Extractor Predicting: 109it [01:10,  1.68it/s]Extractor Predicting: 110it [01:11,  1.68it/s]Extractor Predicting: 111it [01:12,  1.68it/s]Extractor Predicting: 112it [01:12,  1.69it/s]Extractor Predicting: 113it [01:13,  1.68it/s]Extractor Predicting: 114it [01:13,  1.67it/s]Extractor Predicting: 115it [01:14,  1.63it/s]Extractor Predicting: 116it [01:15,  1.59it/s]Extractor Predicting: 117it [01:15,  1.56it/s]Extractor Predicting: 118it [01:16,  1.56it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:17,  1.53it/s]Extractor Predicting: 121it [01:18,  1.55it/s]Extractor Predicting: 122it [01:19,  1.55it/s]Extractor Predicting: 123it [01:19,  1.52it/s]Extractor Predicting: 124it [01:20,  1.56it/s]Extractor Predicting: 125it [01:21,  1.52it/s]Extractor Predicting: 126it [01:21,  1.54it/s]Extractor Predicting: 127it [01:22,  1.52it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.49it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:25,  1.48it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 134it [01:27,  1.50it/s]Extractor Predicting: 135it [01:27,  1.48it/s]Extractor Predicting: 136it [01:28,  1.45it/s]Extractor Predicting: 137it [01:29,  1.37it/s]Extractor Predicting: 138it [01:29,  1.38it/s]Extractor Predicting: 139it [01:30,  1.42it/s]Extractor Predicting: 140it [01:31,  1.43it/s]Extractor Predicting: 141it [01:31,  1.46it/s]Extractor Predicting: 142it [01:32,  1.46it/s]Extractor Predicting: 143it [01:32,  1.82it/s]Extractor Predicting: 143it [01:32,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:48,774 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:48,778 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:48,778 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:48,778 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:48,778 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:21:49,406 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:21:49,407 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:21:49,985 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:21:51,021 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:21:51,021 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:53,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:53,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:53,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:53,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:21:53,913 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:21:54,618 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:21:54,619 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:21:55,202 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:21:55,350 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:21:55,350 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5259026687598116,
  "recall": 0.09824046920821114,
  "score": 0.16555473190017295,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 2it [00:01,  1.35it/s]
[INFO|configuration_utils.py:515] 2023-08-28 11:21:57,162 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:21:57,163 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:21:57,166 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:21:57,166 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 11:21:57,169 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:22:00,082 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 11:22:00,082 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 11:22:00,092 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:22:00,093 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:22:00,098 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:00,101 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:00,101 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:00,101 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:00,101 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:00,101 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:22:00,101 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.25,
  "recall": 0.011111111111111112,
  "score": 0.02127659574468085,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 11:22:00,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:01,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:01,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:02,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:03,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:04,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:05,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:05,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:06,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:07,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:08,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:09,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:09,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:10,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:11,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:12,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:13,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:13,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:14,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:15,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:16,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:16,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:35, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 11:22:17,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:18,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:19,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:19,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:20,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:21,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:22,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:23,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:23,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:24,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:25,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:26,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:27,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:27,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:28,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:29,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:30,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:30,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:31,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:32,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:33,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:14, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 11:22:34,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:34,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:35,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:36,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:37,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:37,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:38,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:39,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:40,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:41,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:42,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:43,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:44,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:45,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:45,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:46,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:47,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:48,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:48,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:49,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:50<01:56, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 11:22:50,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:51,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:51,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:52,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:53,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:53,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:54,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:55,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:55,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:56,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:57,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:57,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:58,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:59,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:22:59,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:00,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:01,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:01,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:02,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:03,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:03,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:04,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:05,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:06,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:06,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:07<01:41, 16.84s/it][WARNING|generation_utils.py:914] 2023-08-28 11:23:07,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:08,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:09,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:09,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:10,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:11,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:12,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:13,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:13,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:14,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:15,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:16,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:16,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:17,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:18,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:19,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:19,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:20,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:21,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:22,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:22,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:23,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:24,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:25, 17.07s/it][WARNING|generation_utils.py:914] 2023-08-28 11:23:25,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:25,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:26,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:27,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:28,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:29,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:29,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:31,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:32,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:32,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:33,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:34,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:35,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:36,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:37,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:38,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:39,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:40,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:41,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:42,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:43,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:43,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:44,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:45<01:12, 18.18s/it][WARNING|generation_utils.py:914] 2023-08-28 11:23:45,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:46,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:46,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:47,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:48,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:49,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:50,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:50,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:51,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:52,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:53,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:54,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:54,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:55,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:56,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:57,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:58,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:58,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:23:59,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:00,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:01,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:01,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:02,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:03,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:04<00:55, 18.52s/it][WARNING|generation_utils.py:914] 2023-08-28 11:24:04,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:05,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:05,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:06,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:07,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:07,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:08,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:09,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:09,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:10,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:11,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:11,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:12,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:13,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:13,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:14,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:15,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:15,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:16,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:16,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:17,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:18,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:18<00:34, 17.22s/it][WARNING|generation_utils.py:914] 2023-08-28 11:24:19,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:19,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:20,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:21,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:22,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:23,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:23,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:24,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:25,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:25,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:26,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:27,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:28,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:28,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:29,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:30,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:31,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:31,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:32,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:33,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:34,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:35,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:35,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:36<00:17, 17.31s/it][WARNING|generation_utils.py:914] 2023-08-28 11:24:36,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:37,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:38,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:38,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:39,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:40,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:40,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:41,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:42,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:43,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:43,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:44,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:45,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:45,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:46,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:47,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:48,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:49,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:50,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:50,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:51,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:52,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:52,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:24:53,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:54<00:00, 17.49s/it]Generating: 100%|██████████| 10/10 [02:54<00:00, 17.41s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:00,464 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:00,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:00,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:00,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:00,470 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:25:01,165 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:25:01,166 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:25:01,743 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:25:02,816 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:25:02,816 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:05,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:05,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:05,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:05,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:25:05,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:25:06,320 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:25:06,321 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:25:06,919 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:25:07,073 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:25:07,073 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : instrument . Context : Later in the year ( October 1887 ) , a song by Henry Ellsworth \'s " I Will Pray " was included on the album . Head Entity : Idyll , Tail Entity : Henry Ellsworth .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 626, 'raw': 768}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8151041666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 9474
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9574, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.55it/s]Extractor Estimating: 2it [00:01,  1.37it/s]Extractor Estimating: 3it [00:02,  1.31it/s]Extractor Estimating: 4it [00:03,  1.29it/s]Extractor Estimating: 5it [00:03,  1.41it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.52it/s]Extractor Estimating: 9it [00:06,  1.38it/s]Extractor Estimating: 10it [00:07,  1.37it/s]Extractor Estimating: 11it [00:07,  1.45it/s]Extractor Estimating: 12it [00:08,  1.43it/s]Extractor Estimating: 13it [00:09,  1.49it/s]Extractor Estimating: 14it [00:09,  1.53it/s]Extractor Estimating: 15it [00:10,  1.52it/s]Extractor Estimating: 16it [00:11,  1.43it/s]Extractor Estimating: 17it [00:11,  1.49it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:13,  1.47it/s]Extractor Estimating: 20it [00:13,  1.47it/s]Extractor Estimating: 21it [00:14,  1.51it/s]Extractor Estimating: 22it [00:15,  1.54it/s]Extractor Estimating: 23it [00:15,  1.56it/s]Extractor Estimating: 24it [00:16,  1.53it/s]Extractor Estimating: 25it [00:17,  1.52it/s]Extractor Estimating: 26it [00:17,  1.48it/s]Extractor Estimating: 27it [00:18,  1.48it/s]Extractor Estimating: 28it [00:19,  1.50it/s]Extractor Estimating: 29it [00:19,  1.51it/s]Extractor Estimating: 30it [00:20,  1.50it/s]Extractor Estimating: 31it [00:21,  1.50it/s]Extractor Estimating: 32it [00:21,  1.45it/s]Extractor Estimating: 33it [00:22,  1.43it/s]Extractor Estimating: 34it [00:23,  1.46it/s]Extractor Estimating: 35it [00:23,  1.42it/s]Extractor Estimating: 36it [00:24,  1.48it/s]Extractor Estimating: 37it [00:25,  1.41it/s]Extractor Estimating: 38it [00:26,  1.42it/s]Extractor Estimating: 39it [00:26,  1.46it/s]Extractor Estimating: 40it [00:27,  1.48it/s]Extractor Estimating: 41it [00:27,  1.53it/s]Extractor Estimating: 42it [00:28,  1.54it/s]Extractor Estimating: 43it [00:29,  1.54it/s]Extractor Estimating: 44it [00:29,  1.56it/s]Extractor Estimating: 45it [00:30,  1.49it/s]Extractor Estimating: 46it [00:31,  1.53it/s]Extractor Estimating: 47it [00:31,  1.54it/s]Extractor Estimating: 48it [00:32,  1.56it/s]Extractor Estimating: 49it [00:33,  1.49it/s]Extractor Estimating: 50it [00:33,  1.51it/s]Extractor Estimating: 51it [00:34,  1.55it/s]Extractor Estimating: 52it [00:35,  1.59it/s]Extractor Estimating: 53it [00:35,  1.55it/s]Extractor Estimating: 54it [00:36,  1.56it/s]Extractor Estimating: 55it [00:36,  1.62it/s]Extractor Estimating: 56it [00:37,  1.67it/s]Extractor Estimating: 57it [00:38,  1.67it/s]Extractor Estimating: 58it [00:38,  1.61it/s]Extractor Estimating: 59it [00:39,  1.60it/s]Extractor Estimating: 60it [00:39,  1.60it/s]Extractor Estimating: 61it [00:40,  1.54it/s]Extractor Estimating: 62it [00:41,  1.55it/s]Extractor Estimating: 63it [00:41,  1.58it/s]Extractor Estimating: 64it [00:42,  1.55it/s]Extractor Estimating: 65it [00:43,  1.56it/s]Extractor Estimating: 66it [00:43,  1.55it/s]Extractor Estimating: 67it [00:44,  1.58it/s]Extractor Estimating: 68it [00:45,  1.61it/s]Extractor Estimating: 69it [00:45,  1.65it/s]Extractor Estimating: 70it [00:46,  1.62it/s]Extractor Estimating: 71it [00:46,  1.63it/s]Extractor Estimating: 72it [00:47,  1.60it/s]Extractor Estimating: 73it [00:48,  1.64it/s]Extractor Estimating: 74it [00:48,  1.56it/s]Extractor Estimating: 75it [00:49,  1.59it/s]Extractor Estimating: 76it [00:50,  1.58it/s]Extractor Estimating: 77it [00:50,  1.61it/s]Extractor Estimating: 78it [00:51,  1.55it/s]Extractor Estimating: 79it [00:51,  1.62it/s]Extractor Estimating: 80it [00:52,  1.64it/s]Extractor Estimating: 81it [00:53,  1.60it/s]Extractor Estimating: 82it [00:53,  1.62it/s]Extractor Estimating: 83it [00:54,  1.65it/s]Extractor Estimating: 84it [00:55,  1.62it/s]Extractor Estimating: 85it [00:55,  1.60it/s]Extractor Estimating: 86it [00:56,  1.58it/s]Extractor Estimating: 87it [00:57,  1.46it/s]Extractor Estimating: 88it [00:57,  1.49it/s]Extractor Estimating: 89it [00:58,  1.58it/s]Extractor Estimating: 90it [00:58,  1.55it/s]Extractor Estimating: 91it [00:59,  1.57it/s]Extractor Estimating: 92it [01:00,  1.58it/s]Extractor Estimating: 93it [01:00,  1.61it/s]Extractor Estimating: 94it [01:01,  1.59it/s]Extractor Estimating: 95it [01:02,  1.60it/s]Extractor Estimating: 96it [01:02,  1.60it/s]Extractor Estimating: 97it [01:03,  1.51it/s]Extractor Estimating: 98it [01:04,  1.51it/s]Extractor Estimating: 99it [01:04,  1.49it/s]Extractor Estimating: 100it [01:05,  1.51it/s]Extractor Estimating: 101it [01:06,  1.53it/s]Extractor Estimating: 102it [01:06,  1.58it/s]Extractor Estimating: 103it [01:07,  1.57it/s]Extractor Estimating: 104it [01:07,  1.54it/s]Extractor Estimating: 105it [01:08,  1.50it/s]Extractor Estimating: 106it [01:09,  1.50it/s]Extractor Estimating: 107it [01:09,  1.54it/s]Extractor Estimating: 108it [01:10,  1.55it/s]Extractor Estimating: 109it [01:11,  1.57it/s]Extractor Estimating: 110it [01:11,  1.54it/s]Extractor Estimating: 111it [01:12,  1.53it/s]Extractor Estimating: 112it [01:13,  1.57it/s]Extractor Estimating: 113it [01:13,  1.59it/s]Extractor Estimating: 114it [01:14,  1.50it/s]Extractor Estimating: 115it [01:15,  1.50it/s]Extractor Estimating: 116it [01:15,  1.50it/s]Extractor Estimating: 117it [01:16,  1.53it/s]Extractor Estimating: 118it [01:17,  1.55it/s]Extractor Estimating: 119it [01:17,  1.57it/s]Extractor Estimating: 120it [01:18,  1.54it/s]Extractor Estimating: 121it [01:19,  1.56it/s]Extractor Estimating: 122it [01:19,  1.59it/s]Extractor Estimating: 123it [01:20,  1.60it/s]Extractor Estimating: 124it [01:20,  1.63it/s]Extractor Estimating: 125it [01:21,  1.64it/s]Extractor Estimating: 126it [01:22,  1.62it/s]Extractor Estimating: 127it [01:22,  1.69it/s]Extractor Estimating: 128it [01:23,  1.67it/s]Extractor Estimating: 129it [01:23,  1.71it/s]Extractor Estimating: 130it [01:24,  1.68it/s]Extractor Estimating: 131it [01:25,  1.66it/s]Extractor Estimating: 132it [01:25,  1.57it/s]Extractor Estimating: 133it [01:26,  1.50it/s]Extractor Estimating: 134it [01:27,  1.51it/s]Extractor Estimating: 135it [01:27,  1.57it/s]Extractor Estimating: 136it [01:28,  1.53it/s]Extractor Estimating: 137it [01:28,  1.57it/s]Extractor Estimating: 138it [01:29,  1.64it/s]Extractor Estimating: 139it [01:30,  1.66it/s]Extractor Estimating: 140it [01:30,  1.53it/s]Extractor Estimating: 141it [01:31,  1.53it/s]Extractor Estimating: 142it [01:32,  1.54it/s]Extractor Estimating: 143it [01:32,  1.59it/s]Extractor Estimating: 144it [01:33,  1.59it/s]Extractor Estimating: 145it [01:34,  1.58it/s]Extractor Estimating: 146it [01:34,  1.55it/s]Extractor Estimating: 147it [01:35,  1.53it/s]Extractor Estimating: 148it [01:36,  1.55it/s]Extractor Estimating: 149it [01:36,  1.51it/s]Extractor Estimating: 150it [01:37,  1.58it/s]Extractor Estimating: 151it [01:37,  1.58it/s]Extractor Estimating: 152it [01:38,  1.59it/s]Extractor Estimating: 153it [01:39,  1.58it/s]Extractor Estimating: 154it [01:39,  1.54it/s]Extractor Estimating: 155it [01:40,  1.54it/s]Extractor Estimating: 156it [01:41,  1.52it/s]Extractor Estimating: 157it [01:41,  1.52it/s]Extractor Estimating: 158it [01:42,  1.50it/s]Extractor Estimating: 159it [01:43,  1.49it/s]Extractor Estimating: 160it [01:43,  1.49it/s]Extractor Estimating: 161it [01:44,  1.51it/s]Extractor Estimating: 162it [01:45,  1.50it/s]Extractor Estimating: 163it [01:45,  1.49it/s]Extractor Estimating: 164it [01:46,  1.54it/s]Extractor Estimating: 165it [01:47,  1.55it/s]Extractor Estimating: 166it [01:47,  1.50it/s]Extractor Estimating: 167it [01:48,  1.50it/s]Extractor Estimating: 168it [01:49,  1.51it/s]Extractor Estimating: 169it [01:49,  1.53it/s]Extractor Estimating: 170it [01:50,  1.57it/s]Extractor Estimating: 171it [01:51,  1.58it/s]Extractor Estimating: 172it [01:51,  1.56it/s]Extractor Estimating: 173it [01:52,  1.40it/s]Extractor Estimating: 174it [01:53,  1.40it/s]Extractor Estimating: 175it [01:53,  1.47it/s]Extractor Estimating: 176it [01:54,  1.52it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:55,  1.53it/s]Extractor Estimating: 179it [01:56,  1.59it/s]Extractor Estimating: 180it [01:56,  1.60it/s]Extractor Estimating: 181it [01:57,  1.63it/s]Extractor Estimating: 182it [01:58,  1.62it/s]Extractor Estimating: 183it [01:58,  1.66it/s]Extractor Estimating: 184it [01:59,  1.67it/s]Extractor Estimating: 185it [01:59,  1.69it/s]Extractor Estimating: 186it [02:00,  1.68it/s]Extractor Estimating: 187it [02:01,  1.70it/s]Extractor Estimating: 188it [02:01,  1.67it/s]Extractor Estimating: 189it [02:02,  1.69it/s]Extractor Estimating: 190it [02:02,  1.64it/s]Extractor Estimating: 191it [02:03,  1.62it/s]Extractor Estimating: 192it [02:04,  1.61it/s]Extractor Estimating: 193it [02:04,  1.55it/s]Extractor Estimating: 194it [02:05,  1.56it/s]Extractor Estimating: 195it [02:06,  1.63it/s]Extractor Estimating: 196it [02:06,  1.63it/s]Extractor Estimating: 197it [02:07,  1.68it/s]Extractor Estimating: 198it [02:07,  1.59it/s]Extractor Estimating: 199it [02:08,  1.56it/s]Extractor Estimating: 200it [02:09,  1.50it/s]Extractor Estimating: 201it [02:09,  1.52it/s]Extractor Estimating: 202it [02:10,  1.54it/s]Extractor Estimating: 203it [02:11,  1.48it/s]Extractor Estimating: 204it [02:11,  1.56it/s]Extractor Estimating: 205it [02:12,  1.58it/s]Extractor Estimating: 206it [02:13,  1.61it/s]Extractor Estimating: 207it [02:13,  1.57it/s]Extractor Estimating: 208it [02:14,  1.58it/s]Extractor Estimating: 209it [02:15,  1.55it/s]Extractor Estimating: 210it [02:15,  1.57it/s]Extractor Estimating: 211it [02:16,  1.59it/s]Extractor Estimating: 212it [02:16,  1.58it/s]Extractor Estimating: 213it [02:17,  1.62it/s]Extractor Estimating: 214it [02:18,  1.64it/s]Extractor Estimating: 215it [02:18,  1.63it/s]Extractor Estimating: 216it [02:19,  1.61it/s]Extractor Estimating: 217it [02:20,  1.58it/s]Extractor Estimating: 218it [02:20,  1.51it/s]Extractor Estimating: 219it [02:21,  1.52it/s]Extractor Estimating: 220it [02:22,  1.57it/s]Extractor Estimating: 221it [02:22,  1.53it/s]Extractor Estimating: 222it [02:23,  1.58it/s]Extractor Estimating: 223it [02:24,  1.53it/s]Extractor Estimating: 224it [02:24,  1.51it/s]Extractor Estimating: 225it [02:25,  1.49it/s]Extractor Estimating: 226it [02:25,  1.52it/s]Extractor Estimating: 227it [02:26,  1.47it/s]Extractor Estimating: 228it [02:27,  1.56it/s]Extractor Estimating: 229it [02:27,  1.60it/s]Extractor Estimating: 230it [02:28,  1.65it/s]Extractor Estimating: 231it [02:29,  1.62it/s]Extractor Estimating: 232it [02:29,  1.56it/s]Extractor Estimating: 233it [02:30,  1.56it/s]Extractor Estimating: 234it [02:31,  1.55it/s]Extractor Estimating: 235it [02:31,  1.55it/s]Extractor Estimating: 236it [02:32,  1.56it/s]Extractor Estimating: 237it [02:32,  1.60it/s]Extractor Estimating: 238it [02:33,  1.58it/s]Extractor Estimating: 239it [02:34,  1.57it/s]Extractor Estimating: 240it [02:34,  1.52it/s]Extractor Estimating: 241it [02:35,  1.49it/s]Extractor Estimating: 242it [02:36,  1.47it/s]Extractor Estimating: 243it [02:36,  1.50it/s]Extractor Estimating: 244it [02:37,  1.53it/s]Extractor Estimating: 245it [02:38,  1.54it/s]Extractor Estimating: 246it [02:38,  1.57it/s]Extractor Estimating: 247it [02:39,  1.59it/s]Extractor Estimating: 248it [02:40,  1.56it/s]Extractor Estimating: 249it [02:40,  1.56it/s]Extractor Estimating: 250it [02:41,  1.49it/s]Extractor Estimating: 250it [02:41,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:00,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:00,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:00,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:00,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:00,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:28:00,894 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:28:00,896 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:28:01,468 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:28:02,525 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:28:02,525 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:05,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:05,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:05,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:05,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:28:05,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:28:06,152 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:28:06,153 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:28:06,725 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:28:06,873 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:28:06,873 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:02:13,277 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:02:13,305 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4997 mean pseudo reward: 0.9517529793750946
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 20621
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20721, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20721, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.086, loss:708.9230
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.088, loss:659.5188
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.095, loss:593.3809
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.100, loss:591.0260
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.103, loss:559.1936
>> valid entity prec:0.6199, rec:0.5566, f1:0.5865
>> valid relation prec:0.3086, rec:0.1013, f1:0.1525
>> valid relation with NER prec:0.3086, rec:0.1013, f1:0.1525
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.443, loss:560.5892
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.079, loss:542.3649
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.089, loss:547.7318
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.079, loss:518.3265
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.090, loss:565.6994
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5948, rec:0.5038, f1:0.5456
>> valid relation prec:0.2284, rec:0.0712, f1:0.1086
>> valid relation with NER prec:0.2284, rec:0.0712, f1:0.1086
g_step 1100, step 55, avg_time 2.430, loss:561.0683
g_step 1200, step 155, avg_time 1.102, loss:536.6520
g_step 1300, step 46, avg_time 1.096, loss:498.2226
g_step 1400, step 146, avg_time 1.091, loss:510.1633
g_step 1500, step 37, avg_time 1.080, loss:518.7595
>> valid entity prec:0.5788, rec:0.5992, f1:0.5888
>> valid relation prec:0.2276, rec:0.0704, f1:0.1075
>> valid relation with NER prec:0.2276, rec:0.0704, f1:0.1075
new max entity f1 on valid!
g_step 1600, step 137, avg_time 2.461, loss:488.1721
g_step 1700, step 28, avg_time 1.086, loss:460.5052
g_step 1800, step 128, avg_time 1.097, loss:474.4470
g_step 1900, step 19, avg_time 1.085, loss:448.4282
g_step 2000, step 119, avg_time 1.084, loss:436.3110
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5856, rec:0.5562, f1:0.5705
>> valid relation prec:0.1692, rec:0.0606, f1:0.0893
>> valid relation with NER prec:0.1692, rec:0.0606, f1:0.0893
g_step 2100, step 10, avg_time 2.458, loss:432.0739
g_step 2200, step 110, avg_time 1.091, loss:415.2969
g_step 2300, step 1, avg_time 1.084, loss:432.0987
g_step 2400, step 101, avg_time 1.101, loss:389.1404
g_step 2500, step 201, avg_time 1.088, loss:418.2646
>> valid entity prec:0.6051, rec:0.5437, f1:0.5727
>> valid relation prec:0.2160, rec:0.0732, f1:0.1094
>> valid relation with NER prec:0.2160, rec:0.0732, f1:0.1094
g_step 2600, step 92, avg_time 2.450, loss:380.2126
g_step 2700, step 192, avg_time 1.093, loss:416.4317
g_step 2800, step 83, avg_time 1.085, loss:352.4968
g_step 2900, step 183, avg_time 1.100, loss:382.9004
g_step 3000, step 74, avg_time 1.075, loss:347.5926
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5669, rec:0.5481, f1:0.5574
>> valid relation prec:0.2275, rec:0.0967, f1:0.1357
>> valid relation with NER prec:0.2275, rec:0.0967, f1:0.1357
g_step 3100, step 174, avg_time 2.460, loss:357.0377
g_step 3200, step 65, avg_time 1.096, loss:353.0419
g_step 3300, step 165, avg_time 1.097, loss:334.2338
g_step 3400, step 56, avg_time 1.077, loss:323.3495
g_step 3500, step 156, avg_time 1.092, loss:345.5012
>> valid entity prec:0.5786, rec:0.5327, f1:0.5547
>> valid relation prec:0.2037, rec:0.0821, f1:0.1170
>> valid relation with NER prec:0.2037, rec:0.0821, f1:0.1170
g_step 3600, step 47, avg_time 2.446, loss:326.8424
g_step 3700, step 147, avg_time 1.094, loss:319.4275
g_step 3800, step 38, avg_time 1.083, loss:308.8196
g_step 3900, step 138, avg_time 1.081, loss:295.4885
g_step 4000, step 29, avg_time 1.110, loss:292.8352
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5592, rec:0.4917, f1:0.5233
>> valid relation prec:0.1988, rec:0.0761, f1:0.1101
>> valid relation with NER prec:0.1988, rec:0.0761, f1:0.1101
g_step 4100, step 129, avg_time 2.442, loss:292.8842
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:02:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:02:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-02-13_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:02:14 - WARNING - datasets.builder -   Using custom data configuration default-74783f478d923c7f
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-74783f478d923c7f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:02:14,609 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:02:14,610 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:02:14,611 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:02:14,612 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:02:14,626 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:02:14,634 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:02:14,634 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:02:14,634 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:02:14,634 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:02:14,634 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:02:14,634 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:02:14,778 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:02:17,979 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:02:17,981 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-74783f478d923c7f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.31ba/s] 40%|████      | 2/5 [00:00<00:00,  4.06ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.36ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.48ba/s]100%|██████████| 5/5 [00:01<00:00,  4.55ba/s]100%|██████████| 5/5 [00:01<00:00,  4.36ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.91ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.26ba/s]100%|██████████| 4/4 [00:00<00:00,  5.30ba/s]100%|██████████| 4/4 [00:00<00:00,  4.82ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.87ba/s]100%|██████████| 5/5 [00:00<00:00, 10.34ba/s]100%|██████████| 5/5 [00:00<00:00, 10.12ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.19ba/s]100%|██████████| 4/4 [00:00<00:00, 11.48ba/s]
[INFO|trainer.py:414] 2023-08-28 13:02:21,202 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:02:21,218 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:02:21,218 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 13:02:21,218 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:02:21,218 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:02:21,218 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:02:21,218 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:02:21,218 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:54,  3.39it/s]  1%|          | 2/390 [00:00<01:52,  3.46it/s]  1%|          | 3/390 [00:00<01:51,  3.48it/s]  1%|          | 4/390 [00:01<01:50,  3.49it/s]  1%|▏         | 5/390 [00:01<01:50,  3.50it/s]  2%|▏         | 6/390 [00:01<01:49,  3.50it/s]  2%|▏         | 7/390 [00:02<01:49,  3.50it/s]  2%|▏         | 8/390 [00:02<01:49,  3.50it/s]  2%|▏         | 9/390 [00:02<01:48,  3.50it/s]  3%|▎         | 10/390 [00:02<01:48,  3.50it/s]  3%|▎         | 11/390 [00:03<01:48,  3.49it/s]  3%|▎         | 12/390 [00:03<01:48,  3.50it/s]  3%|▎         | 13/390 [00:03<01:47,  3.50it/s]  4%|▎         | 14/390 [00:04<01:47,  3.50it/s]  4%|▍         | 15/390 [00:04<01:47,  3.50it/s]  4%|▍         | 16/390 [00:04<01:46,  3.50it/s]  4%|▍         | 17/390 [00:04<01:46,  3.50it/s]  5%|▍         | 18/390 [00:05<01:46,  3.50it/s]  5%|▍         | 19/390 [00:05<01:46,  3.50it/s]  5%|▌         | 20/390 [00:05<01:45,  3.50it/s]  5%|▌         | 21/390 [00:06<01:45,  3.50it/s]  6%|▌         | 22/390 [00:06<01:45,  3.50it/s]  6%|▌         | 23/390 [00:06<01:44,  3.50it/s]  6%|▌         | 24/390 [00:06<01:44,  3.50it/s]  6%|▋         | 25/390 [00:07<01:44,  3.50it/s]  7%|▋         | 26/390 [00:07<01:44,  3.50it/s]  7%|▋         | 27/390 [00:07<01:43,  3.50it/s]  7%|▋         | 28/390 [00:08<01:43,  3.50it/s]  7%|▋         | 29/390 [00:08<01:43,  3.50it/s]  8%|▊         | 30/390 [00:08<01:42,  3.50it/s]  8%|▊         | 31/390 [00:08<01:42,  3.50it/s]  8%|▊         | 32/390 [00:09<01:42,  3.49it/s]  8%|▊         | 33/390 [00:09<01:42,  3.49it/s]  9%|▊         | 34/390 [00:09<01:41,  3.49it/s]  9%|▉         | 35/390 [00:10<01:41,  3.49it/s]  9%|▉         | 36/390 [00:10<01:41,  3.49it/s]  9%|▉         | 37/390 [00:10<01:40,  3.50it/s] 10%|▉         | 38/390 [00:10<01:40,  3.50it/s] 10%|█         | 39/390 [00:11<01:40,  3.50it/s] 10%|█         | 40/390 [00:11<01:40,  3.50it/s] 11%|█         | 41/390 [00:11<01:39,  3.50it/s] 11%|█         | 42/390 [00:12<01:39,  3.50it/s] 11%|█         | 43/390 [00:12<01:39,  3.50it/s] 11%|█▏        | 44/390 [00:12<01:38,  3.50it/s] 12%|█▏        | 45/390 [00:12<01:38,  3.49it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.49it/s] 12%|█▏        | 47/390 [00:13<01:38,  3.49it/s] 12%|█▏        | 48/390 [00:13<01:37,  3.49it/s] 13%|█▎        | 49/390 [00:14<01:37,  3.49it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.50it/s] 13%|█▎        | 51/390 [00:14<01:36,  3.50it/s] 13%|█▎        | 52/390 [00:14<01:36,  3.50it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.50it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.50it/s] 14%|█▍        | 55/390 [00:15<01:35,  3.50it/s] 14%|█▍        | 56/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 58/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 59/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 60/390 [00:17<01:34,  3.49it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.49it/s] 16%|█▌        | 62/390 [00:17<01:33,  3.49it/s] 16%|█▌        | 63/390 [00:18<01:33,  3.50it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.50it/s] 17%|█▋        | 65/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 66/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.49it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.49it/s] 18%|█▊        | 69/390 [00:19<01:31,  3.49it/s] 18%|█▊        | 70/390 [00:20<01:31,  3.49it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.49it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.49it/s] 19%|█▊        | 73/390 [00:20<01:30,  3.49it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.49it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.49it/s] 19%|█▉        | 76/390 [00:21<01:29,  3.49it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.49it/s] 20%|██        | 78/390 [00:22<01:29,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 13:02:43,582 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:02:43,582 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 13:02:43,582 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.76it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.35it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.45it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.59it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.23it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.05it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.77it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.48it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.51it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.46it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.41it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.44it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.33it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.39it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.44it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.39it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.31it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.22it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.20it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.36it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.44it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.31it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.36it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.45it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.30it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.26it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.22it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.26it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.29it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.36it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.26it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.21it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.29it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.31it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.23it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.23it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.30it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.27it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.33it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.31it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.23it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.26it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.34it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.33it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.33it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.16it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.28it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.30it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.28it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.31it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.22it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.23it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.27it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.39it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.30it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.15it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.33it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.26it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.18it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.22it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.23it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.18it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.30it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.38it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.25it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.37it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.25it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.24it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.24it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.24it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.14it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.28it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.22it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.26it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.36it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.28it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.26it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.23it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.20it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.18it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.34it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.29it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.24it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.29it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.32it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.32it/s][A 20%|██        | 78/390 [00:31<01:29,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:02:52,840 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 13:02:52,858 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:02:55,024 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:02:55,035 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:02:55,043 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:38<26:21,  5.09s/it] 21%|██        | 80/390 [00:38<18:50,  3.65s/it] 21%|██        | 81/390 [00:39<13:35,  2.64s/it] 21%|██        | 82/390 [00:39<09:55,  1.93s/it] 21%|██▏       | 83/390 [00:39<07:21,  1.44s/it] 22%|██▏       | 84/390 [00:40<05:34,  1.09s/it] 22%|██▏       | 85/390 [00:40<04:20,  1.17it/s] 22%|██▏       | 86/390 [00:40<03:27,  1.46it/s] 22%|██▏       | 87/390 [00:40<02:50,  1.77it/s] 23%|██▎       | 88/390 [00:41<02:25,  2.08it/s] 23%|██▎       | 89/390 [00:41<02:07,  2.37it/s] 23%|██▎       | 90/390 [00:41<01:54,  2.62it/s] 23%|██▎       | 91/390 [00:42<01:45,  2.83it/s] 24%|██▎       | 92/390 [00:42<01:39,  3.00it/s] 24%|██▍       | 93/390 [00:42<01:34,  3.13it/s] 24%|██▍       | 94/390 [00:42<01:31,  3.23it/s] 24%|██▍       | 95/390 [00:43<01:29,  3.31it/s] 25%|██▍       | 96/390 [00:43<01:27,  3.35it/s] 25%|██▍       | 97/390 [00:43<01:26,  3.39it/s] 25%|██▌       | 98/390 [00:44<01:25,  3.42it/s] 25%|██▌       | 99/390 [00:44<01:24,  3.44it/s] 26%|██▌       | 100/390 [00:44<01:23,  3.45it/s] 26%|██▌       | 101/390 [00:44<01:23,  3.46it/s] 26%|██▌       | 102/390 [00:45<01:22,  3.47it/s] 26%|██▋       | 103/390 [00:45<01:22,  3.48it/s] 27%|██▋       | 104/390 [00:45<01:22,  3.48it/s] 27%|██▋       | 105/390 [00:46<01:21,  3.48it/s] 27%|██▋       | 106/390 [00:46<01:21,  3.49it/s] 27%|██▋       | 107/390 [00:46<01:32,  3.05it/s] 28%|██▊       | 108/390 [00:47<01:29,  3.17it/s] 28%|██▊       | 109/390 [00:47<01:26,  3.26it/s] 28%|██▊       | 110/390 [00:47<01:24,  3.32it/s] 28%|██▊       | 111/390 [00:47<01:22,  3.37it/s] 29%|██▊       | 112/390 [00:48<01:21,  3.41it/s] 29%|██▉       | 113/390 [00:48<01:20,  3.43it/s] 29%|██▉       | 114/390 [00:48<01:20,  3.45it/s] 29%|██▉       | 115/390 [00:49<01:19,  3.46it/s] 30%|██▉       | 116/390 [00:49<01:19,  3.47it/s] 30%|███       | 117/390 [00:49<01:18,  3.46it/s] 30%|███       | 118/390 [00:49<01:18,  3.47it/s] 31%|███       | 119/390 [00:50<01:17,  3.48it/s] 31%|███       | 120/390 [00:50<01:17,  3.48it/s] 31%|███       | 121/390 [00:50<01:17,  3.48it/s] 31%|███▏      | 122/390 [00:51<01:16,  3.48it/s] 32%|███▏      | 123/390 [00:51<01:16,  3.48it/s] 32%|███▏      | 124/390 [00:51<01:16,  3.48it/s] 32%|███▏      | 125/390 [00:51<01:16,  3.48it/s] 32%|███▏      | 126/390 [00:52<01:17,  3.39it/s] 33%|███▎      | 127/390 [00:52<01:17,  3.41it/s] 33%|███▎      | 128/390 [00:52<01:16,  3.42it/s] 33%|███▎      | 129/390 [00:53<01:15,  3.44it/s] 33%|███▎      | 130/390 [00:53<01:15,  3.44it/s] 34%|███▎      | 131/390 [00:53<01:14,  3.45it/s] 34%|███▍      | 132/390 [00:53<01:14,  3.46it/s] 34%|███▍      | 133/390 [00:54<01:14,  3.47it/s] 34%|███▍      | 134/390 [00:54<01:13,  3.47it/s] 35%|███▍      | 135/390 [00:54<01:13,  3.47it/s] 35%|███▍      | 136/390 [00:55<01:13,  3.48it/s] 35%|███▌      | 137/390 [00:55<01:12,  3.48it/s] 35%|███▌      | 138/390 [00:55<01:12,  3.48it/s] 36%|███▌      | 139/390 [00:55<01:12,  3.47it/s] 36%|███▌      | 140/390 [00:56<01:11,  3.47it/s] 36%|███▌      | 141/390 [00:56<01:11,  3.48it/s] 36%|███▋      | 142/390 [00:56<01:11,  3.48it/s] 37%|███▋      | 143/390 [00:57<01:10,  3.48it/s] 37%|███▋      | 144/390 [00:57<01:10,  3.48it/s] 37%|███▋      | 145/390 [00:57<01:10,  3.48it/s] 37%|███▋      | 146/390 [00:58<01:10,  3.48it/s] 38%|███▊      | 147/390 [00:58<01:09,  3.48it/s] 38%|███▊      | 148/390 [00:58<01:09,  3.48it/s] 38%|███▊      | 149/390 [00:58<01:09,  3.48it/s] 38%|███▊      | 150/390 [00:59<01:08,  3.48it/s] 39%|███▊      | 151/390 [00:59<01:08,  3.48it/s] 39%|███▉      | 152/390 [00:59<01:08,  3.49it/s] 39%|███▉      | 153/390 [01:00<01:08,  3.48it/s] 39%|███▉      | 154/390 [01:00<01:07,  3.49it/s] 40%|███▉      | 155/390 [01:00<01:07,  3.49it/s] 40%|████      | 156/390 [01:00<01:07,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 13:03:22,138 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:03:22,139 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 13:03:22,139 >>   Batch size = 8
{'eval_loss': 1.0423665046691895, 'eval_runtime': 9.2434, 'eval_samples_per_second': 378.214, 'eval_steps_per_second': 47.277, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.81it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.03it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.27it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.64it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.19it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.88it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.70it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.38it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.28it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.32it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.27it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.34it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.32it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.33it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.29it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.21it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.11it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.08it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.17it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.13it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.19it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.27it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.24it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.29it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.31it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.10it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.05it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.17it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.10it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.16it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.21it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.19it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.22it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.20it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.14it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.08it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.10it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.11it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.12it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.20it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.16it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.16it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.20it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.26it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.21it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.18it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.14it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.12it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.18it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.17it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.20it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.15it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.17it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.20it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.16it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.14it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.15it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.14it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.21it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.15it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.15it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.08it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.08it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.12it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.14it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.12it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.08it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.14it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.18it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.20it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.25it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.17it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.96it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.04it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.02it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.05it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.22it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.19it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.19it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.19it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.18it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.12it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.05it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.11it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.04it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.13it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.13it/s][A 40%|████      | 156/390 [01:10<01:07,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:03:31,442 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 13:03:31,470 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:03:33,664 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:03:33,683 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:03:33,695 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:17<19:50,  5.11s/it] 41%|████      | 158/390 [01:17<14:09,  3.66s/it] 41%|████      | 159/390 [01:17<10:12,  2.65s/it] 41%|████      | 160/390 [01:18<07:26,  1.94s/it] 41%|████▏     | 161/390 [01:18<05:30,  1.44s/it] 42%|████▏     | 162/390 [01:18<04:10,  1.10s/it] 42%|████▏     | 163/390 [01:18<03:13,  1.17it/s] 42%|████▏     | 164/390 [01:19<02:34,  1.46it/s] 42%|████▏     | 165/390 [01:19<02:07,  1.77it/s] 43%|████▎     | 166/390 [01:19<01:47,  2.08it/s] 43%|████▎     | 167/390 [01:20<01:34,  2.37it/s] 43%|████▎     | 168/390 [01:20<01:24,  2.62it/s] 43%|████▎     | 169/390 [01:20<01:18,  2.81it/s] 44%|████▎     | 170/390 [01:20<01:13,  2.98it/s] 44%|████▍     | 171/390 [01:21<01:10,  3.12it/s] 44%|████▍     | 172/390 [01:21<01:07,  3.22it/s] 44%|████▍     | 173/390 [01:21<01:05,  3.29it/s] 45%|████▍     | 174/390 [01:22<01:04,  3.35it/s] 45%|████▍     | 175/390 [01:22<01:03,  3.39it/s] 45%|████▌     | 176/390 [01:22<01:02,  3.42it/s] 45%|████▌     | 177/390 [01:22<01:01,  3.44it/s] 46%|████▌     | 178/390 [01:23<01:01,  3.45it/s] 46%|████▌     | 179/390 [01:23<01:00,  3.46it/s] 46%|████▌     | 180/390 [01:23<01:00,  3.46it/s] 46%|████▋     | 181/390 [01:24<01:00,  3.46it/s] 47%|████▋     | 182/390 [01:24<00:59,  3.47it/s] 47%|████▋     | 183/390 [01:24<00:59,  3.47it/s] 47%|████▋     | 184/390 [01:24<00:59,  3.48it/s] 47%|████▋     | 185/390 [01:25<00:58,  3.48it/s] 48%|████▊     | 186/390 [01:25<00:58,  3.48it/s] 48%|████▊     | 187/390 [01:25<00:58,  3.48it/s] 48%|████▊     | 188/390 [01:26<00:58,  3.48it/s] 48%|████▊     | 189/390 [01:26<00:57,  3.48it/s] 49%|████▊     | 190/390 [01:26<00:57,  3.48it/s] 49%|████▉     | 191/390 [01:27<00:57,  3.47it/s] 49%|████▉     | 192/390 [01:27<00:57,  3.47it/s] 49%|████▉     | 193/390 [01:27<00:56,  3.47it/s] 50%|████▉     | 194/390 [01:27<00:56,  3.48it/s] 50%|█████     | 195/390 [01:28<00:56,  3.48it/s] 50%|█████     | 196/390 [01:28<00:55,  3.48it/s] 51%|█████     | 197/390 [01:28<00:55,  3.48it/s] 51%|█████     | 198/390 [01:29<00:55,  3.48it/s] 51%|█████     | 199/390 [01:29<00:54,  3.48it/s] 51%|█████▏    | 200/390 [01:29<00:54,  3.47it/s] 52%|█████▏    | 201/390 [01:29<00:54,  3.47it/s] 52%|█████▏    | 202/390 [01:30<00:54,  3.47it/s] 52%|█████▏    | 203/390 [01:30<00:53,  3.48it/s] 52%|█████▏    | 204/390 [01:30<00:53,  3.48it/s] 53%|█████▎    | 205/390 [01:31<00:53,  3.48it/s] 53%|█████▎    | 206/390 [01:31<00:52,  3.48it/s] 53%|█████▎    | 207/390 [01:31<00:52,  3.48it/s] 53%|█████▎    | 208/390 [01:31<00:52,  3.48it/s] 54%|█████▎    | 209/390 [01:32<00:51,  3.48it/s] 54%|█████▍    | 210/390 [01:32<00:51,  3.48it/s] 54%|█████▍    | 211/390 [01:32<00:51,  3.47it/s] 54%|█████▍    | 212/390 [01:33<00:51,  3.47it/s] 55%|█████▍    | 213/390 [01:33<00:50,  3.48it/s] 55%|█████▍    | 214/390 [01:33<00:50,  3.48it/s] 55%|█████▌    | 215/390 [01:33<00:50,  3.48it/s] 55%|█████▌    | 216/390 [01:34<00:50,  3.48it/s] 56%|█████▌    | 217/390 [01:34<00:49,  3.48it/s] 56%|█████▌    | 218/390 [01:34<00:49,  3.48it/s] 56%|█████▌    | 219/390 [01:35<00:49,  3.48it/s] 56%|█████▋    | 220/390 [01:35<00:48,  3.48it/s] 57%|█████▋    | 221/390 [01:35<00:48,  3.48it/s] 57%|█████▋    | 222/390 [01:35<00:48,  3.46it/s] 57%|█████▋    | 223/390 [01:36<00:48,  3.47it/s] 57%|█████▋    | 224/390 [01:36<00:47,  3.47it/s] 58%|█████▊    | 225/390 [01:36<00:47,  3.47it/s] 58%|█████▊    | 226/390 [01:37<00:47,  3.48it/s] 58%|█████▊    | 227/390 [01:37<00:46,  3.48it/s] 58%|█████▊    | 228/390 [01:37<00:46,  3.48it/s] 59%|█████▊    | 229/390 [01:37<00:46,  3.48it/s] 59%|█████▉    | 230/390 [01:38<00:46,  3.48it/s] 59%|█████▉    | 231/390 [01:38<00:45,  3.47it/s] 59%|█████▉    | 232/390 [01:38<00:45,  3.48it/s] 60%|█████▉    | 233/390 [01:39<00:45,  3.47it/s] 60%|██████    | 234/390 [01:39<00:44,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 13:04:00,640 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:04:00,640 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 13:04:00,640 >>   Batch size = 8
{'eval_loss': 1.0579400062561035, 'eval_runtime': 9.2685, 'eval_samples_per_second': 377.193, 'eval_steps_per_second': 47.149, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.27it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.04it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.20it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.31it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.95it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.71it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.45it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.15it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.17it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.16it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.09it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.18it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.05it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.03it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.07it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.97it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.76it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.84it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.73it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.85it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.02it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.02it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.92it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.96it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.97it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.87it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.83it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.86it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.88it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.01it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.99it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.94it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.99it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.91it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.83it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.92it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.90it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.88it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.05it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.01it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.95it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.07it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.99it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.90it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.95it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.91it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.66it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.04it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.99it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.96it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.00it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.95it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.93it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.91it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.89it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.88it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.90it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.84it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.86it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.93it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.89it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.86it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.93it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.90it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.82it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.92it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.88it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.85it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.92it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.86it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.89it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.91it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.89it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.91it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.92it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.86it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.89it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.94it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.87it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.89it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.87it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.89it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.88it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.86it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.82it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.86it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.86it/s][A 60%|██████    | 234/390 [01:48<00:44,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:04:09,967 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:04:09,987 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:04:13,175 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:04:13,193 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:04:13,204 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:57<14:15,  5.52s/it] 61%|██████    | 236/390 [01:57<10:08,  3.95s/it] 61%|██████    | 237/390 [01:57<07:16,  2.85s/it] 61%|██████    | 238/390 [01:57<05:16,  2.08s/it] 61%|██████▏   | 239/390 [01:58<03:53,  1.54s/it] 62%|██████▏   | 240/390 [01:58<02:54,  1.17s/it] 62%|██████▏   | 241/390 [01:58<02:14,  1.11it/s] 62%|██████▏   | 242/390 [01:59<01:46,  1.39it/s] 62%|██████▏   | 243/390 [01:59<01:26,  1.70it/s] 63%|██████▎   | 244/390 [01:59<01:12,  2.01it/s] 63%|██████▎   | 245/390 [01:59<01:03,  2.30it/s] 63%|██████▎   | 246/390 [02:00<00:56,  2.56it/s] 63%|██████▎   | 247/390 [02:00<00:51,  2.78it/s] 64%|██████▎   | 248/390 [02:00<00:48,  2.96it/s] 64%|██████▍   | 249/390 [02:01<00:45,  3.10it/s] 64%|██████▍   | 250/390 [02:01<00:43,  3.20it/s] 64%|██████▍   | 251/390 [02:01<00:42,  3.28it/s] 65%|██████▍   | 252/390 [02:01<00:41,  3.34it/s] 65%|██████▍   | 253/390 [02:02<00:40,  3.38it/s] 65%|██████▌   | 254/390 [02:02<00:39,  3.41it/s] 65%|██████▌   | 255/390 [02:02<00:39,  3.43it/s] 66%|██████▌   | 256/390 [02:03<00:38,  3.45it/s] 66%|██████▌   | 257/390 [02:03<00:38,  3.44it/s] 66%|██████▌   | 258/390 [02:03<00:38,  3.46it/s] 66%|██████▋   | 259/390 [02:03<00:37,  3.46it/s] 67%|██████▋   | 260/390 [02:04<00:37,  3.47it/s] 67%|██████▋   | 261/390 [02:04<00:37,  3.47it/s] 67%|██████▋   | 262/390 [02:04<00:36,  3.48it/s] 67%|██████▋   | 263/390 [02:05<00:36,  3.48it/s] 68%|██████▊   | 264/390 [02:05<00:36,  3.48it/s] 68%|██████▊   | 265/390 [02:05<00:35,  3.48it/s] 68%|██████▊   | 266/390 [02:06<00:35,  3.48it/s] 68%|██████▊   | 267/390 [02:06<00:35,  3.48it/s] 69%|██████▊   | 268/390 [02:06<00:35,  3.47it/s] 69%|██████▉   | 269/390 [02:06<00:34,  3.48it/s] 69%|██████▉   | 270/390 [02:07<00:34,  3.48it/s] 69%|██████▉   | 271/390 [02:07<00:34,  3.48it/s] 70%|██████▉   | 272/390 [02:07<00:33,  3.48it/s] 70%|███████   | 273/390 [02:08<00:33,  3.48it/s] 70%|███████   | 274/390 [02:08<00:33,  3.48it/s] 71%|███████   | 275/390 [02:08<00:33,  3.48it/s] 71%|███████   | 276/390 [02:08<00:32,  3.48it/s] 71%|███████   | 277/390 [02:09<00:32,  3.49it/s] 71%|███████▏  | 278/390 [02:09<00:32,  3.48it/s] 72%|███████▏  | 279/390 [02:09<00:31,  3.47it/s] 72%|███████▏  | 280/390 [02:10<00:31,  3.47it/s] 72%|███████▏  | 281/390 [02:10<00:31,  3.48it/s] 72%|███████▏  | 282/390 [02:10<00:31,  3.48it/s] 73%|███████▎  | 283/390 [02:10<00:30,  3.48it/s] 73%|███████▎  | 284/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 285/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 286/390 [02:11<00:29,  3.48it/s] 74%|███████▎  | 287/390 [02:12<00:29,  3.48it/s] 74%|███████▍  | 288/390 [02:12<00:29,  3.48it/s] 74%|███████▍  | 289/390 [02:12<00:29,  3.48it/s] 74%|███████▍  | 290/390 [02:12<00:28,  3.47it/s] 75%|███████▍  | 291/390 [02:13<00:28,  3.47it/s] 75%|███████▍  | 292/390 [02:13<00:28,  3.47it/s] 75%|███████▌  | 293/390 [02:13<00:27,  3.48it/s] 75%|███████▌  | 294/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 295/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 296/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 297/390 [02:14<00:26,  3.48it/s] 76%|███████▋  | 298/390 [02:15<00:26,  3.48it/s] 77%|███████▋  | 299/390 [02:15<00:26,  3.48it/s] 77%|███████▋  | 300/390 [02:15<00:25,  3.48it/s] 77%|███████▋  | 301/390 [02:16<00:25,  3.46it/s] 77%|███████▋  | 302/390 [02:16<00:25,  3.46it/s] 78%|███████▊  | 303/390 [02:16<00:25,  3.47it/s] 78%|███████▊  | 304/390 [02:16<00:24,  3.47it/s] 78%|███████▊  | 305/390 [02:17<00:24,  3.47it/s] 78%|███████▊  | 306/390 [02:17<00:24,  3.47it/s] 79%|███████▊  | 307/390 [02:17<00:23,  3.47it/s] 79%|███████▉  | 308/390 [02:18<00:23,  3.47it/s] 79%|███████▉  | 309/390 [02:18<00:23,  3.48it/s] 79%|███████▉  | 310/390 [02:18<00:23,  3.48it/s] 80%|███████▉  | 311/390 [02:18<00:22,  3.48it/s] 80%|████████  | 312/390 [02:19<00:22,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 13:04:40,510 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:04:40,510 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 13:04:40,510 >>   Batch size = 8
{'eval_loss': 1.0691461563110352, 'eval_runtime': 9.3146, 'eval_samples_per_second': 375.324, 'eval_steps_per_second': 46.916, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.28it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.86it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.09it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.22it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.90it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.56it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.47it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.17it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.19it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.15it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.16it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.23it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.06it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.14it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.15it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.04it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.98it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.94it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.91it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.92it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.00it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.96it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.97it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.00it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.98it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.93it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.97it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.92it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.75it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.88it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.85it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.92it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.04it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.87it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.93it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.94it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.94it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.89it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.90it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.93it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.83it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.00it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.95it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.88it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.94it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.90it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.93it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.94it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.86it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.91it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.99it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.00it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.90it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.93it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.93it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.95it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.96it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.91it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.89it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.89it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.88it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.90it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.91it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.95it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.87it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.97it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.96it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.94it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.97it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.92it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.93it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.92it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.92it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.92it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.96it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.91it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.93it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.97it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.93it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.89it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.88it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.90it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.90it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.93it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.87it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.90it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.90it/s][A 80%|████████  | 312/390 [02:28<00:22,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:04:49,844 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 13:04:49,864 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:04:52,348 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:04:52,364 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:04:52,377 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:36<06:46,  5.28s/it] 81%|████████  | 314/390 [02:36<04:47,  3.78s/it] 81%|████████  | 315/390 [02:36<03:25,  2.73s/it] 81%|████████  | 316/390 [02:37<02:27,  2.00s/it] 81%|████████▏ | 317/390 [02:37<01:48,  1.49s/it] 82%|████████▏ | 318/390 [02:37<01:21,  1.13s/it] 82%|████████▏ | 319/390 [02:37<01:02,  1.14it/s] 82%|████████▏ | 320/390 [02:38<00:48,  1.43it/s] 82%|████████▏ | 321/390 [02:38<00:39,  1.74it/s] 83%|████████▎ | 322/390 [02:38<00:33,  2.05it/s] 83%|████████▎ | 323/390 [02:39<00:28,  2.34it/s] 83%|████████▎ | 324/390 [02:39<00:25,  2.59it/s] 83%|████████▎ | 325/390 [02:39<00:23,  2.80it/s] 84%|████████▎ | 326/390 [02:39<00:21,  2.98it/s] 84%|████████▍ | 327/390 [02:40<00:20,  3.11it/s] 84%|████████▍ | 328/390 [02:40<00:19,  3.21it/s] 84%|████████▍ | 329/390 [02:40<00:18,  3.29it/s] 85%|████████▍ | 330/390 [02:41<00:17,  3.35it/s] 85%|████████▍ | 331/390 [02:41<00:17,  3.39it/s] 85%|████████▌ | 332/390 [02:41<00:16,  3.42it/s] 85%|████████▌ | 333/390 [02:41<00:16,  3.44it/s] 86%|████████▌ | 334/390 [02:42<00:16,  3.45it/s] 86%|████████▌ | 335/390 [02:42<00:15,  3.46it/s] 86%|████████▌ | 336/390 [02:42<00:15,  3.45it/s] 86%|████████▋ | 337/390 [02:43<00:15,  3.46it/s] 87%|████████▋ | 338/390 [02:43<00:14,  3.47it/s] 87%|████████▋ | 339/390 [02:43<00:14,  3.47it/s] 87%|████████▋ | 340/390 [02:43<00:14,  3.47it/s] 87%|████████▋ | 341/390 [02:44<00:14,  3.48it/s] 88%|████████▊ | 342/390 [02:44<00:13,  3.48it/s] 88%|████████▊ | 343/390 [02:44<00:13,  3.48it/s] 88%|████████▊ | 344/390 [02:45<00:13,  3.48it/s] 88%|████████▊ | 345/390 [02:45<00:12,  3.49it/s] 89%|████████▊ | 346/390 [02:45<00:12,  3.48it/s] 89%|████████▉ | 347/390 [02:45<00:12,  3.48it/s] 89%|████████▉ | 348/390 [02:46<00:12,  3.48it/s] 89%|████████▉ | 349/390 [02:46<00:11,  3.48it/s] 90%|████████▉ | 350/390 [02:46<00:11,  3.48it/s] 90%|█████████ | 351/390 [02:47<00:11,  3.48it/s] 90%|█████████ | 352/390 [02:47<00:10,  3.48it/s] 91%|█████████ | 353/390 [02:47<00:10,  3.48it/s] 91%|█████████ | 354/390 [02:47<00:10,  3.48it/s] 91%|█████████ | 355/390 [02:48<00:10,  3.48it/s] 91%|█████████▏| 356/390 [02:48<00:09,  3.48it/s] 92%|█████████▏| 357/390 [02:48<00:09,  3.48it/s] 92%|█████████▏| 358/390 [02:49<00:09,  3.47it/s] 92%|█████████▏| 359/390 [02:49<00:08,  3.48it/s] 92%|█████████▏| 360/390 [02:49<00:08,  3.48it/s] 93%|█████████▎| 361/390 [02:49<00:08,  3.48it/s] 93%|█████████▎| 362/390 [02:50<00:08,  3.48it/s] 93%|█████████▎| 363/390 [02:50<00:07,  3.48it/s] 93%|█████████▎| 364/390 [02:50<00:07,  3.48it/s] 94%|█████████▎| 365/390 [02:51<00:07,  3.48it/s] 94%|█████████▍| 366/390 [02:51<00:06,  3.48it/s] 94%|█████████▍| 367/390 [02:51<00:06,  3.48it/s] 94%|█████████▍| 368/390 [02:51<00:06,  3.47it/s] 95%|█████████▍| 369/390 [02:52<00:06,  3.47it/s] 95%|█████████▍| 370/390 [02:52<00:05,  3.47it/s] 95%|█████████▌| 371/390 [02:52<00:05,  3.47it/s] 95%|█████████▌| 372/390 [02:53<00:05,  3.48it/s] 96%|█████████▌| 373/390 [02:53<00:04,  3.48it/s] 96%|█████████▌| 374/390 [02:53<00:04,  3.48it/s] 96%|█████████▌| 375/390 [02:53<00:04,  3.48it/s] 96%|█████████▋| 376/390 [02:54<00:04,  3.48it/s] 97%|█████████▋| 377/390 [02:54<00:03,  3.48it/s] 97%|█████████▋| 378/390 [02:54<00:03,  3.48it/s] 97%|█████████▋| 379/390 [02:55<00:03,  3.48it/s] 97%|█████████▋| 380/390 [02:55<00:02,  3.46it/s] 98%|█████████▊| 381/390 [02:55<00:02,  3.47it/s] 98%|█████████▊| 382/390 [02:56<00:02,  3.47it/s] 98%|█████████▊| 383/390 [02:56<00:02,  3.47it/s] 98%|█████████▊| 384/390 [02:56<00:01,  3.48it/s] 99%|█████████▊| 385/390 [02:56<00:01,  3.35it/s] 99%|█████████▉| 386/390 [02:57<00:01,  3.38it/s] 99%|█████████▉| 387/390 [02:57<00:00,  3.41it/s] 99%|█████████▉| 388/390 [02:57<00:00,  3.43it/s]100%|█████████▉| 389/390 [02:58<00:00,  3.44it/s]100%|██████████| 390/390 [02:58<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:05:19,568 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:05:19,568 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 13:05:19,568 >>   Batch size = 8
{'eval_loss': 1.0774381160736084, 'eval_runtime': 9.3125, 'eval_samples_per_second': 375.41, 'eval_steps_per_second': 46.926, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.19it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.89it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.03it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.36it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.98it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.52it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.44it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.08it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.05it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.01it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.00it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.01it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.09it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.16it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.01it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.05it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.95it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.84it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.99it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.82it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.86it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.99it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.02it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.95it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.91it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.91it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.76it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.91it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.93it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.81it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.93it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.92it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.94it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.98it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.96it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.72it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.86it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.98it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.96it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.92it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.93it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.01it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.06it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.04it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.96it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.91it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.94it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.91it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.85it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.95it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.90it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.94it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.91it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.92it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.81it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.89it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.90it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.90it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.83it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.90it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.88it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.88it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.86it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.86it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.78it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.82it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.88it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.92it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.87it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.91it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.91it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.89it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.93it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.92it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.81it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.88it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.85it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.84it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.90it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.93it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.85it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.93it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.93it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.80it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.85it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.88it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.88it/s][A100%|██████████| 390/390 [03:07<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:05:28,896 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 13:05:28,917 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:05:31,323 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:05:31,339 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:05:31,347 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:05:36,384 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:05:36,387 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78 (score: 1.0423665046691895).
                                                 100%|██████████| 390/390 [03:17<00:00,  3.45it/s]100%|██████████| 390/390 [03:17<00:00,  1.98it/s]
[INFO|trainer.py:1894] 2023-08-28 13:05:38,241 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 13:05:38,254 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:05:40,638 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:05:40,649 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:05:40,655 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:05:40,845 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:40,845 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:40,845 >>   train_loss               =     0.6518
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:40,846 >>   train_runtime            = 0:03:17.01
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:40,846 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:40,846 >>   train_samples_per_second =    126.891
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:40,846 >>   train_steps_per_second   =       1.98
{'eval_loss': 1.0802778005599976, 'eval_runtime': 9.3173, 'eval_samples_per_second': 375.217, 'eval_steps_per_second': 46.902, 'epoch': 4.99}
{'train_runtime': 197.0188, 'train_samples_per_second': 126.891, 'train_steps_per_second': 1.98, 'train_loss': 0.6517729930388622, 'epoch': 4.99}
08/28/2023 13:05:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:05:40,878 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:05:40,878 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 13:05:40,878 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.77it/s]  3%|▎         | 12/437 [00:00<00:08, 51.46it/s]  4%|▍         | 18/437 [00:00<00:08, 49.51it/s]  5%|▌         | 23/437 [00:00<00:08, 48.79it/s]  6%|▋         | 28/437 [00:00<00:08, 48.34it/s]  8%|▊         | 33/437 [00:00<00:08, 47.99it/s]  9%|▊         | 38/437 [00:00<00:08, 47.86it/s] 10%|▉         | 43/437 [00:00<00:08, 47.59it/s] 11%|█         | 48/437 [00:00<00:08, 47.43it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.36it/s] 13%|█▎        | 58/437 [00:01<00:07, 47.47it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.43it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.34it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.33it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.42it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.26it/s] 20%|██        | 88/437 [00:01<00:07, 47.28it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.27it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.24it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.23it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.32it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.28it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.28it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.31it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.34it/s] 30%|███       | 133/437 [00:02<00:06, 47.20it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.11it/s] 33%|███▎      | 143/437 [00:02<00:06, 47.22it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.21it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.20it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.28it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.22it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.26it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.25it/s] 41%|████      | 178/437 [00:03<00:05, 47.30it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.18it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.16it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.23it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.12it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.20it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.22it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.27it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.26it/s] 51%|█████     | 223/437 [00:04<00:04, 47.24it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.18it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.16it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.11it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.23it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.25it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.23it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.16it/s] 60%|██████    | 263/437 [00:05<00:03, 47.24it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.22it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.25it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.28it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.17it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.17it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.20it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.91it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.09it/s] 70%|███████   | 308/437 [00:06<00:02, 47.16it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.15it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.18it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.16it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.19it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.10it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.09it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.18it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.23it/s] 81%|████████  | 353/437 [00:07<00:01, 47.21it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.21it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.24it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.23it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.20it/s] 86%|████████▋ | 378/437 [00:07<00:01, 47.24it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.01it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.08it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.19it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.25it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.21it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.19it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.18it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.20it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.20it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.24it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.05it/s]100%|██████████| 437/437 [00:09<00:00, 47.29it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:05:50,141 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   eval_loss               =     1.0424
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   eval_runtime            = 0:00:09.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   eval_samples_per_second =    377.424
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   eval_steps_per_second   =     47.178
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:05:50,141 >>   perplexity              =     2.8359
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:05:56,926 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:05:56,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:05:56,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:05:56,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:05:56,931 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:05:57,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:05:57,551 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:05:58,109 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:05:59,150 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:05:59,150 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:06:02,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:06:02,017 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:06:02,017 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:06:02,017 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:06:02,017 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:06:02,691 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:06:02,692 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:06:03,282 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:06:03,445 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:06:03,445 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.52it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.57it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.49it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:29,  1.55it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:31,  1.56it/s]Extractor Predicting: 48it [00:31,  1.47it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:33,  1.50it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:35,  1.54it/s]Extractor Predicting: 54it [00:35,  1.54it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:37,  1.54it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:45,  1.48it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:46,  1.53it/s]Extractor Predicting: 71it [00:47,  1.51it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:48,  1.51it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.46it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:55,  1.45it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.48it/s]Extractor Predicting: 87it [00:57,  1.45it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [00:59,  1.48it/s]Extractor Predicting: 91it [01:00,  1.53it/s]Extractor Predicting: 92it [01:01,  1.52it/s]Extractor Predicting: 93it [01:01,  1.54it/s]Extractor Predicting: 94it [01:02,  1.52it/s]Extractor Predicting: 95it [01:03,  1.53it/s]Extractor Predicting: 96it [01:03,  1.51it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:08,  1.47it/s]Extractor Predicting: 104it [01:09,  1.47it/s]Extractor Predicting: 105it [01:10,  1.47it/s]Extractor Predicting: 106it [01:10,  1.51it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:12,  1.50it/s]Extractor Predicting: 109it [01:12,  1.50it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:14,  1.49it/s]Extractor Predicting: 112it [01:14,  1.52it/s]Extractor Predicting: 113it [01:15,  1.52it/s]Extractor Predicting: 114it [01:15,  1.51it/s]Extractor Predicting: 115it [01:16,  1.51it/s]Extractor Predicting: 116it [01:17,  1.56it/s]Extractor Predicting: 117it [01:17,  1.54it/s]Extractor Predicting: 118it [01:18,  1.52it/s]Extractor Predicting: 119it [01:19,  1.48it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:21,  1.49it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.47it/s]Extractor Predicting: 125it [01:23,  1.45it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:24,  1.49it/s]Extractor Predicting: 128it [01:25,  1.51it/s]Extractor Predicting: 129it [01:26,  1.40it/s]Extractor Predicting: 130it [01:26,  1.46it/s]Extractor Predicting: 131it [01:27,  1.48it/s]Extractor Predicting: 132it [01:28,  1.49it/s]Extractor Predicting: 133it [01:28,  1.50it/s]Extractor Predicting: 134it [01:29,  1.51it/s]Extractor Predicting: 135it [01:30,  1.52it/s]Extractor Predicting: 136it [01:30,  1.51it/s]Extractor Predicting: 137it [01:31,  1.50it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:32,  1.50it/s]Extractor Predicting: 140it [01:33,  1.49it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:34,  1.47it/s]Extractor Predicting: 143it [01:35,  1.47it/s]Extractor Predicting: 144it [01:35,  1.62it/s]Extractor Predicting: 144it [01:35,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:45,675 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:45,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:45,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:45,681 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:45,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:07:46,415 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:07:46,416 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:07:46,683 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:07:47,722 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:07:47,722 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:50,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:50,573 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:50,573 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:50,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:07:50,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:07:51,242 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:07:51,243 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:07:51,821 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:07:51,986 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:07:51,987 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.63,
  "recall": 0.1261441647597254,
  "score": 0.2102001906577693,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.75it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.71it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.73it/s]Extractor Predicting: 22it [00:12,  1.76it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:14,  1.65it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:15,  1.70it/s]Extractor Predicting: 28it [00:16,  1.61it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:17,  1.53it/s]Extractor Predicting: 31it [00:18,  1.49it/s]Extractor Predicting: 32it [00:19,  1.45it/s]Extractor Predicting: 33it [00:20,  1.44it/s]Extractor Predicting: 34it [00:20,  1.43it/s]Extractor Predicting: 35it [00:21,  1.45it/s]Extractor Predicting: 36it [00:22,  1.46it/s]Extractor Predicting: 37it [00:22,  1.45it/s]Extractor Predicting: 38it [00:23,  1.41it/s]Extractor Predicting: 39it [00:24,  1.42it/s]Extractor Predicting: 40it [00:25,  1.40it/s]Extractor Predicting: 41it [00:25,  1.39it/s]Extractor Predicting: 42it [00:26,  1.38it/s]Extractor Predicting: 43it [00:27,  1.40it/s]Extractor Predicting: 44it [00:27,  1.42it/s]Extractor Predicting: 45it [00:28,  1.42it/s]Extractor Predicting: 46it [00:29,  1.45it/s]Extractor Predicting: 47it [00:30,  1.40it/s]Extractor Predicting: 48it [00:30,  1.39it/s]Extractor Predicting: 49it [00:31,  1.40it/s]Extractor Predicting: 50it [00:32,  1.41it/s]Extractor Predicting: 51it [00:32,  1.40it/s]Extractor Predicting: 52it [00:33,  1.41it/s]Extractor Predicting: 53it [00:34,  1.41it/s]Extractor Predicting: 54it [00:34,  1.42it/s]Extractor Predicting: 55it [00:35,  1.42it/s]Extractor Predicting: 56it [00:36,  1.40it/s]Extractor Predicting: 57it [00:37,  1.42it/s]Extractor Predicting: 58it [00:37,  1.33it/s]Extractor Predicting: 59it [00:38,  1.38it/s]Extractor Predicting: 60it [00:39,  1.42it/s]Extractor Predicting: 61it [00:39,  1.40it/s]Extractor Predicting: 62it [00:40,  1.41it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:42,  1.47it/s]Extractor Predicting: 65it [00:42,  1.48it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:43,  1.51it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.51it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.58it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:48,  1.49it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:49,  1.46it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:51,  1.47it/s]Extractor Predicting: 79it [00:51,  1.47it/s]Extractor Predicting: 80it [00:52,  1.46it/s]Extractor Predicting: 81it [00:53,  1.45it/s]Extractor Predicting: 82it [00:54,  1.47it/s]Extractor Predicting: 83it [00:54,  1.48it/s]Extractor Predicting: 84it [00:55,  1.45it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:56,  1.49it/s]Extractor Predicting: 87it [00:57,  1.53it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:58,  1.58it/s]Extractor Predicting: 90it [00:59,  1.58it/s]Extractor Predicting: 91it [00:59,  1.63it/s]Extractor Predicting: 92it [01:00,  1.61it/s]Extractor Predicting: 93it [01:00,  1.66it/s]Extractor Predicting: 94it [01:01,  1.63it/s]Extractor Predicting: 95it [01:02,  1.62it/s]Extractor Predicting: 96it [01:02,  1.62it/s]Extractor Predicting: 97it [01:03,  1.65it/s]Extractor Predicting: 98it [01:04,  1.67it/s]Extractor Predicting: 99it [01:04,  1.67it/s]Extractor Predicting: 100it [01:05,  1.72it/s]Extractor Predicting: 101it [01:05,  1.71it/s]Extractor Predicting: 102it [01:06,  1.66it/s]Extractor Predicting: 103it [01:07,  1.64it/s]Extractor Predicting: 104it [01:07,  1.65it/s]Extractor Predicting: 105it [01:08,  1.67it/s]Extractor Predicting: 106it [01:08,  1.66it/s]Extractor Predicting: 107it [01:09,  1.72it/s]Extractor Predicting: 108it [01:09,  1.68it/s]Extractor Predicting: 109it [01:10,  1.69it/s]Extractor Predicting: 110it [01:11,  1.69it/s]Extractor Predicting: 111it [01:11,  1.69it/s]Extractor Predicting: 112it [01:12,  1.71it/s]Extractor Predicting: 113it [01:12,  1.70it/s]Extractor Predicting: 114it [01:13,  1.68it/s]Extractor Predicting: 115it [01:14,  1.64it/s]Extractor Predicting: 116it [01:14,  1.61it/s]Extractor Predicting: 117it [01:15,  1.58it/s]Extractor Predicting: 118it [01:16,  1.58it/s]Extractor Predicting: 119it [01:16,  1.57it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.56it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:19,  1.53it/s]Extractor Predicting: 124it [01:19,  1.57it/s]Extractor Predicting: 125it [01:20,  1.54it/s]Extractor Predicting: 126it [01:21,  1.55it/s]Extractor Predicting: 127it [01:21,  1.54it/s]Extractor Predicting: 128it [01:22,  1.52it/s]Extractor Predicting: 129it [01:23,  1.53it/s]Extractor Predicting: 130it [01:23,  1.51it/s]Extractor Predicting: 131it [01:24,  1.49it/s]Extractor Predicting: 132it [01:25,  1.50it/s]Extractor Predicting: 133it [01:25,  1.52it/s]Extractor Predicting: 134it [01:26,  1.52it/s]Extractor Predicting: 135it [01:27,  1.51it/s]Extractor Predicting: 136it [01:27,  1.47it/s]Extractor Predicting: 137it [01:28,  1.39it/s]Extractor Predicting: 138it [01:29,  1.41it/s]Extractor Predicting: 139it [01:30,  1.44it/s]Extractor Predicting: 140it [01:30,  1.45it/s]Extractor Predicting: 141it [01:31,  1.48it/s]Extractor Predicting: 142it [01:32,  1.48it/s]Extractor Predicting: 143it [01:32,  1.83it/s]Extractor Predicting: 143it [01:32,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:31,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:31,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:31,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:31,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:31,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:09:32,301 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:09:32,302 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:09:32,924 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:09:33,941 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:09:33,941 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:36,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:36,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:36,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:36,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:09:36,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:09:37,452 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:09:37,453 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:09:38,009 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:09:38,166 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:09:38,166 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5733333333333334,
  "recall": 0.10087976539589442,
  "score": 0.17157107231920202,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 2it [00:01,  1.36it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:09:39,977 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:09:39,978 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:09:39,985 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:09:39,985 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:09:39,986 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:09:42,918 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:09:42,921 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:09:42,941 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:09:42,941 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:09:42,955 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:09:42,958 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:09:42,958 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:09:42,958 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:09:42,958 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:09:42,958 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:09:42,958 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.25,
  "recall": 0.011111111111111112,
  "score": 0.02127659574468085,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:09:43,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:44,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:44,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:45,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:46,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:47,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:47,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:48,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:49,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:50,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:51,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:52,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:53,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:54,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:54,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:55,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:56,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:56,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:57,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:58,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:09:59,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:33, 17.07s/it][WARNING|generation_utils.py:914] 2023-08-28 13:10:00,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:00,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:01,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:02,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:03,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:04,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:04,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:05,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:06,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:06,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:07,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:08,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:09,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:09,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:10,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:11,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:11,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:12,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:13,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:14,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:04, 15.62s/it][WARNING|generation_utils.py:914] 2023-08-28 13:10:14,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:15,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:16,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:17,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:17,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:18,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:19,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:20,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:20,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:21,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:22,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:24,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:25,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:25,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:26,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:27,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:28,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:28,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:29,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:30,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:48<01:51, 15.94s/it][WARNING|generation_utils.py:914] 2023-08-28 13:10:31,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:31,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:32,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:33,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:33,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:34,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:35,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:36,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:36,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:37,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:37,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:38,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:39,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:40,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:41,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:41,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:42,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:42,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:43,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:44,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:44,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:45,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:02<01:33, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 13:10:46,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:46,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:47,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:48,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:49,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:49,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:50,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:51,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:51,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:52,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:53,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:54,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:55,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:56,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:56,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:57,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:58,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:10:59,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:00,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:00,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:01,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:19<01:19, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 13:11:02,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:03,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:03,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:04,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:05,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:06,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:07,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:08,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:08,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:09,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:11,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:11,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:12,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:13,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:14,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:14,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:16,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:16,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:17,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:18,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:19,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:37<01:06, 16.61s/it][WARNING|generation_utils.py:914] 2023-08-28 13:11:20,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:21,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:22,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:22,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:23,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:24,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:24,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:25,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:26,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:27,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:27,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:28,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:29,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:30,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:31,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:31,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:32,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:33,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:33,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:34,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:35,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:36,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:53<00:49, 16.53s/it][WARNING|generation_utils.py:914] 2023-08-28 13:11:37,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:37,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:38,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:38,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:39,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:40,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:40,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:41,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:42,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:42,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:43,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:44,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:45,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:45,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:46,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:47,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:47,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:48,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:48,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:49,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:50,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:07<00:31, 15.70s/it][WARNING|generation_utils.py:914] 2023-08-28 13:11:50,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:51,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:52,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:53,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:54,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:54,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:55,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:55,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:56,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:57,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:58,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:58,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:11:59,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:00,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:01,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:02,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:02,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:03,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:04,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:05,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:05,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:23<00:15, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-28 13:12:06,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:07,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:08,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:09,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:09,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:10,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:11,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:11,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:12,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:13,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:13,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:14,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:15,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:15,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:16,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:17,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:18,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:18,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:19,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:20,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:21,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:21,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:12:22,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:40<00:00, 16.00s/it]Generating: 100%|██████████| 10/10 [02:40<00:00, 16.01s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:28,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:28,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:28,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:28,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:28,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:12:29,469 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:12:29,470 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:12:29,741 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:12:30,801 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:12:30,801 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:32,917 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:32,920 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:32,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:32,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:12:32,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:12:33,257 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:12:33,258 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:12:33,525 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:12:33,690 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:12:33,691 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8928571428571429, 'errors': {''}}
['Relation : occupation . Context : Later in 1837 he came to London , where he established himself in the Legislative Councils of London , including the Legislative Constituency , the Legislative Councils of Durham and Basingstoke , and in the Legislative Councils of London , including the Legislative Legislative Assemblys of Winchester and Dagenham . Head Entity : John Alexander , Tail Entity : Legislative Councils of London .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 8136
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8236, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.47it/s]Extractor Estimating: 2it [00:01,  1.31it/s]Extractor Estimating: 3it [00:02,  1.36it/s]Extractor Estimating: 4it [00:02,  1.37it/s]Extractor Estimating: 5it [00:03,  1.43it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.43it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:08,  1.54it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:09,  1.49it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:12,  1.55it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.59it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:14,  1.52it/s]Extractor Estimating: 23it [00:15,  1.50it/s]Extractor Estimating: 24it [00:16,  1.45it/s]Extractor Estimating: 25it [00:16,  1.45it/s]Extractor Estimating: 26it [00:17,  1.49it/s]Extractor Estimating: 27it [00:18,  1.47it/s]Extractor Estimating: 28it [00:18,  1.49it/s]Extractor Estimating: 29it [00:19,  1.33it/s]Extractor Estimating: 30it [00:20,  1.34it/s]Extractor Estimating: 31it [00:21,  1.41it/s]Extractor Estimating: 32it [00:21,  1.44it/s]Extractor Estimating: 33it [00:22,  1.47it/s]Extractor Estimating: 34it [00:23,  1.50it/s]Extractor Estimating: 35it [00:23,  1.51it/s]Extractor Estimating: 36it [00:24,  1.50it/s]Extractor Estimating: 37it [00:25,  1.50it/s]Extractor Estimating: 38it [00:25,  1.52it/s]Extractor Estimating: 39it [00:26,  1.53it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:27,  1.51it/s]Extractor Estimating: 42it [00:28,  1.53it/s]Extractor Estimating: 43it [00:28,  1.54it/s]Extractor Estimating: 44it [00:29,  1.49it/s]Extractor Estimating: 45it [00:30,  1.52it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:31,  1.52it/s]Extractor Estimating: 48it [00:32,  1.49it/s]Extractor Estimating: 49it [00:32,  1.47it/s]Extractor Estimating: 50it [00:33,  1.50it/s]Extractor Estimating: 51it [00:34,  1.54it/s]Extractor Estimating: 52it [00:34,  1.59it/s]Extractor Estimating: 53it [00:35,  1.56it/s]Extractor Estimating: 54it [00:36,  1.56it/s]Extractor Estimating: 55it [00:36,  1.55it/s]Extractor Estimating: 56it [00:37,  1.54it/s]Extractor Estimating: 57it [00:38,  1.52it/s]Extractor Estimating: 58it [00:38,  1.54it/s]Extractor Estimating: 59it [00:39,  1.56it/s]Extractor Estimating: 60it [00:40,  1.49it/s]Extractor Estimating: 61it [00:40,  1.51it/s]Extractor Estimating: 62it [00:41,  1.50it/s]Extractor Estimating: 63it [00:42,  1.50it/s]Extractor Estimating: 64it [00:42,  1.54it/s]Extractor Estimating: 65it [00:43,  1.54it/s]Extractor Estimating: 66it [00:43,  1.56it/s]Extractor Estimating: 67it [00:44,  1.56it/s]Extractor Estimating: 68it [00:45,  1.57it/s]Extractor Estimating: 69it [00:45,  1.58it/s]Extractor Estimating: 70it [00:46,  1.53it/s]Extractor Estimating: 71it [00:47,  1.56it/s]Extractor Estimating: 72it [00:47,  1.53it/s]Extractor Estimating: 73it [00:48,  1.51it/s]Extractor Estimating: 74it [00:49,  1.56it/s]Extractor Estimating: 75it [00:49,  1.58it/s]Extractor Estimating: 76it [00:50,  1.56it/s]Extractor Estimating: 77it [00:51,  1.58it/s]Extractor Estimating: 78it [00:51,  1.54it/s]Extractor Estimating: 79it [00:52,  1.55it/s]Extractor Estimating: 80it [00:52,  1.59it/s]Extractor Estimating: 81it [00:53,  1.59it/s]Extractor Estimating: 82it [00:54,  1.60it/s]Extractor Estimating: 83it [00:54,  1.55it/s]Extractor Estimating: 84it [00:55,  1.52it/s]Extractor Estimating: 85it [00:56,  1.52it/s]Extractor Estimating: 86it [00:56,  1.57it/s]Extractor Estimating: 87it [00:57,  1.59it/s]Extractor Estimating: 88it [00:58,  1.55it/s]Extractor Estimating: 89it [00:58,  1.48it/s]Extractor Estimating: 90it [00:59,  1.51it/s]Extractor Estimating: 91it [01:00,  1.51it/s]Extractor Estimating: 92it [01:00,  1.61it/s]Extractor Estimating: 93it [01:01,  1.61it/s]Extractor Estimating: 94it [01:01,  1.64it/s]Extractor Estimating: 95it [01:02,  1.64it/s]Extractor Estimating: 96it [01:03,  1.64it/s]Extractor Estimating: 97it [01:03,  1.57it/s]Extractor Estimating: 98it [01:04,  1.58it/s]Extractor Estimating: 99it [01:04,  1.62it/s]Extractor Estimating: 100it [01:05,  1.66it/s]Extractor Estimating: 101it [01:06,  1.51it/s]Extractor Estimating: 102it [01:06,  1.55it/s]Extractor Estimating: 103it [01:07,  1.58it/s]Extractor Estimating: 104it [01:08,  1.56it/s]Extractor Estimating: 105it [01:08,  1.58it/s]Extractor Estimating: 106it [01:09,  1.60it/s]Extractor Estimating: 107it [01:10,  1.60it/s]Extractor Estimating: 108it [01:10,  1.69it/s]Extractor Estimating: 109it [01:11,  1.70it/s]Extractor Estimating: 110it [01:11,  1.71it/s]Extractor Estimating: 111it [01:12,  1.70it/s]Extractor Estimating: 112it [01:12,  1.70it/s]Extractor Estimating: 113it [01:13,  1.71it/s]Extractor Estimating: 114it [01:14,  1.68it/s]Extractor Estimating: 115it [01:14,  1.70it/s]Extractor Estimating: 116it [01:15,  1.72it/s]Extractor Estimating: 117it [01:15,  1.72it/s]Extractor Estimating: 118it [01:16,  1.72it/s]Extractor Estimating: 119it [01:16,  1.76it/s]Extractor Estimating: 120it [01:17,  1.74it/s]Extractor Estimating: 121it [01:18,  1.70it/s]Extractor Estimating: 122it [01:18,  1.70it/s]Extractor Estimating: 123it [01:19,  1.70it/s]Extractor Estimating: 124it [01:20,  1.65it/s]Extractor Estimating: 125it [01:20,  1.65it/s]Extractor Estimating: 126it [01:21,  1.67it/s]Extractor Estimating: 127it [01:21,  1.66it/s]Extractor Estimating: 128it [01:22,  1.64it/s]Extractor Estimating: 129it [01:23,  1.66it/s]Extractor Estimating: 130it [01:23,  1.70it/s]Extractor Estimating: 131it [01:24,  1.71it/s]Extractor Estimating: 132it [01:24,  1.67it/s]Extractor Estimating: 133it [01:25,  1.63it/s]Extractor Estimating: 134it [01:26,  1.63it/s]Extractor Estimating: 135it [01:26,  1.65it/s]Extractor Estimating: 136it [01:27,  1.58it/s]Extractor Estimating: 137it [01:27,  1.63it/s]Extractor Estimating: 138it [01:28,  1.62it/s]Extractor Estimating: 139it [01:29,  1.70it/s]Extractor Estimating: 140it [01:29,  1.73it/s]Extractor Estimating: 141it [01:30,  1.73it/s]Extractor Estimating: 142it [01:30,  1.70it/s]Extractor Estimating: 143it [01:31,  1.72it/s]Extractor Estimating: 144it [01:31,  1.77it/s]Extractor Estimating: 145it [01:32,  1.78it/s]Extractor Estimating: 146it [01:32,  1.79it/s]Extractor Estimating: 147it [01:33,  1.70it/s]Extractor Estimating: 148it [01:34,  1.71it/s]Extractor Estimating: 149it [01:34,  1.73it/s]Extractor Estimating: 150it [01:35,  1.61it/s]Extractor Estimating: 151it [01:36,  1.56it/s]Extractor Estimating: 152it [01:36,  1.56it/s]Extractor Estimating: 153it [01:37,  1.57it/s]Extractor Estimating: 154it [01:38,  1.61it/s]Extractor Estimating: 155it [01:38,  1.65it/s]Extractor Estimating: 156it [01:39,  1.68it/s]Extractor Estimating: 157it [01:39,  1.65it/s]Extractor Estimating: 158it [01:40,  1.60it/s]Extractor Estimating: 159it [01:41,  1.66it/s]Extractor Estimating: 160it [01:41,  1.62it/s]Extractor Estimating: 161it [01:42,  1.58it/s]Extractor Estimating: 162it [01:43,  1.55it/s]Extractor Estimating: 163it [01:43,  1.57it/s]Extractor Estimating: 164it [01:44,  1.59it/s]Extractor Estimating: 165it [01:44,  1.58it/s]Extractor Estimating: 166it [01:45,  1.59it/s]Extractor Estimating: 167it [01:46,  1.65it/s]Extractor Estimating: 168it [01:46,  1.68it/s]Extractor Estimating: 169it [01:47,  1.50it/s]Extractor Estimating: 170it [01:48,  1.54it/s]Extractor Estimating: 171it [01:48,  1.53it/s]Extractor Estimating: 172it [01:49,  1.57it/s]Extractor Estimating: 173it [01:49,  1.57it/s]Extractor Estimating: 174it [01:50,  1.59it/s]Extractor Estimating: 175it [01:51,  1.59it/s]Extractor Estimating: 176it [01:51,  1.57it/s]Extractor Estimating: 177it [01:52,  1.57it/s]Extractor Estimating: 178it [01:53,  1.59it/s]Extractor Estimating: 179it [01:53,  1.61it/s]Extractor Estimating: 180it [01:54,  1.63it/s]Extractor Estimating: 181it [01:55,  1.58it/s]Extractor Estimating: 182it [01:55,  1.66it/s]Extractor Estimating: 183it [01:56,  1.59it/s]Extractor Estimating: 184it [01:56,  1.62it/s]Extractor Estimating: 185it [01:57,  1.59it/s]Extractor Estimating: 186it [01:58,  1.57it/s]Extractor Estimating: 187it [01:58,  1.61it/s]Extractor Estimating: 188it [01:59,  1.57it/s]Extractor Estimating: 189it [01:59,  1.62it/s]Extractor Estimating: 190it [02:00,  1.63it/s]Extractor Estimating: 191it [02:01,  1.62it/s]Extractor Estimating: 192it [02:01,  1.57it/s]Extractor Estimating: 193it [02:02,  1.62it/s]Extractor Estimating: 194it [02:03,  1.66it/s]Extractor Estimating: 195it [02:03,  1.64it/s]Extractor Estimating: 196it [02:04,  1.64it/s]Extractor Estimating: 197it [02:04,  1.64it/s]Extractor Estimating: 198it [02:05,  1.65it/s]Extractor Estimating: 199it [02:06,  1.67it/s]Extractor Estimating: 200it [02:06,  1.66it/s]Extractor Estimating: 201it [02:07,  1.62it/s]Extractor Estimating: 202it [02:07,  1.64it/s]Extractor Estimating: 203it [02:08,  1.62it/s]Extractor Estimating: 204it [02:09,  1.61it/s]Extractor Estimating: 205it [02:09,  1.59it/s]Extractor Estimating: 206it [02:10,  1.63it/s]Extractor Estimating: 207it [02:10,  1.67it/s]Extractor Estimating: 208it [02:11,  1.69it/s]Extractor Estimating: 209it [02:12,  1.63it/s]Extractor Estimating: 210it [02:12,  1.65it/s]Extractor Estimating: 211it [02:13,  1.63it/s]Extractor Estimating: 212it [02:14,  1.65it/s]Extractor Estimating: 213it [02:14,  1.63it/s]Extractor Estimating: 214it [02:15,  1.61it/s]Extractor Estimating: 215it [02:15,  1.56it/s]Extractor Estimating: 216it [02:16,  1.58it/s]Extractor Estimating: 217it [02:17,  1.59it/s]Extractor Estimating: 218it [02:17,  1.63it/s]Extractor Estimating: 219it [02:18,  1.63it/s]Extractor Estimating: 220it [02:19,  1.57it/s]Extractor Estimating: 221it [02:19,  1.56it/s]Extractor Estimating: 222it [02:20,  1.48it/s]Extractor Estimating: 223it [02:21,  1.50it/s]Extractor Estimating: 224it [02:21,  1.53it/s]Extractor Estimating: 225it [02:22,  1.53it/s]Extractor Estimating: 226it [02:23,  1.56it/s]Extractor Estimating: 227it [02:23,  1.55it/s]Extractor Estimating: 228it [02:24,  1.64it/s]Extractor Estimating: 229it [02:24,  1.67it/s]Extractor Estimating: 230it [02:25,  1.66it/s]Extractor Estimating: 231it [02:26,  1.66it/s]Extractor Estimating: 232it [02:26,  1.57it/s]Extractor Estimating: 233it [02:27,  1.59it/s]Extractor Estimating: 234it [02:27,  1.62it/s]Extractor Estimating: 235it [02:28,  1.64it/s]Extractor Estimating: 236it [02:29,  1.65it/s]Extractor Estimating: 237it [02:29,  1.51it/s]Extractor Estimating: 238it [02:30,  1.58it/s]Extractor Estimating: 239it [02:31,  1.59it/s]Extractor Estimating: 240it [02:31,  1.60it/s]Extractor Estimating: 241it [02:32,  1.58it/s]Extractor Estimating: 242it [02:32,  1.61it/s]Extractor Estimating: 243it [02:33,  1.56it/s]Extractor Estimating: 244it [02:34,  1.62it/s]Extractor Estimating: 245it [02:34,  1.60it/s]Extractor Estimating: 246it [02:35,  1.60it/s]Extractor Estimating: 247it [02:36,  1.62it/s]Extractor Estimating: 248it [02:36,  1.63it/s]Extractor Estimating: 249it [02:37,  1.58it/s]Extractor Estimating: 250it [02:38,  1.54it/s]Extractor Estimating: 250it [02:38,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:23,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:23,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:23,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:23,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:23,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:15:23,789 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:15:23,790 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:15:24,383 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:15:25,406 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:15:25,406 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:28,256 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:28,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:28,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:28,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:15:28,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:15:28,902 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:15:28,903 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:15:29,491 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:15:29,641 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:15:29,641 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 14:49:16,531 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 14:49:16,571 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4998 mean pseudo reward: 0.930816086134818
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 18481
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18581, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18581, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.095, loss:675.3977
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.083, loss:660.0247
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.092, loss:609.9359
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.082, loss:633.3373
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.082, loss:579.0474
>> valid entity prec:0.5756, rec:0.5485, f1:0.5617
>> valid relation prec:0.2195, rec:0.0709, f1:0.1072
>> valid relation with NER prec:0.2195, rec:0.0709, f1:0.1072
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.448, loss:605.8589
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.085, loss:570.5542
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.079, loss:573.6723
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.088, loss:588.4327
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.079, loss:580.3490
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5773, rec:0.5858, f1:0.5815
>> valid relation prec:0.2650, rec:0.1038, f1:0.1492
>> valid relation with NER prec:0.2650, rec:0.1038, f1:0.1492
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 2.441, loss:529.8771
g_step 1200, step 155, avg_time 1.091, loss:564.3572
g_step 1300, step 46, avg_time 1.087, loss:539.9979
g_step 1400, step 146, avg_time 1.081, loss:506.9208
g_step 1500, step 37, avg_time 1.094, loss:495.0941
>> valid entity prec:0.5577, rec:0.5349, f1:0.5461
>> valid relation prec:0.1925, rec:0.0675, f1:0.1000
>> valid relation with NER prec:0.1925, rec:0.0675, f1:0.1000
g_step 1600, step 137, avg_time 2.436, loss:493.4365
g_step 1700, step 28, avg_time 1.086, loss:494.8919
g_step 1800, step 128, avg_time 1.086, loss:445.7450
g_step 1900, step 19, avg_time 1.084, loss:476.2823
g_step 2000, step 119, avg_time 1.087, loss:434.6597
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5614, rec:0.5083, f1:0.5335
>> valid relation prec:0.2130, rec:0.0581, f1:0.0913
>> valid relation with NER prec:0.2130, rec:0.0581, f1:0.0913
g_step 2100, step 10, avg_time 2.439, loss:454.8360
g_step 2200, step 110, avg_time 1.081, loss:419.3108
g_step 2300, step 1, avg_time 1.089, loss:421.9384
g_step 2400, step 101, avg_time 1.103, loss:390.3549
g_step 2500, step 201, avg_time 1.075, loss:407.1087
>> valid entity prec:0.5784, rec:0.5580, f1:0.5680
>> valid relation prec:0.1935, rec:0.0749, f1:0.1080
>> valid relation with NER prec:0.1935, rec:0.0749, f1:0.1080
g_step 2600, step 92, avg_time 2.449, loss:379.1639
g_step 2700, step 192, avg_time 1.082, loss:399.5255
g_step 2800, step 83, avg_time 1.069, loss:352.6203
g_step 2900, step 183, avg_time 1.089, loss:385.3862
g_step 3000, step 74, avg_time 1.087, loss:340.4935
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5293, rec:0.5566, f1:0.5426
>> valid relation prec:0.2093, rec:0.0761, f1:0.1116
>> valid relation with NER prec:0.2093, rec:0.0761, f1:0.1116
g_step 3100, step 174, avg_time 2.454, loss:353.4326
g_step 3200, step 65, avg_time 1.083, loss:315.3483
g_step 3300, step 165, avg_time 1.092, loss:332.6906
g_step 3400, step 56, avg_time 1.075, loss:299.5561
g_step 3500, step 156, avg_time 1.082, loss:342.1759
>> valid entity prec:0.5767, rec:0.4471, f1:0.5037
>> valid relation prec:0.2039, rec:0.0604, f1:0.0931
>> valid relation with NER prec:0.2039, rec:0.0604, f1:0.0931
g_step 3600, step 47, avg_time 2.445, loss:313.2426
g_step 3700, step 147, avg_time 1.087, loss:303.0197
g_step 3800, step 38, avg_time 1.088, loss:295.0755
g_step 3900, step 138, avg_time 1.068, loss:294.9057
g_step 4000, step 29, avg_time 1.110, loss:311.1861
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5524, rec:0.5508, f1:0.5516
>> valid relation prec:0.1941, rec:0.0867, f1:0.1198
>> valid relation with NER prec:0.1941, rec:0.0867, f1:0.1198
g_step 4100, step 129, avg_time 2.455, loss:278.5105
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:49:16 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:49:16 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-49-16_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:49:17 - WARNING - datasets.builder -   Using custom data configuration default-909917a722dffe64
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-909917a722dffe64/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:49:17,869 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:49:17,870 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:49:17,871 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:49:17,872 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:49:17,882 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:49:17,886 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:49:17,886 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:49:17,886 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:49:17,886 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:49:17,886 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:49:17,886 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:49:18,022 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:49:21,247 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:49:21,250 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-909917a722dffe64/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.12ba/s] 40%|████      | 2/5 [00:00<00:00,  3.96ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.36ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.54ba/s]100%|██████████| 5/5 [00:01<00:00,  4.65ba/s]100%|██████████| 5/5 [00:01<00:00,  4.39ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.91ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.27ba/s]100%|██████████| 4/4 [00:00<00:00,  5.32ba/s]100%|██████████| 4/4 [00:00<00:00,  4.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.19ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.97ba/s]100%|██████████| 5/5 [00:00<00:00, 10.46ba/s]100%|██████████| 5/5 [00:00<00:00, 10.20ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.00ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.30ba/s]100%|██████████| 4/4 [00:00<00:00, 11.53ba/s]
[INFO|trainer.py:414] 2023-08-28 14:49:24,451 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:49:24,472 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:49:24,472 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 14:49:24,472 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:49:24,472 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:49:24,472 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:49:24,472 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:49:24,472 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.31it/s]  1%|          | 2/390 [00:00<01:53,  3.42it/s]  1%|          | 3/390 [00:00<01:51,  3.46it/s]  1%|          | 4/390 [00:01<01:50,  3.48it/s]  1%|▏         | 5/390 [00:01<01:50,  3.49it/s]  2%|▏         | 6/390 [00:01<01:49,  3.50it/s]  2%|▏         | 7/390 [00:02<01:49,  3.51it/s]  2%|▏         | 8/390 [00:02<01:48,  3.51it/s]  2%|▏         | 9/390 [00:02<01:48,  3.51it/s]  3%|▎         | 10/390 [00:02<01:48,  3.51it/s]  3%|▎         | 11/390 [00:03<01:47,  3.51it/s]  3%|▎         | 12/390 [00:03<01:47,  3.51it/s]  3%|▎         | 13/390 [00:03<01:47,  3.51it/s]  4%|▎         | 14/390 [00:04<01:47,  3.51it/s]  4%|▍         | 15/390 [00:04<01:46,  3.51it/s]  4%|▍         | 16/390 [00:04<01:46,  3.51it/s]  4%|▍         | 17/390 [00:04<01:46,  3.51it/s]  5%|▍         | 18/390 [00:05<01:45,  3.51it/s]  5%|▍         | 19/390 [00:05<01:45,  3.52it/s]  5%|▌         | 20/390 [00:05<01:46,  3.48it/s]  5%|▌         | 21/390 [00:06<01:45,  3.49it/s]  6%|▌         | 22/390 [00:06<01:45,  3.50it/s]  6%|▌         | 23/390 [00:06<01:44,  3.50it/s]  6%|▌         | 24/390 [00:06<01:44,  3.50it/s]  6%|▋         | 25/390 [00:07<01:44,  3.50it/s]  7%|▋         | 26/390 [00:07<01:43,  3.50it/s]  7%|▋         | 27/390 [00:07<01:43,  3.50it/s]  7%|▋         | 28/390 [00:08<01:43,  3.50it/s]  7%|▋         | 29/390 [00:08<01:43,  3.50it/s]  8%|▊         | 30/390 [00:08<01:42,  3.50it/s]  8%|▊         | 31/390 [00:08<01:42,  3.50it/s]  8%|▊         | 32/390 [00:09<01:42,  3.51it/s]  8%|▊         | 33/390 [00:09<01:41,  3.50it/s]  9%|▊         | 34/390 [00:09<01:41,  3.50it/s]  9%|▉         | 35/390 [00:09<01:41,  3.50it/s]  9%|▉         | 36/390 [00:10<01:40,  3.51it/s]  9%|▉         | 37/390 [00:10<01:40,  3.51it/s] 10%|▉         | 38/390 [00:10<01:40,  3.49it/s] 10%|█         | 39/390 [00:11<01:40,  3.50it/s] 10%|█         | 40/390 [00:11<01:40,  3.50it/s] 11%|█         | 41/390 [00:11<01:39,  3.50it/s] 11%|█         | 42/390 [00:12<01:39,  3.50it/s] 11%|█         | 43/390 [00:12<01:39,  3.50it/s] 11%|█▏        | 44/390 [00:12<01:38,  3.50it/s] 12%|█▏        | 45/390 [00:12<01:38,  3.50it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.50it/s] 12%|█▏        | 47/390 [00:13<01:37,  3.50it/s] 12%|█▏        | 48/390 [00:13<01:37,  3.50it/s] 13%|█▎        | 49/390 [00:13<01:37,  3.50it/s] 13%|█▎        | 50/390 [00:14<01:37,  3.50it/s] 13%|█▎        | 51/390 [00:14<01:36,  3.50it/s] 13%|█▎        | 52/390 [00:14<01:36,  3.50it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.50it/s] 14%|█▍        | 54/390 [00:15<01:35,  3.50it/s] 14%|█▍        | 55/390 [00:15<01:35,  3.50it/s] 14%|█▍        | 56/390 [00:15<01:35,  3.49it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.49it/s] 15%|█▍        | 58/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 59/390 [00:16<01:34,  3.50it/s] 15%|█▌        | 60/390 [00:17<01:34,  3.50it/s] 16%|█▌        | 61/390 [00:17<01:33,  3.50it/s] 16%|█▌        | 62/390 [00:17<01:33,  3.50it/s] 16%|█▌        | 63/390 [00:17<01:33,  3.50it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.50it/s] 17%|█▋        | 65/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 66/390 [00:18<01:32,  3.50it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.50it/s] 17%|█▋        | 68/390 [00:19<01:31,  3.50it/s] 18%|█▊        | 69/390 [00:19<01:31,  3.50it/s] 18%|█▊        | 70/390 [00:19<01:31,  3.50it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.50it/s] 18%|█▊        | 72/390 [00:20<01:30,  3.50it/s] 19%|█▊        | 73/390 [00:20<01:30,  3.50it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.48it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.48it/s] 19%|█▉        | 76/390 [00:21<01:30,  3.49it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.49it/s] 20%|██        | 78/390 [00:22<01:29,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 14:49:46,812 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:49:46,813 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 14:49:46,813 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.92it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.36it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.32it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.59it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.14it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.91it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.77it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.57it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.43it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.34it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.39it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.41it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.37it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.28it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.32it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.37it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.38it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.33it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.35it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.32it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.37it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.35it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.35it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.28it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.32it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.34it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.38it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.34it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.31it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.33it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.34it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.34it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.36it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.21it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.26it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.30it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.33it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.25it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.23it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.22it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.31it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.35it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.23it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.33it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.38it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.17it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.27it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.23it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.17it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.30it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.38it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.33it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.25it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.31it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.36it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.33it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.26it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.27it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.27it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.32it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.40it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.37it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.22it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.26it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.36it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.34it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.32it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.30it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.28it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.26it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.23it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.17it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.21it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.27it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.26it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.12it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.07it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.19it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.23it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.25it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.25it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.24it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.24it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.32it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.23it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.16it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.16it/s][A 20%|██        | 78/390 [00:31<01:29,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:49:56,070 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 14:49:56,088 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:49:58,177 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:49:58,191 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:49:58,201 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:15,  5.26s/it] 21%|██        | 80/390 [00:39<19:28,  3.77s/it] 21%|██        | 81/390 [00:39<14:01,  2.72s/it] 21%|██        | 82/390 [00:40<10:13,  1.99s/it] 21%|██▏       | 83/390 [00:40<07:34,  1.48s/it] 22%|██▏       | 84/390 [00:40<05:43,  1.12s/it] 22%|██▏       | 85/390 [00:40<04:25,  1.15it/s] 22%|██▏       | 86/390 [00:41<03:31,  1.44it/s] 22%|██▏       | 87/390 [00:41<02:53,  1.75it/s] 23%|██▎       | 88/390 [00:41<02:26,  2.06it/s] 23%|██▎       | 89/390 [00:42<02:08,  2.34it/s] 23%|██▎       | 90/390 [00:42<01:55,  2.60it/s] 23%|██▎       | 91/390 [00:42<01:46,  2.81it/s] 24%|██▎       | 92/390 [00:42<01:39,  2.99it/s] 24%|██▍       | 93/390 [00:43<01:35,  3.12it/s] 24%|██▍       | 94/390 [00:43<01:31,  3.23it/s] 24%|██▍       | 95/390 [00:43<01:29,  3.30it/s] 25%|██▍       | 96/390 [00:44<01:27,  3.36it/s] 25%|██▍       | 97/390 [00:44<01:26,  3.40it/s] 25%|██▌       | 98/390 [00:44<01:25,  3.43it/s] 25%|██▌       | 99/390 [00:44<01:24,  3.45it/s] 26%|██▌       | 100/390 [00:45<01:23,  3.46it/s] 26%|██▌       | 101/390 [00:45<01:23,  3.47it/s] 26%|██▌       | 102/390 [00:45<01:23,  3.47it/s] 26%|██▋       | 103/390 [00:46<01:22,  3.48it/s] 27%|██▋       | 104/390 [00:46<01:22,  3.48it/s] 27%|██▋       | 105/390 [00:46<01:21,  3.48it/s] 27%|██▋       | 106/390 [00:46<01:21,  3.49it/s] 27%|██▋       | 107/390 [00:47<01:21,  3.49it/s] 28%|██▊       | 108/390 [00:47<01:20,  3.49it/s] 28%|██▊       | 109/390 [00:47<01:20,  3.49it/s] 28%|██▊       | 110/390 [00:48<01:20,  3.49it/s] 28%|██▊       | 111/390 [00:48<01:19,  3.49it/s] 29%|██▊       | 112/390 [00:48<01:19,  3.49it/s] 29%|██▉       | 113/390 [00:48<01:19,  3.47it/s] 29%|██▉       | 114/390 [00:49<01:19,  3.48it/s] 29%|██▉       | 115/390 [00:49<01:19,  3.48it/s] 30%|██▉       | 116/390 [00:49<01:18,  3.48it/s] 30%|███       | 117/390 [00:50<01:18,  3.49it/s] 30%|███       | 118/390 [00:50<01:18,  3.49it/s] 31%|███       | 119/390 [00:50<01:17,  3.49it/s] 31%|███       | 120/390 [00:50<01:17,  3.49it/s] 31%|███       | 121/390 [00:51<01:17,  3.49it/s] 31%|███▏      | 122/390 [00:51<01:16,  3.49it/s] 32%|███▏      | 123/390 [00:51<01:16,  3.49it/s] 32%|███▏      | 124/390 [00:52<01:16,  3.46it/s] 32%|███▏      | 125/390 [00:52<01:18,  3.36it/s] 32%|███▏      | 126/390 [00:52<01:17,  3.40it/s] 33%|███▎      | 127/390 [00:52<01:16,  3.42it/s] 33%|███▎      | 128/390 [00:53<01:16,  3.45it/s] 33%|███▎      | 129/390 [00:53<01:15,  3.46it/s] 33%|███▎      | 130/390 [00:53<01:14,  3.47it/s] 34%|███▎      | 131/390 [00:54<01:14,  3.48it/s] 34%|███▍      | 132/390 [00:54<01:14,  3.48it/s] 34%|███▍      | 133/390 [00:54<01:13,  3.48it/s] 34%|███▍      | 134/390 [00:54<01:13,  3.48it/s] 35%|███▍      | 135/390 [00:55<01:13,  3.47it/s] 35%|███▍      | 136/390 [00:55<01:13,  3.48it/s] 35%|███▌      | 137/390 [00:55<01:12,  3.48it/s] 35%|███▌      | 138/390 [00:56<01:12,  3.49it/s] 36%|███▌      | 139/390 [00:56<01:11,  3.49it/s] 36%|███▌      | 140/390 [00:56<01:11,  3.49it/s] 36%|███▌      | 141/390 [00:56<01:11,  3.49it/s] 36%|███▋      | 142/390 [00:57<01:11,  3.49it/s] 37%|███▋      | 143/390 [00:57<01:10,  3.49it/s] 37%|███▋      | 144/390 [00:57<01:10,  3.49it/s] 37%|███▋      | 145/390 [00:58<01:10,  3.49it/s] 37%|███▋      | 146/390 [00:58<01:10,  3.48it/s] 38%|███▊      | 147/390 [00:58<01:09,  3.48it/s] 38%|███▊      | 148/390 [00:58<01:09,  3.49it/s] 38%|███▊      | 149/390 [00:59<01:09,  3.49it/s] 38%|███▊      | 150/390 [00:59<01:08,  3.49it/s] 39%|███▊      | 151/390 [00:59<01:08,  3.49it/s] 39%|███▉      | 152/390 [01:00<01:08,  3.49it/s] 39%|███▉      | 153/390 [01:00<01:07,  3.49it/s] 39%|███▉      | 154/390 [01:00<01:07,  3.49it/s] 40%|███▉      | 155/390 [01:00<01:07,  3.49it/s] 40%|████      | 156/390 [01:01<01:06,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 14:50:25,774 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:50:25,774 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 14:50:25,774 >>   Batch size = 8
{'eval_loss': 1.0663018226623535, 'eval_runtime': 9.2433, 'eval_samples_per_second': 378.219, 'eval_steps_per_second': 47.277, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.63it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.91it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.20it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.58it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.17it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.95it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.68it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.32it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.32it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.36it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.30it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.19it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.24it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.27it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.24it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.23it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.12it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.09it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.18it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.23it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.12it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.18it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.24it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.27it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.19it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.17it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.10it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.04it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.12it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.19it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.23it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.15it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.13it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.04it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.96it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.01it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.09it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.18it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.15it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.12it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.26it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.18it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.13it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.08it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.98it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.06it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.15it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.23it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.21it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.18it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.22it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.09it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.04it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.03it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.02it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.07it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.18it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.28it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.14it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.17it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.97it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.99it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.08it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.14it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.08it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.01it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.22it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.25it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.09it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.07it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.06it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.01it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.11it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.08it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.04it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.06it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.05it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.12it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.09it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.03it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.04it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.05it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.15it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.10it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.07it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.07it/s][A 40%|████      | 156/390 [01:10<01:06,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:50:35,068 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 14:50:35,090 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:50:37,470 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:50:37,492 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:50:37,503 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:17<20:07,  5.18s/it] 41%|████      | 158/390 [01:18<14:21,  3.71s/it] 41%|████      | 159/390 [01:18<10:20,  2.69s/it] 41%|████      | 160/390 [01:18<07:32,  1.97s/it] 41%|████▏     | 161/390 [01:19<05:34,  1.46s/it] 42%|████▏     | 162/390 [01:19<04:12,  1.11s/it] 42%|████▏     | 163/390 [01:19<03:15,  1.16it/s] 42%|████▏     | 164/390 [01:19<02:35,  1.45it/s] 42%|████▏     | 165/390 [01:20<02:07,  1.76it/s] 43%|████▎     | 166/390 [01:20<01:48,  2.07it/s] 43%|████▎     | 167/390 [01:20<01:34,  2.36it/s] 43%|████▎     | 168/390 [01:21<01:25,  2.61it/s] 43%|████▎     | 169/390 [01:21<01:18,  2.82it/s] 44%|████▎     | 170/390 [01:21<01:13,  2.99it/s] 44%|████▍     | 171/390 [01:21<01:10,  3.12it/s] 44%|████▍     | 172/390 [01:22<01:07,  3.23it/s] 44%|████▍     | 173/390 [01:22<01:05,  3.30it/s] 45%|████▍     | 174/390 [01:22<01:04,  3.36it/s] 45%|████▍     | 175/390 [01:23<01:03,  3.40it/s] 45%|████▌     | 176/390 [01:23<01:02,  3.43it/s] 45%|████▌     | 177/390 [01:23<01:01,  3.44it/s] 46%|████▌     | 178/390 [01:23<01:01,  3.46it/s] 46%|████▌     | 179/390 [01:24<01:00,  3.47it/s] 46%|████▌     | 180/390 [01:24<01:00,  3.46it/s] 46%|████▋     | 181/390 [01:24<01:00,  3.47it/s] 47%|████▋     | 182/390 [01:25<00:59,  3.48it/s] 47%|████▋     | 183/390 [01:25<00:59,  3.48it/s] 47%|████▋     | 184/390 [01:25<00:59,  3.49it/s] 47%|████▋     | 185/390 [01:25<00:58,  3.49it/s] 48%|████▊     | 186/390 [01:26<00:58,  3.49it/s] 48%|████▊     | 187/390 [01:26<00:58,  3.49it/s] 48%|████▊     | 188/390 [01:26<00:57,  3.49it/s] 48%|████▊     | 189/390 [01:27<00:57,  3.49it/s] 49%|████▊     | 190/390 [01:27<00:57,  3.49it/s] 49%|████▉     | 191/390 [01:27<00:57,  3.48it/s] 49%|████▉     | 192/390 [01:27<00:56,  3.48it/s] 49%|████▉     | 193/390 [01:28<00:56,  3.48it/s] 50%|████▉     | 194/390 [01:28<00:56,  3.49it/s] 50%|█████     | 195/390 [01:28<00:55,  3.49it/s] 50%|█████     | 196/390 [01:29<00:55,  3.49it/s] 51%|█████     | 197/390 [01:29<00:55,  3.49it/s] 51%|█████     | 198/390 [01:29<00:54,  3.49it/s] 51%|█████     | 199/390 [01:29<00:54,  3.49it/s] 51%|█████▏    | 200/390 [01:30<00:54,  3.49it/s] 52%|█████▏    | 201/390 [01:30<00:54,  3.49it/s] 52%|█████▏    | 202/390 [01:30<00:53,  3.48it/s] 52%|█████▏    | 203/390 [01:31<00:53,  3.48it/s] 52%|█████▏    | 204/390 [01:31<00:53,  3.48it/s] 53%|█████▎    | 205/390 [01:31<00:53,  3.49it/s] 53%|█████▎    | 206/390 [01:31<00:52,  3.49it/s] 53%|█████▎    | 207/390 [01:32<00:52,  3.49it/s] 53%|█████▎    | 208/390 [01:32<00:52,  3.49it/s] 54%|█████▎    | 209/390 [01:32<00:51,  3.49it/s] 54%|█████▍    | 210/390 [01:33<00:51,  3.49it/s] 54%|█████▍    | 211/390 [01:33<00:51,  3.49it/s] 54%|█████▍    | 212/390 [01:33<00:51,  3.49it/s] 55%|█████▍    | 213/390 [01:33<00:50,  3.48it/s] 55%|█████▍    | 214/390 [01:34<00:50,  3.48it/s] 55%|█████▌    | 215/390 [01:34<00:50,  3.48it/s] 55%|█████▌    | 216/390 [01:34<00:49,  3.48it/s] 56%|█████▌    | 217/390 [01:35<00:49,  3.49it/s] 56%|█████▌    | 218/390 [01:35<00:49,  3.49it/s] 56%|█████▌    | 219/390 [01:35<00:49,  3.49it/s] 56%|█████▋    | 220/390 [01:35<00:48,  3.49it/s] 57%|█████▋    | 221/390 [01:36<00:48,  3.49it/s] 57%|█████▋    | 222/390 [01:36<00:48,  3.49it/s] 57%|█████▋    | 223/390 [01:36<00:47,  3.49it/s] 57%|█████▋    | 224/390 [01:37<00:47,  3.48it/s] 58%|█████▊    | 225/390 [01:37<00:47,  3.48it/s] 58%|█████▊    | 226/390 [01:37<00:47,  3.48it/s] 58%|█████▊    | 227/390 [01:37<00:46,  3.48it/s] 58%|█████▊    | 228/390 [01:38<00:46,  3.49it/s] 59%|█████▊    | 229/390 [01:38<00:46,  3.49it/s] 59%|█████▉    | 230/390 [01:38<00:45,  3.49it/s] 59%|█████▉    | 231/390 [01:39<00:45,  3.49it/s] 59%|█████▉    | 232/390 [01:39<00:45,  3.49it/s] 60%|█████▉    | 233/390 [01:39<00:45,  3.49it/s] 60%|██████    | 234/390 [01:39<00:44,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 14:51:04,459 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:51:04,460 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 14:51:04,460 >>   Batch size = 8
{'eval_loss': 1.0856149196624756, 'eval_runtime': 9.2735, 'eval_samples_per_second': 376.988, 'eval_steps_per_second': 47.123, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.56it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.83it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.25it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.48it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.95it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.74it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.54it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.24it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.21it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.15it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.13it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.12it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.15it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.13it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.10it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.09it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.06it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.97it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.09it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.10it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.96it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.11it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.01it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.05it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.13it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.08it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.96it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.07it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.07it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.99it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.05it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.14it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.02it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.05it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.07it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.97it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.97it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.67it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.06it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.01it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.11it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.00it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.95it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.07it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.00it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.98it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.04it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.89it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.91it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.05it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.98it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.97it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.01it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.02it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.95it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.98it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.98it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.90it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.05it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.10it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.95it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.94it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.99it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.94it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.88it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.91it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.87it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.84it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.92it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.87it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.91it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.88it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.92it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.77it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.89it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.88it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.91it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.88it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.92it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.90it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.91it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.80it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.86it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.87it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.82it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.77it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.77it/s][A 60%|██████    | 234/390 [01:49<00:44,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:51:13,774 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:51:13,791 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:51:16,230 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:51:16,243 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:51:16,252 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:57<13:51,  5.37s/it] 61%|██████    | 236/390 [01:57<09:51,  3.84s/it] 61%|██████    | 237/390 [01:57<07:04,  2.78s/it] 61%|██████    | 238/390 [01:58<05:08,  2.03s/it] 61%|██████▏   | 239/390 [01:58<03:47,  1.51s/it] 62%|██████▏   | 240/390 [01:58<02:51,  1.14s/it] 62%|██████▏   | 241/390 [01:58<02:11,  1.13it/s] 62%|██████▏   | 242/390 [01:59<01:44,  1.42it/s] 62%|██████▏   | 243/390 [01:59<01:25,  1.73it/s] 63%|██████▎   | 244/390 [01:59<01:11,  2.03it/s] 63%|██████▎   | 245/390 [02:00<01:02,  2.32it/s] 63%|██████▎   | 246/390 [02:00<00:55,  2.58it/s] 63%|██████▎   | 247/390 [02:00<00:51,  2.79it/s] 64%|██████▎   | 248/390 [02:00<00:47,  2.97it/s] 64%|██████▍   | 249/390 [02:01<00:45,  3.11it/s] 64%|██████▍   | 250/390 [02:01<00:43,  3.21it/s] 64%|██████▍   | 251/390 [02:01<00:42,  3.29it/s] 65%|██████▍   | 252/390 [02:02<00:41,  3.35it/s] 65%|██████▍   | 253/390 [02:02<00:40,  3.39it/s] 65%|██████▌   | 254/390 [02:02<00:39,  3.41it/s] 65%|██████▌   | 255/390 [02:02<00:39,  3.44it/s] 66%|██████▌   | 256/390 [02:03<00:38,  3.45it/s] 66%|██████▌   | 257/390 [02:03<00:38,  3.46it/s] 66%|██████▌   | 258/390 [02:03<00:38,  3.46it/s] 66%|██████▋   | 259/390 [02:04<00:37,  3.47it/s] 67%|██████▋   | 260/390 [02:04<00:37,  3.47it/s] 67%|██████▋   | 261/390 [02:04<00:37,  3.48it/s] 67%|██████▋   | 262/390 [02:04<00:36,  3.48it/s] 67%|██████▋   | 263/390 [02:05<00:36,  3.49it/s] 68%|██████▊   | 264/390 [02:05<00:36,  3.48it/s] 68%|██████▊   | 265/390 [02:05<00:35,  3.49it/s] 68%|██████▊   | 266/390 [02:06<00:35,  3.49it/s] 68%|██████▊   | 267/390 [02:06<00:35,  3.49it/s] 69%|██████▊   | 268/390 [02:06<00:35,  3.49it/s] 69%|██████▉   | 269/390 [02:06<00:34,  3.46it/s] 69%|██████▉   | 270/390 [02:07<00:34,  3.47it/s] 69%|██████▉   | 271/390 [02:07<00:34,  3.47it/s] 70%|██████▉   | 272/390 [02:07<00:33,  3.48it/s] 70%|███████   | 273/390 [02:08<00:33,  3.48it/s] 70%|███████   | 274/390 [02:08<00:33,  3.48it/s] 71%|███████   | 275/390 [02:08<00:33,  3.48it/s] 71%|███████   | 276/390 [02:08<00:32,  3.48it/s] 71%|███████   | 277/390 [02:09<00:32,  3.49it/s] 71%|███████▏  | 278/390 [02:09<00:32,  3.49it/s] 72%|███████▏  | 279/390 [02:09<00:31,  3.49it/s] 72%|███████▏  | 280/390 [02:10<00:31,  3.47it/s] 72%|███████▏  | 281/390 [02:10<00:31,  3.48it/s] 72%|███████▏  | 282/390 [02:10<00:31,  3.48it/s] 73%|███████▎  | 283/390 [02:10<00:30,  3.48it/s] 73%|███████▎  | 284/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 285/390 [02:11<00:30,  3.48it/s] 73%|███████▎  | 286/390 [02:11<00:29,  3.49it/s] 74%|███████▎  | 287/390 [02:12<00:29,  3.49it/s] 74%|███████▍  | 288/390 [02:12<00:29,  3.49it/s] 74%|███████▍  | 289/390 [02:12<00:28,  3.49it/s] 74%|███████▍  | 290/390 [02:12<00:28,  3.49it/s] 75%|███████▍  | 291/390 [02:13<00:28,  3.44it/s] 75%|███████▍  | 292/390 [02:13<00:28,  3.46it/s] 75%|███████▌  | 293/390 [02:13<00:27,  3.47it/s] 75%|███████▌  | 294/390 [02:14<00:27,  3.47it/s] 76%|███████▌  | 295/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 296/390 [02:14<00:27,  3.48it/s] 76%|███████▌  | 297/390 [02:14<00:26,  3.48it/s] 76%|███████▋  | 298/390 [02:15<00:26,  3.48it/s] 77%|███████▋  | 299/390 [02:15<00:26,  3.48it/s] 77%|███████▋  | 300/390 [02:15<00:25,  3.48it/s] 77%|███████▋  | 301/390 [02:16<00:25,  3.48it/s] 77%|███████▋  | 302/390 [02:16<00:25,  3.48it/s] 78%|███████▊  | 303/390 [02:16<00:24,  3.48it/s] 78%|███████▊  | 304/390 [02:16<00:24,  3.48it/s] 78%|███████▊  | 305/390 [02:17<00:24,  3.48it/s] 78%|███████▊  | 306/390 [02:17<00:24,  3.48it/s] 79%|███████▊  | 307/390 [02:17<00:23,  3.48it/s] 79%|███████▉  | 308/390 [02:18<00:23,  3.48it/s] 79%|███████▉  | 309/390 [02:18<00:23,  3.49it/s] 79%|███████▉  | 310/390 [02:18<00:22,  3.49it/s] 80%|███████▉  | 311/390 [02:18<00:22,  3.48it/s] 80%|████████  | 312/390 [02:19<00:22,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 14:51:43,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:51:43,797 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 14:51:43,797 >>   Batch size = 8
{'eval_loss': 1.1002800464630127, 'eval_runtime': 9.3031, 'eval_samples_per_second': 375.787, 'eval_steps_per_second': 46.973, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.18it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.80it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.03it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.34it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.86it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.46it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.41it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.18it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.05it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.03it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.16it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.10it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.14it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.22it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.10it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.04it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.98it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.89it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.90it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.06it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.07it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.06it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.01it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.04it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.05it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.96it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.97it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.95it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.99it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.08it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.06it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.04it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.04it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.02it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.99it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.99it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.89it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.04it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.06it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.03it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.04it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.07it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.03it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.03it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.97it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.92it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.94it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.72it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.20it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.17it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.07it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.05it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.06it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.01it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.00it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.95it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.97it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.02it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.01it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.00it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.02it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.96it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.92it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.96it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.93it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.93it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.02it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.09it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.03it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.06it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.00it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.89it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.01it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.02it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.93it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.99it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.12it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.03it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.99it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.02it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.95it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.01it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.06it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.90it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.98it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.03it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.03it/s][A 80%|████████  | 312/390 [02:28<00:22,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:51:53,116 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 14:51:53,139 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:51:55,429 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:51:55,444 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:51:55,451 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:36<06:52,  5.36s/it] 81%|████████  | 314/390 [02:36<04:51,  3.84s/it] 81%|████████  | 315/390 [02:37<03:28,  2.77s/it] 81%|████████  | 316/390 [02:37<02:30,  2.03s/it] 81%|████████▏ | 317/390 [02:37<01:49,  1.51s/it] 82%|████████▏ | 318/390 [02:37<01:22,  1.14s/it] 82%|████████▏ | 319/390 [02:38<01:02,  1.13it/s] 82%|████████▏ | 320/390 [02:38<00:49,  1.42it/s] 82%|████████▏ | 321/390 [02:38<00:39,  1.73it/s] 83%|████████▎ | 322/390 [02:39<00:33,  2.04it/s] 83%|████████▎ | 323/390 [02:39<00:28,  2.33it/s] 83%|████████▎ | 324/390 [02:39<00:25,  2.58it/s] 83%|████████▎ | 325/390 [02:39<00:23,  2.79it/s] 84%|████████▎ | 326/390 [02:40<00:21,  2.97it/s] 84%|████████▍ | 327/390 [02:40<00:20,  3.11it/s] 84%|████████▍ | 328/390 [02:40<00:19,  3.21it/s] 84%|████████▍ | 329/390 [02:41<00:18,  3.29it/s] 85%|████████▍ | 330/390 [02:41<00:17,  3.35it/s] 85%|████████▍ | 331/390 [02:41<00:17,  3.39it/s] 85%|████████▌ | 332/390 [02:41<00:16,  3.42it/s] 85%|████████▌ | 333/390 [02:42<00:16,  3.44it/s] 86%|████████▌ | 334/390 [02:42<00:16,  3.45it/s] 86%|████████▌ | 335/390 [02:42<00:15,  3.46it/s] 86%|████████▌ | 336/390 [02:43<00:15,  3.46it/s] 86%|████████▋ | 337/390 [02:43<00:15,  3.47it/s] 87%|████████▋ | 338/390 [02:43<00:14,  3.47it/s] 87%|████████▋ | 339/390 [02:43<00:14,  3.48it/s] 87%|████████▋ | 340/390 [02:44<00:14,  3.48it/s] 87%|████████▋ | 341/390 [02:44<00:14,  3.48it/s] 88%|████████▊ | 342/390 [02:44<00:13,  3.48it/s] 88%|████████▊ | 343/390 [02:45<00:13,  3.49it/s] 88%|████████▊ | 344/390 [02:45<00:13,  3.49it/s] 88%|████████▊ | 345/390 [02:45<00:12,  3.49it/s] 89%|████████▊ | 346/390 [02:45<00:12,  3.49it/s] 89%|████████▉ | 347/390 [02:46<00:12,  3.46it/s] 89%|████████▉ | 348/390 [02:46<00:12,  3.46it/s] 89%|████████▉ | 349/390 [02:46<00:11,  3.47it/s] 90%|████████▉ | 350/390 [02:47<00:11,  3.48it/s] 90%|█████████ | 351/390 [02:47<00:11,  3.48it/s] 90%|█████████ | 352/390 [02:47<00:10,  3.48it/s] 91%|█████████ | 353/390 [02:47<00:10,  3.48it/s] 91%|█████████ | 354/390 [02:48<00:10,  3.48it/s] 91%|█████████ | 355/390 [02:48<00:10,  3.48it/s] 91%|█████████▏| 356/390 [02:48<00:09,  3.49it/s] 92%|█████████▏| 357/390 [02:49<00:09,  3.49it/s] 92%|█████████▏| 358/390 [02:49<00:09,  3.47it/s] 92%|█████████▏| 359/390 [02:49<00:08,  3.47it/s] 92%|█████████▏| 360/390 [02:49<00:08,  3.48it/s] 93%|█████████▎| 361/390 [02:50<00:08,  3.48it/s] 93%|█████████▎| 362/390 [02:50<00:08,  3.48it/s] 93%|█████████▎| 363/390 [02:50<00:07,  3.48it/s] 93%|█████████▎| 364/390 [02:51<00:07,  3.48it/s] 94%|█████████▎| 365/390 [02:51<00:07,  3.48it/s] 94%|█████████▍| 366/390 [02:51<00:06,  3.48it/s] 94%|█████████▍| 367/390 [02:51<00:06,  3.46it/s] 94%|█████████▍| 368/390 [02:52<00:06,  3.47it/s] 95%|█████████▍| 369/390 [02:52<00:06,  3.45it/s] 95%|█████████▍| 370/390 [02:52<00:05,  3.46it/s] 95%|█████████▌| 371/390 [02:53<00:05,  3.47it/s] 95%|█████████▌| 372/390 [02:53<00:05,  3.47it/s] 96%|█████████▌| 373/390 [02:53<00:04,  3.48it/s] 96%|█████████▌| 374/390 [02:54<00:04,  3.48it/s] 96%|█████████▌| 375/390 [02:54<00:04,  3.48it/s] 96%|█████████▋| 376/390 [02:54<00:04,  3.49it/s] 97%|█████████▋| 377/390 [02:54<00:03,  3.49it/s] 97%|█████████▋| 378/390 [02:55<00:03,  3.48it/s] 97%|█████████▋| 379/390 [02:55<00:03,  3.49it/s] 97%|█████████▋| 380/390 [02:55<00:02,  3.47it/s] 98%|█████████▊| 381/390 [02:56<00:02,  3.47it/s] 98%|█████████▊| 382/390 [02:56<00:02,  3.48it/s] 98%|█████████▊| 383/390 [02:56<00:02,  3.48it/s] 98%|█████████▊| 384/390 [02:56<00:01,  3.32it/s] 99%|█████████▊| 385/390 [02:57<00:01,  3.36it/s] 99%|█████████▉| 386/390 [02:57<00:01,  3.39it/s] 99%|█████████▉| 387/390 [02:57<00:00,  3.42it/s] 99%|█████████▉| 388/390 [02:58<00:00,  3.44it/s]100%|█████████▉| 389/390 [02:58<00:00,  3.45it/s]100%|██████████| 390/390 [02:58<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 14:52:23,133 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:52:23,133 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 14:52:23,133 >>   Batch size = 8
{'eval_loss': 1.1061426401138306, 'eval_runtime': 9.2984, 'eval_samples_per_second': 375.979, 'eval_steps_per_second': 46.997, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.73it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.84it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.93it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.29it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.96it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.63it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.41it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.22it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.06it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.07it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.91it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.98it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.08it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.03it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.09it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.15it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.02it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.99it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.04it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.96it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.92it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.97it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.01it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.02it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.05it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.04it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.00it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.93it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.94it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.97it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.00it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.00it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.02it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.93it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.02it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.01it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.96it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.00it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.93it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.96it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.06it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.03it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.99it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.09it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.98it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.95it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.97it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.92it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.94it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.08it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.96it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.96it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.08it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.95it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.94it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 46.96it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.88it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.93it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.98it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.96it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.95it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.92it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.91it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.92it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.91it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.90it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.92it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.01it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.99it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.97it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.06it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.96it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.90it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.99it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.91it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.90it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.98it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.97it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.94it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.02it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.05it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.96it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.94it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 46.91it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.88it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.90it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.90it/s][A100%|██████████| 390/390 [03:07<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:52:32,457 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 14:52:32,481 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:52:35,066 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:52:35,085 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:52:35,097 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:52:40,081 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:52:40,085 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78 (score: 1.0663018226623535).
                                                 100%|██████████| 390/390 [03:17<00:00,  3.46it/s]100%|██████████| 390/390 [03:17<00:00,  1.97it/s]
[INFO|trainer.py:1894] 2023-08-28 14:52:42,375 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 14:52:42,398 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:52:44,953 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:52:44,973 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:52:44,982 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:52:45,182 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:45,182 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:45,182 >>   train_loss               =     0.5496
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:45,182 >>   train_runtime            = 0:03:17.89
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:45,182 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:45,182 >>   train_samples_per_second =    126.302
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:45,182 >>   train_steps_per_second   =      1.971
{'eval_loss': 1.1115003824234009, 'eval_runtime': 9.3043, 'eval_samples_per_second': 375.739, 'eval_steps_per_second': 46.967, 'epoch': 4.99}
{'train_runtime': 197.8985, 'train_samples_per_second': 126.302, 'train_steps_per_second': 1.971, 'train_loss': 0.5496391100761218, 'epoch': 4.99}
08/28/2023 14:52:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:52:45,213 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:52:45,213 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 14:52:45,213 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 57.89it/s]  3%|▎         | 12/437 [00:00<00:08, 51.40it/s]  4%|▍         | 18/437 [00:00<00:08, 49.55it/s]  5%|▌         | 23/437 [00:00<00:08, 48.74it/s]  6%|▋         | 28/437 [00:00<00:08, 48.26it/s]  8%|▊         | 33/437 [00:00<00:08, 48.00it/s]  9%|▊         | 38/437 [00:00<00:08, 47.80it/s] 10%|▉         | 43/437 [00:00<00:08, 47.64it/s] 11%|█         | 48/437 [00:00<00:08, 47.58it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.38it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.31it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.34it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.37it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.31it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.30it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.33it/s] 20%|██        | 88/437 [00:01<00:07, 47.33it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.32it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.34it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.35it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.20it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.20it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.31it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.29it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.24it/s] 30%|███       | 133/437 [00:02<00:06, 47.23it/s] 32%|███▏      | 138/437 [00:02<00:06, 47.27it/s] 33%|███▎      | 143/437 [00:02<00:06, 47.28it/s] 34%|███▍      | 148/437 [00:03<00:06, 47.28it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.17it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.17it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.22it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.21it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.26it/s] 41%|████      | 178/437 [00:03<00:05, 47.31it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.19it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.26it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.23it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.20it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.21it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.20it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.19it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.21it/s] 51%|█████     | 223/437 [00:04<00:04, 47.25it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.24it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.28it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.27it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.27it/s] 57%|█████▋    | 248/437 [00:05<00:03, 47.26it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.24it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.23it/s] 60%|██████    | 263/437 [00:05<00:03, 47.13it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.21it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.19it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.12it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.13it/s] 66%|██████▌   | 288/437 [00:06<00:03, 47.20it/s] 67%|██████▋   | 293/437 [00:06<00:03, 47.15it/s] 68%|██████▊   | 298/437 [00:06<00:02, 47.17it/s] 69%|██████▉   | 303/437 [00:06<00:02, 47.20it/s] 70%|███████   | 308/437 [00:06<00:02, 47.19it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.10it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.20it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.12it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.12it/s] 76%|███████▌  | 333/437 [00:07<00:02, 47.23it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.17it/s] 78%|███████▊  | 343/437 [00:07<00:01, 47.19it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.20it/s] 81%|████████  | 353/437 [00:07<00:01, 47.24it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.22it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.19it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.11it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.04it/s] 86%|████████▋ | 378/437 [00:07<00:01, 47.14it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.09it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.19it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.25it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.16it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.27it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.06it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.99it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.09it/s] 97%|█████████▋| 423/437 [00:08<00:00, 47.10it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.01it/s] 99%|█████████▉| 433/437 [00:09<00:00, 47.17it/s]100%|██████████| 437/437 [00:09<00:00, 47.29it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:52:54,474 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   eval_loss               =     1.0663
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   eval_runtime            = 0:00:09.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   eval_samples_per_second =    377.495
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   eval_steps_per_second   =     47.187
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:52:54,475 >>   perplexity              =     2.9046
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:00,999 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:01,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:01,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:01,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:01,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:53:01,618 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:53:01,620 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:53:02,195 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:53:03,214 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:53:03,215 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:06,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:06,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:06,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:06,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:53:06,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:53:06,709 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:53:06,710 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:53:07,293 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:53:07,442 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:53:07,443 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.52it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.50it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:29,  1.55it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:31,  1.56it/s]Extractor Predicting: 48it [00:31,  1.58it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:33,  1.56it/s]Extractor Predicting: 51it [00:33,  1.57it/s]Extractor Predicting: 52it [00:34,  1.60it/s]Extractor Predicting: 53it [00:34,  1.57it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:36,  1.59it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:38,  1.52it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:44,  1.46it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:46,  1.52it/s]Extractor Predicting: 72it [00:47,  1.51it/s]Extractor Predicting: 73it [00:48,  1.41it/s]Extractor Predicting: 74it [00:49,  1.43it/s]Extractor Predicting: 75it [00:49,  1.43it/s]Extractor Predicting: 76it [00:50,  1.43it/s]Extractor Predicting: 77it [00:51,  1.46it/s]Extractor Predicting: 78it [00:51,  1.45it/s]Extractor Predicting: 79it [00:52,  1.45it/s]Extractor Predicting: 80it [00:53,  1.45it/s]Extractor Predicting: 81it [00:53,  1.47it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:55,  1.45it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.48it/s]Extractor Predicting: 87it [00:57,  1.45it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [00:59,  1.48it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:01,  1.53it/s]Extractor Predicting: 93it [01:01,  1.54it/s]Extractor Predicting: 94it [01:02,  1.52it/s]Extractor Predicting: 95it [01:03,  1.53it/s]Extractor Predicting: 96it [01:03,  1.51it/s]Extractor Predicting: 97it [01:04,  1.49it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.47it/s]Extractor Predicting: 102it [01:07,  1.44it/s]Extractor Predicting: 103it [01:08,  1.45it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.51it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:11,  1.48it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.47it/s]Extractor Predicting: 111it [01:14,  1.47it/s]Extractor Predicting: 112it [01:14,  1.49it/s]Extractor Predicting: 113it [01:15,  1.49it/s]Extractor Predicting: 114it [01:16,  1.49it/s]Extractor Predicting: 115it [01:16,  1.49it/s]Extractor Predicting: 116it [01:17,  1.54it/s]Extractor Predicting: 117it [01:17,  1.51it/s]Extractor Predicting: 118it [01:18,  1.50it/s]Extractor Predicting: 119it [01:19,  1.46it/s]Extractor Predicting: 120it [01:20,  1.49it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.48it/s]Extractor Predicting: 123it [01:22,  1.48it/s]Extractor Predicting: 124it [01:22,  1.46it/s]Extractor Predicting: 125it [01:23,  1.44it/s]Extractor Predicting: 126it [01:24,  1.47it/s]Extractor Predicting: 127it [01:24,  1.49it/s]Extractor Predicting: 128it [01:25,  1.50it/s]Extractor Predicting: 129it [01:26,  1.51it/s]Extractor Predicting: 130it [01:26,  1.55it/s]Extractor Predicting: 131it [01:27,  1.55it/s]Extractor Predicting: 132it [01:27,  1.54it/s]Extractor Predicting: 133it [01:28,  1.53it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:29,  1.53it/s]Extractor Predicting: 136it [01:30,  1.52it/s]Extractor Predicting: 137it [01:31,  1.51it/s]Extractor Predicting: 138it [01:31,  1.51it/s]Extractor Predicting: 139it [01:32,  1.52it/s]Extractor Predicting: 140it [01:33,  1.50it/s]Extractor Predicting: 141it [01:33,  1.51it/s]Extractor Predicting: 142it [01:34,  1.48it/s]Extractor Predicting: 143it [01:35,  1.48it/s]Extractor Predicting: 144it [01:35,  1.63it/s]Extractor Predicting: 144it [01:35,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:50,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:50,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:50,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:50,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:50,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:54:51,474 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:54:51,475 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:54:52,046 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:54:53,076 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:54:53,076 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:55,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:55,949 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:55,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:55,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:54:55,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:54:56,595 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:54:56,596 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:54:57,179 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:54:57,345 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:54:57,345 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4915254237288136,
  "recall": 0.13272311212814644,
  "score": 0.20900900900900898,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.70it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:13,  1.75it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:14,  1.64it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.61it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.52it/s]Extractor Predicting: 31it [00:18,  1.48it/s]Extractor Predicting: 32it [00:19,  1.43it/s]Extractor Predicting: 33it [00:20,  1.43it/s]Extractor Predicting: 34it [00:21,  1.41it/s]Extractor Predicting: 35it [00:21,  1.43it/s]Extractor Predicting: 36it [00:22,  1.43it/s]Extractor Predicting: 37it [00:23,  1.43it/s]Extractor Predicting: 38it [00:23,  1.40it/s]Extractor Predicting: 39it [00:24,  1.40it/s]Extractor Predicting: 40it [00:25,  1.38it/s]Extractor Predicting: 41it [00:26,  1.37it/s]Extractor Predicting: 42it [00:26,  1.39it/s]Extractor Predicting: 43it [00:27,  1.40it/s]Extractor Predicting: 44it [00:28,  1.42it/s]Extractor Predicting: 45it [00:28,  1.42it/s]Extractor Predicting: 46it [00:29,  1.44it/s]Extractor Predicting: 47it [00:30,  1.43it/s]Extractor Predicting: 48it [00:30,  1.40it/s]Extractor Predicting: 49it [00:31,  1.41it/s]Extractor Predicting: 50it [00:32,  1.41it/s]Extractor Predicting: 51it [00:33,  1.40it/s]Extractor Predicting: 52it [00:33,  1.41it/s]Extractor Predicting: 53it [00:34,  1.41it/s]Extractor Predicting: 54it [00:35,  1.42it/s]Extractor Predicting: 55it [00:35,  1.42it/s]Extractor Predicting: 56it [00:36,  1.39it/s]Extractor Predicting: 57it [00:37,  1.42it/s]Extractor Predicting: 58it [00:37,  1.43it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:40,  1.43it/s]Extractor Predicting: 62it [00:40,  1.43it/s]Extractor Predicting: 63it [00:41,  1.46it/s]Extractor Predicting: 64it [00:42,  1.48it/s]Extractor Predicting: 65it [00:42,  1.49it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:45,  1.53it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:46,  1.59it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:47,  1.52it/s]Extractor Predicting: 74it [00:48,  1.50it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.48it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:51,  1.48it/s]Extractor Predicting: 80it [00:52,  1.48it/s]Extractor Predicting: 81it [00:53,  1.47it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.49it/s]Extractor Predicting: 84it [00:55,  1.47it/s]Extractor Predicting: 85it [00:55,  1.49it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.55it/s]Extractor Predicting: 88it [00:57,  1.59it/s]Extractor Predicting: 89it [00:58,  1.60it/s]Extractor Predicting: 90it [00:59,  1.46it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [01:00,  1.55it/s]Extractor Predicting: 93it [01:01,  1.61it/s]Extractor Predicting: 94it [01:01,  1.59it/s]Extractor Predicting: 95it [01:02,  1.59it/s]Extractor Predicting: 96it [01:02,  1.60it/s]Extractor Predicting: 97it [01:03,  1.64it/s]Extractor Predicting: 98it [01:04,  1.65it/s]Extractor Predicting: 99it [01:04,  1.66it/s]Extractor Predicting: 100it [01:05,  1.71it/s]Extractor Predicting: 101it [01:05,  1.69it/s]Extractor Predicting: 102it [01:06,  1.63it/s]Extractor Predicting: 103it [01:07,  1.61it/s]Extractor Predicting: 104it [01:07,  1.62it/s]Extractor Predicting: 105it [01:08,  1.65it/s]Extractor Predicting: 106it [01:08,  1.64it/s]Extractor Predicting: 107it [01:09,  1.71it/s]Extractor Predicting: 108it [01:10,  1.65it/s]Extractor Predicting: 109it [01:10,  1.67it/s]Extractor Predicting: 110it [01:11,  1.67it/s]Extractor Predicting: 111it [01:11,  1.67it/s]Extractor Predicting: 112it [01:12,  1.69it/s]Extractor Predicting: 113it [01:13,  1.66it/s]Extractor Predicting: 114it [01:13,  1.64it/s]Extractor Predicting: 115it [01:14,  1.61it/s]Extractor Predicting: 116it [01:15,  1.58it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.55it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:17,  1.51it/s]Extractor Predicting: 121it [01:18,  1.53it/s]Extractor Predicting: 122it [01:18,  1.52it/s]Extractor Predicting: 123it [01:19,  1.50it/s]Extractor Predicting: 124it [01:20,  1.55it/s]Extractor Predicting: 125it [01:20,  1.52it/s]Extractor Predicting: 126it [01:21,  1.53it/s]Extractor Predicting: 127it [01:22,  1.51it/s]Extractor Predicting: 128it [01:22,  1.49it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.49it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:25,  1.48it/s]Extractor Predicting: 133it [01:26,  1.50it/s]Extractor Predicting: 134it [01:26,  1.51it/s]Extractor Predicting: 135it [01:27,  1.50it/s]Extractor Predicting: 136it [01:28,  1.46it/s]Extractor Predicting: 137it [01:29,  1.50it/s]Extractor Predicting: 138it [01:29,  1.49it/s]Extractor Predicting: 139it [01:30,  1.49it/s]Extractor Predicting: 140it [01:31,  1.49it/s]Extractor Predicting: 141it [01:31,  1.51it/s]Extractor Predicting: 142it [01:32,  1.49it/s]Extractor Predicting: 143it [01:32,  1.85it/s]Extractor Predicting: 143it [01:32,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:35,486 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:35,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:35,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:35,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:35,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:56:35,852 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:56:35,853 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:56:36,117 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:56:37,130 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:56:37,130 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:40,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:40,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:40,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:40,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:56:40,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:56:40,653 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:56:40,655 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:56:41,255 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:56:41,401 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:56:41,402 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.49453551912568305,
  "recall": 0.106158357771261,
  "score": 0.17479478512795749,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 2it [00:01,  1.34it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:56:43,243 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:56:43,244 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:56:43,246 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:56:43,247 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:56:43,250 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:56:46,398 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:56:46,407 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:56:46,426 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:56:46,427 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:56:46,435 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:56:46,444 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:56:46,444 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:56:46,444 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:56:46,444 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:56:46,444 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:56:46,444 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.05555555555555555,
  "score": 0.09999999999999999,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:56:46,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:47,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:48,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:48,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:49,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:50,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:51,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:52,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:52,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:53,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:54,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:54,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:55,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:56,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:57,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:57,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:58,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:56:59,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:00,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:01,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:02,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:26, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 14:57:03,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:03,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:04,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:05,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:06,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:06,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:07,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:08,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:09,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:09,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:10,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:11,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:12,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:12,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:13,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:14,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:14,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:15,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:16,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:17,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:03, 15.40s/it][WARNING|generation_utils.py:914] 2023-08-28 14:57:17,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:18,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:19,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:20,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:20,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:21,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:22,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:22,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:23,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:24,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:25,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:25,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:26,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:27,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:28,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:29,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:30,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:30,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:31,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:32,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:33,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:47<01:50, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-28 14:57:34,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:34,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:35,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:36,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:37,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:37,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:38,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:39,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:39,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:40,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:40,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:41,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:42,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:43,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:43,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:44,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:45,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:45,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:46,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:47,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:47,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:01<01:30, 15.14s/it][WARNING|generation_utils.py:914] 2023-08-28 14:57:48,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:48,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:49,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:50,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:51,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:51,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:52,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:53,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:53,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:54,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:55,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:55,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:56,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:57,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:58,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:58,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:57:59,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:00,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:01,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:01,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:02,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:03,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:17<01:17, 15.46s/it][WARNING|generation_utils.py:914] 2023-08-28 14:58:04,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:04,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:05,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:06,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:07,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:07,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:08,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:09,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:09,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:10,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:11,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:12,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:12,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:13,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:14,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:14,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:15,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:16,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:17,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:17,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:18,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:19,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:33<01:02, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-28 14:58:20,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:20,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:21,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:22,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:23,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:23,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:24,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:25,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:26,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:26,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:27,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:28,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:28,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:29,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:30,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:31,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:33,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:33,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:34,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:35,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:47, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-28 14:58:35,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:36,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:37,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:37,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:38,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:39,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:39,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:40,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:41,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:41,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:42,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:43,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:43,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:44,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:45,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:46,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:46,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:47,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:48,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:48,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:49,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:03<00:30, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-28 14:58:50,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:50,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:51,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:52,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:52,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:53,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:54,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:55,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:55,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:56,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:57,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:57,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:58,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:59,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:58:59,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:00,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:01,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:02,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:02,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:03,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:04,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:04,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:18<00:15, 15.31s/it][WARNING|generation_utils.py:914] 2023-08-28 14:59:05,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:06,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:07,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:07,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:08,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:09,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:10,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:10,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:11,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:12,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:12,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:14,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:14,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:15,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:16,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:16,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:18,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:18,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:19,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:20,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:20,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:21,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:59:22,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:36<00:00, 15.89s/it]Generating: 100%|██████████| 10/10 [02:36<00:00, 15.62s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:29,036 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:29,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:29,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:29,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:29,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:59:29,655 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:59:29,656 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:59:30,229 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:59:31,287 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:59:31,287 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:34,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:34,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:34,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:34,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:59:34,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:59:34,785 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:59:34,787 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:59:35,341 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:59:35,505 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:59:35,505 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : original language of film or TV show .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8934659090909091, 'errors': {''}}
['Relation : occupation . Context : Later in 1847 , the United States appointed him a brigadier general . Head Entity : William I , Tail Entity : United States .\n']
['Relation : occupation . Context : Later in 1847 , the United States appointed him a brigadier general . Head Entity : William I , Tail Entity : United States .\n', 'Relation : occupation . Context : James Louis Russell Frederick ( 1774–1821 ) was an American lawyer who served as Secretary of the Navy from 1825 to 1828 . Head Entity : James Louis Russell Frederick , Tail Entity : United States Navy .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Anne Aloysius , Duke of Salisbury\', \'occupation\', \'\', "His first wife , Miss Margaret Wills ( 1828–1908 ) , was the daughter of William Henry VII \'s first wife , Anne Aloysius , Duke of Salisbury .")', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 6962
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7062, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.32it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:04,  1.54it/s]Extractor Estimating: 7it [00:04,  1.45it/s]Extractor Estimating: 8it [00:05,  1.47it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.59it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.54it/s]Extractor Estimating: 14it [00:09,  1.52it/s]Extractor Estimating: 15it [00:09,  1.54it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:11,  1.54it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.54it/s]Extractor Estimating: 21it [00:13,  1.52it/s]Extractor Estimating: 22it [00:14,  1.51it/s]Extractor Estimating: 23it [00:15,  1.57it/s]Extractor Estimating: 24it [00:15,  1.57it/s]Extractor Estimating: 25it [00:16,  1.58it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:17,  1.48it/s]Extractor Estimating: 28it [00:18,  1.50it/s]Extractor Estimating: 29it [00:19,  1.51it/s]Extractor Estimating: 30it [00:19,  1.48it/s]Extractor Estimating: 31it [00:20,  1.46it/s]Extractor Estimating: 32it [00:21,  1.44it/s]Extractor Estimating: 33it [00:21,  1.52it/s]Extractor Estimating: 34it [00:22,  1.56it/s]Extractor Estimating: 35it [00:23,  1.53it/s]Extractor Estimating: 36it [00:23,  1.54it/s]Extractor Estimating: 37it [00:24,  1.58it/s]Extractor Estimating: 38it [00:24,  1.55it/s]Extractor Estimating: 39it [00:25,  1.56it/s]Extractor Estimating: 40it [00:26,  1.51it/s]Extractor Estimating: 41it [00:26,  1.53it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:28,  1.51it/s]Extractor Estimating: 44it [00:28,  1.53it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:30,  1.52it/s]Extractor Estimating: 47it [00:30,  1.46it/s]Extractor Estimating: 48it [00:31,  1.46it/s]Extractor Estimating: 49it [00:32,  1.51it/s]Extractor Estimating: 50it [00:32,  1.55it/s]Extractor Estimating: 51it [00:33,  1.56it/s]Extractor Estimating: 52it [00:34,  1.60it/s]Extractor Estimating: 53it [00:34,  1.58it/s]Extractor Estimating: 54it [00:35,  1.56it/s]Extractor Estimating: 55it [00:35,  1.63it/s]Extractor Estimating: 56it [00:36,  1.63it/s]Extractor Estimating: 57it [00:37,  1.64it/s]Extractor Estimating: 58it [00:37,  1.66it/s]Extractor Estimating: 59it [00:38,  1.69it/s]Extractor Estimating: 60it [00:39,  1.56it/s]Extractor Estimating: 61it [00:39,  1.55it/s]Extractor Estimating: 62it [00:40,  1.49it/s]Extractor Estimating: 63it [00:41,  1.52it/s]Extractor Estimating: 64it [00:41,  1.53it/s]Extractor Estimating: 65it [00:42,  1.55it/s]Extractor Estimating: 66it [00:42,  1.59it/s]Extractor Estimating: 67it [00:43,  1.57it/s]Extractor Estimating: 68it [00:44,  1.56it/s]Extractor Estimating: 69it [00:44,  1.59it/s]Extractor Estimating: 70it [00:45,  1.56it/s]Extractor Estimating: 71it [00:46,  1.57it/s]Extractor Estimating: 72it [00:46,  1.56it/s]Extractor Estimating: 73it [00:47,  1.56it/s]Extractor Estimating: 74it [00:48,  1.57it/s]Extractor Estimating: 75it [00:48,  1.54it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:50,  1.54it/s]Extractor Estimating: 78it [00:50,  1.49it/s]Extractor Estimating: 79it [00:51,  1.57it/s]Extractor Estimating: 80it [00:51,  1.55it/s]Extractor Estimating: 81it [00:52,  1.57it/s]Extractor Estimating: 82it [00:53,  1.62it/s]Extractor Estimating: 83it [00:53,  1.58it/s]Extractor Estimating: 84it [00:54,  1.57it/s]Extractor Estimating: 85it [00:55,  1.55it/s]Extractor Estimating: 86it [00:55,  1.57it/s]Extractor Estimating: 87it [00:56,  1.60it/s]Extractor Estimating: 88it [00:56,  1.62it/s]Extractor Estimating: 89it [00:57,  1.59it/s]Extractor Estimating: 90it [00:58,  1.55it/s]Extractor Estimating: 91it [00:58,  1.57it/s]Extractor Estimating: 92it [00:59,  1.63it/s]Extractor Estimating: 93it [01:00,  1.59it/s]Extractor Estimating: 94it [01:00,  1.58it/s]Extractor Estimating: 95it [01:01,  1.60it/s]Extractor Estimating: 96it [01:02,  1.61it/s]Extractor Estimating: 97it [01:02,  1.62it/s]Extractor Estimating: 98it [01:03,  1.59it/s]Extractor Estimating: 99it [01:03,  1.62it/s]Extractor Estimating: 100it [01:04,  1.65it/s]Extractor Estimating: 101it [01:05,  1.65it/s]Extractor Estimating: 102it [01:05,  1.65it/s]Extractor Estimating: 103it [01:06,  1.69it/s]Extractor Estimating: 104it [01:06,  1.63it/s]Extractor Estimating: 105it [01:07,  1.67it/s]Extractor Estimating: 106it [01:08,  1.65it/s]Extractor Estimating: 107it [01:08,  1.73it/s]Extractor Estimating: 108it [01:09,  1.74it/s]Extractor Estimating: 109it [01:09,  1.72it/s]Extractor Estimating: 110it [01:10,  1.72it/s]Extractor Estimating: 111it [01:10,  1.75it/s]Extractor Estimating: 112it [01:11,  1.78it/s]Extractor Estimating: 113it [01:11,  1.82it/s]Extractor Estimating: 114it [01:12,  1.77it/s]Extractor Estimating: 115it [01:13,  1.78it/s]Extractor Estimating: 116it [01:13,  1.73it/s]Extractor Estimating: 117it [01:14,  1.69it/s]Extractor Estimating: 118it [01:14,  1.73it/s]Extractor Estimating: 119it [01:15,  1.56it/s]Extractor Estimating: 120it [01:16,  1.61it/s]Extractor Estimating: 121it [01:16,  1.61it/s]Extractor Estimating: 122it [01:17,  1.63it/s]Extractor Estimating: 123it [01:18,  1.58it/s]Extractor Estimating: 124it [01:18,  1.63it/s]Extractor Estimating: 125it [01:19,  1.68it/s]Extractor Estimating: 126it [01:19,  1.69it/s]Extractor Estimating: 127it [01:20,  1.69it/s]Extractor Estimating: 128it [01:20,  1.73it/s]Extractor Estimating: 129it [01:21,  1.71it/s]Extractor Estimating: 130it [01:22,  1.68it/s]Extractor Estimating: 131it [01:22,  1.68it/s]Extractor Estimating: 132it [01:23,  1.76it/s]Extractor Estimating: 133it [01:23,  1.83it/s]Extractor Estimating: 134it [01:24,  1.82it/s]Extractor Estimating: 135it [01:24,  1.78it/s]Extractor Estimating: 136it [01:25,  1.72it/s]Extractor Estimating: 137it [01:26,  1.74it/s]Extractor Estimating: 138it [01:26,  1.74it/s]Extractor Estimating: 139it [01:27,  1.77it/s]Extractor Estimating: 140it [01:27,  1.77it/s]Extractor Estimating: 141it [01:28,  1.76it/s]Extractor Estimating: 142it [01:28,  1.76it/s]Extractor Estimating: 143it [01:29,  1.81it/s]Extractor Estimating: 144it [01:30,  1.77it/s]Extractor Estimating: 145it [01:30,  1.78it/s]Extractor Estimating: 146it [01:31,  1.74it/s]Extractor Estimating: 147it [01:31,  1.58it/s]Extractor Estimating: 148it [01:32,  1.65it/s]Extractor Estimating: 149it [01:33,  1.67it/s]Extractor Estimating: 150it [01:33,  1.71it/s]Extractor Estimating: 151it [01:34,  1.66it/s]Extractor Estimating: 152it [01:34,  1.66it/s]Extractor Estimating: 153it [01:35,  1.67it/s]Extractor Estimating: 154it [01:36,  1.66it/s]Extractor Estimating: 155it [01:36,  1.67it/s]Extractor Estimating: 156it [01:37,  1.63it/s]Extractor Estimating: 157it [01:37,  1.65it/s]Extractor Estimating: 158it [01:38,  1.65it/s]Extractor Estimating: 159it [01:39,  1.68it/s]Extractor Estimating: 160it [01:39,  1.65it/s]Extractor Estimating: 161it [01:40,  1.66it/s]Extractor Estimating: 162it [01:40,  1.64it/s]Extractor Estimating: 163it [01:41,  1.66it/s]Extractor Estimating: 164it [01:42,  1.68it/s]Extractor Estimating: 165it [01:42,  1.71it/s]Extractor Estimating: 166it [01:43,  1.67it/s]Extractor Estimating: 167it [01:43,  1.68it/s]Extractor Estimating: 168it [01:44,  1.70it/s]Extractor Estimating: 169it [01:45,  1.69it/s]Extractor Estimating: 170it [01:45,  1.65it/s]Extractor Estimating: 171it [01:46,  1.62it/s]Extractor Estimating: 172it [01:47,  1.59it/s]Extractor Estimating: 173it [01:47,  1.58it/s]Extractor Estimating: 174it [01:48,  1.64it/s]Extractor Estimating: 175it [01:48,  1.66it/s]Extractor Estimating: 176it [01:49,  1.63it/s]Extractor Estimating: 177it [01:50,  1.58it/s]Extractor Estimating: 178it [01:50,  1.55it/s]Extractor Estimating: 179it [01:51,  1.53it/s]Extractor Estimating: 180it [01:52,  1.58it/s]Extractor Estimating: 181it [01:52,  1.65it/s]Extractor Estimating: 182it [01:53,  1.64it/s]Extractor Estimating: 183it [01:53,  1.64it/s]Extractor Estimating: 184it [01:54,  1.60it/s]Extractor Estimating: 185it [01:55,  1.56it/s]Extractor Estimating: 186it [01:55,  1.58it/s]Extractor Estimating: 187it [01:56,  1.59it/s]Extractor Estimating: 188it [01:57,  1.58it/s]Extractor Estimating: 189it [01:57,  1.55it/s]Extractor Estimating: 190it [01:58,  1.53it/s]Extractor Estimating: 191it [01:59,  1.54it/s]Extractor Estimating: 192it [01:59,  1.57it/s]Extractor Estimating: 193it [02:00,  1.48it/s]Extractor Estimating: 194it [02:00,  1.55it/s]Extractor Estimating: 195it [02:01,  1.59it/s]Extractor Estimating: 196it [02:02,  1.62it/s]Extractor Estimating: 197it [02:02,  1.58it/s]Extractor Estimating: 198it [02:03,  1.58it/s]Extractor Estimating: 199it [02:04,  1.56it/s]Extractor Estimating: 200it [02:04,  1.56it/s]Extractor Estimating: 201it [02:05,  1.58it/s]Extractor Estimating: 202it [02:05,  1.61it/s]Extractor Estimating: 203it [02:06,  1.61it/s]Extractor Estimating: 204it [02:07,  1.63it/s]Extractor Estimating: 205it [02:07,  1.68it/s]Extractor Estimating: 206it [02:08,  1.67it/s]Extractor Estimating: 207it [02:08,  1.65it/s]Extractor Estimating: 208it [02:09,  1.64it/s]Extractor Estimating: 209it [02:10,  1.67it/s]Extractor Estimating: 210it [02:10,  1.68it/s]Extractor Estimating: 211it [02:11,  1.70it/s]Extractor Estimating: 212it [02:11,  1.70it/s]Extractor Estimating: 213it [02:12,  1.71it/s]Extractor Estimating: 214it [02:13,  1.72it/s]Extractor Estimating: 215it [02:13,  1.77it/s]Extractor Estimating: 216it [02:14,  1.75it/s]Extractor Estimating: 217it [02:14,  1.67it/s]Extractor Estimating: 218it [02:15,  1.68it/s]Extractor Estimating: 219it [02:15,  1.71it/s]Extractor Estimating: 220it [02:16,  1.70it/s]Extractor Estimating: 221it [02:17,  1.53it/s]Extractor Estimating: 222it [02:17,  1.60it/s]Extractor Estimating: 223it [02:18,  1.62it/s]Extractor Estimating: 224it [02:19,  1.69it/s]Extractor Estimating: 225it [02:19,  1.71it/s]Extractor Estimating: 226it [02:20,  1.71it/s]Extractor Estimating: 227it [02:20,  1.65it/s]Extractor Estimating: 228it [02:21,  1.71it/s]Extractor Estimating: 229it [02:22,  1.70it/s]Extractor Estimating: 230it [02:22,  1.71it/s]Extractor Estimating: 231it [02:23,  1.68it/s]Extractor Estimating: 232it [02:23,  1.71it/s]Extractor Estimating: 233it [02:24,  1.76it/s]Extractor Estimating: 234it [02:24,  1.76it/s]Extractor Estimating: 235it [02:25,  1.77it/s]Extractor Estimating: 236it [02:26,  1.73it/s]Extractor Estimating: 237it [02:26,  1.74it/s]Extractor Estimating: 238it [02:27,  1.72it/s]Extractor Estimating: 239it [02:27,  1.74it/s]Extractor Estimating: 240it [02:28,  1.76it/s]Extractor Estimating: 241it [02:28,  1.71it/s]Extractor Estimating: 242it [02:29,  1.68it/s]Extractor Estimating: 243it [02:30,  1.73it/s]Extractor Estimating: 244it [02:30,  1.73it/s]Extractor Estimating: 245it [02:31,  1.70it/s]Extractor Estimating: 246it [02:31,  1.72it/s]Extractor Estimating: 247it [02:32,  1.71it/s]Extractor Estimating: 248it [02:33,  1.72it/s]Extractor Estimating: 249it [02:33,  1.72it/s]Extractor Estimating: 250it [02:34,  1.84it/s]Extractor Estimating: 250it [02:34,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:21,183 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:21,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:21,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:21,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:21,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:02:21,800 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:02:21,801 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:02:22,347 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:02:23,365 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:02:23,365 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:26,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:26,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:26,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:26,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:02:26,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:02:26,835 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:02:26,836 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:02:27,427 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:02:27,568 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:02:27,568 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:36:47,031 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:36:47,114 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4998 mean pseudo reward: 0.9194709356109712
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl'}
train vocab size: 15989
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16089, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16089, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.089, loss:785.2407
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.107, loss:724.9009
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.073, loss:678.4576
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.101, loss:685.2765
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.091, loss:643.8832
>> valid entity prec:0.5824, rec:0.6012, f1:0.5916
>> valid relation prec:0.4321, rec:0.1402, f1:0.2117
>> valid relation with NER prec:0.4321, rec:0.1402, f1:0.2117
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.458, loss:639.9294
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.084, loss:597.2733
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.095, loss:646.0843
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.095, loss:583.3123
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.086, loss:609.2702
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5771, rec:0.5838, f1:0.5804
>> valid relation prec:0.3622, rec:0.1436, f1:0.2057
>> valid relation with NER prec:0.3622, rec:0.1436, f1:0.2057
g_step 1100, step 55, avg_time 2.461, loss:564.9084
g_step 1200, step 155, avg_time 1.102, loss:599.0281
g_step 1300, step 46, avg_time 1.079, loss:541.5455
g_step 1400, step 146, avg_time 1.096, loss:536.7130
g_step 1500, step 37, avg_time 1.095, loss:564.9732
>> valid entity prec:0.5613, rec:0.5395, f1:0.5502
>> valid relation prec:0.2998, rec:0.1304, f1:0.1818
>> valid relation with NER prec:0.2998, rec:0.1304, f1:0.1818
g_step 1600, step 137, avg_time 2.460, loss:501.9507
g_step 1700, step 28, avg_time 1.090, loss:506.3103
g_step 1800, step 128, avg_time 1.094, loss:490.9810
g_step 1900, step 19, avg_time 1.089, loss:509.8541
g_step 2000, step 119, avg_time 1.095, loss:447.9614
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5540, rec:0.5721, f1:0.5629
>> valid relation prec:0.2604, rec:0.1373, f1:0.1798
>> valid relation with NER prec:0.2604, rec:0.1373, f1:0.1798
g_step 2100, step 10, avg_time 2.457, loss:487.6945
g_step 2200, step 110, avg_time 1.094, loss:419.5479
g_step 2300, step 1, avg_time 1.091, loss:448.2897
g_step 2400, step 101, avg_time 1.098, loss:419.3832
g_step 2500, step 201, avg_time 1.099, loss:416.2798
>> valid entity prec:0.5412, rec:0.5684, f1:0.5545
>> valid relation prec:0.2604, rec:0.1324, f1:0.1756
>> valid relation with NER prec:0.2604, rec:0.1324, f1:0.1756
g_step 2600, step 92, avg_time 2.437, loss:378.4318
g_step 2700, step 192, avg_time 1.102, loss:424.0790
g_step 2800, step 83, avg_time 1.091, loss:377.4356
g_step 2900, step 183, avg_time 1.104, loss:400.4791
g_step 3000, step 74, avg_time 1.099, loss:371.9480
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5877, rec:0.5008, f1:0.5408
>> valid relation prec:0.3374, rec:0.1427, f1:0.2006
>> valid relation with NER prec:0.3374, rec:0.1427, f1:0.2006
g_step 3100, step 174, avg_time 2.447, loss:382.8099
g_step 3200, step 65, avg_time 1.082, loss:344.7240
g_step 3300, step 165, avg_time 1.094, loss:351.7702
g_step 3400, step 56, avg_time 1.090, loss:356.3907
g_step 3500, step 156, avg_time 1.096, loss:328.6829
>> valid entity prec:0.5547, rec:0.5508, f1:0.5527
>> valid relation prec:0.2550, rec:0.1322, f1:0.1741
>> valid relation with NER prec:0.2550, rec:0.1322, f1:0.1741
g_step 3600, step 47, avg_time 2.453, loss:316.9410
g_step 3700, step 147, avg_time 1.088, loss:326.2706
g_step 3800, step 38, avg_time 1.088, loss:303.6042
g_step 3900, step 138, avg_time 1.098, loss:306.2213
g_step 4000, step 29, avg_time 1.087, loss:308.7897
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5684, rec:0.5248, f1:0.5457
>> valid relation prec:0.2995, rec:0.1433, f1:0.1938
>> valid relation with NER prec:0.2995, rec:0.1433, f1:0.1938
g_step 4100, step 129, avg_time 2.460, loss:286.4095
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:36:47 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:36:47 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-36-47_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:36:48 - WARNING - datasets.builder -   Using custom data configuration default-6f63a6fdf31f8001
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6f63a6fdf31f8001/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:36:48,736 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:36:48,737 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:36:48,738 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:36:48,739 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:36:48,758 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:36:48,763 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:36:48,763 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:36:48,763 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:36:48,763 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:36:48,763 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:36:48,763 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:36:49,113 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:36:52,371 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:36:52,371 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6f63a6fdf31f8001/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.52ba/s] 40%|████      | 2/5 [00:00<00:01,  2.56ba/s] 60%|██████    | 3/5 [00:01<00:00,  3.29ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.79ba/s]100%|██████████| 5/5 [00:01<00:00,  4.13ba/s]100%|██████████| 5/5 [00:01<00:00,  3.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.08ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.74ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.02ba/s]100%|██████████| 4/4 [00:00<00:00,  5.10ba/s]100%|██████████| 4/4 [00:00<00:00,  4.44ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  7.90ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.67ba/s]100%|██████████| 5/5 [00:00<00:00,  9.99ba/s]100%|██████████| 5/5 [00:00<00:00,  9.78ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.18ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.94ba/s]100%|██████████| 4/4 [00:00<00:00, 11.30ba/s]
[INFO|trainer.py:414] 2023-08-28 16:36:56,236 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:36:56,278 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:36:56,278 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 16:36:56,278 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:36:56,278 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:36:56,278 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:36:56,278 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:36:56,278 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:59,  3.24it/s]  1%|          | 2/390 [00:00<01:54,  3.40it/s]  1%|          | 3/390 [00:00<01:52,  3.45it/s]  1%|          | 4/390 [00:01<01:51,  3.48it/s]  1%|▏         | 5/390 [00:01<01:50,  3.48it/s]  2%|▏         | 6/390 [00:01<01:49,  3.49it/s]  2%|▏         | 7/390 [00:02<01:49,  3.50it/s]  2%|▏         | 8/390 [00:02<01:49,  3.50it/s]  2%|▏         | 9/390 [00:02<01:48,  3.50it/s]  3%|▎         | 10/390 [00:02<01:48,  3.50it/s]  3%|▎         | 11/390 [00:03<01:48,  3.51it/s]  3%|▎         | 12/390 [00:03<01:47,  3.51it/s]  3%|▎         | 13/390 [00:03<01:47,  3.51it/s]  4%|▎         | 14/390 [00:04<01:47,  3.48it/s]  4%|▍         | 15/390 [00:04<01:47,  3.49it/s]  4%|▍         | 16/390 [00:04<01:46,  3.50it/s]  4%|▍         | 17/390 [00:04<01:46,  3.50it/s]  5%|▍         | 18/390 [00:05<01:46,  3.50it/s]  5%|▍         | 19/390 [00:05<01:45,  3.50it/s]  5%|▌         | 20/390 [00:05<01:45,  3.50it/s]  5%|▌         | 21/390 [00:06<01:45,  3.50it/s]  6%|▌         | 22/390 [00:06<01:45,  3.50it/s]  6%|▌         | 23/390 [00:06<01:44,  3.50it/s]  6%|▌         | 24/390 [00:06<01:44,  3.50it/s]  6%|▋         | 25/390 [00:07<01:54,  3.20it/s]  7%|▋         | 26/390 [00:07<01:50,  3.28it/s]  7%|▋         | 27/390 [00:07<01:48,  3.35it/s]  7%|▋         | 28/390 [00:08<01:46,  3.39it/s]  7%|▋         | 29/390 [00:08<01:45,  3.43it/s]  8%|▊         | 30/390 [00:08<01:44,  3.45it/s]  8%|▊         | 31/390 [00:08<01:43,  3.46it/s]  8%|▊         | 32/390 [00:09<01:43,  3.47it/s]  8%|▊         | 33/390 [00:09<01:42,  3.48it/s]  9%|▊         | 34/390 [00:09<01:41,  3.49it/s]  9%|▉         | 35/390 [00:10<01:41,  3.50it/s]  9%|▉         | 36/390 [00:10<01:49,  3.25it/s]  9%|▉         | 37/390 [00:10<01:46,  3.32it/s] 10%|▉         | 38/390 [00:11<01:44,  3.37it/s] 10%|█         | 39/390 [00:11<01:42,  3.41it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:11<01:40,  3.46it/s] 11%|█         | 42/390 [00:12<01:40,  3.47it/s] 11%|█         | 43/390 [00:12<01:39,  3.48it/s] 11%|█▏        | 44/390 [00:12<01:39,  3.49it/s] 12%|█▏        | 45/390 [00:13<01:38,  3.49it/s] 12%|█▏        | 46/390 [00:13<01:38,  3.50it/s] 12%|█▏        | 47/390 [00:13<01:42,  3.36it/s] 12%|█▏        | 48/390 [00:13<01:40,  3.40it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 51/390 [00:14<01:37,  3.47it/s] 13%|█▎        | 52/390 [00:15<01:37,  3.48it/s] 14%|█▎        | 53/390 [00:15<01:36,  3.48it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.49it/s] 14%|█▍        | 55/390 [00:15<01:35,  3.50it/s] 14%|█▍        | 56/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.50it/s] 15%|█▍        | 58/390 [00:16<01:41,  3.28it/s] 15%|█▌        | 59/390 [00:17<01:39,  3.34it/s] 15%|█▌        | 60/390 [00:17<01:37,  3.39it/s] 16%|█▌        | 61/390 [00:17<01:36,  3.42it/s] 16%|█▌        | 62/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.46it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.47it/s] 17%|█▋        | 65/390 [00:18<01:33,  3.48it/s] 17%|█▋        | 66/390 [00:19<01:32,  3.49it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.49it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.49it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.43it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.45it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.46it/s] 18%|█▊        | 72/390 [00:20<01:31,  3.47it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.48it/s] 19%|█▉        | 74/390 [00:21<01:30,  3.49it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.49it/s] 19%|█▉        | 76/390 [00:22<01:29,  3.49it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.49it/s] 20%|██        | 78/390 [00:22<01:29,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 16:37:18,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:37:18,901 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 16:37:18,901 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.32it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.14it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.30it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.63it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.31it/s][A
  8%|▊         | 33/437 [00:00<00:08, 48.03it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.72it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.48it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.40it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.42it/s][A
 13%|█▎        | 58/437 [00:01<00:07, 47.44it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.43it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.31it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.36it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.34it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.39it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.35it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.30it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.35it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.37it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.42it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.36it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.28it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.33it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.23it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.30it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.24it/s][A
 33%|███▎      | 143/437 [00:02<00:06, 47.22it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.27it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.32it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.36it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.36it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.35it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.26it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.19it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.29it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.29it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.15it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.27it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.33it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.36it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.29it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.30it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.24it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.24it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.19it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.27it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.27it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.28it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.23it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.35it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.34it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.30it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.32it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.24it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.23it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.25it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.31it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.27it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.27it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.37it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.31it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.35it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.34it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.29it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.15it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.21it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.22it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.26it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.23it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.39it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.28it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.29it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.31it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.32it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.25it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.13it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.25it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.20it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.22it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.30it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.35it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.26it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.22it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.26it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.27it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 47.27it/s][A 20%|██        | 78/390 [00:31<01:29,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:37:28,169 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 16:37:28,253 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:37:30,796 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:37:31,036 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:37:31,107 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:45<36:50,  7.11s/it] 21%|██        | 80/390 [00:45<26:09,  5.06s/it] 21%|██        | 81/390 [00:46<18:41,  3.63s/it] 21%|██        | 82/390 [00:46<13:28,  2.63s/it] 21%|██▏       | 83/390 [00:46<09:50,  1.92s/it] 22%|██▏       | 84/390 [00:47<07:18,  1.43s/it] 22%|██▏       | 85/390 [00:47<05:31,  1.09s/it] 22%|██▏       | 86/390 [00:47<04:17,  1.18it/s] 22%|██▏       | 87/390 [00:47<03:25,  1.47it/s] 23%|██▎       | 88/390 [00:48<02:49,  1.78it/s] 23%|██▎       | 89/390 [00:48<02:23,  2.09it/s] 23%|██▎       | 90/390 [00:48<02:06,  2.38it/s] 23%|██▎       | 91/390 [00:49<01:54,  2.61it/s] 24%|██▎       | 92/390 [00:49<01:46,  2.81it/s] 24%|██▍       | 93/390 [00:49<01:39,  2.99it/s] 24%|██▍       | 94/390 [00:49<01:34,  3.13it/s] 24%|██▍       | 95/390 [00:50<01:31,  3.23it/s] 25%|██▍       | 96/390 [00:50<01:28,  3.30it/s] 25%|██▍       | 97/390 [00:50<01:27,  3.36it/s] 25%|██▌       | 98/390 [00:51<01:38,  2.96it/s] 25%|██▌       | 99/390 [00:51<01:34,  3.09it/s] 26%|██▌       | 100/390 [00:51<01:30,  3.20it/s] 26%|██▌       | 101/390 [00:52<01:29,  3.22it/s] 26%|██▌       | 102/390 [00:52<01:27,  3.30it/s] 26%|██▋       | 103/390 [00:52<01:25,  3.36it/s] 27%|██▋       | 104/390 [00:52<01:24,  3.40it/s] 27%|██▋       | 105/390 [00:53<01:23,  3.43it/s] 27%|██▋       | 106/390 [00:53<01:22,  3.45it/s] 27%|██▋       | 107/390 [00:53<01:21,  3.46it/s] 28%|██▊       | 108/390 [00:54<01:21,  3.47it/s] 28%|██▊       | 109/390 [00:54<01:20,  3.48it/s] 28%|██▊       | 110/390 [00:54<01:20,  3.48it/s] 28%|██▊       | 111/390 [00:54<01:20,  3.48it/s] 29%|██▊       | 112/390 [00:55<01:21,  3.42it/s] 29%|██▉       | 113/390 [00:55<01:20,  3.45it/s] 29%|██▉       | 114/390 [00:55<01:19,  3.46it/s] 29%|██▉       | 115/390 [00:56<01:19,  3.47it/s] 30%|██▉       | 116/390 [00:56<01:18,  3.48it/s] 30%|███       | 117/390 [00:56<01:18,  3.48it/s] 30%|███       | 118/390 [00:56<01:18,  3.48it/s] 31%|███       | 119/390 [00:57<01:17,  3.49it/s] 31%|███       | 120/390 [00:57<01:17,  3.49it/s] 31%|███       | 121/390 [00:57<01:17,  3.49it/s] 31%|███▏      | 122/390 [00:58<01:16,  3.49it/s] 32%|███▏      | 123/390 [00:58<01:17,  3.46it/s] 32%|███▏      | 124/390 [00:58<01:16,  3.47it/s] 32%|███▏      | 125/390 [00:58<01:16,  3.48it/s] 32%|███▏      | 126/390 [00:59<01:15,  3.48it/s] 33%|███▎      | 127/390 [00:59<01:15,  3.48it/s] 33%|███▎      | 128/390 [00:59<01:15,  3.49it/s] 33%|███▎      | 129/390 [01:00<01:14,  3.49it/s] 33%|███▎      | 130/390 [01:00<01:14,  3.49it/s] 34%|███▎      | 131/390 [01:00<01:14,  3.49it/s] 34%|███▍      | 132/390 [01:00<01:13,  3.49it/s] 34%|███▍      | 133/390 [01:01<01:13,  3.49it/s] 34%|███▍      | 134/390 [01:01<01:14,  3.44it/s] 35%|███▍      | 135/390 [01:01<01:13,  3.46it/s] 35%|███▍      | 136/390 [01:02<01:13,  3.47it/s] 35%|███▌      | 137/390 [01:02<01:12,  3.48it/s] 35%|███▌      | 138/390 [01:02<01:12,  3.48it/s] 36%|███▌      | 139/390 [01:02<01:12,  3.48it/s] 36%|███▌      | 140/390 [01:03<01:11,  3.49it/s] 36%|███▌      | 141/390 [01:03<01:11,  3.49it/s] 36%|███▋      | 142/390 [01:03<01:11,  3.49it/s] 37%|███▋      | 143/390 [01:04<01:10,  3.49it/s] 37%|███▋      | 144/390 [01:04<01:10,  3.49it/s] 37%|███▋      | 145/390 [01:04<01:10,  3.49it/s] 37%|███▋      | 146/390 [01:04<01:09,  3.49it/s] 38%|███▊      | 147/390 [01:05<01:09,  3.50it/s] 38%|███▊      | 148/390 [01:05<01:09,  3.49it/s] 38%|███▊      | 149/390 [01:05<01:09,  3.49it/s] 38%|███▊      | 150/390 [01:06<01:08,  3.49it/s] 39%|███▊      | 151/390 [01:06<01:08,  3.49it/s] 39%|███▉      | 152/390 [01:06<01:08,  3.50it/s] 39%|███▉      | 153/390 [01:07<01:07,  3.49it/s] 39%|███▉      | 154/390 [01:07<01:07,  3.49it/s] 40%|███▉      | 155/390 [01:07<01:07,  3.49it/s] 40%|████      | 156/390 [01:07<01:06,  3.50it/s][INFO|trainer.py:2140] 2023-08-28 16:38:04,183 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:38:04,184 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 16:38:04,184 >>   Batch size = 8
{'eval_loss': 1.1235755681991577, 'eval_runtime': 9.2459, 'eval_samples_per_second': 378.115, 'eval_steps_per_second': 47.264, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.83it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.25it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.35it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.70it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.52it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.46it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.45it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.23it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.31it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.35it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.26it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.34it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.24it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.24it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.28it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.33it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.20it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.14it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.32it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.32it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.21it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.27it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.26it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.25it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.25it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.23it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.26it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.28it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.32it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 47.32it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.18it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.29it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.13it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.17it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.22it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.21it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.19it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.27it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.23it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 47.23it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 47.17it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 47.21it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.21it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.24it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.26it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.25it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.23it/s][A
 57%|█████▋    | 248/437 [00:05<00:03, 47.26it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.28it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.30it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.22it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.22it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.24it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.21it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.28it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.27it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.21it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.25it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.25it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.27it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.24it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.58it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.77it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.91it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.04it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.13it/s][A
 78%|███████▊  | 343/437 [00:07<00:01, 47.15it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 47.21it/s][A
 81%|████████  | 353/437 [00:07<00:01, 47.16it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.18it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.16it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.24it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.21it/s][A
 86%|████████▋ | 378/437 [00:07<00:01, 47.22it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.26it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.22it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.24it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.26it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.28it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.25it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.25it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.21it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.22it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.25it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.23it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.23it/s][A 40%|████      | 156/390 [01:17<01:06,  3.50it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:38:13,475 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 16:38:13,535 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:38:18,545 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:38:18,604 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:38:18,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:33<30:48,  7.94s/it] 41%|████      | 158/390 [01:33<21:49,  5.64s/it] 41%|████      | 159/390 [01:34<15:32,  4.04s/it] 41%|████      | 160/390 [01:34<11:09,  2.91s/it] 41%|████▏     | 161/390 [01:34<08:06,  2.12s/it] 42%|████▏     | 162/390 [01:35<05:58,  1.57s/it] 42%|████▏     | 163/390 [01:35<04:29,  1.19s/it] 42%|████▏     | 164/390 [01:35<03:26,  1.09it/s] 42%|████▏     | 165/390 [01:35<02:43,  1.38it/s] 43%|████▎     | 166/390 [01:36<02:13,  1.68it/s] 43%|████▎     | 167/390 [01:36<01:51,  1.99it/s] 43%|████▎     | 168/390 [01:36<01:36,  2.29it/s] 43%|████▎     | 169/390 [01:37<01:32,  2.38it/s] 44%|████▎     | 170/390 [01:37<01:23,  2.64it/s] 44%|████▍     | 171/390 [01:37<01:16,  2.85it/s] 44%|████▍     | 172/390 [01:38<01:12,  3.02it/s] 44%|████▍     | 173/390 [01:38<01:08,  3.15it/s] 45%|████▍     | 174/390 [01:38<01:06,  3.25it/s] 45%|████▍     | 175/390 [01:38<01:04,  3.32it/s] 45%|████▌     | 176/390 [01:39<01:03,  3.37it/s] 45%|████▌     | 177/390 [01:39<01:02,  3.41it/s] 46%|████▌     | 178/390 [01:39<01:01,  3.44it/s] 46%|████▌     | 179/390 [01:40<01:01,  3.45it/s] 46%|████▌     | 180/390 [01:40<01:06,  3.14it/s] 46%|████▋     | 181/390 [01:40<01:04,  3.24it/s] 47%|████▋     | 182/390 [01:40<01:02,  3.31it/s] 47%|████▋     | 183/390 [01:41<01:01,  3.37it/s] 47%|████▋     | 184/390 [01:41<01:00,  3.40it/s] 47%|████▋     | 185/390 [01:41<00:59,  3.43it/s] 48%|████▊     | 186/390 [01:42<00:59,  3.45it/s] 48%|████▊     | 187/390 [01:42<00:58,  3.46it/s] 48%|████▊     | 188/390 [01:42<00:58,  3.47it/s] 48%|████▊     | 189/390 [01:42<00:57,  3.48it/s] 49%|████▊     | 190/390 [01:43<00:57,  3.48it/s] 49%|████▉     | 191/390 [01:43<01:08,  2.90it/s] 49%|████▉     | 192/390 [01:44<01:04,  3.06it/s] 49%|████▉     | 193/390 [01:44<01:02,  3.18it/s] 50%|████▉     | 194/390 [01:44<00:59,  3.27it/s] 50%|█████     | 195/390 [01:44<00:58,  3.33it/s] 50%|█████     | 196/390 [01:45<00:57,  3.38it/s] 51%|█████     | 197/390 [01:45<00:56,  3.41it/s] 51%|█████     | 198/390 [01:45<00:55,  3.44it/s] 51%|█████     | 199/390 [01:46<00:55,  3.46it/s] 51%|█████▏    | 200/390 [01:46<00:54,  3.47it/s] 52%|█████▏    | 201/390 [01:46<01:02,  3.02it/s] 52%|█████▏    | 202/390 [01:47<00:59,  3.15it/s] 52%|█████▏    | 203/390 [01:47<00:57,  3.24it/s] 52%|█████▏    | 204/390 [01:47<00:56,  3.32it/s] 53%|█████▎    | 205/390 [01:47<00:54,  3.37it/s] 53%|█████▎    | 206/390 [01:48<00:54,  3.41it/s] 53%|█████▎    | 207/390 [01:48<00:53,  3.43it/s] 53%|█████▎    | 208/390 [01:48<00:52,  3.45it/s] 54%|█████▎    | 209/390 [01:49<00:52,  3.46it/s] 54%|█████▍    | 210/390 [01:49<01:03,  2.85it/s] 54%|█████▍    | 211/390 [01:49<00:59,  3.01it/s] 54%|█████▍    | 212/390 [01:50<00:56,  3.14it/s] 55%|█████▍    | 213/390 [01:50<00:54,  3.24it/s] 55%|█████▍    | 214/390 [01:50<00:53,  3.31it/s] 55%|█████▌    | 215/390 [01:50<00:52,  3.36it/s] 55%|█████▌    | 216/390 [01:51<00:52,  3.30it/s] 56%|█████▌    | 217/390 [01:51<00:52,  3.32it/s] 56%|█████▌    | 218/390 [01:51<00:51,  3.37it/s] 56%|█████▌    | 219/390 [01:52<00:50,  3.41it/s] 56%|█████▋    | 220/390 [01:52<00:49,  3.43it/s] 57%|█████▋    | 221/390 [01:52<00:48,  3.45it/s] 57%|█████▋    | 222/390 [01:53<00:48,  3.46it/s] 57%|█████▋    | 223/390 [01:53<00:48,  3.47it/s] 57%|█████▋    | 224/390 [01:53<00:47,  3.48it/s] 58%|█████▊    | 225/390 [01:53<00:47,  3.48it/s] 58%|█████▊    | 226/390 [01:54<00:47,  3.49it/s] 58%|█████▊    | 227/390 [01:54<00:46,  3.49it/s] 58%|█████▊    | 228/390 [01:54<00:46,  3.45it/s] 59%|█████▊    | 229/390 [01:55<00:46,  3.47it/s] 59%|█████▉    | 230/390 [01:55<00:46,  3.47it/s] 59%|█████▉    | 231/390 [01:55<00:45,  3.48it/s] 59%|█████▉    | 232/390 [01:55<00:45,  3.48it/s] 60%|█████▉    | 233/390 [01:56<00:45,  3.49it/s] 60%|██████    | 234/390 [01:56<00:44,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 16:38:52,792 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:38:52,793 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 16:38:52,793 >>   Batch size = 8
{'eval_loss': 1.1514886617660522, 'eval_runtime': 9.2617, 'eval_samples_per_second': 377.47, 'eval_steps_per_second': 47.184, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.57it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.17it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.20it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.49it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.13it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.91it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.71it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.37it/s][A
 11%|█         | 48/437 [00:01<00:08, 47.24it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.17it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.23it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.34it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.35it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.29it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.32it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.35it/s][A
 20%|██        | 88/437 [00:01<00:07, 47.23it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.14it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 47.01it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 47.01it/s][A
 25%|██▍       | 108/437 [00:02<00:06, 47.07it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 47.05it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.17it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.17it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 47.29it/s][A
 30%|███       | 133/437 [00:02<00:06, 47.27it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.10it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.09it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 47.03it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.99it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 47.08it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 47.23it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 47.29it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.24it/s][A
 41%|████      | 178/437 [00:03<00:05, 47.28it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 47.31it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 47.16it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.20it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 45.84it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.28it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.68it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.86it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.94it/s][A
 51%|█████     | 223/437 [00:04<00:04, 47.09it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 47.06it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.05it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.01it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.95it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.05it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.19it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 47.22it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.15it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.28it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.26it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.17it/s][A
 65%|██████▍   | 283/437 [00:05<00:03, 47.02it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.08it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.07it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 47.13it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.22it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.21it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.19it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.28it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.24it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 47.14it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.05it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.05it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.60it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.81it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.99it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 47.04it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 47.16it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.23it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 47.29it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.19it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.10it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.09it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.08it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.09it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.22it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.21it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.22it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.30it/s][A
 97%|█████████▋| 423/437 [00:08<00:00, 47.30it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.28it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.13it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.13it/s][A 60%|██████    | 234/390 [02:05<00:44,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:39:02,136 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:39:02,203 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:39:07,329 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:39:07,413 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:39:07,436 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:25<23:04,  8.93s/it] 61%|██████    | 236/390 [02:25<16:21,  6.37s/it] 61%|██████    | 237/390 [02:26<11:35,  4.55s/it] 61%|██████    | 238/390 [02:26<08:16,  3.27s/it] 61%|██████▏   | 239/390 [02:26<05:58,  2.37s/it] 62%|██████▏   | 240/390 [02:27<04:22,  1.75s/it] 62%|██████▏   | 241/390 [02:27<03:14,  1.31s/it] 62%|██████▏   | 242/390 [02:27<02:28,  1.00s/it] 62%|██████▏   | 243/390 [02:27<01:55,  1.27it/s] 63%|██████▎   | 244/390 [02:28<01:32,  1.57it/s] 63%|██████▎   | 245/390 [02:28<01:16,  1.88it/s] 63%|██████▎   | 246/390 [02:28<01:05,  2.19it/s] 63%|██████▎   | 247/390 [02:29<01:05,  2.17it/s] 64%|██████▎   | 248/390 [02:29<00:57,  2.45it/s] 64%|██████▍   | 249/390 [02:29<00:52,  2.70it/s] 64%|██████▍   | 250/390 [02:30<00:48,  2.89it/s] 64%|██████▍   | 251/390 [02:30<00:45,  3.05it/s] 65%|██████▍   | 252/390 [02:30<00:43,  3.18it/s] 65%|██████▍   | 253/390 [02:31<00:41,  3.27it/s] 65%|██████▌   | 254/390 [02:31<00:40,  3.33it/s] 65%|██████▌   | 255/390 [02:31<00:39,  3.38it/s] 66%|██████▌   | 256/390 [02:31<00:39,  3.42it/s] 66%|██████▌   | 257/390 [02:32<00:38,  3.42it/s] 66%|██████▌   | 258/390 [02:32<00:38,  3.45it/s] 66%|██████▋   | 259/390 [02:32<00:37,  3.46it/s] 67%|██████▋   | 260/390 [02:33<00:37,  3.48it/s] 67%|██████▋   | 261/390 [02:33<00:37,  3.49it/s] 67%|██████▋   | 262/390 [02:33<00:36,  3.49it/s] 67%|██████▋   | 263/390 [02:33<00:36,  3.49it/s] 68%|██████▊   | 264/390 [02:34<00:36,  3.50it/s] 68%|██████▊   | 265/390 [02:34<00:35,  3.50it/s] 68%|██████▊   | 266/390 [02:34<00:35,  3.50it/s] 68%|██████▊   | 267/390 [02:35<00:35,  3.50it/s] 69%|██████▊   | 268/390 [02:35<00:42,  2.89it/s] 69%|██████▉   | 269/390 [02:35<00:39,  3.05it/s] 69%|██████▉   | 270/390 [02:36<00:37,  3.17it/s] 69%|██████▉   | 271/390 [02:36<00:36,  3.26it/s] 70%|██████▉   | 272/390 [02:36<00:35,  3.33it/s] 70%|███████   | 273/390 [02:36<00:34,  3.38it/s] 70%|███████   | 274/390 [02:37<00:33,  3.41it/s] 71%|███████   | 275/390 [02:37<00:33,  3.44it/s] 71%|███████   | 276/390 [02:37<00:32,  3.46it/s] 71%|███████   | 277/390 [02:38<00:32,  3.47it/s] 71%|███████▏  | 278/390 [02:38<00:41,  2.67it/s] 72%|███████▏  | 279/390 [02:38<00:38,  2.88it/s] 72%|███████▏  | 280/390 [02:39<00:36,  3.04it/s] 72%|███████▏  | 281/390 [02:39<00:34,  3.16it/s] 72%|███████▏  | 282/390 [02:39<00:33,  3.26it/s] 73%|███████▎  | 283/390 [02:40<00:32,  3.32it/s] 73%|███████▎  | 284/390 [02:40<00:31,  3.37it/s] 73%|███████▎  | 285/390 [02:40<00:30,  3.41it/s] 73%|███████▎  | 286/390 [02:40<00:30,  3.43it/s] 74%|███████▎  | 287/390 [02:41<00:29,  3.45it/s] 74%|███████▍  | 288/390 [02:41<00:29,  3.43it/s] 74%|███████▍  | 289/390 [02:41<00:29,  3.45it/s] 74%|███████▍  | 290/390 [02:42<00:28,  3.46it/s] 75%|███████▍  | 291/390 [02:42<00:28,  3.47it/s] 75%|███████▍  | 292/390 [02:42<00:28,  3.48it/s] 75%|███████▌  | 293/390 [02:42<00:27,  3.48it/s] 75%|███████▌  | 294/390 [02:43<00:27,  3.48it/s] 76%|███████▌  | 295/390 [02:43<00:27,  3.49it/s] 76%|███████▌  | 296/390 [02:43<00:26,  3.49it/s] 76%|███████▌  | 297/390 [02:44<00:26,  3.49it/s] 76%|███████▋  | 298/390 [02:44<00:26,  3.49it/s] 77%|███████▋  | 299/390 [02:44<00:26,  3.49it/s] 77%|███████▋  | 300/390 [02:44<00:25,  3.49it/s] 77%|███████▋  | 301/390 [02:45<00:25,  3.49it/s] 77%|███████▋  | 302/390 [02:45<00:25,  3.49it/s] 78%|███████▊  | 303/390 [02:45<00:24,  3.49it/s] 78%|███████▊  | 304/390 [02:46<00:24,  3.49it/s] 78%|███████▊  | 305/390 [02:46<00:24,  3.49it/s] 78%|███████▊  | 306/390 [02:46<00:24,  3.49it/s] 79%|███████▊  | 307/390 [02:46<00:23,  3.49it/s] 79%|███████▉  | 308/390 [02:47<00:23,  3.49it/s] 79%|███████▉  | 309/390 [02:47<00:23,  3.49it/s] 79%|███████▉  | 310/390 [02:47<00:23,  3.47it/s] 80%|███████▉  | 311/390 [02:48<00:22,  3.48it/s] 80%|████████  | 312/390 [02:48<00:22,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 16:39:44,712 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:39:44,712 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 16:39:44,712 >>   Batch size = 8
{'eval_loss': 1.1630629301071167, 'eval_runtime': 9.2793, 'eval_samples_per_second': 376.751, 'eval_steps_per_second': 47.094, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.85it/s][A
  3%|▎         | 12/437 [00:00<00:08, 51.05it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.32it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.64it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.22it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.95it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.80it/s][A
 10%|▉         | 43/437 [00:00<00:09, 41.95it/s][A
 11%|█         | 48/437 [00:01<00:08, 43.66it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 44.78it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.55it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 42.92it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 44.19it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 45.03it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 45.70it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.24it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.49it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 46.36it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.64it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.65it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.80it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.98it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.04it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.87it/s][A
 30%|███       | 133/437 [00:02<00:07, 41.34it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 43.10it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 44.24it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 45.01it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 45.69it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.20it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.61it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.87it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 47.00it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.80it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.82it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.95it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 47.05it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 47.06it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 42.98it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 44.27it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 44.95it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 45.80it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.29it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.63it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.85it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 47.09it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.81it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.64it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.69it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.95it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.14it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 47.18it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 47.24it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 47.29it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 47.33it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.11it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 47.01it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.89it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 47.07it/s][A
 70%|███████   | 308/437 [00:06<00:02, 47.01it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 47.13it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 47.23it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 47.25it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 47.31it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 47.32it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 47.18it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 45.92it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.67it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.71it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.79it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 47.02it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 47.13it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 47.17it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 47.09it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 47.03it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 47.05it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 47.12it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 47.18it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 47.02it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 47.05it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 47.20it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 47.22it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 47.18it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 47.15it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 47.15it/s][A 80%|████████  | 312/390 [02:57<00:22,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:39:54,257 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 16:39:54,309 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:39:59,174 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:39:59,213 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:39:59,235 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:19<12:05,  9.42s/it] 81%|████████  | 314/390 [03:19<08:29,  6.70s/it] 81%|████████  | 315/390 [03:19<05:58,  4.78s/it] 81%|████████  | 316/390 [03:20<04:13,  3.43s/it] 81%|████████▏ | 317/390 [03:20<03:01,  2.49s/it] 82%|████████▏ | 318/390 [03:20<02:11,  1.83s/it] 82%|████████▏ | 319/390 [03:20<01:36,  1.36s/it] 82%|████████▏ | 320/390 [03:21<01:12,  1.04s/it] 82%|████████▏ | 321/390 [03:21<00:56,  1.23it/s] 83%|████████▎ | 322/390 [03:21<00:44,  1.53it/s] 83%|████████▎ | 323/390 [03:22<00:36,  1.84it/s] 83%|████████▎ | 324/390 [03:22<00:30,  2.14it/s] 83%|████████▎ | 325/390 [03:22<00:26,  2.42it/s] 84%|████████▎ | 326/390 [03:22<00:23,  2.67it/s] 84%|████████▍ | 327/390 [03:23<00:21,  2.88it/s] 84%|████████▍ | 328/390 [03:23<00:20,  3.04it/s] 84%|████████▍ | 329/390 [03:23<00:22,  2.69it/s] 85%|████████▍ | 330/390 [03:24<00:20,  2.90it/s] 85%|████████▍ | 331/390 [03:24<00:19,  3.05it/s] 85%|████████▌ | 332/390 [03:24<00:18,  3.18it/s] 85%|████████▌ | 333/390 [03:25<00:17,  3.27it/s] 86%|████████▌ | 334/390 [03:25<00:16,  3.34it/s] 86%|████████▌ | 335/390 [03:25<00:16,  3.39it/s] 86%|████████▌ | 336/390 [03:25<00:15,  3.42it/s] 86%|████████▋ | 337/390 [03:26<00:15,  3.45it/s] 87%|████████▋ | 338/390 [03:26<00:15,  3.46it/s] 87%|████████▋ | 339/390 [03:27<00:18,  2.77it/s] 87%|████████▋ | 340/390 [03:27<00:16,  2.96it/s] 87%|████████▋ | 341/390 [03:27<00:15,  3.10it/s] 88%|████████▊ | 342/390 [03:27<00:14,  3.21it/s] 88%|████████▊ | 343/390 [03:28<00:14,  3.30it/s] 88%|████████▊ | 344/390 [03:28<00:13,  3.36it/s] 88%|████████▊ | 345/390 [03:28<00:13,  3.40it/s] 89%|████████▊ | 346/390 [03:29<00:12,  3.43it/s] 89%|████████▉ | 347/390 [03:29<00:12,  3.45it/s] 89%|████████▉ | 348/390 [03:29<00:12,  3.47it/s] 89%|████████▉ | 349/390 [03:30<00:13,  3.02it/s] 90%|████████▉ | 350/390 [03:30<00:12,  3.15it/s] 90%|█████████ | 351/390 [03:30<00:12,  3.25it/s] 90%|█████████ | 352/390 [03:30<00:11,  3.32it/s] 91%|█████████ | 353/390 [03:31<00:10,  3.37it/s] 91%|█████████ | 354/390 [03:31<00:10,  3.41it/s] 91%|█████████ | 355/390 [03:31<00:10,  3.43it/s] 91%|█████████▏| 356/390 [03:32<00:09,  3.46it/s] 92%|█████████▏| 357/390 [03:32<00:09,  3.47it/s] 92%|█████████▏| 358/390 [03:32<00:09,  3.48it/s] 92%|█████████▏| 359/390 [03:32<00:08,  3.47it/s] 92%|█████████▏| 360/390 [03:33<00:08,  3.48it/s] 93%|█████████▎| 361/390 [03:33<00:08,  3.48it/s] 93%|█████████▎| 362/390 [03:33<00:08,  3.49it/s] 93%|█████████▎| 363/390 [03:34<00:07,  3.49it/s] 93%|█████████▎| 364/390 [03:34<00:07,  3.49it/s] 94%|█████████▎| 365/390 [03:34<00:07,  3.49it/s] 94%|█████████▍| 366/390 [03:34<00:06,  3.50it/s] 94%|█████████▍| 367/390 [03:35<00:06,  3.50it/s] 94%|█████████▍| 368/390 [03:35<00:06,  3.50it/s] 95%|█████████▍| 369/390 [03:35<00:06,  3.50it/s] 95%|█████████▍| 370/390 [03:36<00:05,  3.42it/s] 95%|█████████▌| 371/390 [03:36<00:05,  3.44it/s] 95%|█████████▌| 372/390 [03:36<00:05,  3.46it/s] 96%|█████████▌| 373/390 [03:36<00:04,  3.47it/s] 96%|█████████▌| 374/390 [03:37<00:04,  3.48it/s] 96%|█████████▌| 375/390 [03:37<00:04,  3.48it/s] 96%|█████████▋| 376/390 [03:37<00:04,  3.48it/s] 97%|█████████▋| 377/390 [03:38<00:03,  3.48it/s] 97%|█████████▋| 378/390 [03:38<00:03,  3.49it/s] 97%|█████████▋| 379/390 [03:38<00:03,  3.49it/s] 97%|█████████▋| 380/390 [03:38<00:02,  3.49it/s] 98%|█████████▊| 381/390 [03:39<00:02,  3.40it/s] 98%|█████████▊| 382/390 [03:39<00:02,  3.42it/s] 98%|█████████▊| 383/390 [03:39<00:02,  3.45it/s] 98%|█████████▊| 384/390 [03:40<00:01,  3.46it/s] 99%|█████████▊| 385/390 [03:40<00:01,  3.47it/s] 99%|█████████▉| 386/390 [03:40<00:01,  3.48it/s] 99%|█████████▉| 387/390 [03:40<00:00,  3.48it/s] 99%|█████████▉| 388/390 [03:41<00:00,  3.49it/s]100%|█████████▉| 389/390 [03:41<00:00,  3.49it/s]100%|██████████| 390/390 [03:41<00:00,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 16:40:38,102 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:40:38,102 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 16:40:38,102 >>   Batch size = 8
{'eval_loss': 1.1773751974105835, 'eval_runtime': 9.4264, 'eval_samples_per_second': 370.873, 'eval_steps_per_second': 46.359, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.81it/s][A
  3%|▎         | 12/437 [00:00<00:09, 46.65it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.90it/s][A
  5%|▌         | 22/437 [00:00<00:08, 47.08it/s][A
  6%|▌         | 27/437 [00:00<00:08, 47.20it/s][A
  7%|▋         | 32/437 [00:00<00:08, 47.29it/s][A
  8%|▊         | 37/437 [00:00<00:08, 47.37it/s][A
 10%|▉         | 42/437 [00:00<00:08, 47.40it/s][A
 11%|█         | 47/437 [00:00<00:08, 47.24it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 47.24it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 47.17it/s][A
 14%|█▍        | 62/437 [00:01<00:07, 47.20it/s][A
 15%|█▌        | 67/437 [00:01<00:07, 47.19it/s][A
 16%|█▋        | 72/437 [00:01<00:07, 47.29it/s][A
 18%|█▊        | 77/437 [00:01<00:07, 47.36it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 47.38it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 47.35it/s][A
 21%|██        | 92/437 [00:01<00:07, 47.33it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 47.23it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 47.20it/s][A
 24%|██▍       | 107/437 [00:02<00:06, 47.19it/s][A
 26%|██▌       | 112/437 [00:02<00:06, 47.20it/s][A
 27%|██▋       | 117/437 [00:02<00:06, 47.18it/s][A
 28%|██▊       | 122/437 [00:02<00:06, 47.32it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 47.36it/s][A
 30%|███       | 132/437 [00:02<00:06, 47.33it/s][A
 31%|███▏      | 137/437 [00:02<00:06, 47.34it/s][A
 32%|███▏      | 142/437 [00:02<00:06, 47.30it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 47.19it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 47.05it/s][A
 36%|███▌      | 157/437 [00:03<00:05, 47.13it/s][A
 37%|███▋      | 162/437 [00:03<00:05, 47.09it/s][A
 38%|███▊      | 167/437 [00:03<00:05, 47.20it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 47.23it/s][A
 41%|████      | 177/437 [00:03<00:05, 47.33it/s][A
 42%|████▏     | 182/437 [00:03<00:05, 47.32it/s][A
 43%|████▎     | 187/437 [00:03<00:05, 47.30it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 47.22it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 47.17it/s][A
 46%|████▌     | 202/437 [00:04<00:04, 47.17it/s][A
 47%|████▋     | 207/437 [00:04<00:04, 47.22it/s][A
 49%|████▊     | 212/437 [00:04<00:04, 47.11it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 47.19it/s][A
 51%|█████     | 222/437 [00:04<00:04, 47.23it/s][A
 52%|█████▏    | 227/437 [00:04<00:04, 47.35it/s][A
 53%|█████▎    | 232/437 [00:04<00:04, 47.37it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 47.33it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 47.23it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 46.99it/s][A
 58%|█████▊    | 252/437 [00:05<00:03, 47.07it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 47.11it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 46.91it/s][A
 61%|██████    | 267/437 [00:05<00:03, 47.03it/s][A
 62%|██████▏   | 272/437 [00:05<00:03, 47.16it/s][A
 63%|██████▎   | 277/437 [00:05<00:03, 47.27it/s][A
 65%|██████▍   | 282/437 [00:05<00:03, 47.27it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 47.26it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 47.24it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.75it/s][A
 69%|██████▉   | 302/437 [00:06<00:02, 45.51it/s][A
 70%|███████   | 307/437 [00:06<00:02, 46.07it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 46.38it/s][A
 73%|███████▎  | 317/437 [00:06<00:02, 46.68it/s][A
 74%|███████▎  | 322/437 [00:06<00:02, 46.92it/s][A
 75%|███████▍  | 327/437 [00:06<00:02, 47.10it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 47.14it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 46.97it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 46.98it/s][A
 79%|███████▉  | 347/437 [00:07<00:01, 46.98it/s][A
 81%|████████  | 352/437 [00:07<00:01, 47.12it/s][A
 82%|████████▏ | 357/437 [00:07<00:02, 31.27it/s][A
 83%|████████▎ | 362/437 [00:07<00:02, 34.74it/s][A
 84%|████████▍ | 367/437 [00:07<00:01, 37.72it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 40.21it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 42.15it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.60it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.64it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 45.51it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 45.94it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 46.12it/s][A
 93%|█████████▎| 407/437 [00:08<00:00, 46.33it/s][A
 94%|█████████▍| 412/437 [00:08<00:00, 46.53it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 46.63it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 46.92it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.63it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 40.28it/s][A
100%|██████████| 437/437 [00:09<00:00, 22.90it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 22.90it/s][A100%|██████████| 390/390 [03:51<00:00,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:40:48,029 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 16:40:48,103 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:40:53,619 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:40:53,683 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:40:53,707 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:41:09,736 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:41:09,764 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78 (score: 1.1235755681991577).
                                                 100%|██████████| 390/390 [04:26<00:00,  3.49it/s]100%|██████████| 390/390 [04:26<00:00,  1.47it/s]
[INFO|trainer.py:1894] 2023-08-28 16:41:22,451 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 16:41:22,497 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:41:26,913 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:41:26,933 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:41:26,941 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:41:27,297 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:27,298 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:27,298 >>   train_loss               =     0.4213
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:27,298 >>   train_runtime            = 0:04:26.15
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:27,298 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:27,298 >>   train_samples_per_second =     93.931
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:27,298 >>   train_steps_per_second   =      1.465
{'eval_loss': 1.182224154472351, 'eval_runtime': 9.87, 'eval_samples_per_second': 354.204, 'eval_steps_per_second': 44.275, 'epoch': 4.99}
{'train_runtime': 266.1536, 'train_samples_per_second': 93.931, 'train_steps_per_second': 1.465, 'train_loss': 0.42130087828024837, 'epoch': 4.99}
08/28/2023 16:41:27 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:41:27,744 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:41:27,744 >>   Num examples = 3496
[INFO|trainer.py:2145] 2023-08-28 16:41:27,744 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.92it/s]  3%|▎         | 12/437 [00:00<00:08, 51.87it/s]  4%|▍         | 18/437 [00:00<00:08, 50.01it/s]  5%|▌         | 24/437 [00:00<00:08, 49.21it/s]  7%|▋         | 29/437 [00:00<00:08, 48.74it/s]  8%|▊         | 34/437 [00:00<00:08, 48.50it/s]  9%|▉         | 39/437 [00:00<00:08, 48.35it/s] 10%|█         | 44/437 [00:00<00:08, 48.19it/s] 11%|█         | 49/437 [00:01<00:08, 47.98it/s] 12%|█▏        | 54/437 [00:01<00:08, 47.83it/s] 14%|█▎        | 59/437 [00:01<00:07, 47.80it/s] 15%|█▍        | 64/437 [00:01<00:07, 47.69it/s] 16%|█▌        | 69/437 [00:01<00:07, 47.78it/s] 17%|█▋        | 74/437 [00:01<00:07, 47.81it/s] 18%|█▊        | 79/437 [00:01<00:07, 47.72it/s] 19%|█▉        | 84/437 [00:01<00:07, 47.72it/s] 20%|██        | 89/437 [00:01<00:07, 47.76it/s] 22%|██▏       | 94/437 [00:01<00:07, 47.67it/s] 23%|██▎       | 99/437 [00:02<00:07, 47.63it/s] 24%|██▍       | 104/437 [00:02<00:06, 47.57it/s] 25%|██▍       | 109/437 [00:02<00:06, 47.19it/s] 26%|██▌       | 114/437 [00:02<00:06, 47.39it/s] 27%|██▋       | 119/437 [00:02<00:06, 47.50it/s] 28%|██▊       | 124/437 [00:02<00:06, 47.53it/s] 30%|██▉       | 129/437 [00:02<00:06, 47.47it/s] 31%|███       | 134/437 [00:02<00:06, 47.61it/s] 32%|███▏      | 139/437 [00:02<00:06, 47.61it/s] 33%|███▎      | 144/437 [00:02<00:06, 47.59it/s] 34%|███▍      | 149/437 [00:03<00:06, 47.53it/s] 35%|███▌      | 154/437 [00:03<00:05, 47.49it/s] 36%|███▋      | 159/437 [00:03<00:05, 47.60it/s] 38%|███▊      | 164/437 [00:03<00:05, 47.57it/s] 39%|███▊      | 169/437 [00:03<00:05, 47.64it/s] 40%|███▉      | 174/437 [00:03<00:05, 47.51it/s] 41%|████      | 179/437 [00:03<00:05, 47.56it/s] 42%|████▏     | 184/437 [00:03<00:05, 47.39it/s] 43%|████▎     | 189/437 [00:03<00:05, 47.46it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.52it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.54it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.41it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.52it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.52it/s] 50%|█████     | 219/437 [00:04<00:04, 47.64it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.59it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.56it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.56it/s] 55%|█████▍    | 239/437 [00:04<00:04, 47.53it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.51it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.61it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.58it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.53it/s] 60%|██████    | 264/437 [00:05<00:03, 47.46it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.57it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.57it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.55it/s] 65%|██████▍   | 284/437 [00:05<00:03, 47.54it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.51it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.53it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.54it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.53it/s] 71%|███████   | 309/437 [00:06<00:02, 47.44it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.40it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.46it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.25it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.50it/s] 76%|███████▋  | 334/437 [00:06<00:02, 47.52it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.47it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.55it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.54it/s] 81%|████████  | 354/437 [00:07<00:01, 47.54it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.48it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.47it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.51it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.39it/s] 87%|████████▋ | 379/437 [00:07<00:01, 47.49it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.53it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.51it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.52it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.48it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.47it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.43it/s] 95%|█████████▍| 414/437 [00:08<00:00, 47.40it/s] 96%|█████████▌| 419/437 [00:08<00:00, 47.45it/s] 97%|█████████▋| 424/437 [00:08<00:00, 47.39it/s] 98%|█████████▊| 429/437 [00:08<00:00, 47.40it/s] 99%|█████████▉| 434/437 [00:09<00:00, 47.41it/s]100%|██████████| 437/437 [00:09<00:00, 47.62it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:41:36,943 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,943 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,943 >>   eval_loss               =     1.1236
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,944 >>   eval_runtime            = 0:00:09.19
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,944 >>   eval_samples            =       3496
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,944 >>   eval_samples_per_second =    380.021
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,944 >>   eval_steps_per_second   =     47.503
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:41:36,944 >>   perplexity              =     3.0758
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:44,017 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:44,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:44,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:44,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:44,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:41:44,858 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:41:44,859 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:41:45,478 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:41:46,532 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:41:46,532 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:48,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:48,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:48,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:48,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:48,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:41:49,250 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:41:49,251 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:41:49,994 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:41:50,207 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:41:50,207 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/dev.jsonl', 'labels': ['field of work', 'instrument', 'located on terrain feature', 'original language of film or TV show', 'owned by'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13219
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13319, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.52it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:21,  1.34it/s]Extractor Predicting: 32it [00:21,  1.37it/s]Extractor Predicting: 33it [00:22,  1.39it/s]Extractor Predicting: 34it [00:23,  1.42it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.39it/s]Extractor Predicting: 37it [00:25,  1.42it/s]Extractor Predicting: 38it [00:25,  1.44it/s]Extractor Predicting: 39it [00:26,  1.49it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.53it/s]Extractor Predicting: 43it [00:29,  1.55it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.54it/s]Extractor Predicting: 49it [00:33,  1.51it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:34,  1.54it/s]Extractor Predicting: 52it [00:34,  1.58it/s]Extractor Predicting: 53it [00:35,  1.55it/s]Extractor Predicting: 54it [00:36,  1.54it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:37,  1.54it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.49it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:42,  1.47it/s]Extractor Predicting: 64it [00:43,  1.49it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.42it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:46,  1.46it/s]Extractor Predicting: 70it [00:47,  1.51it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:48,  1.49it/s]Extractor Predicting: 73it [00:49,  1.51it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:50,  1.49it/s]Extractor Predicting: 76it [00:51,  1.46it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.47it/s]Extractor Predicting: 79it [00:53,  1.47it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:54,  1.49it/s]Extractor Predicting: 82it [00:55,  1.50it/s]Extractor Predicting: 83it [00:55,  1.53it/s]Extractor Predicting: 84it [00:56,  1.46it/s]Extractor Predicting: 85it [00:57,  1.47it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:58,  1.46it/s]Extractor Predicting: 88it [00:59,  1.51it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:01,  1.55it/s]Extractor Predicting: 92it [01:01,  1.55it/s]Extractor Predicting: 93it [01:02,  1.56it/s]Extractor Predicting: 94it [01:03,  1.53it/s]Extractor Predicting: 95it [01:03,  1.54it/s]Extractor Predicting: 96it [01:04,  1.53it/s]Extractor Predicting: 97it [01:05,  1.50it/s]Extractor Predicting: 98it [01:05,  1.49it/s]Extractor Predicting: 99it [01:06,  1.51it/s]Extractor Predicting: 100it [01:07,  1.49it/s]Extractor Predicting: 101it [01:07,  1.49it/s]Extractor Predicting: 102it [01:08,  1.46it/s]Extractor Predicting: 103it [01:09,  1.46it/s]Extractor Predicting: 104it [01:09,  1.47it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:11,  1.51it/s]Extractor Predicting: 107it [01:11,  1.50it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:13,  1.47it/s]Extractor Predicting: 110it [01:13,  1.47it/s]Extractor Predicting: 111it [01:14,  1.48it/s]Extractor Predicting: 112it [01:15,  1.51it/s]Extractor Predicting: 113it [01:15,  1.50it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:17,  1.50it/s]Extractor Predicting: 116it [01:17,  1.56it/s]Extractor Predicting: 117it [01:18,  1.42it/s]Extractor Predicting: 118it [01:19,  1.39it/s]Extractor Predicting: 119it [01:20,  1.39it/s]Extractor Predicting: 120it [01:20,  1.43it/s]Extractor Predicting: 121it [01:21,  1.46it/s]Extractor Predicting: 122it [01:22,  1.45it/s]Extractor Predicting: 123it [01:22,  1.43it/s]Extractor Predicting: 124it [01:23,  1.43it/s]Extractor Predicting: 125it [01:24,  1.41it/s]Extractor Predicting: 126it [01:24,  1.45it/s]Extractor Predicting: 127it [01:25,  1.47it/s]Extractor Predicting: 128it [01:26,  1.47it/s]Extractor Predicting: 129it [01:26,  1.48it/s]Extractor Predicting: 130it [01:27,  1.51it/s]Extractor Predicting: 131it [01:28,  1.53it/s]Extractor Predicting: 132it [01:28,  1.52it/s]Extractor Predicting: 133it [01:29,  1.51it/s]Extractor Predicting: 134it [01:30,  1.52it/s]Extractor Predicting: 135it [01:30,  1.52it/s]Extractor Predicting: 136it [01:31,  1.50it/s]Extractor Predicting: 137it [01:32,  1.49it/s]Extractor Predicting: 138it [01:32,  1.48it/s]Extractor Predicting: 139it [01:33,  1.49it/s]Extractor Predicting: 140it [01:34,  1.48it/s]Extractor Predicting: 141it [01:34,  1.48it/s]Extractor Predicting: 142it [01:35,  1.46it/s]Extractor Predicting: 143it [01:36,  1.44it/s]Extractor Predicting: 144it [01:36,  1.59it/s]Extractor Predicting: 144it [01:36,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:35,432 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:35,436 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:35,436 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:35,436 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:35,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:43:35,731 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:43:35,732 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:43:35,994 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:43:37,016 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:43:37,016 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:38,332 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:38,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:38,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:38,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:43:38,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:43:38,661 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:43:38,666 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:43:38,926 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:43:39,079 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:43:39,079 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.48091603053435117,
  "recall": 0.14416475972540047,
  "score": 0.22183098591549297,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 11674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.70it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:08,  1.71it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:14,  1.64it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.61it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.52it/s]Extractor Predicting: 31it [00:18,  1.48it/s]Extractor Predicting: 32it [00:19,  1.44it/s]Extractor Predicting: 33it [00:20,  1.35it/s]Extractor Predicting: 34it [00:21,  1.36it/s]Extractor Predicting: 35it [00:21,  1.40it/s]Extractor Predicting: 36it [00:22,  1.42it/s]Extractor Predicting: 37it [00:23,  1.42it/s]Extractor Predicting: 38it [00:23,  1.39it/s]Extractor Predicting: 39it [00:24,  1.40it/s]Extractor Predicting: 40it [00:25,  1.38it/s]Extractor Predicting: 41it [00:26,  1.37it/s]Extractor Predicting: 42it [00:26,  1.39it/s]Extractor Predicting: 43it [00:27,  1.40it/s]Extractor Predicting: 44it [00:28,  1.42it/s]Extractor Predicting: 45it [00:28,  1.41it/s]Extractor Predicting: 46it [00:29,  1.44it/s]Extractor Predicting: 47it [00:30,  1.42it/s]Extractor Predicting: 48it [00:31,  1.40it/s]Extractor Predicting: 49it [00:31,  1.39it/s]Extractor Predicting: 50it [00:32,  1.40it/s]Extractor Predicting: 51it [00:33,  1.39it/s]Extractor Predicting: 52it [00:33,  1.40it/s]Extractor Predicting: 53it [00:34,  1.40it/s]Extractor Predicting: 54it [00:35,  1.40it/s]Extractor Predicting: 55it [00:36,  1.40it/s]Extractor Predicting: 56it [00:36,  1.38it/s]Extractor Predicting: 57it [00:37,  1.40it/s]Extractor Predicting: 58it [00:38,  1.41it/s]Extractor Predicting: 59it [00:38,  1.42it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:40,  1.41it/s]Extractor Predicting: 62it [00:41,  1.41it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:43,  1.10it/s]Extractor Predicting: 65it [00:43,  1.19it/s]Extractor Predicting: 66it [00:44,  1.29it/s]Extractor Predicting: 67it [00:45,  1.35it/s]Extractor Predicting: 68it [00:45,  1.33it/s]Extractor Predicting: 69it [00:46,  1.37it/s]Extractor Predicting: 70it [00:47,  1.43it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:48,  1.48it/s]Extractor Predicting: 73it [00:49,  1.38it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:50,  1.43it/s]Extractor Predicting: 76it [00:51,  1.43it/s]Extractor Predicting: 77it [00:51,  1.44it/s]Extractor Predicting: 78it [00:52,  1.39it/s]Extractor Predicting: 79it [00:53,  1.42it/s]Extractor Predicting: 80it [00:54,  1.43it/s]Extractor Predicting: 81it [00:54,  1.43it/s]Extractor Predicting: 82it [00:55,  1.46it/s]Extractor Predicting: 83it [00:56,  1.26it/s]Extractor Predicting: 84it [00:57,  1.31it/s]Extractor Predicting: 85it [00:57,  1.37it/s]Extractor Predicting: 86it [00:58,  1.42it/s]Extractor Predicting: 87it [00:59,  1.47it/s]Extractor Predicting: 88it [00:59,  1.52it/s]Extractor Predicting: 89it [01:00,  1.56it/s]Extractor Predicting: 90it [01:00,  1.57it/s]Extractor Predicting: 91it [01:01,  1.63it/s]Extractor Predicting: 92it [01:02,  1.61it/s]Extractor Predicting: 93it [01:02,  1.66it/s]Extractor Predicting: 94it [01:03,  1.64it/s]Extractor Predicting: 95it [01:03,  1.62it/s]Extractor Predicting: 96it [01:04,  1.63it/s]Extractor Predicting: 97it [01:05,  1.55it/s]Extractor Predicting: 98it [01:05,  1.60it/s]Extractor Predicting: 99it [01:06,  1.63it/s]Extractor Predicting: 100it [01:07,  1.69it/s]Extractor Predicting: 101it [01:07,  1.69it/s]Extractor Predicting: 102it [01:08,  1.64it/s]Extractor Predicting: 103it [01:08,  1.62it/s]Extractor Predicting: 104it [01:09,  1.65it/s]Extractor Predicting: 105it [01:10,  1.67it/s]Extractor Predicting: 106it [01:10,  1.67it/s]Extractor Predicting: 107it [01:11,  1.73it/s]Extractor Predicting: 108it [01:11,  1.68it/s]Extractor Predicting: 109it [01:12,  1.69it/s]Extractor Predicting: 110it [01:13,  1.66it/s]Extractor Predicting: 111it [01:13,  1.67it/s]Extractor Predicting: 112it [01:14,  1.70it/s]Extractor Predicting: 113it [01:14,  1.69it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:16,  1.49it/s]Extractor Predicting: 117it [01:17,  1.49it/s]Extractor Predicting: 118it [01:18,  1.51it/s]Extractor Predicting: 119it [01:18,  1.52it/s]Extractor Predicting: 120it [01:19,  1.48it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:20,  1.52it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.54it/s]Extractor Predicting: 125it [01:22,  1.50it/s]Extractor Predicting: 126it [01:23,  1.52it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:25,  1.50it/s]Extractor Predicting: 130it [01:26,  1.48it/s]Extractor Predicting: 131it [01:26,  1.47it/s]Extractor Predicting: 132it [01:27,  1.47it/s]Extractor Predicting: 133it [01:28,  1.50it/s]Extractor Predicting: 134it [01:28,  1.49it/s]Extractor Predicting: 135it [01:29,  1.46it/s]Extractor Predicting: 136it [01:30,  1.43it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:31,  1.46it/s]Extractor Predicting: 139it [01:32,  1.47it/s]Extractor Predicting: 140it [01:33,  1.45it/s]Extractor Predicting: 141it [01:33,  1.47it/s]Extractor Predicting: 142it [01:34,  1.45it/s]Extractor Predicting: 143it [01:34,  1.80it/s]Extractor Predicting: 143it [01:34,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:26,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:27,061 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:27,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:45:27,705 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:45:27,706 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:45:28,313 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:45:29,339 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:45:29,339 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:32,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:32,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:32,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:32,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:32,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:45:33,108 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:45:33,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:45:33,679 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:45:33,832 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:45:33,833 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5034013605442177,
  "recall": 0.10850439882697947,
  "score": 0.17852834740651385,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 2it [00:01,  1.31it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.05555555555555555,
  "score": 0.09999999999999999,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/', 'labels': ['father', 'heritage designation', 'licensed to broadcast to', 'occupant', 'occupation'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
