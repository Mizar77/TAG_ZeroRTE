/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:36, 36.30s/it]Extractor Estimating: 2it [00:37, 15.68s/it]Extractor Estimating: 3it [00:38,  8.82s/it]Extractor Estimating: 4it [00:38,  5.57s/it]Extractor Estimating: 5it [00:39,  3.77s/it]Extractor Estimating: 6it [00:39,  2.67s/it]Extractor Estimating: 7it [00:42,  2.54s/it]Extractor Estimating: 8it [00:42,  1.93s/it]Extractor Estimating: 9it [00:43,  1.52s/it]Extractor Estimating: 10it [00:44,  1.25s/it]Extractor Estimating: 11it [00:44,  1.07s/it]Extractor Estimating: 12it [00:45,  1.09it/s]Extractor Estimating: 13it [00:45,  1.20it/s]Extractor Estimating: 14it [00:46,  1.29it/s]Extractor Estimating: 15it [00:47,  1.40it/s]Extractor Estimating: 16it [00:47,  1.51it/s]Extractor Estimating: 17it [00:48,  1.59it/s]Extractor Estimating: 18it [00:48,  1.59it/s]Extractor Estimating: 19it [00:49,  1.62it/s]Extractor Estimating: 20it [00:50,  1.61it/s]Extractor Estimating: 21it [00:50,  1.60it/s]Extractor Estimating: 22it [00:51,  1.63it/s]Extractor Estimating: 23it [00:51,  1.64it/s]Extractor Estimating: 24it [00:52,  1.62it/s]Extractor Estimating: 25it [00:53,  1.62it/s]Extractor Estimating: 26it [00:53,  1.60it/s]Extractor Estimating: 27it [00:54,  1.64it/s]Extractor Estimating: 28it [00:54,  1.68it/s]Extractor Estimating: 29it [00:55,  1.62it/s]Extractor Estimating: 30it [00:56,  1.62it/s]Extractor Estimating: 31it [00:56,  1.65it/s]Extractor Estimating: 32it [00:57,  1.64it/s]Extractor Estimating: 33it [00:58,  1.60it/s]Extractor Estimating: 34it [00:58,  1.61it/s]Extractor Estimating: 35it [00:59,  1.59it/s]Extractor Estimating: 36it [01:00,  1.56it/s]Extractor Estimating: 37it [01:00,  1.53it/s]Extractor Estimating: 38it [01:01,  1.61it/s]Extractor Estimating: 39it [01:01,  1.54it/s]Extractor Estimating: 40it [01:02,  1.51it/s]Extractor Estimating: 41it [01:03,  1.58it/s]Extractor Estimating: 42it [01:03,  1.56it/s]Extractor Estimating: 43it [01:04,  1.57it/s]Extractor Estimating: 44it [01:05,  1.58it/s]Extractor Estimating: 45it [01:05,  1.58it/s]Extractor Estimating: 46it [01:06,  1.57it/s]Extractor Estimating: 47it [01:06,  1.62it/s]Extractor Estimating: 48it [01:07,  1.57it/s]Extractor Estimating: 49it [01:08,  1.57it/s]Extractor Estimating: 50it [01:08,  1.64it/s]Extractor Estimating: 51it [01:09,  1.61it/s]Extractor Estimating: 52it [01:10,  1.58it/s]Extractor Estimating: 53it [01:10,  1.51it/s]Extractor Estimating: 54it [01:11,  1.56it/s]Extractor Estimating: 55it [01:12,  1.60it/s]Extractor Estimating: 56it [01:12,  1.61it/s]Extractor Estimating: 57it [01:13,  1.55it/s]Extractor Estimating: 58it [01:13,  1.61it/s]Extractor Estimating: 59it [01:14,  1.59it/s]Extractor Estimating: 60it [01:15,  1.56it/s]Extractor Estimating: 61it [01:15,  1.56it/s]Extractor Estimating: 62it [01:16,  1.54it/s]Extractor Estimating: 63it [01:17,  1.60it/s]Extractor Estimating: 64it [01:17,  1.57it/s]Extractor Estimating: 65it [01:18,  1.53it/s]Extractor Estimating: 66it [01:19,  1.50it/s]Extractor Estimating: 67it [01:19,  1.50it/s]Extractor Estimating: 68it [01:20,  1.52it/s]Extractor Estimating: 69it [01:21,  1.48it/s]Extractor Estimating: 70it [01:21,  1.50it/s]Extractor Estimating: 71it [01:22,  1.56it/s]Extractor Estimating: 72it [01:23,  1.57it/s]Extractor Estimating: 73it [01:23,  1.56it/s]Extractor Estimating: 74it [01:24,  1.53it/s]Extractor Estimating: 75it [01:25,  1.55it/s]Extractor Estimating: 76it [01:25,  1.55it/s]Extractor Estimating: 77it [01:27,  1.17it/s]Extractor Estimating: 78it [01:27,  1.25it/s]Extractor Estimating: 79it [01:28,  1.35it/s]Extractor Estimating: 80it [01:28,  1.39it/s]Extractor Estimating: 81it [01:29,  1.44it/s]Extractor Estimating: 82it [01:30,  1.47it/s]Extractor Estimating: 83it [01:30,  1.50it/s]Extractor Estimating: 84it [01:31,  1.48it/s]Extractor Estimating: 85it [01:32,  1.50it/s]Extractor Estimating: 86it [01:32,  1.56it/s]Extractor Estimating: 87it [01:33,  1.57it/s]Extractor Estimating: 88it [01:34,  1.47it/s]Extractor Estimating: 89it [01:34,  1.50it/s]Extractor Estimating: 90it [01:35,  1.54it/s]Extractor Estimating: 91it [01:35,  1.62it/s]Extractor Estimating: 92it [01:36,  1.58it/s]Extractor Estimating: 93it [01:37,  1.50it/s]Extractor Estimating: 94it [01:38,  1.54it/s]Extractor Estimating: 95it [01:38,  1.56it/s]Extractor Estimating: 96it [01:39,  1.57it/s]Extractor Estimating: 97it [01:40,  1.50it/s]Extractor Estimating: 98it [01:40,  1.53it/s]Extractor Estimating: 99it [01:41,  1.55it/s]Extractor Estimating: 100it [01:41,  1.54it/s]Extractor Estimating: 101it [01:42,  1.50it/s]Extractor Estimating: 102it [01:43,  1.54it/s]Extractor Estimating: 103it [01:43,  1.62it/s]Extractor Estimating: 104it [01:44,  1.59it/s]Extractor Estimating: 105it [01:45,  1.63it/s]Extractor Estimating: 106it [01:45,  1.61it/s]Extractor Estimating: 107it [01:46,  1.62it/s]Extractor Estimating: 108it [01:46,  1.63it/s]Extractor Estimating: 109it [01:47,  1.66it/s]Extractor Estimating: 110it [01:48,  1.58it/s]Extractor Estimating: 111it [01:48,  1.51it/s]Extractor Estimating: 112it [01:49,  1.52it/s]Extractor Estimating: 113it [01:50,  1.59it/s]Extractor Estimating: 114it [01:50,  1.63it/s]Extractor Estimating: 115it [01:51,  1.56it/s]Extractor Estimating: 116it [01:52,  1.54it/s]Extractor Estimating: 117it [01:52,  1.52it/s]Extractor Estimating: 118it [01:53,  1.55it/s]Extractor Estimating: 119it [01:53,  1.57it/s]Extractor Estimating: 120it [01:54,  1.57it/s]Extractor Estimating: 121it [01:55,  1.61it/s]Extractor Estimating: 122it [01:55,  1.64it/s]Extractor Estimating: 123it [01:56,  1.65it/s]Extractor Estimating: 124it [01:56,  1.69it/s]Extractor Estimating: 125it [01:57,  1.67it/s]Extractor Estimating: 126it [01:58,  1.70it/s]Extractor Estimating: 127it [01:58,  1.56it/s]Extractor Estimating: 128it [01:59,  1.58it/s]Extractor Estimating: 129it [02:00,  1.56it/s]Extractor Estimating: 130it [02:00,  1.54it/s]Extractor Estimating: 131it [02:01,  1.56it/s]Extractor Estimating: 132it [02:02,  1.50it/s]Extractor Estimating: 133it [02:02,  1.52it/s]Extractor Estimating: 134it [02:03,  1.52it/s]Extractor Estimating: 135it [02:04,  1.51it/s]Extractor Estimating: 136it [02:04,  1.56it/s]Extractor Estimating: 137it [02:05,  1.54it/s]Extractor Estimating: 138it [02:05,  1.56it/s]Extractor Estimating: 139it [02:06,  1.62it/s]Extractor Estimating: 140it [02:07,  1.64it/s]Extractor Estimating: 141it [02:07,  1.58it/s]Extractor Estimating: 142it [02:08,  1.54it/s]Extractor Estimating: 143it [02:09,  1.54it/s]Extractor Estimating: 144it [02:09,  1.50it/s]Extractor Estimating: 145it [02:10,  1.53it/s]Extractor Estimating: 146it [02:11,  1.50it/s]Extractor Estimating: 147it [02:11,  1.53it/s]Extractor Estimating: 148it [02:12,  1.49it/s]Extractor Estimating: 149it [02:13,  1.53it/s]Extractor Estimating: 150it [02:13,  1.53it/s]Extractor Estimating: 151it [02:14,  1.56it/s]Extractor Estimating: 152it [02:14,  1.62it/s]Extractor Estimating: 153it [02:15,  1.65it/s]Extractor Estimating: 154it [02:16,  1.71it/s]Extractor Estimating: 155it [02:16,  1.73it/s]Extractor Estimating: 156it [02:17,  1.69it/s]Extractor Estimating: 157it [02:17,  1.71it/s]Extractor Estimating: 158it [02:18,  1.69it/s]Extractor Estimating: 159it [02:19,  1.67it/s]Extractor Estimating: 160it [02:19,  1.70it/s]Extractor Estimating: 161it [02:20,  1.74it/s]Extractor Estimating: 162it [02:20,  1.72it/s]Extractor Estimating: 163it [02:21,  1.75it/s]Extractor Estimating: 164it [02:21,  1.77it/s]Extractor Estimating: 165it [02:22,  1.74it/s]Extractor Estimating: 166it [02:23,  1.68it/s]Extractor Estimating: 167it [02:23,  1.67it/s]Extractor Estimating: 168it [02:24,  1.71it/s]Extractor Estimating: 169it [02:24,  1.76it/s]Extractor Estimating: 170it [02:25,  1.72it/s]Extractor Estimating: 171it [02:28,  1.32s/it]Extractor Estimating: 172it [02:29,  1.12s/it]Extractor Estimating: 173it [02:29,  1.05it/s]Extractor Estimating: 174it [02:30,  1.19it/s]Extractor Estimating: 175it [02:30,  1.31it/s]Extractor Estimating: 176it [02:31,  1.40it/s]Extractor Estimating: 177it [02:31,  1.51it/s]Extractor Estimating: 178it [02:32,  1.53it/s]Extractor Estimating: 179it [02:33,  1.55it/s]Extractor Estimating: 180it [02:33,  1.57it/s]Extractor Estimating: 181it [02:34,  1.61it/s]Extractor Estimating: 182it [02:34,  1.67it/s]Extractor Estimating: 183it [02:35,  1.64it/s]Extractor Estimating: 184it [02:36,  1.56it/s]Extractor Estimating: 185it [02:36,  1.64it/s]Extractor Estimating: 186it [02:37,  1.61it/s]Extractor Estimating: 187it [02:38,  1.58it/s]Extractor Estimating: 188it [02:38,  1.58it/s]Extractor Estimating: 189it [02:39,  1.57it/s]Extractor Estimating: 190it [02:40,  1.60it/s]Extractor Estimating: 191it [02:40,  1.63it/s]Extractor Estimating: 192it [02:41,  1.66it/s]Extractor Estimating: 193it [02:41,  1.69it/s]Extractor Estimating: 194it [02:42,  1.55it/s]Extractor Estimating: 195it [02:43,  1.56it/s]Extractor Estimating: 196it [02:43,  1.58it/s]Extractor Estimating: 197it [02:44,  1.66it/s]Extractor Estimating: 198it [02:44,  1.64it/s]Extractor Estimating: 199it [02:45,  1.67it/s]Extractor Estimating: 200it [02:46,  1.64it/s]Extractor Estimating: 201it [02:46,  1.62it/s]Extractor Estimating: 202it [02:47,  1.49it/s]Extractor Estimating: 203it [02:48,  1.55it/s]Extractor Estimating: 204it [02:48,  1.55it/s]Extractor Estimating: 205it [02:49,  1.55it/s]Extractor Estimating: 206it [02:50,  1.56it/s]Extractor Estimating: 207it [02:50,  1.43it/s]Extractor Estimating: 208it [02:51,  1.45it/s]Extractor Estimating: 209it [02:52,  1.50it/s]Extractor Estimating: 210it [02:52,  1.52it/s]Extractor Estimating: 211it [02:53,  1.50it/s]Extractor Estimating: 212it [02:54,  1.42it/s]Extractor Estimating: 213it [02:54,  1.47it/s]Extractor Estimating: 214it [02:55,  1.51it/s]Extractor Estimating: 215it [02:56,  1.48it/s]Extractor Estimating: 216it [02:56,  1.46it/s]Extractor Estimating: 217it [02:57,  1.45it/s]Extractor Estimating: 218it [02:58,  1.46it/s]Extractor Estimating: 219it [02:59,  1.45it/s]Extractor Estimating: 220it [02:59,  1.50it/s]Extractor Estimating: 221it [03:00,  1.55it/s]Extractor Estimating: 222it [03:01,  1.46it/s]Extractor Estimating: 223it [03:01,  1.53it/s]Extractor Estimating: 224it [03:02,  1.53it/s]Extractor Estimating: 225it [03:02,  1.50it/s]Extractor Estimating: 226it [03:03,  1.49it/s]Extractor Estimating: 227it [03:04,  1.48it/s]Extractor Estimating: 228it [03:04,  1.53it/s]Extractor Estimating: 229it [03:05,  1.58it/s]Extractor Estimating: 230it [03:06,  1.54it/s]Extractor Estimating: 231it [03:06,  1.55it/s]Extractor Estimating: 232it [03:07,  1.53it/s]Extractor Estimating: 233it [03:08,  1.52it/s]Extractor Estimating: 234it [03:08,  1.59it/s]Extractor Estimating: 235it [03:09,  1.59it/s]Extractor Estimating: 236it [03:10,  1.59it/s]Extractor Estimating: 237it [03:10,  1.57it/s]Extractor Estimating: 238it [03:11,  1.56it/s]Extractor Estimating: 239it [03:11,  1.58it/s]Extractor Estimating: 240it [03:12,  1.60it/s]Extractor Estimating: 241it [03:13,  1.57it/s]Extractor Estimating: 242it [03:13,  1.59it/s]Extractor Estimating: 243it [03:14,  1.59it/s]Extractor Estimating: 244it [03:15,  1.56it/s]Extractor Estimating: 245it [03:15,  1.56it/s]Extractor Estimating: 246it [03:16,  1.54it/s]Extractor Estimating: 247it [03:16,  1.59it/s]Extractor Estimating: 248it [03:17,  1.52it/s]Extractor Estimating: 249it [03:18,  1.50it/s]Extractor Estimating: 250it [03:19,  1.55it/s]Extractor Estimating: 251it [03:19,  1.55it/s]Extractor Estimating: 252it [03:20,  1.53it/s]Extractor Estimating: 253it [03:20,  1.53it/s]Extractor Estimating: 254it [03:21,  1.58it/s]Extractor Estimating: 255it [03:22,  1.56it/s]Extractor Estimating: 256it [03:22,  1.54it/s]Extractor Estimating: 257it [03:23,  1.56it/s]Extractor Estimating: 258it [03:24,  1.55it/s]Extractor Estimating: 259it [03:24,  1.58it/s]Extractor Estimating: 260it [03:25,  1.56it/s]Extractor Estimating: 261it [03:26,  1.53it/s]Extractor Estimating: 262it [03:26,  1.54it/s]Extractor Estimating: 263it [03:27,  1.50it/s]Extractor Estimating: 264it [03:28,  1.53it/s]Extractor Estimating: 265it [03:28,  1.53it/s]Extractor Estimating: 266it [03:29,  1.51it/s]Extractor Estimating: 267it [03:29,  1.57it/s]Extractor Estimating: 268it [03:30,  1.58it/s]Extractor Estimating: 269it [03:31,  1.65it/s]Extractor Estimating: 270it [03:31,  1.60it/s]Extractor Estimating: 271it [03:32,  1.61it/s]Extractor Estimating: 272it [03:33,  1.44it/s]Extractor Estimating: 273it [03:33,  1.50it/s]Extractor Estimating: 274it [03:34,  1.51it/s]Extractor Estimating: 275it [03:35,  1.50it/s]Extractor Estimating: 276it [03:35,  1.48it/s]Extractor Estimating: 277it [03:36,  1.54it/s]Extractor Estimating: 278it [03:37,  1.56it/s]Extractor Estimating: 279it [03:37,  1.57it/s]Extractor Estimating: 280it [03:38,  1.54it/s]Extractor Estimating: 281it [03:39,  1.59it/s]Extractor Estimating: 282it [03:39,  1.57it/s]Extractor Estimating: 283it [03:40,  1.55it/s]Extractor Estimating: 284it [03:41,  1.55it/s]Extractor Estimating: 285it [03:41,  1.54it/s]Extractor Estimating: 286it [03:42,  1.56it/s]Extractor Estimating: 287it [03:42,  1.54it/s]Extractor Estimating: 288it [03:43,  1.52it/s]Extractor Estimating: 289it [03:44,  1.53it/s]Extractor Estimating: 290it [03:44,  1.50it/s]Extractor Estimating: 291it [03:45,  1.50it/s]Extractor Estimating: 292it [03:46,  1.48it/s]Extractor Estimating: 293it [03:46,  1.51it/s]Extractor Estimating: 294it [03:47,  1.53it/s]Extractor Estimating: 295it [03:48,  1.53it/s]Extractor Estimating: 296it [03:48,  1.47it/s]Extractor Estimating: 297it [03:49,  1.43it/s]Extractor Estimating: 298it [03:50,  1.44it/s]Extractor Estimating: 299it [03:51,  1.45it/s]Extractor Estimating: 300it [03:51,  1.49it/s]Extractor Estimating: 301it [03:52,  1.56it/s]Extractor Estimating: 302it [03:52,  1.61it/s]Extractor Estimating: 303it [03:53,  1.62it/s]Extractor Estimating: 304it [03:54,  1.66it/s]Extractor Estimating: 305it [03:54,  1.65it/s]Extractor Estimating: 306it [03:55,  1.65it/s]Extractor Estimating: 307it [03:55,  1.64it/s]Extractor Estimating: 308it [03:56,  1.65it/s]Extractor Estimating: 309it [03:57,  1.72it/s]Extractor Estimating: 310it [03:57,  1.72it/s]Extractor Estimating: 311it [03:58,  1.71it/s]Extractor Estimating: 312it [03:58,  1.67it/s]Extractor Estimating: 313it [03:59,  1.68it/s]Extractor Estimating: 314it [04:00,  1.66it/s]Extractor Estimating: 315it [04:00,  1.69it/s]Extractor Estimating: 316it [04:01,  1.73it/s]Extractor Estimating: 317it [04:01,  1.73it/s]Extractor Estimating: 318it [04:02,  1.70it/s]Extractor Estimating: 319it [04:02,  1.67it/s]Extractor Estimating: 320it [04:03,  1.66it/s]Extractor Estimating: 321it [04:04,  1.67it/s]Extractor Estimating: 322it [04:04,  1.68it/s]Extractor Estimating: 323it [04:05,  1.71it/s]Extractor Estimating: 324it [04:05,  1.71it/s]Extractor Estimating: 325it [04:06,  1.71it/s]Extractor Estimating: 326it [04:07,  1.70it/s]Extractor Estimating: 327it [04:07,  1.65it/s]Extractor Estimating: 328it [04:08,  1.59it/s]Extractor Estimating: 329it [04:09,  1.54it/s]Extractor Estimating: 330it [04:09,  1.58it/s]Extractor Estimating: 331it [04:10,  1.67it/s]Extractor Estimating: 332it [04:10,  1.74it/s]Extractor Estimating: 333it [04:11,  1.69it/s]Extractor Estimating: 334it [04:12,  1.65it/s]Extractor Estimating: 335it [04:12,  1.60it/s]Extractor Estimating: 336it [04:13,  1.61it/s]Extractor Estimating: 337it [04:13,  1.59it/s]Extractor Estimating: 338it [04:14,  1.57it/s]Extractor Estimating: 339it [04:15,  1.64it/s]Extractor Estimating: 340it [04:15,  1.61it/s]Extractor Estimating: 341it [04:16,  1.61it/s]Extractor Estimating: 342it [04:16,  1.68it/s]Extractor Estimating: 343it [04:17,  1.68it/s]Extractor Estimating: 344it [04:18,  1.66it/s]Extractor Estimating: 345it [04:18,  1.64it/s]Extractor Estimating: 346it [04:19,  1.65it/s]Extractor Estimating: 347it [04:19,  1.71it/s]Extractor Estimating: 348it [04:20,  1.68it/s]Extractor Estimating: 349it [04:21,  1.63it/s]Extractor Estimating: 350it [04:21,  1.60it/s]Extractor Estimating: 351it [04:22,  1.62it/s]Extractor Estimating: 352it [04:23,  1.58it/s]Extractor Estimating: 353it [04:23,  1.57it/s]Extractor Estimating: 354it [04:24,  1.62it/s]Extractor Estimating: 355it [04:24,  1.62it/s]Extractor Estimating: 356it [04:25,  1.64it/s]Extractor Estimating: 357it [04:26,  1.64it/s]Extractor Estimating: 358it [04:26,  1.64it/s]Extractor Estimating: 359it [04:27,  1.66it/s]Extractor Estimating: 360it [04:28,  1.57it/s]Extractor Estimating: 361it [04:28,  1.55it/s]Extractor Estimating: 362it [04:29,  1.58it/s]Extractor Estimating: 363it [04:29,  1.60it/s]Extractor Estimating: 364it [04:30,  1.59it/s]Extractor Estimating: 365it [04:31,  1.51it/s]Extractor Estimating: 366it [04:31,  1.55it/s]Extractor Estimating: 367it [04:32,  1.37it/s]Extractor Estimating: 368it [04:33,  1.43it/s]Extractor Estimating: 369it [04:34,  1.33it/s]Extractor Estimating: 370it [04:34,  1.38it/s]Extractor Estimating: 371it [04:35,  1.43it/s]Extractor Estimating: 372it [04:36,  1.50it/s]Extractor Estimating: 373it [04:36,  1.54it/s]Extractor Estimating: 374it [04:37,  1.60it/s]Extractor Estimating: 375it [04:38,  1.59it/s]Extractor Estimating: 376it [04:38,  1.60it/s]Extractor Estimating: 377it [04:39,  1.58it/s]Extractor Estimating: 378it [04:39,  1.58it/s]Extractor Estimating: 379it [04:40,  1.62it/s]Extractor Estimating: 380it [04:41,  1.62it/s]Extractor Estimating: 381it [04:41,  1.57it/s]Extractor Estimating: 382it [04:42,  1.60it/s]Extractor Estimating: 383it [04:43,  1.61it/s]Extractor Estimating: 384it [04:43,  1.57it/s]Extractor Estimating: 385it [04:44,  1.54it/s]Extractor Estimating: 386it [04:45,  1.55it/s]Extractor Estimating: 387it [04:45,  1.54it/s]Extractor Estimating: 388it [04:46,  1.57it/s]Extractor Estimating: 389it [04:47,  1.51it/s]Extractor Estimating: 390it [04:47,  1.51it/s]Extractor Estimating: 391it [04:48,  1.51it/s]Extractor Estimating: 392it [04:48,  1.52it/s]Extractor Estimating: 393it [04:49,  1.49it/s]Extractor Estimating: 394it [04:50,  1.52it/s]Extractor Estimating: 395it [04:50,  1.55it/s]Extractor Estimating: 396it [04:51,  1.48it/s]Extractor Estimating: 397it [04:52,  1.50it/s]Extractor Estimating: 398it [04:53,  1.49it/s]Extractor Estimating: 399it [04:53,  1.50it/s]Extractor Estimating: 400it [04:54,  1.54it/s]Extractor Estimating: 401it [04:54,  1.57it/s]Extractor Estimating: 402it [04:55,  1.54it/s]Extractor Estimating: 403it [04:56,  1.61it/s]Extractor Estimating: 404it [04:56,  1.57it/s]Extractor Estimating: 405it [04:57,  1.62it/s]Extractor Estimating: 406it [04:57,  1.63it/s]Extractor Estimating: 407it [04:58,  1.62it/s]Extractor Estimating: 408it [04:59,  1.65it/s]Extractor Estimating: 409it [04:59,  1.63it/s]Extractor Estimating: 410it [05:00,  1.62it/s]Extractor Estimating: 411it [05:01,  1.61it/s]Extractor Estimating: 412it [05:01,  1.64it/s]Extractor Estimating: 413it [05:02,  1.63it/s]Extractor Estimating: 414it [05:02,  1.58it/s]Extractor Estimating: 415it [05:03,  1.59it/s]Extractor Estimating: 416it [05:04,  1.55it/s]Extractor Estimating: 417it [05:04,  1.62it/s]Extractor Estimating: 418it [05:05,  1.57it/s]Extractor Estimating: 419it [05:06,  1.58it/s]Extractor Estimating: 420it [05:06,  1.58it/s]Extractor Estimating: 421it [05:07,  1.57it/s]Extractor Estimating: 422it [05:08,  1.53it/s]Extractor Estimating: 423it [05:08,  1.51it/s]Extractor Estimating: 424it [05:09,  1.55it/s]Extractor Estimating: 425it [05:10,  1.55it/s]Extractor Estimating: 426it [05:10,  1.57it/s]Extractor Estimating: 427it [05:11,  1.59it/s]Extractor Estimating: 428it [05:11,  1.59it/s]Extractor Estimating: 429it [05:12,  1.60it/s]Extractor Estimating: 430it [05:13,  1.56it/s]Extractor Estimating: 431it [05:13,  1.54it/s]Extractor Estimating: 432it [05:14,  1.56it/s]Extractor Estimating: 433it [05:15,  1.51it/s]Extractor Estimating: 434it [05:15,  1.49it/s]Extractor Estimating: 435it [05:16,  1.49it/s]Extractor Estimating: 436it [05:17,  1.53it/s]Extractor Estimating: 437it [05:17,  1.56it/s]Extractor Estimating: 438it [05:18,  1.55it/s]Extractor Estimating: 439it [05:19,  1.52it/s]Extractor Estimating: 440it [05:19,  1.52it/s]Extractor Estimating: 441it [05:20,  1.50it/s]Extractor Estimating: 442it [05:21,  1.51it/s]Extractor Estimating: 443it [05:21,  1.53it/s]Extractor Estimating: 444it [05:22,  1.52it/s]Extractor Estimating: 445it [05:23,  1.49it/s]Extractor Estimating: 446it [05:23,  1.47it/s]Extractor Estimating: 447it [05:24,  1.47it/s]Extractor Estimating: 448it [05:25,  1.45it/s]Extractor Estimating: 449it [05:25,  1.43it/s]Extractor Estimating: 450it [05:26,  1.50it/s]Extractor Estimating: 451it [05:27,  1.55it/s]Extractor Estimating: 452it [05:27,  1.56it/s]Extractor Estimating: 453it [05:28,  1.54it/s]Extractor Estimating: 454it [05:29,  1.58it/s]Extractor Estimating: 455it [05:29,  1.60it/s]Extractor Estimating: 456it [05:30,  1.45it/s]Extractor Estimating: 457it [05:31,  1.48it/s]Extractor Estimating: 458it [05:31,  1.49it/s]Extractor Estimating: 459it [05:32,  1.56it/s]Extractor Estimating: 460it [05:32,  1.62it/s]Extractor Estimating: 461it [05:33,  1.57it/s]Extractor Estimating: 462it [05:34,  1.58it/s]Extractor Estimating: 463it [05:34,  1.59it/s]Extractor Estimating: 464it [05:35,  1.60it/s]Extractor Estimating: 465it [05:36,  1.56it/s]Extractor Estimating: 466it [05:36,  1.60it/s]Extractor Estimating: 467it [05:37,  1.60it/s]Extractor Estimating: 468it [05:37,  1.62it/s]Extractor Estimating: 469it [05:38,  1.64it/s]Extractor Estimating: 470it [05:39,  1.65it/s]Extractor Estimating: 471it [05:39,  1.63it/s]Extractor Estimating: 472it [05:40,  1.55it/s]Extractor Estimating: 473it [05:41,  1.56it/s]Extractor Estimating: 474it [05:41,  1.59it/s]Extractor Estimating: 475it [05:42,  1.63it/s]Extractor Estimating: 476it [05:42,  1.57it/s]Extractor Estimating: 477it [05:43,  1.56it/s]Extractor Estimating: 478it [05:44,  1.59it/s]Extractor Estimating: 479it [05:44,  1.49it/s]Extractor Estimating: 480it [05:45,  1.50it/s]Extractor Estimating: 481it [05:46,  1.50it/s]Extractor Estimating: 482it [05:46,  1.54it/s]Extractor Estimating: 483it [05:47,  1.56it/s]Extractor Estimating: 484it [05:48,  1.59it/s]Extractor Estimating: 485it [05:48,  1.57it/s]Extractor Estimating: 486it [05:49,  1.59it/s]Extractor Estimating: 487it [05:50,  1.56it/s]Extractor Estimating: 488it [05:50,  1.55it/s]Extractor Estimating: 489it [05:51,  1.55it/s]Extractor Estimating: 490it [05:51,  1.59it/s]Extractor Estimating: 491it [05:52,  1.60it/s]Extractor Estimating: 492it [05:53,  1.58it/s]Extractor Estimating: 493it [05:53,  1.59it/s]Extractor Estimating: 494it [05:54,  1.57it/s]Extractor Estimating: 495it [05:55,  1.58it/s]Extractor Estimating: 496it [05:55,  1.57it/s]Extractor Estimating: 497it [05:56,  1.47it/s]Extractor Estimating: 498it [05:57,  1.51it/s]Extractor Estimating: 499it [05:57,  1.51it/s]Extractor Estimating: 500it [05:58,  1.51it/s]Extractor Estimating: 500it [05:58,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9995 mean pseudo reward: 0.9556932049332086
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 30966
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31066, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31066, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.239, loss:3731.3351
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.011, loss:2511.7714
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.969, loss:1908.2851
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.982, loss:1836.3793
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.972, loss:1626.4153
>> valid entity prec:0.5900, rec:0.5722, f1:0.5810
>> valid relation prec:0.1494, rec:0.0141, f1:0.0257
>> valid relation with NER prec:0.1494, rec:0.0141, f1:0.0257
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.298, loss:1588.4146
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.979, loss:1479.2715
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.976, loss:1419.3003
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.989, loss:1245.9977
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.979, loss:1199.2189
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5039, rec:0.5585, f1:0.5298
>> valid relation prec:0.2038, rec:0.0215, f1:0.0390
>> valid relation with NER prec:0.2038, rec:0.0215, f1:0.0390
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.272, loss:1208.0653
g_step 1200, step 366, avg_time 0.983, loss:1193.5451
g_step 1300, step 49, avg_time 0.962, loss:1068.9258
g_step 1400, step 149, avg_time 0.973, loss:1033.7102
g_step 1500, step 249, avg_time 0.984, loss:1049.9402
>> valid entity prec:0.5713, rec:0.5440, f1:0.5573
>> valid relation prec:0.1763, rec:0.0244, f1:0.0429
>> valid relation with NER prec:0.1763, rec:0.0244, f1:0.0429
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.268, loss:1040.9091
g_step 1700, step 32, avg_time 0.978, loss:1020.6017
g_step 1800, step 132, avg_time 0.972, loss:945.6475
g_step 1900, step 232, avg_time 0.982, loss:953.1382
g_step 2000, step 332, avg_time 0.980, loss:994.9516
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6204, rec:0.4878, f1:0.5462
>> valid relation prec:0.1395, rec:0.0190, f1:0.0334
>> valid relation with NER prec:0.1395, rec:0.0190, f1:0.0334
g_step 2100, step 15, avg_time 2.260, loss:928.4244
g_step 2200, step 115, avg_time 0.983, loss:922.9169
g_step 2300, step 215, avg_time 0.973, loss:877.5829
g_step 2400, step 315, avg_time 0.977, loss:892.8160
g_step 2500, step 415, avg_time 0.975, loss:868.2802
>> valid entity prec:0.5557, rec:0.6374, f1:0.5937
>> valid relation prec:0.1893, rec:0.0517, f1:0.0812
>> valid relation with NER prec:0.1893, rec:0.0517, f1:0.0812
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.280, loss:838.6766
g_step 2700, step 198, avg_time 0.974, loss:830.7523
g_step 2800, step 298, avg_time 0.983, loss:861.1138
g_step 2900, step 398, avg_time 0.980, loss:850.7925
g_step 3000, step 81, avg_time 0.969, loss:815.8955
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5582, rec:0.5779, f1:0.5679
>> valid relation prec:0.1181, rec:0.0319, f1:0.0502
>> valid relation with NER prec:0.1181, rec:0.0319, f1:0.0502
g_step 3100, step 181, avg_time 2.272, loss:799.5527
g_step 3200, step 281, avg_time 0.974, loss:794.4677
g_step 3300, step 381, avg_time 0.983, loss:795.3596
g_step 3400, step 64, avg_time 0.971, loss:729.4796
g_step 3500, step 164, avg_time 0.975, loss:746.3784
>> valid entity prec:0.6028, rec:0.5266, f1:0.5622
>> valid relation prec:0.0797, rec:0.0132, f1:0.0227
>> valid relation with NER prec:0.0797, rec:0.0132, f1:0.0227
g_step 3600, step 264, avg_time 2.252, loss:762.6468
g_step 3700, step 364, avg_time 0.985, loss:764.1158
g_step 3800, step 47, avg_time 0.972, loss:789.5734
g_step 3900, step 147, avg_time 0.977, loss:714.8335
g_step 4000, step 247, avg_time 0.976, loss:736.9750
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5485, rec:0.5216, f1:0.5347
>> valid relation prec:0.0906, rec:0.0218, f1:0.0352
>> valid relation with NER prec:0.0906, rec:0.0218, f1:0.0352
g_step 4100, step 347, avg_time 2.268, loss:725.9220
g_step 4200, step 30, avg_time 0.974, loss:739.9034
g_step 4300, step 130, avg_time 0.975, loss:657.2406
g_step 4400, step 230, avg_time 0.979, loss:694.1652
g_step 4500, step 330, avg_time 0.986, loss:724.5112
>> valid entity prec:0.5751, rec:0.5304, f1:0.5518
>> valid relation prec:0.0839, rec:0.0149, f1:0.0254
>> valid relation with NER prec:0.0839, rec:0.0149, f1:0.0254
g_step 4600, step 13, avg_time 2.252, loss:702.2094
g_step 4700, step 113, avg_time 0.982, loss:646.2767
g_step 4800, step 213, avg_time 0.979, loss:682.5184
g_step 4900, step 313, avg_time 0.979, loss:674.1172
g_step 5000, step 413, avg_time 0.963, loss:662.3029
learning rate was adjusted to 0.0008
>> valid entity prec:0.5574, rec:0.5730, f1:0.5651
>> valid relation prec:0.1458, rec:0.0264, f1:0.0447
>> valid relation with NER prec:0.1458, rec:0.0264, f1:0.0447
g_step 5100, step 96, avg_time 2.263, loss:625.0899
g_step 5200, step 196, avg_time 0.973, loss:625.1131
g_step 5300, step 296, avg_time 0.977, loss:674.7294
g_step 5400, step 396, avg_time 0.974, loss:659.1239
g_step 5500, step 79, avg_time 0.970, loss:591.4028
>> valid entity prec:0.5623, rec:0.5059, f1:0.5326
>> valid relation prec:0.0776, rec:0.0155, f1:0.0258
>> valid relation with NER prec:0.0776, rec:0.0155, f1:0.0258
g_step 5600, step 179, avg_time 2.259, loss:593.9264
g_step 5700, step 279, avg_time 0.967, loss:602.5581
g_step 5800, step 379, avg_time 0.984, loss:651.2832
g_step 5900, step 62, avg_time 0.986, loss:598.1502
g_step 6000, step 162, avg_time 0.977, loss:586.6864
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5601, rec:0.5310, f1:0.5451
>> valid relation prec:0.1236, rec:0.0316, f1:0.0503
>> valid relation with NER prec:0.1236, rec:0.0316, f1:0.0503
g_step 6100, step 262, avg_time 2.252, loss:600.0484
g_step 6200, step 362, avg_time 0.979, loss:618.5677
g_step 6300, step 45, avg_time 0.977, loss:569.0389
g_step 6400, step 145, avg_time 0.975, loss:520.6188
g_step 6500, step 245, avg_time 0.975, loss:568.6028
>> valid entity prec:0.5524, rec:0.5669, f1:0.5595
>> valid relation prec:0.1192, rec:0.0339, f1:0.0528
>> valid relation with NER prec:0.1192, rec:0.0339, f1:0.0528
g_step 6600, step 345, avg_time 2.269, loss:572.9563
g_step 6700, step 28, avg_time 0.974, loss:575.9765
g_step 6800, step 128, avg_time 0.989, loss:543.0290
g_step 6900, step 228, avg_time 0.970, loss:527.3565
g_step 7000, step 328, avg_time 0.972, loss:549.0585
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5824, rec:0.5497, f1:0.5655
>> valid relation prec:0.0717, rec:0.0198, f1:0.0311
>> valid relation with NER prec:0.0717, rec:0.0198, f1:0.0311
g_step 7100, step 11, avg_time 2.261, loss:569.6459
g_step 7200, step 111, avg_time 0.986, loss:492.9528
g_step 7300, step 211, avg_time 0.964, loss:524.3210
g_step 7400, step 311, avg_time 0.973, loss:496.1334
g_step 7500, step 411, avg_time 0.982, loss:546.7109
>> valid entity prec:0.5680, rec:0.5195, f1:0.5427
>> valid relation prec:0.0995, rec:0.0302, f1:0.0463
>> valid relation with NER prec:0.0995, rec:0.0302, f1:0.0463
g_step 7600, step 94, avg_time 2.272, loss:487.5927
g_step 7700, step 194, avg_time 0.980, loss:528.0068
g_step 7800, step 294, avg_time 0.973, loss:521.2529
g_step 7900, step 394, avg_time 0.973, loss:519.2073
g_step 8000, step 77, avg_time 0.960, loss:474.6697
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5820, rec:0.5349, f1:0.5574
>> valid relation prec:0.0719, rec:0.0207, f1:0.0321
>> valid relation with NER prec:0.0719, rec:0.0207, f1:0.0321
g_step 8100, step 177, avg_time 2.264, loss:495.5348
g_step 8200, step 277, avg_time 0.979, loss:501.9225
g_step 8300, step 377, avg_time 0.972, loss:510.0596
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:57:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:57:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-57-48_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:57:50 - WARNING - datasets.builder -   Using custom data configuration default-a396c161e0ea158f
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a396c161e0ea158f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:57:51,438 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:57:51,440 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:57:51,470 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:57:51,472 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:57:51,540 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:57:51,594 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:57:51,594 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:57:51,594 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:57:51,594 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:57:51,594 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:57:51,594 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:57:51,945 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:58:04,719 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:58:04,780 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a396c161e0ea158f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:58:04 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14d8e080b320> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:08,  1.17ba/s] 18%|        | 2/11 [00:01<00:04,  2.11ba/s] 27%|       | 3/11 [00:01<00:02,  2.78ba/s] 36%|      | 4/11 [00:01<00:02,  3.29ba/s] 45%|     | 5/11 [00:01<00:01,  3.66ba/s] 55%|    | 6/11 [00:01<00:01,  3.89ba/s] 64%|   | 7/11 [00:02<00:00,  4.08ba/s] 73%|  | 8/11 [00:02<00:00,  4.22ba/s] 82%| | 9/11 [00:02<00:00,  4.31ba/s] 91%| | 10/11 [00:02<00:00,  4.37ba/s]100%|| 11/11 [00:02<00:00,  3.87ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.14ba/s] 50%|     | 2/4 [00:00<00:00,  3.76ba/s] 75%|  | 3/4 [00:00<00:00,  4.04ba/s]100%|| 4/4 [00:00<00:00,  5.14ba/s]100%|| 4/4 [00:00<00:00,  4.51ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.69ba/s] 27%|       | 3/11 [00:00<00:01,  6.13ba/s] 45%|     | 5/11 [00:00<00:00,  7.97ba/s] 64%|   | 7/11 [00:00<00:00,  8.96ba/s] 82%| | 9/11 [00:01<00:00,  9.64ba/s]100%|| 11/11 [00:01<00:00,  9.22ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.90ba/s] 75%|  | 3/4 [00:00<00:00,  7.42ba/s]100%|| 4/4 [00:00<00:00,  8.24ba/s]
[INFO|trainer.py:414] 2023-08-27 23:58:12,171 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:58:12,233 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:58:12,234 >>   Num examples = 10037
[INFO|trainer.py:1149] 2023-08-27 23:58:12,234 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:58:12,234 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:58:12,234 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:58:12,234 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:58:12,234 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:57,  3.30it/s]  0%|          | 2/785 [00:00<03:50,  3.39it/s]  0%|          | 3/785 [00:00<03:48,  3.42it/s]  1%|          | 4/785 [00:01<03:47,  3.44it/s]  1%|          | 5/785 [00:01<03:46,  3.44it/s]  1%|          | 6/785 [00:01<03:46,  3.45it/s]  1%|          | 7/785 [00:02<03:45,  3.45it/s]  1%|          | 8/785 [00:02<03:45,  3.45it/s]  1%|          | 9/785 [00:02<03:44,  3.45it/s]  1%|         | 10/785 [00:02<03:44,  3.45it/s]  1%|         | 11/785 [00:03<03:44,  3.45it/s]  2%|         | 12/785 [00:03<03:43,  3.45it/s]  2%|         | 13/785 [00:03<03:43,  3.46it/s]  2%|         | 14/785 [00:04<03:43,  3.45it/s]  2%|         | 15/785 [00:04<03:42,  3.45it/s]  2%|         | 16/785 [00:04<03:42,  3.45it/s]  2%|         | 17/785 [00:04<03:50,  3.33it/s]  2%|         | 18/785 [00:05<03:47,  3.37it/s]  2%|         | 19/785 [00:05<03:45,  3.40it/s]  3%|         | 20/785 [00:05<03:44,  3.41it/s]  3%|         | 21/785 [00:06<03:43,  3.42it/s]  3%|         | 22/785 [00:06<03:42,  3.43it/s]  3%|         | 23/785 [00:06<03:41,  3.44it/s]  3%|         | 24/785 [00:06<03:41,  3.44it/s]  3%|         | 25/785 [00:07<03:40,  3.44it/s]  3%|         | 26/785 [00:07<03:40,  3.45it/s]  3%|         | 27/785 [00:07<03:39,  3.45it/s]  4%|         | 28/785 [00:08<03:39,  3.45it/s]  4%|         | 29/785 [00:08<03:39,  3.45it/s]  4%|         | 30/785 [00:08<03:38,  3.45it/s]  4%|         | 31/785 [00:09<03:38,  3.45it/s]  4%|         | 32/785 [00:09<03:38,  3.45it/s]  4%|         | 33/785 [00:09<03:38,  3.44it/s]  4%|         | 34/785 [00:09<03:38,  3.44it/s]  4%|         | 35/785 [00:10<03:37,  3.45it/s]  5%|         | 36/785 [00:10<03:37,  3.45it/s]  5%|         | 37/785 [00:10<03:36,  3.45it/s]  5%|         | 38/785 [00:11<03:36,  3.45it/s]  5%|         | 39/785 [00:11<03:36,  3.45it/s]  5%|         | 40/785 [00:11<03:35,  3.45it/s]  5%|         | 41/785 [00:11<03:35,  3.45it/s]  5%|         | 42/785 [00:12<03:35,  3.45it/s]  5%|         | 43/785 [00:12<03:35,  3.45it/s]  6%|         | 44/785 [00:12<03:34,  3.45it/s]  6%|         | 45/785 [00:13<03:34,  3.45it/s]  6%|         | 46/785 [00:13<03:34,  3.45it/s]  6%|         | 47/785 [00:13<03:33,  3.45it/s]  6%|         | 48/785 [00:13<03:33,  3.45it/s]  6%|         | 49/785 [00:14<03:33,  3.45it/s]  6%|         | 50/785 [00:14<03:32,  3.45it/s]  6%|         | 51/785 [00:14<03:32,  3.45it/s]  7%|         | 52/785 [00:15<03:32,  3.45it/s]  7%|         | 53/785 [00:15<03:32,  3.45it/s]  7%|         | 54/785 [00:15<03:31,  3.45it/s]  7%|         | 55/785 [00:15<03:31,  3.45it/s]  7%|         | 56/785 [00:16<03:31,  3.45it/s]  7%|         | 57/785 [00:16<03:30,  3.45it/s]  7%|         | 58/785 [00:16<03:30,  3.45it/s]  8%|         | 59/785 [00:17<03:30,  3.45it/s]  8%|         | 60/785 [00:17<03:30,  3.45it/s]  8%|         | 61/785 [00:17<03:29,  3.45it/s]  8%|         | 62/785 [00:18<03:29,  3.45it/s]  8%|         | 63/785 [00:18<03:29,  3.45it/s]  8%|         | 64/785 [00:18<03:28,  3.45it/s]  8%|         | 65/785 [00:18<03:28,  3.45it/s]  8%|         | 66/785 [00:19<03:28,  3.45it/s]  9%|         | 67/785 [00:19<03:28,  3.45it/s]  9%|         | 68/785 [00:19<03:27,  3.45it/s]  9%|         | 69/785 [00:20<03:27,  3.45it/s]  9%|         | 70/785 [00:20<03:27,  3.45it/s]  9%|         | 71/785 [00:20<03:27,  3.45it/s]  9%|         | 72/785 [00:20<03:27,  3.44it/s]  9%|         | 73/785 [00:21<03:26,  3.45it/s]  9%|         | 74/785 [00:21<03:26,  3.45it/s] 10%|         | 75/785 [00:21<03:25,  3.45it/s] 10%|         | 76/785 [00:22<03:25,  3.45it/s] 10%|         | 77/785 [00:22<03:25,  3.45it/s] 10%|         | 78/785 [00:22<03:24,  3.45it/s] 10%|         | 79/785 [00:22<03:24,  3.45it/s] 10%|         | 80/785 [00:23<03:24,  3.45it/s] 10%|         | 81/785 [00:23<03:24,  3.45it/s] 10%|         | 82/785 [00:23<03:23,  3.45it/s] 11%|         | 83/785 [00:24<03:23,  3.45it/s] 11%|         | 84/785 [00:24<03:23,  3.45it/s] 11%|         | 85/785 [00:24<03:22,  3.45it/s] 11%|         | 86/785 [00:24<03:22,  3.45it/s] 11%|         | 87/785 [00:25<03:22,  3.45it/s] 11%|         | 88/785 [00:25<03:22,  3.45it/s] 11%|        | 89/785 [00:25<03:21,  3.45it/s] 11%|        | 90/785 [00:26<03:21,  3.45it/s] 12%|        | 91/785 [00:26<03:21,  3.45it/s] 12%|        | 92/785 [00:26<03:20,  3.45it/s] 12%|        | 93/785 [00:27<03:20,  3.45it/s] 12%|        | 94/785 [00:27<03:20,  3.45it/s] 12%|        | 95/785 [00:27<03:20,  3.45it/s] 12%|        | 96/785 [00:27<03:19,  3.45it/s] 12%|        | 97/785 [00:28<03:19,  3.45it/s] 12%|        | 98/785 [00:28<03:19,  3.45it/s] 13%|        | 99/785 [00:28<03:18,  3.45it/s] 13%|        | 100/785 [00:29<03:18,  3.45it/s] 13%|        | 101/785 [00:29<03:18,  3.45it/s] 13%|        | 102/785 [00:29<03:18,  3.45it/s] 13%|        | 103/785 [00:29<03:17,  3.45it/s] 13%|        | 104/785 [00:30<03:17,  3.44it/s] 13%|        | 105/785 [00:30<03:17,  3.44it/s] 14%|        | 106/785 [00:30<03:17,  3.44it/s] 14%|        | 107/785 [00:31<03:16,  3.44it/s] 14%|        | 108/785 [00:31<03:16,  3.44it/s] 14%|        | 109/785 [00:31<03:16,  3.44it/s] 14%|        | 110/785 [00:31<03:16,  3.44it/s] 14%|        | 111/785 [00:32<03:15,  3.44it/s] 14%|        | 112/785 [00:32<03:15,  3.44it/s] 14%|        | 113/785 [00:32<03:15,  3.44it/s] 15%|        | 114/785 [00:33<03:14,  3.44it/s] 15%|        | 115/785 [00:33<03:14,  3.44it/s] 15%|        | 116/785 [00:33<03:14,  3.44it/s] 15%|        | 117/785 [00:33<03:14,  3.44it/s] 15%|        | 118/785 [00:34<03:13,  3.44it/s] 15%|        | 119/785 [00:34<03:13,  3.44it/s] 15%|        | 120/785 [00:34<03:13,  3.45it/s] 15%|        | 121/785 [00:35<03:12,  3.44it/s] 16%|        | 122/785 [00:35<03:12,  3.45it/s] 16%|        | 123/785 [00:35<03:12,  3.44it/s] 16%|        | 124/785 [00:36<03:12,  3.44it/s] 16%|        | 125/785 [00:36<03:11,  3.44it/s] 16%|        | 126/785 [00:36<03:11,  3.44it/s] 16%|        | 127/785 [00:36<03:11,  3.44it/s] 16%|        | 128/785 [00:37<03:10,  3.44it/s] 16%|        | 129/785 [00:37<03:10,  3.44it/s] 17%|        | 130/785 [00:37<03:10,  3.44it/s] 17%|        | 131/785 [00:38<03:09,  3.44it/s] 17%|        | 132/785 [00:38<03:09,  3.44it/s] 17%|        | 133/785 [00:38<03:09,  3.44it/s] 17%|        | 134/785 [00:38<03:09,  3.44it/s] 17%|        | 135/785 [00:39<03:08,  3.44it/s] 17%|        | 136/785 [00:39<03:08,  3.44it/s] 17%|        | 137/785 [00:39<03:08,  3.44it/s] 18%|        | 138/785 [00:40<03:07,  3.44it/s] 18%|        | 139/785 [00:40<03:07,  3.44it/s] 18%|        | 140/785 [00:40<03:07,  3.44it/s] 18%|        | 141/785 [00:40<03:07,  3.44it/s] 18%|        | 142/785 [00:41<03:06,  3.44it/s] 18%|        | 143/785 [00:41<03:06,  3.44it/s] 18%|        | 144/785 [00:41<03:10,  3.37it/s] 18%|        | 145/785 [00:42<03:08,  3.40it/s] 19%|        | 146/785 [00:42<03:07,  3.41it/s] 19%|        | 147/785 [00:42<03:06,  3.42it/s] 19%|        | 148/785 [00:42<03:06,  3.42it/s] 19%|        | 149/785 [00:43<03:05,  3.43it/s] 19%|        | 150/785 [00:43<03:04,  3.43it/s] 19%|        | 151/785 [00:43<03:04,  3.44it/s] 19%|        | 152/785 [00:44<03:04,  3.44it/s] 19%|        | 153/785 [00:44<03:03,  3.44it/s] 20%|        | 154/785 [00:44<03:03,  3.44it/s] 20%|        | 155/785 [00:45<03:06,  3.37it/s] 20%|        | 156/785 [00:45<03:05,  3.40it/s] 20%|        | 157/785 [00:45<02:56,  3.57it/s][INFO|trainer.py:2140] 2023-08-27 23:58:57,822 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:58:57,822 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-27 23:58:57,822 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.63it/s][A
  3%|         | 12/436 [00:00<00:08, 48.08it/s][A
  4%|         | 17/436 [00:00<00:09, 46.37it/s][A
  5%|         | 22/436 [00:00<00:09, 45.68it/s][A
  6%|         | 27/436 [00:00<00:08, 45.47it/s][A
  7%|         | 32/436 [00:00<00:08, 45.21it/s][A
  8%|         | 37/436 [00:00<00:08, 45.00it/s][A
 10%|         | 42/436 [00:00<00:08, 44.84it/s][A
 11%|         | 47/436 [00:01<00:08, 43.37it/s][A
 12%|        | 52/436 [00:01<00:08, 44.97it/s][A
 13%|        | 57/436 [00:01<00:08, 44.91it/s][A
 14%|        | 62/436 [00:01<00:08, 44.61it/s][A
 15%|        | 67/436 [00:01<00:08, 44.63it/s][A
 17%|        | 72/436 [00:01<00:08, 44.75it/s][A
 18%|        | 77/436 [00:01<00:08, 44.72it/s][A
 19%|        | 82/436 [00:01<00:07, 44.72it/s][A
 20%|        | 87/436 [00:01<00:07, 44.59it/s][A
 21%|        | 92/436 [00:02<00:07, 44.56it/s][A
 22%|       | 97/436 [00:02<00:07, 44.79it/s][A
 23%|       | 102/436 [00:02<00:07, 44.72it/s][A
 25%|       | 107/436 [00:02<00:07, 44.67it/s][A
 26%|       | 112/436 [00:02<00:07, 44.67it/s][A
 27%|       | 117/436 [00:02<00:07, 44.72it/s][A
 28%|       | 122/436 [00:02<00:07, 44.58it/s][A
 29%|       | 127/436 [00:02<00:06, 44.59it/s][A
 30%|       | 132/436 [00:02<00:06, 44.57it/s][A
 31%|      | 137/436 [00:03<00:06, 44.63it/s][A
 33%|      | 142/436 [00:03<00:06, 44.70it/s][A
 34%|      | 147/436 [00:03<00:06, 44.71it/s][A
 35%|      | 152/436 [00:03<00:06, 44.65it/s][A
 36%|      | 157/436 [00:03<00:06, 44.64it/s][A
 37%|      | 162/436 [00:03<00:06, 44.74it/s][A
 38%|      | 167/436 [00:03<00:06, 44.64it/s][A
 39%|      | 172/436 [00:03<00:05, 44.74it/s][A
 41%|      | 177/436 [00:03<00:05, 44.73it/s][A
 42%|     | 182/436 [00:04<00:06, 37.60it/s][A
 43%|     | 186/436 [00:04<00:08, 28.67it/s][A
 44%|     | 190/436 [00:04<00:09, 26.70it/s][A
 44%|     | 193/436 [00:04<00:09, 26.81it/s][A
 45%|     | 198/436 [00:04<00:07, 31.35it/s][A
 47%|     | 203/436 [00:04<00:06, 34.91it/s][A
 48%|     | 208/436 [00:04<00:06, 37.66it/s][A
 49%|     | 213/436 [00:05<00:05, 39.68it/s][A
 50%|     | 218/436 [00:05<00:05, 41.17it/s][A
 51%|     | 223/436 [00:05<00:05, 42.37it/s][A
 52%|    | 228/436 [00:05<00:04, 43.14it/s][A
 53%|    | 233/436 [00:05<00:05, 40.04it/s][A
 55%|    | 238/436 [00:05<00:04, 41.25it/s][A
 56%|    | 243/436 [00:05<00:04, 42.26it/s][A
 57%|    | 248/436 [00:05<00:04, 43.10it/s][A
 58%|    | 253/436 [00:06<00:04, 43.72it/s][A
 59%|    | 258/436 [00:06<00:04, 44.14it/s][A
 60%|    | 263/436 [00:06<00:03, 44.34it/s][A
 61%|   | 268/436 [00:06<00:03, 44.53it/s][A
 63%|   | 273/436 [00:06<00:03, 44.20it/s][A
 64%|   | 278/436 [00:06<00:03, 44.11it/s][A
 65%|   | 283/436 [00:06<00:03, 44.08it/s][A
 66%|   | 288/436 [00:06<00:03, 44.31it/s][A
 67%|   | 293/436 [00:06<00:03, 44.47it/s][A
 68%|   | 298/436 [00:07<00:03, 44.79it/s][A
 69%|   | 303/436 [00:07<00:02, 44.90it/s][A
 71%|   | 308/436 [00:07<00:02, 44.95it/s][A
 72%|  | 313/436 [00:07<00:02, 44.71it/s][A
 73%|  | 318/436 [00:07<00:02, 44.53it/s][A
 74%|  | 323/436 [00:07<00:02, 44.31it/s][A
 75%|  | 328/436 [00:07<00:02, 43.92it/s][A
 76%|  | 333/436 [00:07<00:02, 44.50it/s][A
 78%|  | 338/436 [00:07<00:02, 44.62it/s][A
 79%|  | 343/436 [00:08<00:02, 44.85it/s][A
 80%|  | 348/436 [00:08<00:01, 44.90it/s][A
 81%|  | 353/436 [00:08<00:01, 45.02it/s][A
 82%| | 358/436 [00:08<00:01, 44.78it/s][A
 83%| | 363/436 [00:08<00:01, 44.50it/s][A
 84%| | 368/436 [00:08<00:01, 42.30it/s][A
 86%| | 373/436 [00:08<00:01, 43.04it/s][A
 87%| | 378/436 [00:08<00:01, 43.57it/s][A
 88%| | 383/436 [00:08<00:01, 44.06it/s][A
 89%| | 388/436 [00:09<00:01, 44.35it/s][A
 90%| | 393/436 [00:09<00:00, 44.60it/s][A
 91%|| 398/436 [00:09<00:00, 44.66it/s][A
 92%|| 403/436 [00:09<00:00, 44.52it/s][A
 94%|| 408/436 [00:09<00:00, 44.25it/s][A
 95%|| 413/436 [00:09<00:00, 44.11it/s][A
 96%|| 418/436 [00:09<00:00, 44.22it/s][A
 97%|| 423/436 [00:09<00:00, 44.37it/s][A
 98%|| 428/436 [00:09<00:00, 44.64it/s][A
 99%|| 433/436 [00:10<00:00, 44.75it/s][A                                                 
                                                 [A 20%|        | 157/785 [00:55<02:56,  3.57it/s]
100%|| 436/436 [00:10<00:00, 44.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:59:08,520 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-27 23:59:08,855 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:59:12,564 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:59:13,041 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:59:13,250 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|        | 158/785 [01:08<1:13:20,  7.02s/it] 20%|        | 159/785 [01:08<52:15,  5.01s/it]   20%|        | 160/785 [01:08<37:26,  3.60s/it] 21%|        | 161/785 [01:09<27:05,  2.60s/it] 21%|        | 162/785 [01:09<19:51,  1.91s/it] 21%|        | 163/785 [01:09<14:47,  1.43s/it] 21%|        | 164/785 [01:10<11:15,  1.09s/it] 21%|        | 165/785 [01:10<08:46,  1.18it/s] 21%|        | 166/785 [01:10<07:02,  1.46it/s] 21%|       | 167/785 [01:11<05:50,  1.77it/s] 21%|       | 168/785 [01:11<04:59,  2.06it/s] 22%|       | 169/785 [01:11<04:23,  2.33it/s] 22%|       | 170/785 [01:11<04:03,  2.53it/s] 22%|       | 171/785 [01:12<03:44,  2.74it/s] 22%|       | 172/785 [01:12<03:30,  2.91it/s] 22%|       | 173/785 [01:12<03:21,  3.04it/s] 22%|       | 174/785 [01:13<03:14,  3.14it/s] 22%|       | 175/785 [01:13<03:10,  3.21it/s] 22%|       | 176/785 [01:13<03:06,  3.26it/s] 23%|       | 177/785 [01:13<03:04,  3.30it/s] 23%|       | 178/785 [01:14<03:02,  3.33it/s] 23%|       | 179/785 [01:14<03:00,  3.35it/s] 23%|       | 180/785 [01:14<03:00,  3.36it/s] 23%|       | 181/785 [01:15<03:04,  3.28it/s] 23%|       | 182/785 [01:15<03:01,  3.31it/s] 23%|       | 183/785 [01:15<03:00,  3.34it/s] 23%|       | 184/785 [01:16<02:59,  3.36it/s] 24%|       | 185/785 [01:16<02:58,  3.37it/s] 24%|       | 186/785 [01:16<02:57,  3.37it/s] 24%|       | 187/785 [01:16<02:56,  3.38it/s] 24%|       | 188/785 [01:17<02:56,  3.38it/s] 24%|       | 189/785 [01:17<02:56,  3.38it/s] 24%|       | 190/785 [01:17<02:55,  3.39it/s] 24%|       | 191/785 [01:18<02:55,  3.39it/s] 24%|       | 192/785 [01:18<03:01,  3.26it/s] 25%|       | 193/785 [01:18<02:59,  3.30it/s] 25%|       | 194/785 [01:19<02:57,  3.33it/s] 25%|       | 195/785 [01:19<02:56,  3.35it/s] 25%|       | 196/785 [01:19<02:55,  3.36it/s] 25%|       | 197/785 [01:19<02:54,  3.37it/s] 25%|       | 198/785 [01:20<02:53,  3.38it/s] 25%|       | 199/785 [01:20<02:53,  3.38it/s] 25%|       | 200/785 [01:20<02:52,  3.38it/s] 26%|       | 201/785 [01:21<02:52,  3.39it/s] 26%|       | 202/785 [01:21<02:51,  3.39it/s] 26%|       | 203/785 [01:21<02:51,  3.39it/s] 26%|       | 204/785 [01:22<02:51,  3.38it/s] 26%|       | 205/785 [01:22<02:55,  3.31it/s] 26%|       | 206/785 [01:22<02:53,  3.33it/s] 26%|       | 207/785 [01:22<02:52,  3.35it/s] 26%|       | 208/785 [01:23<02:51,  3.36it/s] 27%|       | 209/785 [01:23<02:51,  3.37it/s] 27%|       | 210/785 [01:23<02:50,  3.37it/s] 27%|       | 211/785 [01:24<02:49,  3.38it/s] 27%|       | 212/785 [01:24<02:49,  3.39it/s] 27%|       | 213/785 [01:24<02:48,  3.39it/s] 27%|       | 214/785 [01:24<02:48,  3.39it/s] 27%|       | 215/785 [01:25<02:48,  3.39it/s] 28%|       | 216/785 [01:25<02:56,  3.23it/s] 28%|       | 217/785 [01:25<02:53,  3.28it/s] 28%|       | 218/785 [01:26<02:51,  3.31it/s] 28%|       | 219/785 [01:26<02:49,  3.33it/s] 28%|       | 220/785 [01:26<02:48,  3.35it/s] 28%|       | 221/785 [01:27<02:47,  3.36it/s] 28%|       | 222/785 [01:27<02:46,  3.38it/s] 28%|       | 223/785 [01:27<02:45,  3.39it/s] 29%|       | 224/785 [01:27<02:44,  3.41it/s] 29%|       | 225/785 [01:28<02:43,  3.42it/s] 29%|       | 226/785 [01:28<02:43,  3.42it/s] 29%|       | 227/785 [01:28<02:49,  3.29it/s] 29%|       | 228/785 [01:29<02:46,  3.34it/s] 29%|       | 229/785 [01:29<02:45,  3.37it/s] 29%|       | 230/785 [01:29<02:43,  3.39it/s] 29%|       | 231/785 [01:30<02:42,  3.41it/s] 30%|       | 232/785 [01:30<02:41,  3.42it/s] 30%|       | 233/785 [01:30<02:41,  3.42it/s] 30%|       | 234/785 [01:30<02:40,  3.43it/s] 30%|       | 235/785 [01:31<02:40,  3.43it/s] 30%|       | 236/785 [01:31<02:39,  3.44it/s] 30%|       | 237/785 [01:31<02:39,  3.44it/s] 30%|       | 238/785 [01:32<02:41,  3.39it/s] 30%|       | 239/785 [01:32<02:40,  3.41it/s] 31%|       | 240/785 [01:32<02:39,  3.42it/s] 31%|       | 241/785 [01:32<02:38,  3.42it/s] 31%|       | 242/785 [01:33<02:38,  3.43it/s] 31%|       | 243/785 [01:33<02:37,  3.43it/s] 31%|       | 244/785 [01:33<02:37,  3.44it/s] 31%|       | 245/785 [01:34<02:37,  3.44it/s] 31%|      | 246/785 [01:34<02:36,  3.44it/s] 31%|      | 247/785 [01:34<02:36,  3.44it/s] 32%|      | 248/785 [01:34<02:36,  3.44it/s] 32%|      | 249/785 [01:35<02:39,  3.37it/s] 32%|      | 250/785 [01:35<02:37,  3.39it/s] 32%|      | 251/785 [01:35<02:36,  3.40it/s] 32%|      | 252/785 [01:36<02:36,  3.41it/s] 32%|      | 253/785 [01:36<02:35,  3.42it/s] 32%|      | 254/785 [01:36<02:34,  3.43it/s] 32%|      | 255/785 [01:37<02:34,  3.43it/s] 33%|      | 256/785 [01:37<02:34,  3.43it/s] 33%|      | 257/785 [01:37<02:33,  3.44it/s] 33%|      | 258/785 [01:37<02:33,  3.44it/s] 33%|      | 259/785 [01:38<02:33,  3.44it/s] 33%|      | 260/785 [01:38<02:36,  3.35it/s] 33%|      | 261/785 [01:38<02:35,  3.38it/s] 33%|      | 262/785 [01:39<02:33,  3.40it/s] 34%|      | 263/785 [01:39<02:33,  3.41it/s] 34%|      | 264/785 [01:39<02:32,  3.42it/s] 34%|      | 265/785 [01:39<02:31,  3.42it/s] 34%|      | 266/785 [01:40<02:31,  3.43it/s] 34%|      | 267/785 [01:40<02:30,  3.43it/s] 34%|      | 268/785 [01:40<02:30,  3.43it/s] 34%|      | 269/785 [01:41<02:30,  3.44it/s] 34%|      | 270/785 [01:41<02:29,  3.44it/s] 35%|      | 271/785 [01:41<02:36,  3.28it/s] 35%|      | 272/785 [01:42<02:34,  3.33it/s] 35%|      | 273/785 [01:42<02:32,  3.36it/s] 35%|      | 274/785 [01:42<02:31,  3.38it/s] 35%|      | 275/785 [01:42<02:29,  3.40it/s] 35%|      | 276/785 [01:43<02:28,  3.42it/s] 35%|      | 277/785 [01:43<02:28,  3.42it/s] 35%|      | 278/785 [01:43<02:27,  3.43it/s] 36%|      | 279/785 [01:44<02:27,  3.43it/s] 36%|      | 280/785 [01:44<02:26,  3.44it/s] 36%|      | 281/785 [01:44<02:26,  3.44it/s] 36%|      | 282/785 [01:45<02:34,  3.26it/s] 36%|      | 283/785 [01:45<02:31,  3.32it/s] 36%|      | 284/785 [01:45<02:29,  3.35it/s] 36%|      | 285/785 [01:45<02:28,  3.38it/s] 36%|      | 286/785 [01:46<02:26,  3.40it/s] 37%|      | 287/785 [01:46<02:26,  3.41it/s] 37%|      | 288/785 [01:46<02:25,  3.42it/s] 37%|      | 289/785 [01:47<02:24,  3.43it/s] 37%|      | 290/785 [01:47<02:24,  3.43it/s] 37%|      | 291/785 [01:47<02:23,  3.43it/s] 37%|      | 292/785 [01:47<02:23,  3.44it/s] 37%|      | 293/785 [01:48<02:27,  3.35it/s] 37%|      | 294/785 [01:48<02:25,  3.37it/s] 38%|      | 295/785 [01:48<02:24,  3.39it/s] 38%|      | 296/785 [01:49<02:23,  3.41it/s] 38%|      | 297/785 [01:49<02:22,  3.42it/s] 38%|      | 298/785 [01:49<03:03,  2.66it/s] 38%|      | 299/785 [01:50<03:19,  2.44it/s] 38%|      | 300/785 [01:50<03:01,  2.67it/s] 38%|      | 301/785 [01:51<02:48,  2.87it/s] 38%|      | 302/785 [01:51<02:40,  3.02it/s] 39%|      | 303/785 [01:51<02:33,  3.13it/s] 39%|      | 304/785 [01:51<02:29,  3.22it/s] 39%|      | 305/785 [01:52<02:26,  3.28it/s] 39%|      | 306/785 [01:52<02:23,  3.33it/s] 39%|      | 307/785 [01:52<02:27,  3.25it/s] 39%|      | 308/785 [01:53<02:24,  3.30it/s] 39%|      | 309/785 [01:53<02:22,  3.34it/s] 39%|      | 310/785 [01:53<02:20,  3.37it/s] 40%|      | 311/785 [01:53<02:19,  3.39it/s] 40%|      | 312/785 [01:54<02:18,  3.41it/s] 40%|      | 313/785 [01:54<02:18,  3.42it/s] 40%|      | 314/785 [01:54<02:11,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 00:00:07,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:00:07,047 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 00:00:07,047 >>   Batch size = 8
{'eval_loss': 1.0367066860198975, 'eval_runtime': 10.1462, 'eval_samples_per_second': 343.184, 'eval_steps_per_second': 42.972, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.47it/s][A
  3%|         | 12/436 [00:00<00:08, 48.48it/s][A
  4%|         | 17/436 [00:00<00:08, 46.59it/s][A
  5%|         | 22/436 [00:00<00:09, 45.79it/s][A
  6%|         | 27/436 [00:00<00:09, 45.32it/s][A
  7%|         | 32/436 [00:00<00:08, 44.96it/s][A
  8%|         | 37/436 [00:00<00:10, 38.49it/s][A
 10%|         | 42/436 [00:00<00:09, 40.37it/s][A
 11%|         | 47/436 [00:01<00:09, 41.59it/s][A
 12%|        | 52/436 [00:01<00:08, 42.71it/s][A
 13%|        | 57/436 [00:01<00:08, 43.46it/s][A
 14%|        | 62/436 [00:01<00:08, 44.11it/s][A
 15%|        | 67/436 [00:01<00:08, 44.41it/s][A
 17%|        | 72/436 [00:01<00:08, 44.33it/s][A
 18%|        | 77/436 [00:01<00:08, 43.98it/s][A
 19%|        | 82/436 [00:01<00:08, 43.91it/s][A
 20%|        | 87/436 [00:01<00:07, 44.14it/s][A
 21%|        | 92/436 [00:02<00:07, 44.30it/s][A
 22%|       | 97/436 [00:02<00:07, 44.61it/s][A
 23%|       | 102/436 [00:02<00:07, 44.84it/s][A
 25%|       | 107/436 [00:02<00:07, 44.77it/s][A
 26%|       | 112/436 [00:02<00:07, 44.80it/s][A
 27%|       | 117/436 [00:02<00:07, 44.47it/s][A
 28%|       | 122/436 [00:02<00:07, 44.36it/s][A
 29%|       | 127/436 [00:02<00:06, 44.22it/s][A
 30%|       | 132/436 [00:02<00:06, 44.23it/s][A
 31%|      | 137/436 [00:03<00:06, 44.46it/s][A
 33%|      | 142/436 [00:03<00:06, 44.71it/s][A
 34%|      | 147/436 [00:03<00:06, 44.88it/s][A
 35%|      | 152/436 [00:03<00:06, 45.06it/s][A
 36%|      | 157/436 [00:03<00:06, 44.89it/s][A
 37%|      | 162/436 [00:03<00:06, 44.78it/s][A
 38%|      | 167/436 [00:03<00:06, 44.48it/s][A
 39%|      | 172/436 [00:03<00:06, 38.28it/s][A
 41%|      | 177/436 [00:04<00:06, 40.11it/s][A
 42%|     | 182/436 [00:04<00:06, 41.48it/s][A
 43%|     | 187/436 [00:04<00:05, 42.56it/s][A
 44%|     | 192/436 [00:04<00:05, 43.34it/s][A
 45%|     | 197/436 [00:04<00:05, 43.94it/s][A
 46%|     | 202/436 [00:04<00:05, 44.22it/s][A
 47%|     | 207/436 [00:04<00:05, 44.36it/s][A
 49%|     | 212/436 [00:04<00:05, 44.09it/s][A
 50%|     | 217/436 [00:04<00:04, 43.92it/s][A
 51%|     | 222/436 [00:05<00:04, 44.09it/s][A
 52%|    | 227/436 [00:05<00:04, 44.40it/s][A
 53%|    | 232/436 [00:05<00:04, 44.65it/s][A
 54%|    | 237/436 [00:05<00:04, 44.85it/s][A
 56%|    | 242/436 [00:05<00:04, 44.96it/s][A
 57%|    | 247/436 [00:05<00:04, 44.98it/s][A
 58%|    | 252/436 [00:05<00:04, 44.73it/s][A
 59%|    | 257/436 [00:05<00:04, 44.45it/s][A
 60%|    | 262/436 [00:05<00:03, 44.29it/s][A
 61%|    | 267/436 [00:06<00:03, 44.31it/s][A
 62%|   | 272/436 [00:06<00:03, 44.53it/s][A
 64%|   | 277/436 [00:06<00:03, 44.71it/s][A
 65%|   | 282/436 [00:06<00:03, 44.88it/s][A
 66%|   | 287/436 [00:06<00:03, 44.92it/s][A
 67%|   | 292/436 [00:06<00:03, 44.93it/s][A
 68%|   | 297/436 [00:06<00:03, 44.55it/s][A
 69%|   | 302/436 [00:06<00:03, 44.46it/s][A
 70%|   | 307/436 [00:07<00:03, 40.54it/s][A
 72%|  | 312/436 [00:07<00:02, 41.88it/s][A
 73%|  | 317/436 [00:07<00:02, 42.88it/s][A
 74%|  | 322/436 [00:07<00:02, 43.59it/s][A
 75%|  | 327/436 [00:07<00:02, 44.08it/s][A
 76%|  | 332/436 [00:07<00:02, 44.42it/s][A
 77%|  | 337/436 [00:07<00:02, 44.52it/s][A
 78%|  | 342/436 [00:07<00:02, 44.40it/s][A
 80%|  | 347/436 [00:07<00:02, 44.10it/s][A
 81%|  | 352/436 [00:08<00:01, 43.98it/s][A
 82%| | 357/436 [00:08<00:01, 44.22it/s][A
 83%| | 362/436 [00:08<00:01, 44.45it/s][A
 84%| | 367/436 [00:08<00:01, 44.61it/s][A
 85%| | 372/436 [00:08<00:01, 44.82it/s][A
 86%| | 377/436 [00:08<00:01, 44.90it/s][A
 88%| | 382/436 [00:08<00:01, 44.96it/s][A
 89%| | 387/436 [00:08<00:01, 44.70it/s][A
 90%| | 392/436 [00:08<00:00, 44.45it/s][A
 91%| | 397/436 [00:09<00:00, 44.39it/s][A
 92%|| 402/436 [00:09<00:00, 44.42it/s][A
 93%|| 407/436 [00:09<00:00, 44.51it/s][A
 94%|| 412/436 [00:09<00:00, 44.76it/s][A
 96%|| 417/436 [00:09<00:00, 44.81it/s][A
 97%|| 422/436 [00:09<00:00, 44.94it/s][A
 98%|| 427/436 [00:09<00:00, 44.80it/s][A
 99%|| 432/436 [00:09<00:00, 44.65it/s][A                                                 
                                                 [A 40%|      | 314/785 [02:04<02:11,  3.59it/s]
100%|| 436/436 [00:09<00:00, 44.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:00:17,118 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 00:00:17,297 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:00:22,494 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:00:22,559 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:00:22,608 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|      | 315/785 [02:18<57:38,  7.36s/it] 40%|      | 316/785 [02:19<41:01,  5.25s/it] 40%|      | 317/785 [02:19<29:20,  3.76s/it] 41%|      | 318/785 [02:19<21:11,  2.72s/it] 41%|      | 319/785 [02:19<15:29,  1.99s/it] 41%|      | 320/785 [02:20<11:29,  1.48s/it] 41%|      | 321/785 [02:20<08:42,  1.13s/it] 41%|      | 322/785 [02:20<06:46,  1.14it/s] 41%|      | 323/785 [02:21<05:24,  1.42it/s] 41%|     | 324/785 [02:21<04:27,  1.72it/s] 41%|     | 325/785 [02:21<03:47,  2.02it/s] 42%|     | 326/785 [02:21<03:19,  2.30it/s] 42%|     | 327/785 [02:22<02:59,  2.55it/s] 42%|     | 328/785 [02:22<02:45,  2.76it/s] 42%|     | 329/785 [02:22<02:35,  2.94it/s] 42%|     | 330/785 [02:23<02:28,  3.07it/s] 42%|     | 331/785 [02:23<02:22,  3.18it/s] 42%|     | 332/785 [02:23<02:21,  3.19it/s] 42%|     | 333/785 [02:24<02:18,  3.27it/s] 43%|     | 334/785 [02:24<02:15,  3.32it/s] 43%|     | 335/785 [02:24<02:14,  3.36it/s] 43%|     | 336/785 [02:24<02:12,  3.38it/s] 43%|     | 337/785 [02:25<02:11,  3.40it/s] 43%|     | 338/785 [02:25<02:10,  3.41it/s] 43%|     | 339/785 [02:25<02:10,  3.42it/s] 43%|     | 340/785 [02:26<02:09,  3.43it/s] 43%|     | 341/785 [02:26<02:09,  3.44it/s] 44%|     | 342/785 [02:26<02:08,  3.44it/s] 44%|     | 343/785 [02:26<02:10,  3.39it/s] 44%|     | 344/785 [02:27<02:09,  3.40it/s] 44%|     | 345/785 [02:27<02:08,  3.42it/s] 44%|     | 346/785 [02:27<02:08,  3.42it/s] 44%|     | 347/785 [02:28<02:07,  3.43it/s] 44%|     | 348/785 [02:28<02:07,  3.43it/s] 44%|     | 349/785 [02:28<02:06,  3.44it/s] 45%|     | 350/785 [02:28<02:06,  3.44it/s] 45%|     | 351/785 [02:29<02:06,  3.44it/s] 45%|     | 352/785 [02:29<02:05,  3.44it/s] 45%|     | 353/785 [02:29<02:05,  3.44it/s] 45%|     | 354/785 [02:30<02:07,  3.38it/s] 45%|     | 355/785 [02:30<02:06,  3.40it/s] 45%|     | 356/785 [02:30<02:05,  3.41it/s] 45%|     | 357/785 [02:31<02:05,  3.42it/s] 46%|     | 358/785 [02:31<02:04,  3.43it/s] 46%|     | 359/785 [02:31<02:04,  3.43it/s] 46%|     | 360/785 [02:31<02:03,  3.44it/s] 46%|     | 361/785 [02:32<02:03,  3.44it/s] 46%|     | 362/785 [02:32<02:02,  3.44it/s] 46%|     | 363/785 [02:32<02:02,  3.44it/s] 46%|     | 364/785 [02:33<02:02,  3.44it/s] 46%|     | 365/785 [02:33<02:03,  3.40it/s] 47%|     | 366/785 [02:33<02:02,  3.41it/s] 47%|     | 367/785 [02:33<02:02,  3.42it/s] 47%|     | 368/785 [02:34<02:01,  3.43it/s] 47%|     | 369/785 [02:34<02:01,  3.43it/s] 47%|     | 370/785 [02:34<02:00,  3.44it/s] 47%|     | 371/785 [02:35<02:00,  3.44it/s] 47%|     | 372/785 [02:35<02:00,  3.44it/s] 48%|     | 373/785 [02:35<01:59,  3.44it/s] 48%|     | 374/785 [02:35<01:59,  3.45it/s] 48%|     | 375/785 [02:36<01:59,  3.44it/s] 48%|     | 376/785 [02:36<02:03,  3.31it/s] 48%|     | 377/785 [02:36<02:01,  3.35it/s] 48%|     | 378/785 [02:37<02:00,  3.38it/s] 48%|     | 379/785 [02:37<01:59,  3.40it/s] 48%|     | 380/785 [02:37<01:58,  3.41it/s] 49%|     | 381/785 [02:38<01:58,  3.42it/s] 49%|     | 382/785 [02:38<01:57,  3.43it/s] 49%|     | 383/785 [02:38<01:57,  3.43it/s] 49%|     | 384/785 [02:38<01:56,  3.44it/s] 49%|     | 385/785 [02:39<01:56,  3.43it/s] 49%|     | 386/785 [02:39<01:56,  3.44it/s] 49%|     | 387/785 [02:39<01:59,  3.33it/s] 49%|     | 388/785 [02:40<01:57,  3.37it/s] 50%|     | 389/785 [02:40<01:56,  3.39it/s] 50%|     | 390/785 [02:40<01:55,  3.41it/s] 50%|     | 391/785 [02:40<01:55,  3.42it/s] 50%|     | 392/785 [02:41<01:54,  3.43it/s] 50%|     | 393/785 [02:41<01:54,  3.43it/s] 50%|     | 394/785 [02:41<01:53,  3.44it/s] 50%|     | 395/785 [02:42<01:53,  3.44it/s] 50%|     | 396/785 [02:42<01:53,  3.44it/s] 51%|     | 397/785 [02:42<01:52,  3.44it/s] 51%|     | 398/785 [02:43<01:56,  3.34it/s] 51%|     | 399/785 [02:43<01:54,  3.37it/s] 51%|     | 400/785 [02:43<01:53,  3.38it/s] 51%|     | 401/785 [02:43<01:52,  3.40it/s] 51%|     | 402/785 [02:44<01:52,  3.41it/s] 51%|    | 403/785 [02:44<01:51,  3.42it/s] 51%|    | 404/785 [02:44<01:51,  3.43it/s] 52%|    | 405/785 [02:45<01:50,  3.44it/s] 52%|    | 406/785 [02:45<01:50,  3.44it/s] 52%|    | 407/785 [02:45<01:49,  3.44it/s] 52%|    | 408/785 [02:45<01:49,  3.44it/s] 52%|    | 409/785 [02:46<01:53,  3.33it/s] 52%|    | 410/785 [02:46<01:51,  3.36it/s] 52%|    | 411/785 [02:46<01:50,  3.38it/s] 52%|    | 412/785 [02:47<01:49,  3.40it/s] 53%|    | 413/785 [02:47<01:49,  3.41it/s] 53%|    | 414/785 [02:47<01:48,  3.42it/s] 53%|    | 415/785 [02:48<01:47,  3.43it/s] 53%|    | 416/785 [02:48<01:47,  3.43it/s] 53%|    | 417/785 [02:48<01:47,  3.43it/s] 53%|    | 418/785 [02:48<01:46,  3.44it/s] 53%|    | 419/785 [02:49<01:46,  3.44it/s] 54%|    | 420/785 [02:49<01:50,  3.31it/s] 54%|    | 421/785 [02:49<01:48,  3.35it/s] 54%|    | 422/785 [02:50<02:07,  2.85it/s] 54%|    | 423/785 [02:50<02:28,  2.44it/s] 54%|    | 424/785 [02:51<02:14,  2.68it/s] 54%|    | 425/785 [02:51<02:05,  2.87it/s] 54%|    | 426/785 [02:51<01:58,  3.02it/s] 54%|    | 427/785 [02:51<01:54,  3.13it/s] 55%|    | 428/785 [02:52<01:50,  3.22it/s] 55%|    | 429/785 [02:52<01:48,  3.28it/s] 55%|    | 430/785 [02:52<01:46,  3.33it/s] 55%|    | 431/785 [02:53<01:45,  3.36it/s] 55%|    | 432/785 [02:53<01:44,  3.39it/s] 55%|    | 433/785 [02:53<01:43,  3.40it/s] 55%|    | 434/785 [02:53<01:42,  3.42it/s] 55%|    | 435/785 [02:54<01:47,  3.27it/s] 56%|    | 436/785 [02:54<01:45,  3.32it/s] 56%|    | 437/785 [02:54<01:43,  3.36it/s] 56%|    | 438/785 [02:55<01:42,  3.38it/s] 56%|    | 439/785 [02:55<01:41,  3.40it/s] 56%|    | 440/785 [02:55<01:41,  3.41it/s] 56%|    | 441/785 [02:56<01:40,  3.42it/s] 56%|    | 442/785 [02:56<01:40,  3.43it/s] 56%|    | 443/785 [02:56<01:39,  3.43it/s] 57%|    | 444/785 [02:56<01:39,  3.43it/s] 57%|    | 445/785 [02:57<01:39,  3.43it/s] 57%|    | 446/785 [02:57<01:44,  3.25it/s] 57%|    | 447/785 [02:57<01:42,  3.31it/s] 57%|    | 448/785 [02:58<01:40,  3.35it/s] 57%|    | 449/785 [02:58<01:39,  3.37it/s] 57%|    | 450/785 [02:58<01:38,  3.39it/s] 57%|    | 451/785 [02:59<01:38,  3.41it/s] 58%|    | 452/785 [02:59<01:37,  3.42it/s] 58%|    | 453/785 [02:59<01:37,  3.42it/s] 58%|    | 454/785 [02:59<01:36,  3.43it/s] 58%|    | 455/785 [03:00<01:36,  3.43it/s] 58%|    | 456/785 [03:00<01:35,  3.44it/s] 58%|    | 457/785 [03:00<01:39,  3.29it/s] 58%|    | 458/785 [03:01<01:37,  3.34it/s] 58%|    | 459/785 [03:01<01:36,  3.37it/s] 59%|    | 460/785 [03:01<01:35,  3.39it/s] 59%|    | 461/785 [03:01<01:35,  3.41it/s] 59%|    | 462/785 [03:02<01:34,  3.42it/s] 59%|    | 463/785 [03:02<01:34,  3.42it/s] 59%|    | 464/785 [03:02<01:33,  3.43it/s] 59%|    | 465/785 [03:03<01:33,  3.43it/s] 59%|    | 466/785 [03:03<01:32,  3.44it/s] 59%|    | 467/785 [03:03<01:32,  3.44it/s] 60%|    | 468/785 [03:04<01:33,  3.38it/s] 60%|    | 469/785 [03:04<01:33,  3.40it/s] 60%|    | 470/785 [03:04<01:32,  3.41it/s] 60%|    | 471/785 [03:04<01:27,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 00:01:17,105 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:01:17,105 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 00:01:17,105 >>   Batch size = 8
{'eval_loss': 1.0355809926986694, 'eval_runtime': 9.9009, 'eval_samples_per_second': 351.684, 'eval_steps_per_second': 44.036, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.92it/s][A
  3%|         | 12/436 [00:00<00:08, 48.51it/s][A
  4%|         | 17/436 [00:00<00:08, 46.66it/s][A
  5%|         | 22/436 [00:00<00:09, 45.73it/s][A
  6%|         | 27/436 [00:00<00:09, 45.33it/s][A
  7%|         | 32/436 [00:00<00:08, 45.02it/s][A
  8%|         | 37/436 [00:00<00:08, 44.83it/s][A
 10%|         | 42/436 [00:00<00:08, 44.78it/s][A
 11%|         | 47/436 [00:01<00:08, 44.81it/s][A
 12%|        | 52/436 [00:01<00:08, 44.89it/s][A
 13%|        | 57/436 [00:01<00:08, 44.83it/s][A
 14%|        | 62/436 [00:01<00:08, 44.65it/s][A
 15%|        | 67/436 [00:01<00:08, 44.59it/s][A
 17%|        | 72/436 [00:01<00:08, 44.53it/s][A
 18%|        | 77/436 [00:01<00:08, 44.52it/s][A
 19%|        | 82/436 [00:01<00:07, 44.50it/s][A
 20%|        | 87/436 [00:01<00:07, 44.53it/s][A
 21%|        | 92/436 [00:02<00:07, 44.00it/s][A
 22%|       | 97/436 [00:02<00:07, 44.39it/s][A
 23%|       | 102/436 [00:02<00:07, 44.49it/s][A
 25%|       | 107/436 [00:02<00:07, 44.17it/s][A
 26%|       | 112/436 [00:02<00:07, 44.31it/s][A
 27%|       | 117/436 [00:02<00:07, 44.19it/s][A
 28%|       | 122/436 [00:02<00:07, 44.33it/s][A
 29%|       | 127/436 [00:02<00:06, 44.29it/s][A
 30%|       | 132/436 [00:02<00:06, 44.37it/s][A
 31%|      | 137/436 [00:03<00:06, 44.60it/s][A
 33%|      | 142/436 [00:03<00:06, 44.71it/s][A
 34%|      | 147/436 [00:03<00:06, 44.82it/s][A
 35%|      | 152/436 [00:03<00:06, 44.86it/s][A
 36%|      | 157/436 [00:03<00:06, 44.71it/s][A
 37%|      | 162/436 [00:03<00:06, 44.57it/s][A
 38%|      | 167/436 [00:03<00:06, 44.47it/s][A
 39%|      | 172/436 [00:03<00:05, 44.44it/s][A
 41%|      | 177/436 [00:03<00:05, 44.48it/s][A
 42%|     | 182/436 [00:04<00:05, 44.61it/s][A
 43%|     | 187/436 [00:04<00:05, 44.68it/s][A
 44%|     | 192/436 [00:04<00:05, 44.74it/s][A
 45%|     | 197/436 [00:04<00:05, 44.75it/s][A
 46%|     | 202/436 [00:04<00:05, 44.64it/s][A
 47%|     | 207/436 [00:04<00:05, 44.63it/s][A
 49%|     | 212/436 [00:04<00:05, 44.50it/s][A
 50%|     | 217/436 [00:04<00:04, 44.50it/s][A
 51%|     | 222/436 [00:04<00:04, 44.55it/s][A
 52%|    | 227/436 [00:05<00:04, 44.58it/s][A
 53%|    | 232/436 [00:05<00:04, 44.66it/s][A
 54%|    | 237/436 [00:05<00:04, 44.80it/s][A
 56%|    | 242/436 [00:05<00:04, 44.76it/s][A
 57%|    | 247/436 [00:05<00:04, 44.53it/s][A
 58%|    | 252/436 [00:05<00:04, 44.43it/s][A
 59%|    | 257/436 [00:05<00:04, 44.48it/s][A
 60%|    | 262/436 [00:05<00:03, 44.49it/s][A
 61%|    | 267/436 [00:05<00:03, 44.51it/s][A
 62%|   | 272/436 [00:06<00:03, 44.63it/s][A
 64%|   | 277/436 [00:06<00:03, 44.71it/s][A
 65%|   | 282/436 [00:06<00:03, 44.87it/s][A
 66%|   | 287/436 [00:06<00:03, 44.83it/s][A
 67%|   | 292/436 [00:06<00:03, 44.67it/s][A
 68%|   | 297/436 [00:06<00:03, 44.63it/s][A
 69%|   | 302/436 [00:06<00:03, 44.50it/s][A
 70%|   | 307/436 [00:06<00:02, 44.48it/s][A
 72%|  | 312/436 [00:06<00:02, 44.54it/s][A
 73%|  | 317/436 [00:07<00:02, 44.41it/s][A
 74%|  | 322/436 [00:07<00:02, 44.63it/s][A
 75%|  | 327/436 [00:07<00:02, 44.76it/s][A
 76%|  | 332/436 [00:07<00:02, 44.73it/s][A
 77%|  | 337/436 [00:07<00:02, 44.80it/s][A
 78%|  | 342/436 [00:07<00:02, 44.64it/s][A
 80%|  | 347/436 [00:07<00:01, 44.54it/s][A
 81%|  | 352/436 [00:07<00:01, 44.59it/s][A
 82%| | 357/436 [00:08<00:01, 44.51it/s][A
 83%| | 362/436 [00:08<00:01, 43.97it/s][A
 84%| | 367/436 [00:08<00:01, 44.31it/s][A
 85%| | 372/436 [00:08<00:01, 44.51it/s][A
 86%| | 377/436 [00:08<00:01, 44.54it/s][A
 88%| | 382/436 [00:08<00:01, 44.58it/s][A
 89%| | 387/436 [00:08<00:01, 44.58it/s][A
 90%| | 392/436 [00:08<00:00, 44.55it/s][A
 91%| | 397/436 [00:08<00:00, 44.50it/s][A
 92%|| 402/436 [00:08<00:00, 44.34it/s][A
 93%|| 407/436 [00:09<00:00, 44.48it/s][A
 94%|| 412/436 [00:09<00:00, 44.47it/s][A
 96%|| 417/436 [00:09<00:00, 44.72it/s][A
 97%|| 422/436 [00:09<00:00, 44.76it/s][A
 98%|| 427/436 [00:09<00:00, 44.75it/s][A
 99%|| 432/436 [00:09<00:00, 44.66it/s][A                                                 
                                                 [A 60%|    | 471/785 [03:14<01:27,  3.58it/s]
100%|| 436/436 [00:09<00:00, 44.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:01:27,056 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 00:01:27,288 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:01:31,639 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:01:31,816 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:01:31,911 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|    | 472/785 [03:27<36:46,  7.05s/it] 60%|    | 473/785 [03:28<26:09,  5.03s/it] 60%|    | 474/785 [03:28<18:42,  3.61s/it] 61%|    | 475/785 [03:28<13:29,  2.61s/it] 61%|    | 476/785 [03:28<09:52,  1.92s/it] 61%|    | 477/785 [03:29<07:19,  1.43s/it] 61%|    | 478/785 [03:29<05:33,  1.09s/it] 61%|    | 479/785 [03:29<04:19,  1.18it/s] 61%|    | 480/785 [03:30<03:27,  1.47it/s] 61%|   | 481/785 [03:30<02:51,  1.77it/s] 61%|   | 482/785 [03:30<02:25,  2.08it/s] 62%|   | 483/785 [03:30<02:08,  2.36it/s] 62%|   | 484/785 [03:31<01:58,  2.55it/s] 62%|   | 485/785 [03:31<01:48,  2.76it/s] 62%|   | 486/785 [03:31<01:41,  2.94it/s] 62%|   | 487/785 [03:32<01:37,  3.07it/s] 62%|   | 488/785 [03:32<01:33,  3.17it/s] 62%|   | 489/785 [03:32<01:31,  3.25it/s] 62%|   | 490/785 [03:32<01:29,  3.31it/s] 63%|   | 491/785 [03:33<01:27,  3.35it/s] 63%|   | 492/785 [03:33<01:26,  3.38it/s] 63%|   | 493/785 [03:33<01:25,  3.40it/s] 63%|   | 494/785 [03:34<01:25,  3.41it/s] 63%|   | 495/785 [03:34<01:27,  3.33it/s] 63%|   | 496/785 [03:34<01:25,  3.36it/s] 63%|   | 497/785 [03:35<01:25,  3.39it/s] 63%|   | 498/785 [03:35<01:24,  3.41it/s] 64%|   | 499/785 [03:35<01:23,  3.42it/s] 64%|   | 500/785 [03:35<01:23,  3.43it/s]                                                  64%|   | 500/785 [03:35<01:23,  3.43it/s] 64%|   | 501/785 [03:36<01:22,  3.43it/s] 64%|   | 502/785 [03:36<01:22,  3.43it/s] 64%|   | 503/785 [03:36<01:22,  3.43it/s] 64%|   | 504/785 [03:37<01:21,  3.44it/s] 64%|   | 505/785 [03:37<01:21,  3.44it/s] 64%|   | 506/785 [03:37<01:25,  3.28it/s] 65%|   | 507/785 [03:38<01:23,  3.33it/s] 65%|   | 508/785 [03:38<01:22,  3.36it/s] 65%|   | 509/785 [03:38<01:21,  3.39it/s] 65%|   | 510/785 [03:38<01:20,  3.40it/s] 65%|   | 511/785 [03:39<01:20,  3.42it/s] 65%|   | 512/785 [03:39<01:19,  3.42it/s] 65%|   | 513/785 [03:39<01:19,  3.43it/s] 65%|   | 514/785 [03:40<01:18,  3.43it/s] 66%|   | 515/785 [03:40<01:18,  3.44it/s] 66%|   | 516/785 [03:40<01:18,  3.44it/s] 66%|   | 517/785 [03:40<01:19,  3.35it/s] 66%|   | 518/785 [03:41<01:18,  3.38it/s] 66%|   | 519/785 [03:41<01:18,  3.40it/s] 66%|   | 520/785 [03:41<01:17,  3.41it/s] 66%|   | 521/785 [03:42<01:17,  3.42it/s] 66%|   | 522/785 [03:42<01:16,  3.43it/s] 67%|   | 523/785 [03:42<01:16,  3.43it/s] 67%|   | 524/785 [03:42<01:15,  3.44it/s] 67%|   | 525/785 [03:43<01:15,  3.44it/s] 67%|   | 526/785 [03:43<01:15,  3.44it/s] 67%|   | 527/785 [03:43<01:14,  3.44it/s] 67%|   | 528/785 [03:44<01:16,  3.35it/s] 67%|   | 529/785 [03:44<01:15,  3.38it/s] 68%|   | 530/785 [03:44<01:15,  3.40it/s] 68%|   | 531/785 [03:45<01:14,  3.41it/s] 68%|   | 532/785 [03:45<01:13,  3.42it/s] 68%|   | 533/785 [03:45<01:13,  3.43it/s] 68%|   | 534/785 [03:45<01:13,  3.43it/s] 68%|   | 535/785 [03:46<01:12,  3.44it/s] 68%|   | 536/785 [03:46<01:12,  3.44it/s] 68%|   | 537/785 [03:46<01:13,  3.37it/s] 69%|   | 538/785 [03:47<01:12,  3.39it/s] 69%|   | 539/785 [03:47<01:14,  3.30it/s] 69%|   | 540/785 [03:47<01:13,  3.34it/s] 69%|   | 541/785 [03:47<01:12,  3.37it/s] 69%|   | 542/785 [03:48<01:11,  3.39it/s] 69%|   | 543/785 [03:48<01:11,  3.41it/s] 69%|   | 544/785 [03:48<01:10,  3.42it/s] 69%|   | 545/785 [03:49<01:10,  3.42it/s] 70%|   | 546/785 [03:49<01:09,  3.43it/s] 70%|   | 547/785 [03:49<01:12,  3.29it/s] 70%|   | 548/785 [03:50<01:26,  2.75it/s] 70%|   | 549/785 [03:50<01:29,  2.64it/s] 70%|   | 550/785 [03:51<01:29,  2.64it/s] 70%|   | 551/785 [03:51<01:22,  2.84it/s] 70%|   | 552/785 [03:51<01:17,  3.00it/s] 70%|   | 553/785 [03:51<01:14,  3.12it/s] 71%|   | 554/785 [03:52<01:12,  3.21it/s] 71%|   | 555/785 [03:52<01:10,  3.28it/s] 71%|   | 556/785 [03:52<01:08,  3.33it/s] 71%|   | 557/785 [03:53<01:07,  3.36it/s] 71%|   | 558/785 [03:53<01:07,  3.38it/s] 71%|   | 559/785 [03:53<01:06,  3.40it/s] 71%|  | 560/785 [03:53<01:05,  3.41it/s] 71%|  | 561/785 [03:54<01:05,  3.42it/s] 72%|  | 562/785 [03:54<01:05,  3.43it/s] 72%|  | 563/785 [03:54<01:04,  3.43it/s] 72%|  | 564/785 [03:55<01:04,  3.44it/s] 72%|  | 565/785 [03:55<01:03,  3.44it/s] 72%|  | 566/785 [03:55<01:07,  3.26it/s] 72%|  | 567/785 [03:56<01:05,  3.31it/s] 72%|  | 568/785 [03:56<01:04,  3.36it/s] 72%|  | 569/785 [03:56<01:03,  3.38it/s] 73%|  | 570/785 [03:56<01:03,  3.40it/s] 73%|  | 571/785 [03:57<01:02,  3.41it/s] 73%|  | 572/785 [03:57<01:02,  3.42it/s] 73%|  | 573/785 [03:57<01:01,  3.43it/s] 73%|  | 574/785 [03:58<01:01,  3.43it/s] 73%|  | 575/785 [03:58<01:01,  3.44it/s] 73%|  | 576/785 [03:58<01:00,  3.44it/s] 74%|  | 577/785 [03:59<01:03,  3.26it/s] 74%|  | 578/785 [03:59<01:02,  3.32it/s] 74%|  | 579/785 [03:59<01:01,  3.35it/s] 74%|  | 580/785 [03:59<01:00,  3.38it/s] 74%|  | 581/785 [04:00<00:59,  3.40it/s] 74%|  | 582/785 [04:00<00:59,  3.41it/s] 74%|  | 583/785 [04:00<00:59,  3.42it/s] 74%|  | 584/785 [04:01<00:58,  3.43it/s] 75%|  | 585/785 [04:01<00:58,  3.43it/s] 75%|  | 586/785 [04:01<00:57,  3.43it/s] 75%|  | 587/785 [04:01<00:57,  3.44it/s] 75%|  | 588/785 [04:02<00:59,  3.29it/s] 75%|  | 589/785 [04:02<00:58,  3.33it/s] 75%|  | 590/785 [04:02<00:57,  3.36it/s] 75%|  | 591/785 [04:03<00:57,  3.39it/s] 75%|  | 592/785 [04:03<00:56,  3.40it/s] 76%|  | 593/785 [04:03<00:56,  3.41it/s] 76%|  | 594/785 [04:03<00:55,  3.42it/s] 76%|  | 595/785 [04:04<00:55,  3.43it/s] 76%|  | 596/785 [04:04<00:55,  3.43it/s] 76%|  | 597/785 [04:04<00:54,  3.44it/s] 76%|  | 598/785 [04:05<00:54,  3.44it/s] 76%|  | 599/785 [04:05<00:57,  3.24it/s] 76%|  | 600/785 [04:05<00:56,  3.30it/s] 77%|  | 601/785 [04:06<00:55,  3.34it/s] 77%|  | 602/785 [04:06<00:54,  3.37it/s] 77%|  | 603/785 [04:06<00:53,  3.39it/s] 77%|  | 604/785 [04:06<00:53,  3.40it/s] 77%|  | 605/785 [04:07<00:52,  3.42it/s] 77%|  | 606/785 [04:07<00:52,  3.42it/s] 77%|  | 607/785 [04:07<00:51,  3.43it/s] 77%|  | 608/785 [04:08<00:51,  3.43it/s] 78%|  | 609/785 [04:08<00:51,  3.44it/s] 78%|  | 610/785 [04:08<00:53,  3.26it/s] 78%|  | 611/785 [04:09<00:52,  3.31it/s] 78%|  | 612/785 [04:09<00:51,  3.35it/s] 78%|  | 613/785 [04:09<00:50,  3.38it/s] 78%|  | 614/785 [04:09<00:50,  3.40it/s] 78%|  | 615/785 [04:10<00:49,  3.41it/s] 78%|  | 616/785 [04:10<00:49,  3.42it/s] 79%|  | 617/785 [04:10<00:49,  3.43it/s] 79%|  | 618/785 [04:11<00:48,  3.43it/s] 79%|  | 619/785 [04:11<00:48,  3.43it/s] 79%|  | 620/785 [04:11<00:48,  3.43it/s] 79%|  | 621/785 [04:11<00:49,  3.32it/s] 79%|  | 622/785 [04:12<00:48,  3.36it/s] 79%|  | 623/785 [04:12<00:47,  3.38it/s] 79%|  | 624/785 [04:12<00:47,  3.40it/s] 80%|  | 625/785 [04:13<00:46,  3.41it/s] 80%|  | 626/785 [04:13<00:46,  3.42it/s] 80%|  | 627/785 [04:13<00:46,  3.43it/s] 80%|  | 628/785 [04:13<00:43,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 00:02:26,211 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:02:26,212 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 00:02:26,212 >>   Batch size = 8
{'eval_loss': 1.0413364171981812, 'eval_runtime': 9.775, 'eval_samples_per_second': 356.216, 'eval_steps_per_second': 44.604, 'epoch': 3.0}
{'loss': 0.8204, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.19it/s][A
  3%|         | 12/436 [00:00<00:08, 48.71it/s][A
  4%|         | 17/436 [00:00<00:08, 46.87it/s][A
  5%|         | 22/436 [00:00<00:09, 45.97it/s][A
  6%|         | 27/436 [00:00<00:09, 45.36it/s][A
  7%|         | 32/436 [00:00<00:08, 45.06it/s][A
  8%|         | 37/436 [00:00<00:09, 42.74it/s][A
 10%|         | 42/436 [00:00<00:09, 43.45it/s][A
 11%|         | 47/436 [00:01<00:08, 43.90it/s][A
 12%|        | 52/436 [00:01<00:08, 44.25it/s][A
 13%|        | 57/436 [00:01<00:08, 44.45it/s][A
 14%|        | 62/436 [00:01<00:08, 44.61it/s][A
 15%|        | 67/436 [00:01<00:08, 44.56it/s][A
 17%|        | 72/436 [00:01<00:08, 44.51it/s][A
 18%|        | 77/436 [00:01<00:08, 44.35it/s][A
 19%|        | 82/436 [00:01<00:07, 44.39it/s][A
 20%|        | 87/436 [00:01<00:07, 44.49it/s][A
 21%|        | 92/436 [00:02<00:07, 44.56it/s][A
 22%|       | 97/436 [00:02<00:07, 44.64it/s][A
 23%|       | 102/436 [00:02<00:07, 44.84it/s][A
 25%|       | 107/436 [00:02<00:07, 44.77it/s][A
 26%|       | 112/436 [00:02<00:07, 44.70it/s][A
 27%|       | 117/436 [00:02<00:07, 44.51it/s][A
 28%|       | 122/436 [00:02<00:07, 44.33it/s][A
 29%|       | 127/436 [00:02<00:06, 44.27it/s][A
 30%|       | 132/436 [00:02<00:06, 44.37it/s][A
 31%|      | 137/436 [00:03<00:06, 44.58it/s][A
 33%|      | 142/436 [00:03<00:06, 44.67it/s][A
 34%|      | 147/436 [00:03<00:06, 44.72it/s][A
 35%|      | 152/436 [00:03<00:06, 44.71it/s][A
 36%|      | 157/436 [00:03<00:06, 44.71it/s][A
 37%|      | 162/436 [00:03<00:06, 44.55it/s][A
 38%|      | 167/436 [00:03<00:06, 44.52it/s][A
 39%|      | 172/436 [00:03<00:06, 39.54it/s][A
 41%|      | 177/436 [00:04<00:06, 41.06it/s][A
 42%|     | 182/436 [00:04<00:06, 42.28it/s][A
 43%|     | 187/436 [00:04<00:05, 43.15it/s][A
 44%|     | 192/436 [00:04<00:05, 43.78it/s][A
 45%|     | 197/436 [00:04<00:05, 44.22it/s][A
 46%|     | 202/436 [00:04<00:05, 44.45it/s][A
 47%|     | 207/436 [00:04<00:05, 44.39it/s][A
 49%|     | 212/436 [00:04<00:05, 44.06it/s][A
 50%|     | 217/436 [00:04<00:04, 43.97it/s][A
 51%|     | 222/436 [00:05<00:05, 37.27it/s][A
 52%|    | 227/436 [00:05<00:05, 39.58it/s][A
 53%|    | 232/436 [00:05<00:04, 41.12it/s][A
 54%|    | 237/436 [00:05<00:04, 42.33it/s][A
 56%|    | 242/436 [00:05<00:04, 43.15it/s][A
 57%|    | 247/436 [00:05<00:04, 43.78it/s][A
 58%|    | 252/436 [00:05<00:04, 44.16it/s][A
 59%|    | 257/436 [00:05<00:04, 44.52it/s][A
 60%|    | 262/436 [00:05<00:03, 44.33it/s][A
 61%|    | 267/436 [00:06<00:03, 44.02it/s][A
 62%|   | 272/436 [00:06<00:03, 43.86it/s][A
 64%|   | 277/436 [00:06<00:03, 44.13it/s][A
 65%|   | 282/436 [00:06<00:03, 44.45it/s][A
 66%|   | 287/436 [00:06<00:03, 44.72it/s][A
 67%|   | 292/436 [00:06<00:03, 44.85it/s][A
 68%|   | 297/436 [00:06<00:03, 44.98it/s][A
 69%|   | 302/436 [00:06<00:03, 39.97it/s][A
 70%|   | 307/436 [00:07<00:03, 41.46it/s][A
 72%|  | 312/436 [00:07<00:02, 42.33it/s][A
 73%|  | 317/436 [00:07<00:02, 42.92it/s][A
 74%|  | 322/436 [00:07<00:02, 43.58it/s][A
 75%|  | 327/436 [00:07<00:02, 44.01it/s][A
 76%|  | 332/436 [00:07<00:02, 44.24it/s][A
 77%|  | 337/436 [00:07<00:02, 44.41it/s][A
 78%|  | 342/436 [00:07<00:02, 44.12it/s][A
 80%|  | 347/436 [00:07<00:02, 44.27it/s][A
 81%|  | 352/436 [00:08<00:01, 44.45it/s][A
 82%| | 357/436 [00:08<00:01, 44.44it/s][A
 83%| | 362/436 [00:08<00:01, 44.64it/s][A
 84%| | 367/436 [00:08<00:01, 44.66it/s][A
 85%| | 372/436 [00:08<00:01, 44.88it/s][A
 86%| | 377/436 [00:08<00:01, 44.82it/s][A
 88%| | 382/436 [00:08<00:01, 44.76it/s][A
 89%| | 387/436 [00:08<00:01, 44.52it/s][A
 90%| | 392/436 [00:08<00:00, 44.43it/s][A
 91%| | 397/436 [00:09<00:00, 44.55it/s][A
 92%|| 402/436 [00:09<00:00, 44.61it/s][A
 93%|| 407/436 [00:09<00:00, 44.69it/s][A
 94%|| 412/436 [00:09<00:00, 44.80it/s][A
 96%|| 417/436 [00:09<00:00, 44.80it/s][A
 97%|| 422/436 [00:09<00:00, 44.78it/s][A
 98%|| 427/436 [00:09<00:00, 44.66it/s][A
 99%|| 432/436 [00:09<00:00, 44.50it/s][A                                                 
                                                 [A 80%|  | 628/785 [04:23<00:43,  3.59it/s]
100%|| 436/436 [00:09<00:00, 44.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:02:36,286 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 00:02:36,420 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:02:39,728 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:02:39,918 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:02:40,025 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|  | 629/785 [04:35<16:59,  6.53s/it] 80%|  | 630/785 [04:35<12:04,  4.67s/it] 80%|  | 631/785 [04:35<08:37,  3.36s/it] 81%|  | 632/785 [04:36<06:13,  2.44s/it] 81%|  | 633/785 [04:36<04:33,  1.80s/it] 81%|  | 634/785 [04:36<03:23,  1.35s/it] 81%|  | 635/785 [04:36<02:34,  1.03s/it] 81%|  | 636/785 [04:37<02:00,  1.23it/s] 81%|  | 637/785 [04:37<01:36,  1.53it/s] 81%| | 638/785 [04:37<01:20,  1.83it/s] 81%| | 639/785 [04:38<01:08,  2.12it/s] 82%| | 640/785 [04:38<01:00,  2.39it/s] 82%| | 641/785 [04:38<00:56,  2.55it/s] 82%| | 642/785 [04:39<00:51,  2.77it/s] 82%| | 643/785 [04:39<00:48,  2.94it/s] 82%| | 644/785 [04:39<00:45,  3.08it/s] 82%| | 645/785 [04:39<00:44,  3.18it/s] 82%| | 646/785 [04:40<00:42,  3.26it/s] 82%| | 647/785 [04:40<00:41,  3.31it/s] 83%| | 648/785 [04:40<00:40,  3.35it/s] 83%| | 649/785 [04:41<00:40,  3.38it/s] 83%| | 650/785 [04:41<00:39,  3.40it/s] 83%| | 651/785 [04:41<00:39,  3.41it/s] 83%| | 652/785 [04:41<00:40,  3.28it/s] 83%| | 653/785 [04:42<00:39,  3.33it/s] 83%| | 654/785 [04:42<00:38,  3.36it/s] 83%| | 655/785 [04:42<00:38,  3.39it/s] 84%| | 656/785 [04:43<00:37,  3.41it/s] 84%| | 657/785 [04:43<00:37,  3.42it/s] 84%| | 658/785 [04:43<00:37,  3.42it/s] 84%| | 659/785 [04:43<00:36,  3.43it/s] 84%| | 660/785 [04:44<00:36,  3.43it/s] 84%| | 661/785 [04:44<00:36,  3.44it/s] 84%| | 662/785 [04:44<00:35,  3.44it/s] 84%| | 663/785 [04:45<00:36,  3.32it/s] 85%| | 664/785 [04:45<00:36,  3.36it/s] 85%| | 665/785 [04:45<00:35,  3.38it/s] 85%| | 666/785 [04:46<00:34,  3.40it/s] 85%| | 667/785 [04:46<00:34,  3.41it/s] 85%| | 668/785 [04:46<00:34,  3.42it/s] 85%| | 669/785 [04:46<00:33,  3.43it/s] 85%| | 670/785 [04:47<00:33,  3.43it/s] 85%| | 671/785 [04:47<00:33,  3.44it/s] 86%| | 672/785 [04:47<00:32,  3.44it/s] 86%| | 673/785 [04:48<00:32,  3.44it/s] 86%| | 674/785 [04:48<00:33,  3.27it/s] 86%| | 675/785 [04:48<00:33,  3.32it/s] 86%| | 676/785 [04:48<00:32,  3.36it/s] 86%| | 677/785 [04:49<00:31,  3.39it/s] 86%| | 678/785 [04:49<00:31,  3.40it/s] 86%| | 679/785 [04:49<00:31,  3.42it/s] 87%| | 680/785 [04:50<00:33,  3.11it/s] 87%| | 681/785 [04:50<00:32,  3.21it/s] 87%| | 682/785 [04:50<00:31,  3.27it/s] 87%| | 683/785 [04:51<00:30,  3.33it/s] 87%| | 684/785 [04:51<00:31,  3.25it/s] 87%| | 685/785 [04:51<00:30,  3.30it/s] 87%| | 686/785 [04:52<00:29,  3.34it/s] 88%| | 687/785 [04:52<00:29,  3.38it/s] 88%| | 688/785 [04:52<00:28,  3.40it/s] 88%| | 689/785 [04:52<00:28,  3.41it/s] 88%| | 690/785 [04:53<00:27,  3.42it/s] 88%| | 691/785 [04:53<00:27,  3.43it/s] 88%| | 692/785 [04:53<00:27,  3.44it/s] 88%| | 693/785 [04:54<00:28,  3.28it/s] 88%| | 694/785 [04:54<00:27,  3.33it/s] 89%| | 695/785 [04:54<00:26,  3.36it/s] 89%| | 696/785 [04:54<00:26,  3.39it/s] 89%| | 697/785 [04:55<00:25,  3.40it/s] 89%| | 698/785 [04:55<00:25,  3.42it/s] 89%| | 699/785 [04:55<00:25,  3.42it/s] 89%| | 700/785 [04:56<00:24,  3.43it/s] 89%| | 701/785 [04:56<00:24,  3.43it/s] 89%| | 702/785 [04:56<00:24,  3.44it/s] 90%| | 703/785 [04:57<00:23,  3.44it/s] 90%| | 704/785 [04:57<00:25,  3.20it/s] 90%| | 705/785 [04:57<00:24,  3.27it/s] 90%| | 706/785 [04:57<00:23,  3.32it/s] 90%| | 707/785 [04:58<00:23,  3.36it/s] 90%| | 708/785 [04:58<00:22,  3.38it/s] 90%| | 709/785 [04:58<00:22,  3.40it/s] 90%| | 710/785 [04:59<00:21,  3.41it/s] 91%| | 711/785 [04:59<00:21,  3.42it/s] 91%| | 712/785 [04:59<00:21,  3.43it/s] 91%| | 713/785 [04:59<00:20,  3.43it/s] 91%| | 714/785 [05:00<00:20,  3.44it/s] 91%| | 715/785 [05:00<00:21,  3.28it/s] 91%| | 716/785 [05:00<00:20,  3.32it/s] 91%|| 717/785 [05:01<00:20,  3.36it/s] 91%|| 718/785 [05:01<00:19,  3.39it/s] 92%|| 719/785 [05:01<00:19,  3.41it/s] 92%|| 720/785 [05:02<00:19,  3.42it/s] 92%|| 721/785 [05:02<00:18,  3.43it/s] 92%|| 722/785 [05:02<00:18,  3.43it/s] 92%|| 723/785 [05:02<00:18,  3.44it/s] 92%|| 724/785 [05:03<00:17,  3.44it/s] 92%|| 725/785 [05:03<00:17,  3.44it/s] 92%|| 726/785 [05:03<00:18,  3.24it/s] 93%|| 727/785 [05:04<00:17,  3.30it/s] 93%|| 728/785 [05:04<00:17,  3.34it/s] 93%|| 729/785 [05:04<00:16,  3.37it/s] 93%|| 730/785 [05:05<00:16,  3.40it/s] 93%|| 731/785 [05:05<00:15,  3.41it/s] 93%|| 732/785 [05:05<00:15,  3.43it/s] 93%|| 733/785 [05:05<00:15,  3.43it/s] 94%|| 734/785 [05:06<00:14,  3.44it/s] 94%|| 735/785 [05:06<00:14,  3.44it/s] 94%|| 736/785 [05:06<00:14,  3.44it/s] 94%|| 737/785 [05:07<00:14,  3.34it/s] 94%|| 738/785 [05:07<00:13,  3.37it/s] 94%|| 739/785 [05:07<00:13,  3.40it/s] 94%|| 740/785 [05:07<00:13,  3.41it/s] 94%|| 741/785 [05:08<00:12,  3.42it/s] 95%|| 742/785 [05:08<00:12,  3.43it/s] 95%|| 743/785 [05:08<00:12,  3.44it/s] 95%|| 744/785 [05:09<00:11,  3.44it/s] 95%|| 745/785 [05:09<00:11,  3.44it/s] 95%|| 746/785 [05:09<00:11,  3.44it/s] 95%|| 747/785 [05:09<00:11,  3.45it/s] 95%|| 748/785 [05:10<00:11,  3.33it/s] 95%|| 749/785 [05:10<00:10,  3.36it/s] 96%|| 750/785 [05:10<00:10,  3.39it/s] 96%|| 751/785 [05:11<00:09,  3.41it/s] 96%|| 752/785 [05:11<00:09,  3.42it/s] 96%|| 753/785 [05:11<00:09,  3.43it/s] 96%|| 754/785 [05:12<00:09,  3.44it/s] 96%|| 755/785 [05:12<00:08,  3.44it/s] 96%|| 756/785 [05:12<00:08,  3.44it/s] 96%|| 757/785 [05:12<00:08,  3.44it/s] 97%|| 758/785 [05:13<00:07,  3.44it/s] 97%|| 759/785 [05:13<00:07,  3.37it/s] 97%|| 760/785 [05:13<00:07,  3.39it/s] 97%|| 761/785 [05:14<00:07,  3.41it/s] 97%|| 762/785 [05:14<00:06,  3.42it/s] 97%|| 763/785 [05:14<00:06,  3.43it/s] 97%|| 764/785 [05:14<00:06,  3.43it/s] 97%|| 765/785 [05:15<00:05,  3.44it/s] 98%|| 766/785 [05:15<00:05,  3.44it/s] 98%|| 767/785 [05:15<00:05,  3.44it/s] 98%|| 768/785 [05:16<00:04,  3.44it/s] 98%|| 769/785 [05:16<00:04,  3.44it/s] 98%|| 770/785 [05:16<00:04,  3.35it/s] 98%|| 771/785 [05:17<00:04,  3.38it/s] 98%|| 772/785 [05:17<00:03,  3.40it/s] 98%|| 773/785 [05:17<00:03,  3.41it/s] 99%|| 774/785 [05:17<00:03,  3.42it/s] 99%|| 775/785 [05:18<00:02,  3.43it/s] 99%|| 776/785 [05:18<00:02,  3.44it/s] 99%|| 777/785 [05:18<00:02,  3.44it/s] 99%|| 778/785 [05:19<00:02,  3.34it/s] 99%|| 779/785 [05:19<00:01,  3.38it/s] 99%|| 780/785 [05:19<00:01,  3.39it/s] 99%|| 781/785 [05:19<00:01,  3.30it/s]100%|| 782/785 [05:20<00:00,  3.34it/s]100%|| 783/785 [05:20<00:00,  3.37it/s]100%|| 784/785 [05:20<00:00,  3.39it/s]100%|| 785/785 [05:21<00:00,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 00:03:33,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:03:33,354 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 00:03:33,354 >>   Batch size = 8
{'eval_loss': 1.0471917390823364, 'eval_runtime': 9.9213, 'eval_samples_per_second': 350.963, 'eval_steps_per_second': 43.946, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.20it/s][A
  3%|         | 12/436 [00:00<00:08, 48.77it/s][A
  4%|         | 17/436 [00:00<00:08, 46.91it/s][A
  5%|         | 22/436 [00:00<00:09, 45.83it/s][A
  6%|         | 27/436 [00:00<00:09, 45.41it/s][A
  7%|         | 32/436 [00:00<00:08, 45.12it/s][A
  8%|         | 37/436 [00:00<00:08, 44.89it/s][A
 10%|         | 42/436 [00:00<00:08, 44.82it/s][A
 11%|         | 47/436 [00:01<00:08, 44.92it/s][A
 12%|        | 52/436 [00:01<00:08, 45.00it/s][A
 13%|        | 57/436 [00:01<00:08, 45.03it/s][A
 14%|        | 62/436 [00:01<00:08, 44.69it/s][A
 15%|        | 67/436 [00:01<00:08, 44.73it/s][A
 17%|        | 72/436 [00:01<00:08, 44.70it/s][A
 18%|        | 77/436 [00:01<00:08, 43.12it/s][A
 19%|        | 82/436 [00:01<00:08, 43.62it/s][A
 20%|        | 87/436 [00:01<00:07, 43.95it/s][A
 21%|        | 92/436 [00:02<00:07, 44.40it/s][A
 22%|       | 97/436 [00:02<00:07, 44.64it/s][A
 23%|       | 102/436 [00:02<00:07, 44.71it/s][A
 25%|       | 107/436 [00:02<00:07, 44.53it/s][A
 26%|       | 112/436 [00:02<00:07, 44.61it/s][A
 27%|       | 117/436 [00:02<00:07, 44.29it/s][A
 28%|       | 122/436 [00:02<00:07, 44.41it/s][A
 29%|       | 127/436 [00:02<00:06, 44.52it/s][A
 30%|       | 132/436 [00:02<00:06, 44.60it/s][A
 31%|      | 137/436 [00:03<00:06, 44.72it/s][A
 33%|      | 142/436 [00:03<00:06, 44.88it/s][A
 34%|      | 147/436 [00:03<00:06, 44.72it/s][A
 35%|      | 152/436 [00:03<00:06, 44.74it/s][A
 36%|      | 157/436 [00:03<00:06, 44.70it/s][A
 37%|      | 162/436 [00:03<00:06, 44.64it/s][A
 38%|      | 167/436 [00:03<00:06, 44.56it/s][A
 39%|      | 172/436 [00:03<00:05, 44.63it/s][A
 41%|      | 177/436 [00:03<00:05, 44.69it/s][A
 42%|     | 182/436 [00:04<00:05, 44.76it/s][A
 43%|     | 187/436 [00:04<00:05, 44.86it/s][A
 44%|     | 192/436 [00:04<00:05, 44.70it/s][A
 45%|     | 197/436 [00:04<00:05, 44.74it/s][A
 46%|     | 202/436 [00:04<00:05, 44.70it/s][A
 47%|     | 207/436 [00:04<00:05, 44.64it/s][A
 49%|     | 212/436 [00:04<00:05, 44.66it/s][A
 50%|     | 217/436 [00:04<00:04, 44.60it/s][A
 51%|     | 222/436 [00:04<00:04, 44.64it/s][A
 52%|    | 227/436 [00:05<00:04, 44.63it/s][A
 53%|    | 232/436 [00:05<00:04, 44.84it/s][A
 54%|    | 237/436 [00:05<00:04, 44.71it/s][A
 56%|    | 242/436 [00:05<00:04, 44.70it/s][A
 57%|    | 247/436 [00:05<00:04, 44.76it/s][A
 58%|    | 252/436 [00:05<00:04, 44.74it/s][A
 59%|    | 257/436 [00:05<00:04, 44.68it/s][A
 60%|    | 262/436 [00:05<00:03, 44.61it/s][A
 61%|    | 267/436 [00:05<00:03, 44.63it/s][A
 62%|   | 272/436 [00:06<00:03, 44.73it/s][A
 64%|   | 277/436 [00:06<00:03, 44.82it/s][A
 65%|   | 282/436 [00:06<00:03, 44.70it/s][A
 66%|   | 287/436 [00:06<00:03, 44.78it/s][A
 67%|   | 292/436 [00:06<00:03, 42.70it/s][A
 68%|   | 297/436 [00:06<00:03, 43.48it/s][A
 69%|   | 302/436 [00:06<00:03, 43.86it/s][A
 70%|   | 307/436 [00:06<00:02, 44.13it/s][A
 72%|  | 312/436 [00:06<00:02, 44.30it/s][A
 73%|  | 317/436 [00:07<00:02, 44.55it/s][A
 74%|  | 322/436 [00:07<00:02, 44.55it/s][A
 75%|  | 327/436 [00:07<00:02, 44.56it/s][A
 76%|  | 332/436 [00:07<00:02, 44.38it/s][A
 77%|  | 337/436 [00:07<00:02, 44.52it/s][A
 78%|  | 342/436 [00:07<00:02, 44.72it/s][A
 80%|  | 347/436 [00:07<00:01, 44.73it/s][A
 81%|  | 352/436 [00:07<00:01, 44.79it/s][A
 82%| | 357/436 [00:07<00:01, 44.82it/s][A
 83%| | 362/436 [00:08<00:01, 44.80it/s][A
 84%| | 367/436 [00:08<00:01, 44.66it/s][A
 85%| | 372/436 [00:08<00:01, 44.61it/s][A
 86%| | 377/436 [00:08<00:01, 44.43it/s][A
 88%| | 382/436 [00:08<00:01, 44.56it/s][A
 89%| | 387/436 [00:08<00:01, 44.68it/s][A
 90%| | 392/436 [00:08<00:00, 44.79it/s][A
 91%| | 397/436 [00:08<00:00, 44.80it/s][A
 92%|| 402/436 [00:08<00:00, 44.82it/s][A
 93%|| 407/436 [00:09<00:00, 44.78it/s][A
 94%|| 412/436 [00:09<00:00, 44.76it/s][A
 96%|| 417/436 [00:09<00:00, 44.62it/s][A
 97%|| 422/436 [00:09<00:00, 44.53it/s][A
 98%|| 427/436 [00:09<00:00, 35.42it/s][A
 99%|| 432/436 [00:09<00:00, 37.95it/s][A                                                 
                                                 [A100%|| 785/785 [05:30<00:00,  3.57it/s]
100%|| 436/436 [00:09<00:00, 37.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:03:43,437 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 00:03:43,738 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:03:47,476 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:03:47,625 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:03:47,700 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:03:54,091 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:03:54,116 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314 (score: 1.0355809926986694).
                                                 100%|| 785/785 [05:52<00:00,  3.57it/s]100%|| 785/785 [05:52<00:00,  2.23it/s]
[INFO|trainer.py:1894] 2023-08-28 00:04:04,297 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:04:04,439 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:04:07,777 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:04:07,961 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:04:08,078 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:04:09,019 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:09,019 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:09,019 >>   train_loss               =      0.806
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:09,019 >>   train_runtime            = 0:05:52.00
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:09,020 >>   train_samples            =      10037
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:09,020 >>   train_samples_per_second =    142.567
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:09,020 >>   train_steps_per_second   =       2.23
{'eval_loss': 1.0504004955291748, 'eval_runtime': 9.8654, 'eval_samples_per_second': 352.952, 'eval_steps_per_second': 44.195, 'epoch': 5.0}
{'train_runtime': 352.009, 'train_samples_per_second': 142.567, 'train_steps_per_second': 2.23, 'train_loss': 0.8060468345690684, 'epoch': 5.0}
08/28/2023 00:04:09 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:04:09,511 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:04:09,511 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 00:04:09,511 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.95it/s]  3%|         | 12/436 [00:00<00:08, 49.53it/s]  4%|         | 18/436 [00:00<00:08, 47.59it/s]  5%|         | 23/436 [00:00<00:08, 46.71it/s]  6%|         | 28/436 [00:00<00:08, 46.32it/s]  8%|         | 33/436 [00:00<00:08, 46.06it/s]  9%|         | 38/436 [00:00<00:08, 45.80it/s] 10%|         | 43/436 [00:00<00:08, 45.21it/s] 11%|         | 48/436 [00:01<00:09, 41.34it/s] 12%|        | 53/436 [00:01<00:09, 42.48it/s] 13%|        | 58/436 [00:01<00:08, 43.22it/s] 14%|        | 63/436 [00:01<00:08, 43.90it/s] 16%|        | 68/436 [00:01<00:08, 44.31it/s] 17%|        | 73/436 [00:01<00:08, 44.70it/s] 18%|        | 78/436 [00:01<00:08, 44.61it/s] 19%|        | 83/436 [00:01<00:07, 44.76it/s] 20%|        | 88/436 [00:01<00:07, 44.46it/s] 21%|       | 93/436 [00:02<00:07, 44.40it/s] 22%|       | 98/436 [00:02<00:07, 44.57it/s] 24%|       | 103/436 [00:02<00:07, 44.66it/s] 25%|       | 108/436 [00:02<00:07, 44.86it/s] 26%|       | 113/436 [00:02<00:07, 45.01it/s] 27%|       | 118/436 [00:02<00:07, 45.07it/s] 28%|       | 123/436 [00:02<00:06, 45.18it/s] 29%|       | 128/436 [00:02<00:06, 45.03it/s] 31%|       | 133/436 [00:02<00:06, 44.82it/s] 32%|      | 138/436 [00:03<00:06, 44.71it/s] 33%|      | 143/436 [00:03<00:06, 44.69it/s] 34%|      | 148/436 [00:03<00:06, 44.74it/s] 35%|      | 153/436 [00:03<00:06, 44.89it/s] 36%|      | 158/436 [00:03<00:06, 45.09it/s] 37%|      | 163/436 [00:03<00:06, 45.15it/s] 39%|      | 168/436 [00:03<00:05, 45.13it/s] 40%|      | 173/436 [00:03<00:05, 44.98it/s] 41%|      | 178/436 [00:03<00:05, 44.87it/s] 42%|     | 183/436 [00:04<00:06, 39.23it/s] 43%|     | 188/436 [00:04<00:06, 40.90it/s] 44%|     | 193/436 [00:04<00:05, 42.07it/s] 45%|     | 198/436 [00:04<00:05, 43.03it/s] 47%|     | 203/436 [00:04<00:05, 43.70it/s] 48%|     | 208/436 [00:04<00:05, 44.21it/s] 49%|     | 213/436 [00:04<00:05, 44.52it/s] 50%|     | 218/436 [00:04<00:04, 44.74it/s] 51%|     | 223/436 [00:05<00:04, 44.46it/s] 52%|    | 228/436 [00:05<00:04, 44.16it/s] 53%|    | 233/436 [00:05<00:04, 44.41it/s] 55%|    | 238/436 [00:05<00:04, 44.66it/s] 56%|    | 243/436 [00:05<00:04, 44.82it/s] 57%|    | 248/436 [00:05<00:04, 44.98it/s] 58%|    | 253/436 [00:05<00:04, 45.05it/s] 59%|    | 258/436 [00:05<00:03, 45.19it/s] 60%|    | 263/436 [00:05<00:03, 45.08it/s] 61%|   | 268/436 [00:06<00:03, 44.65it/s] 63%|   | 273/436 [00:06<00:03, 44.49it/s] 64%|   | 278/436 [00:06<00:03, 44.68it/s] 65%|   | 283/436 [00:06<00:03, 44.78it/s] 66%|   | 288/436 [00:06<00:03, 44.99it/s] 67%|   | 293/436 [00:06<00:03, 44.95it/s] 68%|   | 298/436 [00:06<00:03, 45.04it/s] 69%|   | 303/436 [00:06<00:02, 44.91it/s] 71%|   | 308/436 [00:06<00:02, 44.81it/s] 72%|  | 313/436 [00:07<00:02, 44.53it/s] 73%|  | 318/436 [00:07<00:02, 39.72it/s] 74%|  | 323/436 [00:07<00:02, 41.29it/s] 75%|  | 328/436 [00:07<00:02, 42.50it/s] 76%|  | 333/436 [00:07<00:02, 43.22it/s] 78%|  | 338/436 [00:07<00:02, 43.85it/s] 79%|  | 343/436 [00:07<00:02, 44.29it/s] 80%|  | 348/436 [00:07<00:01, 44.63it/s] 81%|  | 353/436 [00:07<00:01, 44.61it/s] 82%| | 358/436 [00:08<00:01, 44.31it/s] 83%| | 363/436 [00:08<00:01, 44.24it/s] 84%| | 368/436 [00:08<00:01, 44.37it/s] 86%| | 373/436 [00:08<00:01, 44.53it/s] 87%| | 378/436 [00:08<00:01, 44.68it/s] 88%| | 383/436 [00:08<00:01, 44.89it/s] 89%| | 388/436 [00:08<00:01, 45.08it/s] 90%| | 393/436 [00:08<00:00, 45.11it/s] 91%|| 398/436 [00:08<00:00, 45.01it/s] 92%|| 403/436 [00:09<00:00, 44.77it/s] 94%|| 408/436 [00:09<00:00, 44.57it/s] 95%|| 413/436 [00:09<00:00, 44.60it/s] 96%|| 418/436 [00:09<00:00, 44.55it/s] 97%|| 423/436 [00:09<00:00, 44.82it/s] 98%|| 428/436 [00:09<00:00, 44.93it/s] 99%|| 433/436 [00:09<00:00, 45.04it/s]100%|| 436/436 [00:09<00:00, 44.46it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:04:19,334 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   eval_loss               =     1.0356
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   eval_runtime            = 0:00:09.82
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   eval_samples_per_second =    354.472
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   eval_steps_per_second   =     44.385
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:04:19,335 >>   perplexity              =     2.8167
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_2', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:21<06:52, 21.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:35<05:11, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:55<05:15, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:11<04:35, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:28<04:17, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:44<03:56, 16.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:02<03:44, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:20<03:28, 17.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:38<03:14, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:55<02:54, 17.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:11<02:34, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:29<02:17, 17.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:45<01:58, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:02<01:42, 17.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:19<01:24, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:37<01:09, 17.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:53<00:50, 16.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:09<00:33, 16.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:27<00:16, 16.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:42<00:00, 16.50s/it]Generating: 100%|| 20/20 [05:42<00:00, 17.14s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of Lpez 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of Lpez 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Dmontagne " , including Boulogne \'s " Montessemble des deux de Chteau des Gains " . Head Entity : Montessemble des deux de Chteau des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of Franois Renoul .\')', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:30, 30.48s/it]Extractor Estimating: 2it [00:31, 13.29s/it]Extractor Estimating: 3it [00:32,  7.52s/it]Extractor Estimating: 4it [00:32,  4.79s/it]Extractor Estimating: 5it [00:33,  3.31s/it]Extractor Estimating: 6it [00:34,  2.38s/it]Extractor Estimating: 7it [00:36,  2.36s/it]Extractor Estimating: 8it [00:37,  1.80s/it]Extractor Estimating: 9it [00:37,  1.44s/it]Extractor Estimating: 10it [00:38,  1.20s/it]Extractor Estimating: 11it [00:39,  1.04s/it]Extractor Estimating: 12it [00:39,  1.11it/s]Extractor Estimating: 13it [00:40,  1.24it/s]Extractor Estimating: 14it [00:41,  1.30it/s]Extractor Estimating: 15it [00:41,  1.40it/s]Extractor Estimating: 16it [00:42,  1.50it/s]Extractor Estimating: 17it [00:42,  1.58it/s]Extractor Estimating: 18it [00:43,  1.57it/s]Extractor Estimating: 19it [00:43,  1.60it/s]Extractor Estimating: 20it [00:44,  1.58it/s]Extractor Estimating: 21it [00:45,  1.57it/s]Extractor Estimating: 22it [00:45,  1.60it/s]Extractor Estimating: 23it [00:46,  1.60it/s]Extractor Estimating: 24it [00:47,  1.61it/s]Extractor Estimating: 25it [00:47,  1.60it/s]Extractor Estimating: 26it [00:48,  1.64it/s]Extractor Estimating: 27it [00:48,  1.66it/s]Extractor Estimating: 28it [00:49,  1.68it/s]Extractor Estimating: 29it [00:50,  1.63it/s]Extractor Estimating: 30it [00:50,  1.62it/s]Extractor Estimating: 31it [00:51,  1.61it/s]Extractor Estimating: 32it [00:52,  1.60it/s]Extractor Estimating: 33it [00:52,  1.57it/s]Extractor Estimating: 34it [00:53,  1.60it/s]Extractor Estimating: 35it [00:53,  1.58it/s]Extractor Estimating: 36it [00:54,  1.51it/s]Extractor Estimating: 37it [00:55,  1.49it/s]Extractor Estimating: 38it [00:55,  1.57it/s]Extractor Estimating: 39it [00:56,  1.53it/s]Extractor Estimating: 40it [00:57,  1.49it/s]Extractor Estimating: 41it [00:57,  1.55it/s]Extractor Estimating: 42it [00:58,  1.53it/s]Extractor Estimating: 43it [00:59,  1.54it/s]Extractor Estimating: 44it [00:59,  1.57it/s]Extractor Estimating: 45it [01:00,  1.57it/s]Extractor Estimating: 46it [01:01,  1.53it/s]Extractor Estimating: 47it [01:01,  1.59it/s]Extractor Estimating: 48it [01:02,  1.55it/s]Extractor Estimating: 49it [01:02,  1.59it/s]Extractor Estimating: 50it [01:03,  1.65it/s]Extractor Estimating: 51it [01:04,  1.37it/s]Extractor Estimating: 52it [01:05,  1.40it/s]Extractor Estimating: 53it [01:05,  1.39it/s]Extractor Estimating: 54it [01:06,  1.47it/s]Extractor Estimating: 55it [01:07,  1.49it/s]Extractor Estimating: 56it [01:07,  1.52it/s]Extractor Estimating: 57it [01:08,  1.49it/s]Extractor Estimating: 58it [01:09,  1.55it/s]Extractor Estimating: 59it [01:09,  1.56it/s]Extractor Estimating: 60it [01:10,  1.52it/s]Extractor Estimating: 61it [01:11,  1.53it/s]Extractor Estimating: 62it [01:11,  1.51it/s]Extractor Estimating: 63it [01:12,  1.57it/s]Extractor Estimating: 64it [01:13,  1.55it/s]Extractor Estimating: 65it [01:13,  1.50it/s]Extractor Estimating: 66it [01:14,  1.48it/s]Extractor Estimating: 67it [01:15,  1.48it/s]Extractor Estimating: 68it [01:15,  1.51it/s]Extractor Estimating: 69it [01:16,  1.49it/s]Extractor Estimating: 70it [01:17,  1.50it/s]Extractor Estimating: 71it [01:17,  1.55it/s]Extractor Estimating: 72it [01:18,  1.56it/s]Extractor Estimating: 73it [01:18,  1.53it/s]Extractor Estimating: 74it [01:19,  1.53it/s]Extractor Estimating: 75it [01:20,  1.54it/s]Extractor Estimating: 76it [01:20,  1.54it/s]Extractor Estimating: 77it [01:22,  1.16it/s]Extractor Estimating: 78it [01:22,  1.23it/s]Extractor Estimating: 79it [01:23,  1.33it/s]Extractor Estimating: 80it [01:24,  1.38it/s]Extractor Estimating: 81it [01:24,  1.42it/s]Extractor Estimating: 82it [01:25,  1.45it/s]Extractor Estimating: 83it [01:26,  1.48it/s]Extractor Estimating: 84it [01:26,  1.46it/s]Extractor Estimating: 85it [01:27,  1.47it/s]Extractor Estimating: 86it [01:28,  1.53it/s]Extractor Estimating: 87it [01:29,  1.42it/s]Extractor Estimating: 88it [01:29,  1.37it/s]Extractor Estimating: 89it [01:30,  1.42it/s]Extractor Estimating: 90it [01:31,  1.47it/s]Extractor Estimating: 91it [01:31,  1.56it/s]Extractor Estimating: 92it [01:32,  1.54it/s]Extractor Estimating: 93it [01:33,  1.46it/s]Extractor Estimating: 94it [01:33,  1.51it/s]Extractor Estimating: 95it [01:34,  1.52it/s]Extractor Estimating: 96it [01:34,  1.54it/s]Extractor Estimating: 97it [01:35,  1.47it/s]Extractor Estimating: 98it [01:36,  1.48it/s]Extractor Estimating: 99it [01:36,  1.50it/s]Extractor Estimating: 100it [01:37,  1.50it/s]Extractor Estimating: 101it [01:38,  1.51it/s]Extractor Estimating: 102it [01:38,  1.53it/s]Extractor Estimating: 103it [01:39,  1.58it/s]Extractor Estimating: 104it [01:40,  1.55it/s]Extractor Estimating: 105it [01:40,  1.58it/s]Extractor Estimating: 106it [01:41,  1.58it/s]Extractor Estimating: 107it [01:42,  1.60it/s]Extractor Estimating: 108it [01:42,  1.59it/s]Extractor Estimating: 109it [01:43,  1.63it/s]Extractor Estimating: 110it [01:43,  1.71it/s]Extractor Estimating: 111it [01:44,  1.60it/s]Extractor Estimating: 112it [01:45,  1.58it/s]Extractor Estimating: 113it [01:45,  1.62it/s]Extractor Estimating: 114it [01:46,  1.64it/s]Extractor Estimating: 115it [01:47,  1.56it/s]Extractor Estimating: 116it [01:47,  1.55it/s]Extractor Estimating: 117it [01:48,  1.52it/s]Extractor Estimating: 118it [01:49,  1.55it/s]Extractor Estimating: 119it [01:49,  1.55it/s]Extractor Estimating: 120it [01:50,  1.56it/s]Extractor Estimating: 121it [01:50,  1.61it/s]Extractor Estimating: 122it [01:51,  1.64it/s]Extractor Estimating: 123it [01:52,  1.64it/s]Extractor Estimating: 124it [01:52,  1.66it/s]Extractor Estimating: 125it [01:53,  1.64it/s]Extractor Estimating: 126it [01:53,  1.67it/s]Extractor Estimating: 127it [01:54,  1.56it/s]Extractor Estimating: 128it [01:55,  1.57it/s]Extractor Estimating: 129it [01:55,  1.52it/s]Extractor Estimating: 130it [01:56,  1.49it/s]Extractor Estimating: 131it [01:57,  1.53it/s]Extractor Estimating: 132it [01:57,  1.49it/s]Extractor Estimating: 133it [01:58,  1.51it/s]Extractor Estimating: 134it [01:59,  1.49it/s]Extractor Estimating: 135it [01:59,  1.48it/s]Extractor Estimating: 136it [02:00,  1.53it/s]Extractor Estimating: 137it [02:01,  1.55it/s]Extractor Estimating: 138it [02:01,  1.56it/s]Extractor Estimating: 139it [02:02,  1.59it/s]Extractor Estimating: 140it [02:03,  1.61it/s]Extractor Estimating: 141it [02:03,  1.54it/s]Extractor Estimating: 142it [02:04,  1.52it/s]Extractor Estimating: 143it [02:05,  1.52it/s]Extractor Estimating: 144it [02:06,  1.33it/s]Extractor Estimating: 145it [02:06,  1.40it/s]Extractor Estimating: 146it [02:07,  1.41it/s]Extractor Estimating: 147it [02:08,  1.45it/s]Extractor Estimating: 148it [02:08,  1.43it/s]Extractor Estimating: 149it [02:09,  1.46it/s]Extractor Estimating: 150it [02:10,  1.48it/s]Extractor Estimating: 151it [02:10,  1.52it/s]Extractor Estimating: 152it [02:11,  1.59it/s]Extractor Estimating: 153it [02:11,  1.63it/s]Extractor Estimating: 154it [02:12,  1.68it/s]Extractor Estimating: 155it [02:12,  1.70it/s]Extractor Estimating: 156it [02:13,  1.66it/s]Extractor Estimating: 157it [02:14,  1.67it/s]Extractor Estimating: 158it [02:14,  1.66it/s]Extractor Estimating: 159it [02:15,  1.64it/s]Extractor Estimating: 160it [02:15,  1.68it/s]Extractor Estimating: 161it [02:16,  1.72it/s]Extractor Estimating: 162it [02:17,  1.70it/s]Extractor Estimating: 163it [02:17,  1.73it/s]Extractor Estimating: 164it [02:18,  1.74it/s]Extractor Estimating: 165it [02:18,  1.71it/s]Extractor Estimating: 166it [02:19,  1.70it/s]Extractor Estimating: 167it [02:20,  1.67it/s]Extractor Estimating: 168it [02:20,  1.70it/s]Extractor Estimating: 169it [02:21,  1.74it/s]Extractor Estimating: 170it [02:21,  1.70it/s]Extractor Estimating: 171it [02:24,  1.33s/it]Extractor Estimating: 172it [02:25,  1.12s/it]Extractor Estimating: 173it [02:26,  1.04it/s]Extractor Estimating: 174it [02:26,  1.18it/s]Extractor Estimating: 175it [02:27,  1.30it/s]Extractor Estimating: 176it [02:27,  1.38it/s]Extractor Estimating: 177it [02:28,  1.47it/s]Extractor Estimating: 178it [02:28,  1.55it/s]Extractor Estimating: 179it [02:29,  1.55it/s]Extractor Estimating: 180it [02:30,  1.43it/s]Extractor Estimating: 181it [02:31,  1.49it/s]Extractor Estimating: 182it [02:31,  1.56it/s]Extractor Estimating: 183it [02:32,  1.58it/s]Extractor Estimating: 184it [02:32,  1.51it/s]Extractor Estimating: 185it [02:33,  1.59it/s]Extractor Estimating: 186it [02:34,  1.57it/s]Extractor Estimating: 187it [02:34,  1.53it/s]Extractor Estimating: 188it [02:35,  1.57it/s]Extractor Estimating: 189it [02:36,  1.56it/s]Extractor Estimating: 190it [02:36,  1.59it/s]Extractor Estimating: 191it [02:37,  1.62it/s]Extractor Estimating: 192it [02:37,  1.62it/s]Extractor Estimating: 193it [02:38,  1.66it/s]Extractor Estimating: 194it [02:39,  1.67it/s]Extractor Estimating: 195it [02:39,  1.64it/s]Extractor Estimating: 196it [02:40,  1.65it/s]Extractor Estimating: 197it [02:40,  1.66it/s]Extractor Estimating: 198it [02:41,  1.63it/s]Extractor Estimating: 199it [02:42,  1.65it/s]Extractor Estimating: 200it [02:42,  1.63it/s]Extractor Estimating: 201it [02:43,  1.61it/s]Extractor Estimating: 202it [02:44,  1.47it/s]Extractor Estimating: 203it [02:44,  1.53it/s]Extractor Estimating: 204it [02:45,  1.53it/s]Extractor Estimating: 205it [02:46,  1.53it/s]Extractor Estimating: 206it [02:46,  1.54it/s]Extractor Estimating: 207it [02:47,  1.41it/s]Extractor Estimating: 208it [02:48,  1.43it/s]Extractor Estimating: 209it [02:48,  1.47it/s]Extractor Estimating: 210it [02:49,  1.49it/s]Extractor Estimating: 211it [02:50,  1.48it/s]Extractor Estimating: 212it [02:51,  1.42it/s]Extractor Estimating: 213it [02:51,  1.46it/s]Extractor Estimating: 214it [02:52,  1.49it/s]Extractor Estimating: 215it [02:53,  1.46it/s]Extractor Estimating: 216it [02:53,  1.44it/s]Extractor Estimating: 217it [02:54,  1.45it/s]Extractor Estimating: 218it [02:55,  1.43it/s]Extractor Estimating: 219it [02:55,  1.42it/s]Extractor Estimating: 220it [02:56,  1.47it/s]Extractor Estimating: 221it [02:57,  1.53it/s]Extractor Estimating: 222it [02:57,  1.46it/s]Extractor Estimating: 223it [02:58,  1.52it/s]Extractor Estimating: 224it [02:59,  1.52it/s]Extractor Estimating: 225it [02:59,  1.49it/s]Extractor Estimating: 226it [03:00,  1.49it/s]Extractor Estimating: 227it [03:01,  1.49it/s]Extractor Estimating: 228it [03:01,  1.52it/s]Extractor Estimating: 229it [03:02,  1.56it/s]Extractor Estimating: 230it [03:03,  1.53it/s]Extractor Estimating: 231it [03:03,  1.53it/s]Extractor Estimating: 232it [03:04,  1.54it/s]Extractor Estimating: 233it [03:05,  1.49it/s]Extractor Estimating: 234it [03:05,  1.56it/s]Extractor Estimating: 235it [03:06,  1.56it/s]Extractor Estimating: 236it [03:06,  1.57it/s]Extractor Estimating: 237it [03:07,  1.56it/s]Extractor Estimating: 238it [03:08,  1.53it/s]Extractor Estimating: 239it [03:08,  1.54it/s]Extractor Estimating: 240it [03:09,  1.57it/s]Extractor Estimating: 241it [03:10,  1.54it/s]Extractor Estimating: 242it [03:10,  1.57it/s]Extractor Estimating: 243it [03:11,  1.55it/s]Extractor Estimating: 244it [03:12,  1.52it/s]Extractor Estimating: 245it [03:12,  1.53it/s]Extractor Estimating: 246it [03:13,  1.52it/s]Extractor Estimating: 247it [03:14,  1.57it/s]Extractor Estimating: 248it [03:14,  1.46it/s]Extractor Estimating: 249it [03:15,  1.46it/s]Extractor Estimating: 250it [03:16,  1.51it/s]Extractor Estimating: 251it [03:16,  1.52it/s]Extractor Estimating: 252it [03:17,  1.50it/s]Extractor Estimating: 253it [03:18,  1.48it/s]Extractor Estimating: 254it [03:18,  1.53it/s]Extractor Estimating: 255it [03:19,  1.52it/s]Extractor Estimating: 256it [03:20,  1.51it/s]Extractor Estimating: 257it [03:20,  1.54it/s]Extractor Estimating: 258it [03:21,  1.56it/s]Extractor Estimating: 259it [03:21,  1.56it/s]Extractor Estimating: 260it [03:22,  1.53it/s]Extractor Estimating: 261it [03:23,  1.51it/s]Extractor Estimating: 262it [03:24,  1.39it/s]Extractor Estimating: 263it [03:24,  1.44it/s]Extractor Estimating: 264it [03:25,  1.45it/s]Extractor Estimating: 265it [03:26,  1.46it/s]Extractor Estimating: 266it [03:26,  1.45it/s]Extractor Estimating: 267it [03:27,  1.52it/s]Extractor Estimating: 268it [03:28,  1.57it/s]Extractor Estimating: 269it [03:28,  1.63it/s]Extractor Estimating: 270it [03:29,  1.58it/s]Extractor Estimating: 271it [03:29,  1.60it/s]Extractor Estimating: 272it [03:30,  1.57it/s]Extractor Estimating: 273it [03:31,  1.60it/s]Extractor Estimating: 274it [03:31,  1.55it/s]Extractor Estimating: 275it [03:32,  1.53it/s]Extractor Estimating: 276it [03:33,  1.49it/s]Extractor Estimating: 277it [03:33,  1.54it/s]Extractor Estimating: 278it [03:34,  1.58it/s]Extractor Estimating: 279it [03:35,  1.58it/s]Extractor Estimating: 280it [03:35,  1.54it/s]Extractor Estimating: 281it [03:36,  1.57it/s]Extractor Estimating: 282it [03:37,  1.56it/s]Extractor Estimating: 283it [03:37,  1.58it/s]Extractor Estimating: 284it [03:38,  1.55it/s]Extractor Estimating: 285it [03:38,  1.53it/s]Extractor Estimating: 286it [03:39,  1.54it/s]Extractor Estimating: 287it [03:40,  1.52it/s]Extractor Estimating: 288it [03:40,  1.52it/s]Extractor Estimating: 289it [03:41,  1.50it/s]Extractor Estimating: 290it [03:42,  1.49it/s]Extractor Estimating: 291it [03:42,  1.48it/s]Extractor Estimating: 292it [03:43,  1.46it/s]Extractor Estimating: 293it [03:44,  1.50it/s]Extractor Estimating: 294it [03:45,  1.37it/s]Extractor Estimating: 295it [03:45,  1.42it/s]Extractor Estimating: 296it [03:46,  1.40it/s]Extractor Estimating: 297it [03:47,  1.37it/s]Extractor Estimating: 298it [03:48,  1.37it/s]Extractor Estimating: 299it [03:48,  1.40it/s]Extractor Estimating: 300it [03:49,  1.47it/s]Extractor Estimating: 301it [03:49,  1.54it/s]Extractor Estimating: 302it [03:50,  1.59it/s]Extractor Estimating: 303it [03:51,  1.60it/s]Extractor Estimating: 304it [03:51,  1.64it/s]Extractor Estimating: 305it [03:52,  1.63it/s]Extractor Estimating: 306it [03:52,  1.62it/s]Extractor Estimating: 307it [03:53,  1.62it/s]Extractor Estimating: 308it [03:54,  1.62it/s]Extractor Estimating: 309it [03:54,  1.69it/s]Extractor Estimating: 310it [03:55,  1.70it/s]Extractor Estimating: 311it [03:55,  1.69it/s]Extractor Estimating: 312it [03:56,  1.64it/s]Extractor Estimating: 313it [03:57,  1.65it/s]Extractor Estimating: 314it [03:57,  1.63it/s]Extractor Estimating: 315it [03:58,  1.66it/s]Extractor Estimating: 316it [03:58,  1.71it/s]Extractor Estimating: 317it [03:59,  1.71it/s]Extractor Estimating: 318it [04:00,  1.68it/s]Extractor Estimating: 319it [04:00,  1.64it/s]Extractor Estimating: 320it [04:01,  1.62it/s]Extractor Estimating: 321it [04:01,  1.64it/s]Extractor Estimating: 322it [04:02,  1.66it/s]Extractor Estimating: 323it [04:03,  1.66it/s]Extractor Estimating: 324it [04:03,  1.68it/s]Extractor Estimating: 325it [04:04,  1.67it/s]Extractor Estimating: 326it [04:04,  1.67it/s]Extractor Estimating: 327it [04:05,  1.62it/s]Extractor Estimating: 328it [04:06,  1.55it/s]Extractor Estimating: 329it [04:07,  1.51it/s]Extractor Estimating: 330it [04:07,  1.56it/s]Extractor Estimating: 331it [04:08,  1.64it/s]Extractor Estimating: 332it [04:08,  1.71it/s]Extractor Estimating: 333it [04:09,  1.63it/s]Extractor Estimating: 334it [04:10,  1.60it/s]Extractor Estimating: 335it [04:10,  1.59it/s]Extractor Estimating: 336it [04:11,  1.61it/s]Extractor Estimating: 337it [04:11,  1.58it/s]Extractor Estimating: 338it [04:12,  1.51it/s]Extractor Estimating: 339it [04:13,  1.59it/s]Extractor Estimating: 340it [04:13,  1.58it/s]Extractor Estimating: 341it [04:14,  1.58it/s]Extractor Estimating: 342it [04:15,  1.65it/s]Extractor Estimating: 343it [04:15,  1.61it/s]Extractor Estimating: 344it [04:16,  1.60it/s]Extractor Estimating: 345it [04:16,  1.59it/s]Extractor Estimating: 346it [04:17,  1.61it/s]Extractor Estimating: 347it [04:18,  1.52it/s]Extractor Estimating: 348it [04:18,  1.53it/s]Extractor Estimating: 349it [04:19,  1.52it/s]Extractor Estimating: 350it [04:20,  1.54it/s]Extractor Estimating: 351it [04:20,  1.56it/s]Extractor Estimating: 352it [04:21,  1.54it/s]Extractor Estimating: 353it [04:22,  1.54it/s]Extractor Estimating: 354it [04:22,  1.59it/s]Extractor Estimating: 355it [04:23,  1.60it/s]Extractor Estimating: 356it [04:23,  1.62it/s]Extractor Estimating: 357it [04:24,  1.61it/s]Extractor Estimating: 358it [04:25,  1.62it/s]Extractor Estimating: 359it [04:25,  1.64it/s]Extractor Estimating: 360it [04:26,  1.55it/s]Extractor Estimating: 361it [04:27,  1.53it/s]Extractor Estimating: 362it [04:27,  1.56it/s]Extractor Estimating: 363it [04:28,  1.59it/s]Extractor Estimating: 364it [04:29,  1.58it/s]Extractor Estimating: 365it [04:29,  1.54it/s]Extractor Estimating: 366it [04:30,  1.57it/s]Extractor Estimating: 367it [04:31,  1.52it/s]Extractor Estimating: 368it [04:31,  1.54it/s]Extractor Estimating: 369it [04:32,  1.39it/s]Extractor Estimating: 370it [04:33,  1.43it/s]Extractor Estimating: 371it [04:33,  1.46it/s]Extractor Estimating: 372it [04:34,  1.52it/s]Extractor Estimating: 373it [04:35,  1.55it/s]Extractor Estimating: 374it [04:35,  1.60it/s]Extractor Estimating: 375it [04:36,  1.58it/s]Extractor Estimating: 376it [04:36,  1.58it/s]Extractor Estimating: 377it [04:37,  1.56it/s]Extractor Estimating: 378it [04:38,  1.57it/s]Extractor Estimating: 379it [04:38,  1.60it/s]Extractor Estimating: 380it [04:39,  1.60it/s]Extractor Estimating: 381it [04:40,  1.55it/s]Extractor Estimating: 382it [04:40,  1.58it/s]Extractor Estimating: 383it [04:41,  1.59it/s]Extractor Estimating: 384it [04:42,  1.55it/s]Extractor Estimating: 385it [04:42,  1.48it/s]Extractor Estimating: 386it [04:43,  1.50it/s]Extractor Estimating: 387it [04:44,  1.50it/s]Extractor Estimating: 388it [04:44,  1.54it/s]Extractor Estimating: 389it [04:45,  1.51it/s]Extractor Estimating: 390it [04:46,  1.47it/s]Extractor Estimating: 391it [04:46,  1.48it/s]Extractor Estimating: 392it [04:47,  1.49it/s]Extractor Estimating: 393it [04:48,  1.47it/s]Extractor Estimating: 394it [04:48,  1.52it/s]Extractor Estimating: 395it [04:49,  1.52it/s]Extractor Estimating: 396it [04:50,  1.46it/s]Extractor Estimating: 397it [04:50,  1.48it/s]Extractor Estimating: 398it [04:51,  1.47it/s]Extractor Estimating: 399it [04:52,  1.49it/s]Extractor Estimating: 400it [04:52,  1.53it/s]Extractor Estimating: 401it [04:53,  1.55it/s]Extractor Estimating: 402it [04:54,  1.50it/s]Extractor Estimating: 403it [04:54,  1.58it/s]Extractor Estimating: 404it [04:55,  1.56it/s]Extractor Estimating: 405it [04:55,  1.60it/s]Extractor Estimating: 406it [04:56,  1.62it/s]Extractor Estimating: 407it [04:57,  1.57it/s]Extractor Estimating: 408it [04:57,  1.60it/s]Extractor Estimating: 409it [04:58,  1.61it/s]Extractor Estimating: 410it [04:59,  1.60it/s]Extractor Estimating: 411it [04:59,  1.59it/s]Extractor Estimating: 412it [05:00,  1.59it/s]Extractor Estimating: 413it [05:00,  1.59it/s]Extractor Estimating: 414it [05:01,  1.56it/s]Extractor Estimating: 415it [05:02,  1.57it/s]Extractor Estimating: 416it [05:02,  1.53it/s]Extractor Estimating: 417it [05:03,  1.58it/s]Extractor Estimating: 418it [05:04,  1.54it/s]Extractor Estimating: 419it [05:04,  1.58it/s]Extractor Estimating: 420it [05:05,  1.57it/s]Extractor Estimating: 421it [05:06,  1.55it/s]Extractor Estimating: 422it [05:06,  1.49it/s]Extractor Estimating: 423it [05:07,  1.48it/s]Extractor Estimating: 424it [05:08,  1.54it/s]Extractor Estimating: 425it [05:08,  1.53it/s]Extractor Estimating: 426it [05:09,  1.55it/s]Extractor Estimating: 427it [05:10,  1.53it/s]Extractor Estimating: 428it [05:10,  1.55it/s]Extractor Estimating: 429it [05:11,  1.59it/s]Extractor Estimating: 430it [05:11,  1.55it/s]Extractor Estimating: 431it [05:12,  1.53it/s]Extractor Estimating: 432it [05:13,  1.52it/s]Extractor Estimating: 433it [05:14,  1.48it/s]Extractor Estimating: 434it [05:14,  1.46it/s]Extractor Estimating: 435it [05:15,  1.47it/s]Extractor Estimating: 436it [05:16,  1.51it/s]Extractor Estimating: 437it [05:16,  1.51it/s]Extractor Estimating: 438it [05:17,  1.51it/s]Extractor Estimating: 439it [05:18,  1.48it/s]Extractor Estimating: 440it [05:18,  1.35it/s]Extractor Estimating: 441it [05:19,  1.39it/s]Extractor Estimating: 442it [05:20,  1.41it/s]Extractor Estimating: 443it [05:20,  1.45it/s]Extractor Estimating: 444it [05:21,  1.45it/s]Extractor Estimating: 445it [05:22,  1.44it/s]Extractor Estimating: 446it [05:23,  1.45it/s]Extractor Estimating: 447it [05:23,  1.46it/s]Extractor Estimating: 448it [05:24,  1.44it/s]Extractor Estimating: 449it [05:25,  1.39it/s]Extractor Estimating: 450it [05:25,  1.47it/s]Extractor Estimating: 451it [05:26,  1.56it/s]Extractor Estimating: 452it [05:26,  1.56it/s]Extractor Estimating: 453it [05:27,  1.53it/s]Extractor Estimating: 454it [05:28,  1.55it/s]Extractor Estimating: 455it [05:28,  1.58it/s]Extractor Estimating: 456it [05:29,  1.62it/s]Extractor Estimating: 457it [05:30,  1.60it/s]Extractor Estimating: 458it [05:30,  1.58it/s]Extractor Estimating: 459it [05:31,  1.58it/s]Extractor Estimating: 460it [05:31,  1.63it/s]Extractor Estimating: 461it [05:32,  1.61it/s]Extractor Estimating: 462it [05:33,  1.60it/s]Extractor Estimating: 463it [05:33,  1.61it/s]Extractor Estimating: 464it [05:34,  1.57it/s]Extractor Estimating: 465it [05:35,  1.54it/s]Extractor Estimating: 466it [05:35,  1.62it/s]Extractor Estimating: 467it [05:36,  1.60it/s]Extractor Estimating: 468it [05:36,  1.63it/s]Extractor Estimating: 469it [05:37,  1.61it/s]Extractor Estimating: 470it [05:38,  1.62it/s]Extractor Estimating: 471it [05:38,  1.63it/s]Extractor Estimating: 472it [05:39,  1.55it/s]Extractor Estimating: 473it [05:40,  1.55it/s]Extractor Estimating: 474it [05:40,  1.57it/s]Extractor Estimating: 475it [05:41,  1.59it/s]Extractor Estimating: 476it [05:42,  1.56it/s]Extractor Estimating: 477it [05:42,  1.55it/s]Extractor Estimating: 478it [05:43,  1.57it/s]Extractor Estimating: 479it [05:44,  1.53it/s]Extractor Estimating: 480it [05:44,  1.52it/s]Extractor Estimating: 481it [05:45,  1.52it/s]Extractor Estimating: 482it [05:46,  1.54it/s]Extractor Estimating: 483it [05:46,  1.56it/s]Extractor Estimating: 484it [05:47,  1.58it/s]Extractor Estimating: 485it [05:47,  1.56it/s]Extractor Estimating: 486it [05:48,  1.58it/s]Extractor Estimating: 487it [05:49,  1.54it/s]Extractor Estimating: 488it [05:49,  1.53it/s]Extractor Estimating: 489it [05:50,  1.53it/s]Extractor Estimating: 490it [05:51,  1.58it/s]Extractor Estimating: 491it [05:51,  1.58it/s]Extractor Estimating: 492it [05:52,  1.56it/s]Extractor Estimating: 493it [05:53,  1.57it/s]Extractor Estimating: 494it [05:53,  1.57it/s]Extractor Estimating: 495it [05:54,  1.57it/s]Extractor Estimating: 496it [05:54,  1.56it/s]Extractor Estimating: 497it [05:55,  1.41it/s]Extractor Estimating: 498it [05:56,  1.46it/s]Extractor Estimating: 499it [05:57,  1.48it/s]Extractor Estimating: 500it [05:57,  1.59it/s]Extractor Estimating: 500it [05:57,  1.40it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 10273 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 31254
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31354, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_15_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31354, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.327, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.003, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.040, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.012, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 71, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 171, avg_time 2.106, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 271, avg_time 1.011, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 371, avg_time 1.013, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 42, avg_time 1.017, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 142, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 242, avg_time 2.104, loss:nan
g_step 1200, step 342, avg_time 1.025, loss:nan
g_step 1300, step 13, avg_time 1.002, loss:nan
g_step 1400, step 113, avg_time 1.009, loss:nan
g_step 1500, step 213, avg_time 1.014, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 313, avg_time 2.108, loss:nan
g_step 1700, step 413, avg_time 1.013, loss:nan
g_step 1800, step 84, avg_time 1.002, loss:nan
g_step 1900, step 184, avg_time 1.015, loss:nan
g_step 2000, step 284, avg_time 1.010, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 384, avg_time 2.114, loss:nan
g_step 2200, step 55, avg_time 1.002, loss:nan
g_step 2300, step 155, avg_time 1.006, loss:nan
g_step 2400, step 255, avg_time 1.011, loss:nan
g_step 2500, step 355, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 26, avg_time 2.104, loss:nan
g_step 2700, step 126, avg_time 1.011, loss:nan
g_step 2800, step 226, avg_time 1.015, loss:nan
g_step 2900, step 326, avg_time 1.025, loss:nan
g_step 3000, step 426, avg_time 1.010, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 97, avg_time 2.101, loss:nan
g_step 3200, step 197, avg_time 1.020, loss:nan
g_step 3300, step 297, avg_time 1.005, loss:nan
g_step 3400, step 397, avg_time 1.013, loss:nan
g_step 3500, step 68, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 168, avg_time 2.092, loss:nan
g_step 3700, step 268, avg_time 1.013, loss:nan
g_step 3800, step 368, avg_time 1.008, loss:nan
g_step 3900, step 39, avg_time 1.014, loss:nan
g_step 4000, step 139, avg_time 1.012, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 239, avg_time 2.109, loss:nan
g_step 4200, step 339, avg_time 1.009, loss:nan
g_step 4300, step 10, avg_time 1.010, loss:nan
g_step 4400, step 110, avg_time 1.007, loss:nan
g_step 4500, step 210, avg_time 1.021, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.099, loss:nan
g_step 4700, step 410, avg_time 1.013, loss:nan
g_step 4800, step 81, avg_time 1.012, loss:nan
g_step 4900, step 181, avg_time 1.004, loss:nan
g_step 5000, step 281, avg_time 1.027, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 381, avg_time 2.093, loss:nan
g_step 5200, step 52, avg_time 1.016, loss:nan
g_step 5300, step 152, avg_time 1.019, loss:nan
g_step 5400, step 252, avg_time 1.004, loss:nan
g_step 5500, step 352, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 23, avg_time 2.102, loss:nan
g_step 5700, step 123, avg_time 1.014, loss:nan
g_step 5800, step 223, avg_time 1.007, loss:nan
g_step 5900, step 323, avg_time 1.010, loss:nan
g_step 6000, step 423, avg_time 1.020, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 94, avg_time 2.094, loss:nan
g_step 6200, step 194, avg_time 1.006, loss:nan
g_step 6300, step 294, avg_time 1.017, loss:nan
g_step 6400, step 394, avg_time 1.024, loss:nan
g_step 6500, step 65, avg_time 0.998, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 165, avg_time 2.098, loss:nan
g_step 6700, step 265, avg_time 1.017, loss:nan
g_step 6800, step 365, avg_time 1.005, loss:nan
g_step 6900, step 36, avg_time 1.012, loss:nan
g_step 7000, step 136, avg_time 1.002, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 236, avg_time 2.107, loss:nan
g_step 7200, step 336, avg_time 1.017, loss:nan
g_step 7300, step 7, avg_time 1.008, loss:nan
g_step 7400, step 107, avg_time 1.015, loss:nan
g_step 7500, step 207, avg_time 1.010, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 307, avg_time 2.103, loss:nan
g_step 7700, step 407, avg_time 1.008, loss:nan
g_step 7800, step 78, avg_time 1.011, loss:nan
g_step 7900, step 178, avg_time 1.003, loss:nan
g_step 8000, step 278, avg_time 1.015, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 378, avg_time 2.110, loss:nan
g_step 8200, step 49, avg_time 0.994, loss:nan
g_step 8300, step 149, avg_time 1.008, loss:nan
g_step 8400, step 249, avg_time 1.014, loss:nan
g_step 8500, step 349, avg_time 1.014, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:21:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:21:03 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-21-03_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:21:04 - WARNING - datasets.builder -   Using custom data configuration default-9f6c66f28e351de8
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9f6c66f28e351de8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:21:05,533 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:21:05,534 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:21:05,535 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:21:05,536 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:21:05,598 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:05,636 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:05,636 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:05,636 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:05,636 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:05,636 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:05,636 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:21:05,864 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:21:08,987 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:21:09,008 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9f6c66f28e351de8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 03:21:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14aad2df7200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:06,  1.48ba/s] 18%|        | 2/11 [00:00<00:03,  2.47ba/s] 27%|       | 3/11 [00:01<00:02,  3.11ba/s] 36%|      | 4/11 [00:01<00:02,  2.93ba/s] 45%|     | 5/11 [00:01<00:01,  3.35ba/s] 55%|    | 6/11 [00:01<00:01,  3.67ba/s] 64%|   | 7/11 [00:02<00:01,  3.91ba/s] 73%|  | 8/11 [00:02<00:00,  4.07ba/s] 82%| | 9/11 [00:02<00:00,  4.20ba/s] 91%| | 10/11 [00:02<00:00,  4.28ba/s]100%|| 11/11 [00:02<00:00,  3.80ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.22ba/s] 50%|     | 2/4 [00:00<00:00,  3.84ba/s] 75%|  | 3/4 [00:00<00:00,  4.08ba/s]100%|| 4/4 [00:00<00:00,  5.19ba/s]100%|| 4/4 [00:00<00:00,  4.57ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.06ba/s] 18%|        | 2/11 [00:00<00:01,  7.08ba/s] 36%|      | 4/11 [00:00<00:00,  8.79ba/s] 55%|    | 6/11 [00:00<00:00,  9.44ba/s] 73%|  | 8/11 [00:00<00:00,  9.70ba/s] 91%| | 10/11 [00:01<00:00,  9.88ba/s]100%|| 11/11 [00:01<00:00,  9.87ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.08ba/s] 75%|  | 3/4 [00:00<00:00,  8.02ba/s]100%|| 4/4 [00:00<00:00,  9.02ba/s]
[INFO|trainer.py:414] 2023-08-28 03:21:15,793 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:21:15,854 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:21:15,855 >>   Num examples = 10320
[INFO|trainer.py:1149] 2023-08-28 03:21:15,855 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:21:15,855 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:21:15,855 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:21:15,855 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:21:15,855 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:00<03:52,  3.46it/s]  0%|          | 2/805 [00:00<03:45,  3.55it/s]  0%|          | 3/805 [00:00<03:44,  3.58it/s]  0%|          | 4/805 [00:01<03:42,  3.59it/s]  1%|          | 5/805 [00:01<03:41,  3.60it/s]  1%|          | 6/805 [00:01<03:41,  3.61it/s]  1%|          | 7/805 [00:01<03:40,  3.61it/s]  1%|          | 8/805 [00:02<03:40,  3.61it/s]  1%|          | 9/805 [00:02<03:40,  3.61it/s]  1%|          | 10/805 [00:02<03:40,  3.61it/s]  1%|         | 11/805 [00:03<03:39,  3.61it/s]  1%|         | 12/805 [00:03<03:43,  3.54it/s]  2%|         | 13/805 [00:03<03:42,  3.57it/s]  2%|         | 14/805 [00:03<03:40,  3.58it/s]  2%|         | 15/805 [00:04<03:39,  3.59it/s]  2%|         | 16/805 [00:04<03:39,  3.60it/s]  2%|         | 17/805 [00:04<03:38,  3.60it/s]  2%|         | 18/805 [00:05<03:37,  3.61it/s]  2%|         | 19/805 [00:05<03:37,  3.61it/s]  2%|         | 20/805 [00:05<03:37,  3.61it/s]  3%|         | 21/805 [00:05<03:36,  3.61it/s]  3%|         | 22/805 [00:06<03:36,  3.62it/s]  3%|         | 23/805 [00:06<03:43,  3.50it/s]  3%|         | 24/805 [00:06<03:40,  3.53it/s]  3%|         | 25/805 [00:06<03:39,  3.56it/s]  3%|         | 26/805 [00:07<03:37,  3.58it/s]  3%|         | 27/805 [00:07<03:36,  3.59it/s]  3%|         | 28/805 [00:07<03:36,  3.59it/s]  4%|         | 29/805 [00:08<03:35,  3.59it/s]  4%|         | 30/805 [00:08<03:35,  3.60it/s]  4%|         | 31/805 [00:08<03:34,  3.60it/s]  4%|         | 32/805 [00:08<03:34,  3.61it/s]  4%|         | 33/805 [00:09<03:33,  3.61it/s]  4%|         | 34/805 [00:09<03:34,  3.59it/s]  4%|         | 35/805 [00:09<03:33,  3.60it/s]  4%|         | 36/805 [00:10<03:33,  3.61it/s]  5%|         | 37/805 [00:10<03:33,  3.60it/s]  5%|         | 38/805 [00:10<03:32,  3.60it/s]  5%|         | 39/805 [00:10<03:32,  3.61it/s]  5%|         | 40/805 [00:11<03:31,  3.61it/s]  5%|         | 41/805 [00:11<03:31,  3.62it/s]  5%|         | 42/805 [00:11<03:30,  3.62it/s]  5%|         | 43/805 [00:11<03:30,  3.62it/s]  5%|         | 44/805 [00:12<03:30,  3.62it/s]  6%|         | 45/805 [00:12<03:37,  3.49it/s]  6%|         | 46/805 [00:12<03:35,  3.52it/s]  6%|         | 47/805 [00:13<03:33,  3.55it/s]  6%|         | 48/805 [00:13<03:31,  3.57it/s]  6%|         | 49/805 [00:13<03:31,  3.58it/s]  6%|         | 50/805 [00:13<03:30,  3.59it/s]  6%|         | 51/805 [00:14<03:29,  3.60it/s]  6%|         | 52/805 [00:14<03:29,  3.60it/s]  7%|         | 53/805 [00:14<03:28,  3.60it/s]  7%|         | 54/805 [00:15<03:28,  3.60it/s]  7%|         | 55/805 [00:15<03:27,  3.61it/s]  7%|         | 56/805 [00:15<03:32,  3.52it/s]  7%|         | 57/805 [00:15<03:30,  3.55it/s]  7%|         | 58/805 [00:16<03:29,  3.57it/s]  7%|         | 59/805 [00:16<03:28,  3.58it/s]  7%|         | 60/805 [00:16<03:27,  3.59it/s]  8%|         | 61/805 [00:16<03:27,  3.59it/s]  8%|         | 62/805 [00:17<03:26,  3.59it/s]  8%|         | 63/805 [00:17<03:26,  3.60it/s]  8%|         | 64/805 [00:17<03:25,  3.60it/s]  8%|         | 65/805 [00:18<03:25,  3.61it/s]  8%|         | 66/805 [00:18<03:24,  3.61it/s]  8%|         | 67/805 [00:18<03:29,  3.52it/s]  8%|         | 68/805 [00:18<03:27,  3.54it/s]  9%|         | 69/805 [00:19<03:26,  3.56it/s]  9%|         | 70/805 [00:19<03:25,  3.57it/s]  9%|         | 71/805 [00:19<03:33,  3.43it/s]  9%|         | 72/805 [00:20<03:30,  3.48it/s]  9%|         | 73/805 [00:20<03:28,  3.51it/s]  9%|         | 74/805 [00:20<03:26,  3.54it/s]  9%|         | 75/805 [00:20<03:24,  3.57it/s]  9%|         | 76/805 [00:21<03:24,  3.57it/s] 10%|         | 77/805 [00:21<03:23,  3.58it/s] 10%|         | 78/805 [00:21<03:22,  3.59it/s] 10%|         | 79/805 [00:22<03:21,  3.60it/s] 10%|         | 80/805 [00:22<03:21,  3.60it/s] 10%|         | 81/805 [00:22<03:20,  3.61it/s] 10%|         | 82/805 [00:22<03:26,  3.50it/s] 10%|         | 83/805 [00:23<03:24,  3.53it/s] 10%|         | 84/805 [00:23<03:22,  3.55it/s] 11%|         | 85/805 [00:23<03:21,  3.57it/s] 11%|         | 86/805 [00:24<03:20,  3.58it/s] 11%|         | 87/805 [00:24<03:20,  3.59it/s] 11%|         | 88/805 [00:24<03:19,  3.59it/s] 11%|         | 89/805 [00:24<03:19,  3.60it/s] 11%|         | 90/805 [00:25<03:18,  3.60it/s] 11%|        | 91/805 [00:25<03:17,  3.61it/s] 11%|        | 92/805 [00:25<03:17,  3.61it/s] 12%|        | 93/805 [00:25<03:16,  3.61it/s] 12%|        | 94/805 [00:26<03:16,  3.61it/s] 12%|        | 95/805 [00:26<03:16,  3.61it/s] 12%|        | 96/805 [00:26<03:16,  3.61it/s] 12%|        | 97/805 [00:27<03:15,  3.62it/s] 12%|        | 98/805 [00:27<03:15,  3.62it/s] 12%|        | 99/805 [00:27<03:25,  3.43it/s] 12%|        | 100/805 [00:27<03:27,  3.40it/s] 13%|        | 101/805 [00:28<03:23,  3.46it/s] 13%|        | 102/805 [00:28<03:20,  3.51it/s] 13%|        | 103/805 [00:28<03:18,  3.54it/s] 13%|        | 104/805 [00:29<03:16,  3.56it/s] 13%|        | 105/805 [00:29<03:15,  3.58it/s] 13%|        | 106/805 [00:29<03:14,  3.59it/s] 13%|        | 107/805 [00:29<03:13,  3.60it/s] 13%|        | 108/805 [00:30<03:13,  3.61it/s] 14%|        | 109/805 [00:30<03:12,  3.61it/s] 14%|        | 110/805 [00:30<03:12,  3.61it/s] 14%|        | 111/805 [00:31<03:11,  3.62it/s] 14%|        | 112/805 [00:31<03:11,  3.62it/s] 14%|        | 113/805 [00:31<03:11,  3.62it/s] 14%|        | 114/805 [00:31<03:10,  3.62it/s] 14%|        | 115/805 [00:32<03:10,  3.62it/s] 14%|        | 116/805 [00:32<03:10,  3.62it/s] 15%|        | 117/805 [00:32<03:10,  3.62it/s] 15%|        | 118/805 [00:32<03:09,  3.62it/s] 15%|        | 119/805 [00:33<03:21,  3.40it/s] 15%|        | 120/805 [00:33<03:18,  3.46it/s] 15%|        | 121/805 [00:33<03:15,  3.50it/s] 15%|        | 122/805 [00:34<03:13,  3.54it/s] 15%|        | 123/805 [00:34<03:11,  3.57it/s] 15%|        | 124/805 [00:34<03:10,  3.58it/s] 16%|        | 125/805 [00:34<03:09,  3.59it/s] 16%|        | 126/805 [00:35<03:16,  3.46it/s] 16%|        | 127/805 [00:35<03:13,  3.51it/s] 16%|        | 128/805 [00:35<03:11,  3.54it/s] 16%|        | 129/805 [00:36<03:09,  3.56it/s] 16%|        | 130/805 [00:36<03:08,  3.58it/s] 16%|        | 131/805 [00:36<03:07,  3.59it/s] 16%|        | 132/805 [00:36<03:07,  3.60it/s] 17%|        | 133/805 [00:37<03:06,  3.61it/s] 17%|        | 134/805 [00:37<03:05,  3.61it/s] 17%|        | 135/805 [00:37<03:05,  3.61it/s] 17%|        | 136/805 [00:38<03:05,  3.61it/s] 17%|        | 137/805 [00:38<03:14,  3.43it/s] 17%|        | 138/805 [00:38<03:11,  3.49it/s] 17%|        | 139/805 [00:38<03:08,  3.52it/s] 17%|        | 140/805 [00:39<03:07,  3.55it/s] 18%|        | 141/805 [00:39<03:06,  3.57it/s] 18%|        | 142/805 [00:39<03:05,  3.58it/s] 18%|        | 143/805 [00:39<03:04,  3.59it/s] 18%|        | 144/805 [00:40<03:03,  3.60it/s] 18%|        | 145/805 [00:40<03:03,  3.60it/s] 18%|        | 146/805 [00:40<03:02,  3.61it/s] 18%|        | 147/805 [00:41<03:02,  3.61it/s] 18%|        | 148/805 [00:41<03:12,  3.42it/s] 19%|        | 149/805 [00:41<03:08,  3.47it/s] 19%|        | 150/805 [00:41<03:06,  3.52it/s] 19%|        | 151/805 [00:42<03:04,  3.55it/s] 19%|        | 152/805 [00:42<03:02,  3.57it/s] 19%|        | 153/805 [00:42<03:01,  3.58it/s] 19%|        | 154/805 [00:43<03:01,  3.59it/s] 19%|        | 155/805 [00:43<03:00,  3.60it/s] 19%|        | 156/805 [00:43<03:00,  3.60it/s] 20%|        | 157/805 [00:43<02:59,  3.61it/s] 20%|        | 158/805 [00:44<02:59,  3.61it/s] 20%|        | 159/805 [00:44<03:00,  3.57it/s] 20%|        | 160/805 [00:44<02:59,  3.59it/s] 20%|        | 161/805 [00:45<02:59,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 03:22:00,928 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:22:00,928 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 03:22:00,928 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.85it/s][A
  3%|         | 12/436 [00:00<00:08, 49.03it/s][A
  4%|         | 17/436 [00:00<00:08, 47.02it/s][A
  5%|         | 22/436 [00:00<00:08, 46.10it/s][A
  6%|         | 27/436 [00:00<00:09, 45.37it/s][A
  7%|         | 32/436 [00:00<00:08, 45.00it/s][A
  8%|         | 37/436 [00:00<00:08, 44.99it/s][A
 10%|         | 42/436 [00:00<00:08, 44.61it/s][A
 11%|         | 47/436 [00:01<00:08, 44.73it/s][A
 12%|        | 52/436 [00:01<00:08, 44.75it/s][A
 13%|        | 57/436 [00:01<00:08, 44.98it/s][A
 14%|        | 62/436 [00:01<00:08, 45.06it/s][A
 15%|        | 67/436 [00:01<00:08, 44.84it/s][A
 17%|        | 72/436 [00:01<00:08, 44.73it/s][A
 18%|        | 77/436 [00:01<00:08, 44.59it/s][A
 19%|        | 82/436 [00:01<00:07, 44.50it/s][A
 20%|        | 87/436 [00:01<00:07, 44.49it/s][A
 21%|        | 92/436 [00:02<00:07, 44.54it/s][A
 22%|       | 97/436 [00:02<00:07, 44.65it/s][A
 23%|       | 102/436 [00:02<00:07, 43.90it/s][A
 25%|       | 107/436 [00:02<00:07, 44.31it/s][A
 26%|       | 112/436 [00:02<00:07, 44.54it/s][A
 27%|       | 117/436 [00:02<00:07, 44.57it/s][A
 28%|       | 122/436 [00:02<00:07, 44.46it/s][A
 29%|       | 127/436 [00:02<00:06, 44.42it/s][A
 30%|       | 132/436 [00:02<00:06, 44.49it/s][A
 31%|      | 137/436 [00:03<00:06, 44.57it/s][A
 33%|      | 142/436 [00:03<00:06, 44.62it/s][A
 34%|      | 147/436 [00:03<00:06, 44.65it/s][A
 35%|      | 152/436 [00:03<00:06, 44.86it/s][A
 36%|      | 157/436 [00:03<00:06, 44.89it/s][A
 37%|      | 162/436 [00:03<00:06, 44.87it/s][A
 38%|      | 167/436 [00:03<00:06, 44.74it/s][A
 39%|      | 172/436 [00:03<00:05, 44.53it/s][A
 41%|      | 177/436 [00:03<00:05, 44.58it/s][A
 42%|     | 182/436 [00:04<00:05, 44.46it/s][A
 43%|     | 187/436 [00:04<00:05, 44.61it/s][A
 44%|     | 192/436 [00:04<00:05, 44.66it/s][A
 45%|     | 197/436 [00:04<00:05, 44.86it/s][A
 46%|     | 202/436 [00:04<00:05, 44.93it/s][A
 47%|     | 207/436 [00:04<00:05, 44.79it/s][A
 49%|     | 212/436 [00:04<00:05, 44.75it/s][A
 50%|     | 217/436 [00:04<00:04, 44.71it/s][A
 51%|     | 222/436 [00:04<00:04, 44.73it/s][A
 52%|    | 227/436 [00:05<00:04, 44.75it/s][A
 53%|    | 232/436 [00:05<00:04, 44.57it/s][A
 54%|    | 237/436 [00:05<00:04, 42.52it/s][A
 56%|    | 242/436 [00:05<00:04, 43.34it/s][A
 57%|    | 247/436 [00:05<00:04, 43.94it/s][A
 58%|    | 252/436 [00:05<00:04, 44.15it/s][A
 59%|    | 257/436 [00:05<00:04, 44.27it/s][A
 60%|    | 262/436 [00:05<00:03, 44.42it/s][A
 61%|    | 267/436 [00:05<00:03, 44.48it/s][A
 62%|   | 272/436 [00:06<00:03, 44.49it/s][A
 64%|   | 277/436 [00:06<00:03, 44.30it/s][A
 65%|   | 282/436 [00:06<00:03, 44.44it/s][A
 66%|   | 287/436 [00:06<00:03, 44.64it/s][A
 67%|   | 292/436 [00:06<00:03, 44.80it/s][A
 68%|   | 297/436 [00:06<00:03, 44.86it/s][A
 69%|   | 302/436 [00:06<00:02, 44.77it/s][A
 70%|   | 307/436 [00:06<00:02, 44.83it/s][A
 72%|  | 312/436 [00:06<00:02, 44.80it/s][A
 73%|  | 317/436 [00:07<00:02, 44.62it/s][A
 74%|  | 322/436 [00:07<00:02, 44.42it/s][A
 75%|  | 327/436 [00:07<00:02, 44.48it/s][A
 76%|  | 332/436 [00:07<00:02, 44.69it/s][A
 77%|  | 337/436 [00:07<00:02, 44.83it/s][A
 78%|  | 342/436 [00:07<00:02, 44.90it/s][A
 80%|  | 347/436 [00:07<00:01, 44.88it/s][A
 81%|  | 352/436 [00:07<00:01, 44.93it/s][A
 82%| | 357/436 [00:07<00:01, 44.87it/s][A
 83%| | 362/436 [00:08<00:01, 44.65it/s][A
 84%| | 367/436 [00:08<00:01, 44.35it/s][A
 85%| | 372/436 [00:08<00:01, 41.40it/s][A
 86%| | 377/436 [00:08<00:01, 42.56it/s][A
 88%| | 382/436 [00:08<00:01, 43.40it/s][A
 89%| | 387/436 [00:08<00:01, 43.97it/s][A
 90%| | 392/436 [00:08<00:00, 44.36it/s][A
 91%| | 397/436 [00:08<00:00, 44.61it/s][A
 92%|| 402/436 [00:09<00:00, 44.59it/s][A
 93%|| 407/436 [00:09<00:00, 44.44it/s][A
 94%|| 412/436 [00:09<00:00, 44.16it/s][A
 96%|| 417/436 [00:09<00:00, 44.19it/s][A
 97%|| 422/436 [00:09<00:00, 44.42it/s][A
 98%|| 427/436 [00:09<00:00, 44.57it/s][A
 99%|| 432/436 [00:09<00:00, 44.73it/s][A                                                 
                                                 [A 20%|        | 161/805 [00:54<02:59,  3.60it/s]
100%|| 436/436 [00:09<00:00, 44.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:22:11,004 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 03:22:11,275 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:22:16,161 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:22:16,281 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:22:16,361 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161/special_tokens_map.json
 20%|        | 162/805 [01:01<56:03,  5.23s/it] 20%|        | 163/805 [01:02<40:04,  3.75s/it] 20%|        | 164/805 [01:02<28:54,  2.71s/it] 20%|        | 165/805 [01:02<21:06,  1.98s/it] 21%|        | 166/805 [01:02<15:39,  1.47s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|        | 167/805 [01:03<12:02,  1.13s/it] 21%|        | 168/805 [01:03<09:18,  1.14it/s] 21%|        | 169/805 [01:03<07:29,  1.41it/s] 21%|        | 170/805 [01:04<06:07,  1.73it/s] 21%|        | 171/805 [01:04<05:10,  2.04it/s] 21%|       | 172/805 [01:04<04:30,  2.34it/s] 21%|       | 173/805 [01:05<04:02,  2.61it/s] 22%|       | 174/805 [01:05<03:42,  2.84it/s] 22%|       | 175/805 [01:05<03:27,  3.03it/s] 22%|       | 176/805 [01:05<03:17,  3.19it/s] 22%|       | 177/805 [01:06<03:11,  3.28it/s] 22%|       | 178/805 [01:06<03:05,  3.37it/s] 22%|       | 179/805 [01:06<03:01,  3.44it/s] 22%|       | 180/805 [01:06<02:59,  3.49it/s] 22%|       | 181/805 [01:07<02:56,  3.53it/s] 23%|       | 182/805 [01:07<02:55,  3.56it/s] 23%|       | 183/805 [01:07<02:53,  3.58it/s] 23%|       | 184/805 [01:08<02:53,  3.58it/s] 23%|       | 185/805 [01:08<02:52,  3.59it/s] 23%|       | 186/805 [01:08<02:51,  3.60it/s] 23%|       | 187/805 [01:08<03:02,  3.39it/s] 23%|       | 188/805 [01:09<02:58,  3.45it/s] 23%|       | 189/805 [01:09<02:55,  3.50it/s] 24%|       | 190/805 [01:09<02:53,  3.53it/s] 24%|       | 191/805 [01:10<02:52,  3.56it/s] 24%|       | 192/805 [01:10<02:51,  3.58it/s] 24%|       | 193/805 [01:10<02:50,  3.60it/s] 24%|       | 194/805 [01:10<02:49,  3.60it/s] 24%|       | 195/805 [01:11<02:49,  3.60it/s] 24%|       | 196/805 [01:11<02:49,  3.60it/s] 24%|       | 197/805 [01:11<02:48,  3.61it/s] 25%|       | 198/805 [01:12<02:57,  3.42it/s] 25%|       | 199/805 [01:12<02:54,  3.47it/s] 25%|       | 200/805 [01:12<02:52,  3.51it/s] 25%|       | 201/805 [01:12<02:50,  3.54it/s] 25%|       | 202/805 [01:13<02:49,  3.57it/s] 25%|       | 203/805 [01:13<02:48,  3.58it/s] 25%|       | 204/805 [01:13<02:47,  3.59it/s] 25%|       | 205/805 [01:13<02:46,  3.60it/s] 26%|       | 206/805 [01:14<02:46,  3.60it/s] 26%|       | 207/805 [01:14<02:46,  3.60it/s] 26%|       | 208/805 [01:14<02:45,  3.60it/s] 26%|       | 209/805 [01:15<02:50,  3.50it/s] 26%|       | 210/805 [01:15<02:48,  3.53it/s] 26%|       | 211/805 [01:15<02:47,  3.55it/s] 26%|       | 212/805 [01:15<02:45,  3.57it/s] 26%|       | 213/805 [01:16<02:44,  3.59it/s] 27%|       | 214/805 [01:16<02:44,  3.60it/s] 27%|       | 215/805 [01:16<02:44,  3.60it/s] 27%|       | 216/805 [01:17<02:43,  3.60it/s] 27%|       | 217/805 [01:17<02:42,  3.61it/s] 27%|       | 218/805 [01:17<02:42,  3.61it/s] 27%|       | 219/805 [01:17<02:42,  3.62it/s] 27%|       | 220/805 [01:18<02:46,  3.52it/s] 27%|       | 221/805 [01:18<02:44,  3.55it/s] 28%|       | 222/805 [01:18<02:43,  3.57it/s] 28%|       | 223/805 [01:19<02:42,  3.58it/s] 28%|       | 224/805 [01:19<02:41,  3.59it/s] 28%|       | 225/805 [01:19<02:41,  3.60it/s] 28%|       | 226/805 [01:19<02:40,  3.60it/s] 28%|       | 227/805 [01:20<02:40,  3.60it/s] 28%|       | 228/805 [01:20<02:40,  3.60it/s] 28%|       | 229/805 [01:20<02:39,  3.61it/s] 29%|       | 230/805 [01:20<02:39,  3.61it/s] 29%|       | 231/805 [01:21<02:43,  3.52it/s] 29%|       | 232/805 [01:21<02:41,  3.55it/s] 29%|       | 233/805 [01:21<02:40,  3.57it/s] 29%|       | 234/805 [01:22<02:39,  3.58it/s] 29%|       | 235/805 [01:22<02:38,  3.59it/s] 29%|       | 236/805 [01:22<02:38,  3.60it/s] 29%|       | 237/805 [01:22<02:37,  3.60it/s] 30%|       | 238/805 [01:23<02:37,  3.60it/s] 30%|       | 239/805 [01:23<02:37,  3.60it/s] 30%|       | 240/805 [01:23<02:36,  3.61it/s] 30%|       | 241/805 [01:24<02:36,  3.61it/s] 30%|       | 242/805 [01:24<02:39,  3.53it/s] 30%|       | 243/805 [01:24<02:38,  3.55it/s] 30%|       | 244/805 [01:24<02:37,  3.57it/s] 30%|       | 245/805 [01:25<02:36,  3.58it/s] 31%|       | 246/805 [01:25<02:35,  3.59it/s] 31%|       | 247/805 [01:25<02:35,  3.59it/s] 31%|       | 248/805 [01:25<02:34,  3.60it/s] 31%|       | 249/805 [01:26<02:34,  3.60it/s] 31%|       | 250/805 [01:26<02:33,  3.61it/s] 31%|       | 251/805 [01:26<02:33,  3.61it/s] 31%|      | 252/805 [01:27<02:33,  3.60it/s] 31%|      | 253/805 [01:27<02:34,  3.56it/s] 32%|      | 254/805 [01:27<02:44,  3.35it/s] 32%|      | 255/805 [01:28<02:58,  3.09it/s] 32%|      | 256/805 [01:28<02:50,  3.23it/s] 32%|      | 257/805 [01:28<02:44,  3.33it/s] 32%|      | 258/805 [01:28<02:40,  3.41it/s] 32%|      | 259/805 [01:29<02:37,  3.47it/s] 32%|      | 260/805 [01:29<02:35,  3.51it/s] 32%|      | 261/805 [01:29<02:33,  3.54it/s] 33%|      | 262/805 [01:30<02:32,  3.57it/s] 33%|      | 263/805 [01:30<02:31,  3.59it/s] 33%|      | 264/805 [01:30<02:34,  3.50it/s] 33%|      | 265/805 [01:30<02:32,  3.53it/s] 33%|      | 266/805 [01:31<02:31,  3.56it/s] 33%|      | 267/805 [01:31<02:30,  3.57it/s] 33%|      | 268/805 [01:31<02:29,  3.59it/s] 33%|      | 269/805 [01:32<02:28,  3.60it/s] 34%|      | 270/805 [01:32<02:28,  3.61it/s] 34%|      | 271/805 [01:32<02:27,  3.61it/s] 34%|      | 272/805 [01:32<02:27,  3.61it/s] 34%|      | 273/805 [01:33<02:27,  3.60it/s] 34%|      | 274/805 [01:33<02:27,  3.61it/s] 34%|      | 275/805 [01:33<02:34,  3.44it/s] 34%|      | 276/805 [01:33<02:31,  3.49it/s] 34%|      | 277/805 [01:34<02:29,  3.53it/s] 35%|      | 278/805 [01:34<02:28,  3.56it/s] 35%|      | 279/805 [01:34<02:27,  3.57it/s] 35%|      | 280/805 [01:35<02:26,  3.59it/s] 35%|      | 281/805 [01:35<02:25,  3.59it/s] 35%|      | 282/805 [01:35<02:25,  3.60it/s] 35%|      | 283/805 [01:35<02:24,  3.61it/s] 35%|      | 284/805 [01:36<02:24,  3.61it/s] 35%|      | 285/805 [01:36<02:24,  3.60it/s] 36%|      | 286/805 [01:36<02:23,  3.61it/s] 36%|      | 287/805 [01:37<02:23,  3.61it/s] 36%|      | 288/805 [01:37<02:23,  3.61it/s] 36%|      | 289/805 [01:37<02:22,  3.61it/s] 36%|      | 290/805 [01:37<02:22,  3.62it/s] 36%|      | 291/805 [01:38<02:22,  3.62it/s] 36%|      | 292/805 [01:38<02:21,  3.62it/s] 36%|      | 293/805 [01:38<02:21,  3.61it/s] 37%|      | 294/805 [01:38<02:21,  3.62it/s] 37%|      | 295/805 [01:39<02:20,  3.62it/s] 37%|      | 296/805 [01:39<02:20,  3.62it/s] 37%|      | 297/805 [01:39<02:29,  3.41it/s] 37%|      | 298/805 [01:40<02:26,  3.47it/s] 37%|      | 299/805 [01:40<02:24,  3.51it/s] 37%|      | 300/805 [01:40<02:22,  3.54it/s] 37%|      | 301/805 [01:40<02:21,  3.56it/s] 38%|      | 302/805 [01:41<02:20,  3.58it/s] 38%|      | 303/805 [01:41<02:19,  3.59it/s] 38%|      | 304/805 [01:41<02:19,  3.59it/s] 38%|      | 305/805 [01:42<02:18,  3.60it/s] 38%|      | 306/805 [01:42<02:18,  3.61it/s] 38%|      | 307/805 [01:42<02:17,  3.61it/s] 38%|      | 308/805 [01:42<02:24,  3.45it/s] 38%|      | 309/805 [01:43<02:21,  3.50it/s] 39%|      | 310/805 [01:43<02:20,  3.53it/s] 39%|      | 311/805 [01:43<02:18,  3.56it/s] 39%|      | 312/805 [01:44<02:17,  3.58it/s] 39%|      | 313/805 [01:44<02:17,  3.59it/s] 39%|      | 314/805 [01:44<02:16,  3.60it/s] 39%|      | 315/805 [01:44<02:16,  3.60it/s] 39%|      | 316/805 [01:45<02:15,  3.61it/s] 39%|      | 317/805 [01:45<02:15,  3.60it/s] 40%|      | 318/805 [01:45<02:14,  3.61it/s] 40%|      | 319/805 [01:45<02:17,  3.54it/s] 40%|      | 320/805 [01:46<02:16,  3.56it/s] 40%|      | 321/805 [01:46<02:15,  3.57it/s] 40%|      | 322/805 [01:46<02:14,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 03:23:02,724 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:23:02,724 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 03:23:02,724 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8223, 'eval_samples_per_second': 354.501, 'eval_steps_per_second': 44.389, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.51it/s][A
  3%|         | 12/436 [00:00<00:08, 49.22it/s][A
  4%|         | 17/436 [00:00<00:08, 47.38it/s][A
  5%|         | 22/436 [00:00<00:08, 46.26it/s][A
  6%|         | 27/436 [00:00<00:08, 45.57it/s][A
  7%|         | 32/436 [00:00<00:08, 45.13it/s][A
  8%|         | 37/436 [00:00<00:08, 44.71it/s][A
 10%|         | 42/436 [00:00<00:08, 44.37it/s][A
 11%|         | 47/436 [00:01<00:08, 44.64it/s][A
 12%|        | 52/436 [00:01<00:08, 44.77it/s][A
 13%|        | 57/436 [00:01<00:08, 45.02it/s][A
 14%|        | 62/436 [00:01<00:08, 45.03it/s][A
 15%|        | 67/436 [00:01<00:08, 45.03it/s][A
 17%|        | 72/436 [00:01<00:08, 44.96it/s][A
 18%|        | 77/436 [00:01<00:08, 44.66it/s][A
 19%|        | 82/436 [00:01<00:07, 44.50it/s][A
 20%|        | 87/436 [00:01<00:08, 42.17it/s][A
 21%|        | 92/436 [00:02<00:07, 43.11it/s][A
 22%|       | 97/436 [00:02<00:07, 43.78it/s][A
 23%|       | 102/436 [00:02<00:07, 44.21it/s][A
 25%|       | 107/436 [00:02<00:07, 44.50it/s][A
 26%|       | 112/436 [00:02<00:07, 44.50it/s][A
 27%|       | 117/436 [00:02<00:07, 44.42it/s][A
 28%|       | 122/436 [00:02<00:07, 44.33it/s][A
 29%|       | 127/436 [00:02<00:07, 44.11it/s][A
 30%|       | 132/436 [00:02<00:06, 44.21it/s][A
 31%|      | 137/436 [00:03<00:06, 44.48it/s][A
 33%|      | 142/436 [00:03<00:06, 44.72it/s][A
 34%|      | 147/436 [00:03<00:06, 44.89it/s][A
 35%|      | 152/436 [00:03<00:06, 45.01it/s][A
 36%|      | 157/436 [00:03<00:06, 44.94it/s][A
 37%|      | 162/436 [00:03<00:06, 44.73it/s][A
 38%|      | 167/436 [00:03<00:06, 44.59it/s][A
 39%|      | 172/436 [00:03<00:05, 44.35it/s][A
 41%|      | 177/436 [00:03<00:05, 44.30it/s][A
 42%|     | 182/436 [00:04<00:05, 44.50it/s][A
 43%|     | 187/436 [00:04<00:05, 44.57it/s][A
 44%|     | 192/436 [00:04<00:05, 44.72it/s][A
 45%|     | 197/436 [00:04<00:05, 44.85it/s][A
 46%|     | 202/436 [00:04<00:05, 44.96it/s][A
 47%|     | 207/436 [00:04<00:05, 44.99it/s][A
 49%|     | 212/436 [00:04<00:05, 44.67it/s][A
 50%|     | 217/436 [00:04<00:04, 44.50it/s][A
 51%|     | 222/436 [00:04<00:04, 43.21it/s][A
 52%|    | 227/436 [00:05<00:04, 43.75it/s][A
 53%|    | 232/436 [00:05<00:04, 43.93it/s][A
 54%|    | 237/436 [00:05<00:04, 44.27it/s][A
 56%|    | 242/436 [00:05<00:04, 44.54it/s][A
 57%|    | 247/436 [00:05<00:04, 44.72it/s][A
 58%|    | 252/436 [00:05<00:04, 44.76it/s][A
 59%|    | 257/436 [00:05<00:04, 44.66it/s][A
 60%|    | 262/436 [00:05<00:03, 44.35it/s][A
 61%|    | 267/436 [00:05<00:03, 44.37it/s][A
 62%|   | 272/436 [00:06<00:03, 44.54it/s][A
 64%|   | 277/436 [00:06<00:03, 44.68it/s][A
 65%|   | 282/436 [00:06<00:03, 44.72it/s][A
 66%|   | 287/436 [00:06<00:03, 44.80it/s][A
 67%|   | 292/436 [00:06<00:03, 44.76it/s][A
 68%|   | 297/436 [00:06<00:03, 44.87it/s][A
 69%|   | 302/436 [00:06<00:02, 44.74it/s][A
 70%|   | 307/436 [00:06<00:02, 44.56it/s][A
 72%|  | 312/436 [00:06<00:02, 44.49it/s][A
 73%|  | 317/436 [00:07<00:02, 44.38it/s][A
 74%|  | 322/436 [00:07<00:02, 44.61it/s][A
 75%|  | 327/436 [00:07<00:02, 44.79it/s][A
 76%|  | 332/436 [00:07<00:02, 44.80it/s][A
 77%|  | 337/436 [00:07<00:02, 44.98it/s][A
 78%|  | 342/436 [00:07<00:02, 44.88it/s][A
 80%|  | 347/436 [00:07<00:01, 44.61it/s][A
 81%|  | 352/436 [00:07<00:01, 44.62it/s][A
 82%| | 357/436 [00:08<00:01, 42.20it/s][A
 83%| | 362/436 [00:08<00:01, 43.05it/s][A
 84%| | 367/436 [00:08<00:01, 43.52it/s][A
 85%| | 372/436 [00:08<00:01, 43.99it/s][A
 86%| | 377/436 [00:08<00:01, 44.30it/s][A
 88%| | 382/436 [00:08<00:01, 44.55it/s][A
 89%| | 387/436 [00:08<00:01, 44.60it/s][A
 90%| | 392/436 [00:08<00:00, 44.57it/s][A
 91%| | 397/436 [00:08<00:00, 44.29it/s][A
 92%|| 402/436 [00:09<00:00, 44.36it/s][A
 93%|| 407/436 [00:09<00:00, 44.44it/s][A
 94%|| 412/436 [00:09<00:00, 44.69it/s][A
 96%|| 417/436 [00:09<00:00, 44.73it/s][A
 97%|| 422/436 [00:09<00:00, 44.84it/s][A
 98%|| 427/436 [00:09<00:00, 44.84it/s][A
 99%|| 432/436 [00:09<00:00, 44.85it/s][A                                                 
                                                 [A 40%|      | 322/805 [01:56<02:14,  3.58it/s]
100%|| 436/436 [00:09<00:00, 44.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:23:12,720 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 03:23:12,921 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:23:16,939 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:23:17,147 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:23:17,261 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-322/special_tokens_map.json
 40%|      | 323/805 [02:03<40:51,  5.09s/it] 40%|      | 324/805 [02:03<29:12,  3.64s/it] 40%|      | 325/805 [02:03<21:04,  2.63s/it] 40%|      | 326/805 [02:03<15:23,  1.93s/it] 41%|      | 327/805 [02:04<11:25,  1.43s/it] 41%|      | 328/805 [02:04<08:42,  1.10s/it] 41%|      | 329/805 [02:04<06:45,  1.17it/s] 41%|      | 330/805 [02:05<05:23,  1.47it/s] 41%|      | 331/805 [02:05<04:25,  1.78it/s] 41%|      | 332/805 [02:05<03:45,  2.10it/s] 41%|     | 333/805 [02:05<03:17,  2.39it/s] 41%|     | 334/805 [02:06<02:57,  2.65it/s] 42%|     | 335/805 [02:06<02:43,  2.87it/s] 42%|     | 336/805 [02:06<02:33,  3.05it/s] 42%|     | 337/805 [02:07<02:26,  3.18it/s] 42%|     | 338/805 [02:07<02:21,  3.29it/s] 42%|     | 339/805 [02:07<02:18,  3.37it/s] 42%|     | 340/805 [02:07<02:15,  3.42it/s] 42%|     | 341/805 [02:08<02:13,  3.47it/s] 42%|     | 342/805 [02:08<02:12,  3.50it/s] 43%|     | 343/805 [02:08<02:11,  3.52it/s] 43%|     | 344/805 [02:09<02:10,  3.53it/s] 43%|     | 345/805 [02:09<02:09,  3.54it/s] 43%|     | 346/805 [02:09<02:09,  3.55it/s] 43%|     | 347/805 [02:09<02:08,  3.55it/s] 43%|     | 348/805 [02:10<02:08,  3.56it/s] 43%|     | 349/805 [02:10<02:17,  3.33it/s] 43%|     | 350/805 [02:10<02:14,  3.39it/s] 44%|     | 351/805 [02:11<02:12,  3.44it/s] 44%|     | 352/805 [02:11<02:10,  3.47it/s] 44%|     | 353/805 [02:11<02:09,  3.50it/s] 44%|     | 354/805 [02:11<02:08,  3.52it/s] 44%|     | 355/805 [02:12<02:07,  3.53it/s] 44%|     | 356/805 [02:12<02:06,  3.54it/s] 44%|     | 357/805 [02:12<02:06,  3.55it/s] 44%|     | 358/805 [02:13<02:05,  3.55it/s] 45%|     | 359/805 [02:13<02:05,  3.56it/s] 45%|     | 360/805 [02:13<02:06,  3.51it/s] 45%|     | 361/805 [02:13<02:06,  3.52it/s] 45%|     | 362/805 [02:14<02:05,  3.53it/s] 45%|     | 363/805 [02:14<02:04,  3.54it/s] 45%|     | 364/805 [02:14<02:04,  3.55it/s] 45%|     | 365/805 [02:15<02:03,  3.55it/s] 45%|     | 366/805 [02:15<02:03,  3.56it/s] 46%|     | 367/805 [02:15<02:03,  3.56it/s] 46%|     | 368/805 [02:15<02:02,  3.56it/s] 46%|     | 369/805 [02:16<02:02,  3.56it/s] 46%|     | 370/805 [02:16<02:02,  3.56it/s] 46%|     | 371/805 [02:16<02:06,  3.44it/s] 46%|     | 372/805 [02:17<02:04,  3.48it/s] 46%|     | 373/805 [02:17<02:03,  3.50it/s] 46%|     | 374/805 [02:17<02:02,  3.52it/s] 47%|     | 375/805 [02:17<02:01,  3.53it/s] 47%|     | 376/805 [02:18<02:01,  3.54it/s] 47%|     | 377/805 [02:18<02:00,  3.55it/s] 47%|     | 378/805 [02:18<02:00,  3.55it/s] 47%|     | 379/805 [02:18<01:59,  3.56it/s] 47%|     | 380/805 [02:19<01:59,  3.56it/s] 47%|     | 381/805 [02:19<01:59,  3.55it/s] 47%|     | 382/805 [02:19<02:02,  3.46it/s] 48%|     | 383/805 [02:20<02:00,  3.49it/s] 48%|     | 384/805 [02:20<01:59,  3.51it/s] 48%|     | 385/805 [02:20<01:59,  3.52it/s] 48%|     | 386/805 [02:20<01:58,  3.53it/s] 48%|     | 387/805 [02:21<01:57,  3.54it/s] 48%|     | 388/805 [02:21<01:57,  3.55it/s] 48%|     | 389/805 [02:21<01:57,  3.55it/s] 48%|     | 390/805 [02:22<01:56,  3.56it/s] 49%|     | 391/805 [02:22<01:56,  3.56it/s] 49%|     | 392/805 [02:22<01:55,  3.56it/s] 49%|     | 393/805 [02:22<01:57,  3.51it/s] 49%|     | 394/805 [02:23<01:56,  3.52it/s] 49%|     | 395/805 [02:23<01:55,  3.54it/s] 49%|     | 396/805 [02:23<01:55,  3.54it/s] 49%|     | 397/805 [02:24<01:54,  3.55it/s] 49%|     | 398/805 [02:24<01:54,  3.56it/s] 50%|     | 399/805 [02:24<01:54,  3.56it/s] 50%|     | 400/805 [02:24<01:53,  3.56it/s] 50%|     | 401/805 [02:25<01:53,  3.56it/s] 50%|     | 402/805 [02:25<01:53,  3.54it/s] 50%|     | 403/805 [02:25<01:53,  3.53it/s] 50%|     | 404/805 [02:26<01:57,  3.43it/s] 50%|     | 405/805 [02:26<01:55,  3.46it/s] 50%|     | 406/805 [02:26<01:54,  3.49it/s] 51%|     | 407/805 [02:26<01:53,  3.51it/s] 51%|     | 408/805 [02:27<01:52,  3.52it/s] 51%|     | 409/805 [02:27<01:54,  3.45it/s] 51%|     | 410/805 [02:28<02:19,  2.84it/s] 51%|     | 411/805 [02:28<02:10,  3.02it/s] 51%|     | 412/805 [02:28<02:04,  3.16it/s] 51%|    | 413/805 [02:28<01:59,  3.27it/s] 51%|    | 414/805 [02:29<02:00,  3.25it/s] 52%|    | 415/805 [02:29<01:57,  3.33it/s] 52%|    | 416/805 [02:29<01:54,  3.40it/s] 52%|    | 417/805 [02:30<01:52,  3.44it/s] 52%|    | 418/805 [02:30<01:51,  3.48it/s] 52%|    | 419/805 [02:30<01:50,  3.50it/s] 52%|    | 420/805 [02:30<01:49,  3.52it/s] 52%|    | 421/805 [02:31<01:48,  3.54it/s] 52%|    | 422/805 [02:31<01:48,  3.55it/s] 53%|    | 423/805 [02:31<01:47,  3.55it/s] 53%|    | 424/805 [02:31<01:47,  3.56it/s] 53%|    | 425/805 [02:32<01:50,  3.43it/s] 53%|    | 426/805 [02:32<01:49,  3.47it/s] 53%|    | 427/805 [02:32<01:48,  3.50it/s] 53%|    | 428/805 [02:33<01:47,  3.52it/s] 53%|    | 429/805 [02:33<01:46,  3.53it/s] 53%|    | 430/805 [02:33<01:45,  3.54it/s] 54%|    | 431/805 [02:33<01:45,  3.55it/s] 54%|    | 432/805 [02:34<01:44,  3.56it/s] 54%|    | 433/805 [02:34<01:44,  3.56it/s] 54%|    | 434/805 [02:34<01:44,  3.56it/s] 54%|    | 435/805 [02:35<01:43,  3.56it/s] 54%|    | 436/805 [02:35<01:48,  3.41it/s] 54%|    | 437/805 [02:35<01:46,  3.46it/s] 54%|    | 438/805 [02:35<01:45,  3.49it/s] 55%|    | 439/805 [02:36<01:44,  3.51it/s] 55%|    | 440/805 [02:36<01:43,  3.52it/s] 55%|    | 441/805 [02:36<01:43,  3.53it/s] 55%|    | 442/805 [02:37<01:42,  3.54it/s] 55%|    | 443/805 [02:37<01:42,  3.54it/s] 55%|    | 444/805 [02:37<01:41,  3.55it/s] 55%|    | 445/805 [02:37<01:41,  3.55it/s] 55%|    | 446/805 [02:38<01:41,  3.55it/s] 56%|    | 447/805 [02:38<01:40,  3.55it/s] 56%|    | 448/805 [02:38<01:40,  3.56it/s] 56%|    | 449/805 [02:39<01:39,  3.56it/s] 56%|    | 450/805 [02:39<01:39,  3.56it/s] 56%|    | 451/805 [02:39<01:39,  3.57it/s] 56%|    | 452/805 [02:39<01:38,  3.57it/s] 56%|    | 453/805 [02:40<01:38,  3.57it/s] 56%|    | 454/805 [02:40<01:38,  3.57it/s] 57%|    | 455/805 [02:40<01:38,  3.57it/s] 57%|    | 456/805 [02:41<01:43,  3.38it/s] 57%|    | 457/805 [02:41<01:41,  3.43it/s] 57%|    | 458/805 [02:41<01:40,  3.47it/s] 57%|    | 459/805 [02:41<01:39,  3.49it/s] 57%|    | 460/805 [02:42<01:38,  3.51it/s] 57%|    | 461/805 [02:42<01:37,  3.53it/s] 57%|    | 462/805 [02:42<01:36,  3.54it/s] 58%|    | 463/805 [02:43<01:36,  3.54it/s] 58%|    | 464/805 [02:43<01:36,  3.55it/s] 58%|    | 465/805 [02:43<01:35,  3.55it/s] 58%|    | 466/805 [02:43<01:35,  3.56it/s] 58%|    | 467/805 [02:44<01:42,  3.30it/s] 58%|    | 468/805 [02:44<01:39,  3.38it/s] 58%|    | 469/805 [02:44<01:37,  3.43it/s] 58%|    | 470/805 [02:45<01:36,  3.47it/s] 59%|    | 471/805 [02:45<01:35,  3.50it/s] 59%|    | 472/805 [02:45<01:34,  3.52it/s] 59%|    | 473/805 [02:45<01:34,  3.53it/s] 59%|    | 474/805 [02:46<01:33,  3.54it/s] 59%|    | 475/805 [02:46<01:33,  3.55it/s] 59%|    | 476/805 [02:46<01:32,  3.55it/s] 59%|    | 477/805 [02:47<01:32,  3.56it/s] 59%|    | 478/805 [02:47<01:35,  3.43it/s] 60%|    | 479/805 [02:47<01:33,  3.47it/s] 60%|    | 480/805 [02:47<01:32,  3.50it/s] 60%|    | 481/805 [02:48<01:32,  3.52it/s] 60%|    | 482/805 [02:48<01:31,  3.53it/s] 60%|    | 483/805 [02:48<01:30,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 03:24:04,677 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:24:04,678 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 03:24:04,678 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8252, 'eval_samples_per_second': 354.397, 'eval_steps_per_second': 44.376, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.92it/s][A
  3%|         | 12/436 [00:00<00:08, 48.84it/s][A
  4%|         | 17/436 [00:00<00:08, 47.16it/s][A
  5%|         | 22/436 [00:00<00:09, 45.62it/s][A
  6%|         | 27/436 [00:00<00:09, 45.21it/s][A
  7%|         | 32/436 [00:00<00:08, 44.90it/s][A
  8%|         | 37/436 [00:00<00:08, 44.70it/s][A
 10%|         | 42/436 [00:00<00:08, 44.54it/s][A
 11%|         | 47/436 [00:01<00:08, 44.72it/s][A
 12%|        | 52/436 [00:01<00:08, 44.86it/s][A
 13%|        | 57/436 [00:01<00:08, 44.98it/s][A
 14%|        | 62/436 [00:01<00:08, 43.10it/s][A
 15%|        | 67/436 [00:01<00:08, 43.66it/s][A
 17%|        | 72/436 [00:01<00:08, 43.83it/s][A
 18%|        | 77/436 [00:01<00:08, 44.00it/s][A
 19%|        | 82/436 [00:01<00:08, 44.03it/s][A
 20%|        | 87/436 [00:01<00:07, 44.17it/s][A
 21%|        | 92/436 [00:02<00:07, 44.46it/s][A
 22%|       | 97/436 [00:02<00:07, 44.51it/s][A
 23%|       | 102/436 [00:02<00:07, 44.70it/s][A
 25%|       | 107/436 [00:02<00:07, 44.69it/s][A
 26%|       | 112/436 [00:02<00:07, 44.88it/s][A
 27%|       | 117/436 [00:02<00:07, 44.71it/s][A
 28%|       | 122/436 [00:02<00:07, 44.60it/s][A
 29%|       | 127/436 [00:02<00:06, 44.59it/s][A
 30%|       | 132/436 [00:02<00:06, 44.55it/s][A
 31%|      | 137/436 [00:03<00:06, 44.64it/s][A
 33%|      | 142/436 [00:03<00:06, 44.75it/s][A
 34%|      | 147/436 [00:03<00:06, 44.79it/s][A
 35%|      | 152/436 [00:03<00:06, 44.81it/s][A
 36%|      | 157/436 [00:03<00:06, 44.88it/s][A
 37%|      | 162/436 [00:03<00:06, 44.78it/s][A
 38%|      | 167/436 [00:03<00:06, 44.68it/s][A
 39%|      | 172/436 [00:03<00:05, 44.64it/s][A
 41%|      | 177/436 [00:03<00:05, 44.62it/s][A
 42%|     | 182/436 [00:04<00:05, 44.64it/s][A
 43%|     | 187/436 [00:04<00:05, 44.74it/s][A
 44%|     | 192/436 [00:04<00:05, 44.70it/s][A
 45%|     | 197/436 [00:04<00:05, 43.25it/s][A
 46%|     | 202/436 [00:04<00:05, 43.62it/s][A
 47%|     | 207/436 [00:04<00:05, 44.11it/s][A
 49%|     | 212/436 [00:04<00:05, 44.22it/s][A
 50%|     | 217/436 [00:04<00:04, 44.36it/s][A
 51%|     | 222/436 [00:04<00:04, 44.42it/s][A
 52%|    | 227/436 [00:05<00:04, 44.38it/s][A
 53%|    | 232/436 [00:05<00:04, 44.54it/s][A
 54%|    | 237/436 [00:05<00:04, 44.50it/s][A
 56%|    | 242/436 [00:05<00:04, 44.59it/s][A
 57%|    | 247/436 [00:05<00:04, 44.77it/s][A
 58%|    | 252/436 [00:05<00:04, 44.74it/s][A
 59%|    | 257/436 [00:05<00:03, 44.78it/s][A
 60%|    | 262/436 [00:05<00:03, 44.64it/s][A
 61%|    | 267/436 [00:05<00:03, 44.65it/s][A
 62%|   | 272/436 [00:06<00:03, 44.72it/s][A
 64%|   | 277/436 [00:06<00:03, 44.66it/s][A
 65%|   | 282/436 [00:06<00:03, 44.68it/s][A
 66%|   | 287/436 [00:06<00:03, 44.65it/s][A
 67%|   | 292/436 [00:06<00:03, 44.83it/s][A
 68%|   | 297/436 [00:06<00:03, 44.71it/s][A
 69%|   | 302/436 [00:06<00:02, 44.77it/s][A
 70%|   | 307/436 [00:06<00:02, 44.69it/s][A
 72%|  | 312/436 [00:06<00:02, 44.63it/s][A
 73%|  | 317/436 [00:07<00:02, 44.74it/s][A
 74%|  | 322/436 [00:07<00:02, 44.60it/s][A
 75%|  | 327/436 [00:07<00:02, 44.72it/s][A
 76%|  | 332/436 [00:07<00:02, 43.45it/s][A
 77%|  | 337/436 [00:07<00:02, 44.02it/s][A
 78%|  | 342/436 [00:07<00:02, 44.13it/s][A
 80%|  | 347/436 [00:07<00:02, 44.42it/s][A
 81%|  | 352/436 [00:07<00:01, 44.48it/s][A
 82%| | 357/436 [00:08<00:01, 44.46it/s][A
 83%| | 362/436 [00:08<00:01, 44.56it/s][A
 84%| | 367/436 [00:08<00:01, 44.48it/s][A
 85%| | 372/436 [00:08<00:01, 44.50it/s][A
 86%| | 377/436 [00:08<00:01, 44.55it/s][A
 88%| | 382/436 [00:08<00:01, 44.55it/s][A
 89%| | 387/436 [00:08<00:01, 44.77it/s][A
 90%| | 392/436 [00:08<00:00, 44.80it/s][A
 91%| | 397/436 [00:08<00:00, 44.87it/s][A
 92%|| 402/436 [00:09<00:00, 44.79it/s][A
 93%|| 407/436 [00:09<00:00, 44.70it/s][A
 94%|| 412/436 [00:09<00:00, 44.64it/s][A
 96%|| 417/436 [00:09<00:00, 44.56it/s][A
 97%|| 422/436 [00:09<00:00, 44.68it/s][A
 98%|| 427/436 [00:09<00:00, 44.77it/s][A
 99%|| 432/436 [00:09<00:00, 44.84it/s][A                                                 
                                                 [A 60%|    | 483/805 [02:58<01:30,  3.54it/s]
100%|| 436/436 [00:09<00:00, 44.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:24:14,771 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 03:24:15,038 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:24:18,424 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:24:18,581 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:24:18,660 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-483/special_tokens_map.json
 60%|    | 484/805 [03:04<26:16,  4.91s/it] 60%|    | 485/805 [03:04<18:47,  3.52s/it] 60%|    | 486/805 [03:05<13:33,  2.55s/it] 60%|    | 487/805 [03:05<09:54,  1.87s/it] 61%|    | 488/805 [03:05<07:21,  1.39s/it] 61%|    | 489/805 [03:05<05:38,  1.07s/it] 61%|    | 490/805 [03:06<04:22,  1.20it/s] 61%|    | 491/805 [03:06<03:29,  1.50it/s] 61%|    | 492/805 [03:06<02:52,  1.81it/s] 61%|    | 493/805 [03:07<02:26,  2.13it/s] 61%|   | 494/805 [03:07<02:08,  2.42it/s] 61%|   | 495/805 [03:07<01:55,  2.68it/s] 62%|   | 496/805 [03:07<01:46,  2.89it/s] 62%|   | 497/805 [03:08<01:40,  3.06it/s] 62%|   | 498/805 [03:08<01:36,  3.19it/s] 62%|   | 499/805 [03:08<01:43,  2.97it/s] 62%|   | 500/805 [03:09<01:37,  3.14it/s]                                                  62%|   | 500/805 [03:09<01:37,  3.14it/s] 62%|   | 501/805 [03:09<01:33,  3.27it/s] 62%|   | 502/805 [03:09<01:30,  3.37it/s] 62%|   | 503/805 [03:09<01:27,  3.44it/s] 63%|   | 504/805 [03:10<01:26,  3.49it/s] 63%|   | 505/805 [03:10<01:25,  3.53it/s] 63%|   | 506/805 [03:10<01:24,  3.56it/s] 63%|   | 507/805 [03:11<01:23,  3.57it/s] 63%|   | 508/805 [03:11<01:22,  3.59it/s] 63%|   | 509/805 [03:11<01:22,  3.59it/s] 63%|   | 510/805 [03:12<01:45,  2.80it/s] 63%|   | 511/805 [03:12<01:37,  3.00it/s] 64%|   | 512/805 [03:12<01:32,  3.16it/s] 64%|   | 513/805 [03:12<01:28,  3.29it/s] 64%|   | 514/805 [03:13<01:26,  3.38it/s] 64%|   | 515/805 [03:13<01:24,  3.45it/s] 64%|   | 516/805 [03:13<01:22,  3.50it/s] 64%|   | 517/805 [03:14<01:21,  3.53it/s] 64%|   | 518/805 [03:14<01:20,  3.56it/s] 64%|   | 519/805 [03:14<01:19,  3.58it/s] 65%|   | 520/805 [03:14<01:21,  3.48it/s] 65%|   | 521/805 [03:15<01:20,  3.52it/s] 65%|   | 522/805 [03:15<01:19,  3.55it/s] 65%|   | 523/805 [03:15<01:19,  3.57it/s] 65%|   | 524/805 [03:16<01:18,  3.58it/s] 65%|   | 525/805 [03:16<01:17,  3.59it/s] 65%|   | 526/805 [03:16<01:17,  3.60it/s] 65%|   | 527/805 [03:16<01:17,  3.60it/s] 66%|   | 528/805 [03:17<01:16,  3.61it/s] 66%|   | 529/805 [03:17<01:16,  3.61it/s] 66%|   | 530/805 [03:17<01:16,  3.61it/s] 66%|   | 531/805 [03:18<01:18,  3.48it/s] 66%|   | 532/805 [03:18<01:17,  3.52it/s] 66%|   | 533/805 [03:18<01:16,  3.55it/s] 66%|   | 534/805 [03:18<01:15,  3.57it/s] 66%|   | 535/805 [03:19<01:15,  3.58it/s] 67%|   | 536/805 [03:19<01:14,  3.59it/s] 67%|   | 537/805 [03:19<01:14,  3.60it/s] 67%|   | 538/805 [03:19<01:14,  3.60it/s] 67%|   | 539/805 [03:20<01:13,  3.61it/s] 67%|   | 540/805 [03:20<01:13,  3.61it/s] 67%|   | 541/805 [03:20<01:13,  3.61it/s] 67%|   | 542/805 [03:21<01:15,  3.47it/s] 67%|   | 543/805 [03:21<01:14,  3.51it/s] 68%|   | 544/805 [03:21<01:13,  3.55it/s] 68%|   | 545/805 [03:21<01:12,  3.57it/s] 68%|   | 546/805 [03:22<01:12,  3.58it/s] 68%|   | 547/805 [03:22<01:11,  3.59it/s] 68%|   | 548/805 [03:22<01:11,  3.60it/s] 68%|   | 549/805 [03:23<01:11,  3.60it/s] 68%|   | 550/805 [03:23<01:10,  3.61it/s] 68%|   | 551/805 [03:23<01:10,  3.61it/s] 69%|   | 552/805 [03:23<01:10,  3.61it/s] 69%|   | 553/805 [03:24<01:11,  3.50it/s] 69%|   | 554/805 [03:24<01:11,  3.53it/s] 69%|   | 555/805 [03:24<01:10,  3.56it/s] 69%|   | 556/805 [03:25<01:09,  3.57it/s] 69%|   | 557/805 [03:25<01:09,  3.59it/s] 69%|   | 558/805 [03:25<01:10,  3.52it/s] 69%|   | 559/805 [03:25<01:09,  3.54it/s] 70%|   | 560/805 [03:26<01:08,  3.56it/s] 70%|   | 561/805 [03:26<01:08,  3.58it/s] 70%|   | 562/805 [03:26<01:07,  3.59it/s] 70%|   | 563/805 [03:26<01:07,  3.60it/s] 70%|   | 564/805 [03:27<01:08,  3.51it/s] 70%|   | 565/805 [03:27<01:09,  3.47it/s] 70%|   | 566/805 [03:27<01:09,  3.45it/s] 70%|   | 567/805 [03:28<01:14,  3.20it/s] 71%|   | 568/805 [03:28<01:11,  3.30it/s] 71%|   | 569/805 [03:28<01:09,  3.39it/s] 71%|   | 570/805 [03:29<01:08,  3.45it/s] 71%|   | 571/805 [03:29<01:06,  3.50it/s] 71%|   | 572/805 [03:29<01:05,  3.54it/s] 71%|   | 573/805 [03:29<01:05,  3.56it/s] 71%|  | 574/805 [03:30<01:04,  3.58it/s] 71%|  | 575/805 [03:30<01:06,  3.45it/s] 72%|  | 576/805 [03:30<01:05,  3.50it/s] 72%|  | 577/805 [03:31<01:04,  3.53it/s] 72%|  | 578/805 [03:31<01:03,  3.55it/s] 72%|  | 579/805 [03:31<01:03,  3.57it/s] 72%|  | 580/805 [03:31<01:02,  3.59it/s] 72%|  | 581/805 [03:32<01:02,  3.60it/s] 72%|  | 582/805 [03:32<01:01,  3.60it/s] 72%|  | 583/805 [03:32<01:01,  3.60it/s] 73%|  | 584/805 [03:32<01:01,  3.60it/s] 73%|  | 585/805 [03:33<01:00,  3.61it/s] 73%|  | 586/805 [03:33<01:02,  3.48it/s] 73%|  | 587/805 [03:33<01:01,  3.52it/s] 73%|  | 588/805 [03:34<01:01,  3.55it/s] 73%|  | 589/805 [03:34<01:00,  3.57it/s] 73%|  | 590/805 [03:34<01:00,  3.58it/s] 73%|  | 591/805 [03:34<00:59,  3.59it/s] 74%|  | 592/805 [03:35<00:59,  3.60it/s] 74%|  | 593/805 [03:35<00:58,  3.61it/s] 74%|  | 594/805 [03:35<00:58,  3.61it/s] 74%|  | 595/805 [03:36<00:58,  3.61it/s] 74%|  | 596/805 [03:36<00:57,  3.61it/s] 74%|  | 597/805 [03:36<01:00,  3.45it/s] 74%|  | 598/805 [03:36<00:59,  3.50it/s] 74%|  | 599/805 [03:37<00:58,  3.53it/s] 75%|  | 600/805 [03:37<00:57,  3.56it/s] 75%|  | 601/805 [03:37<00:57,  3.57it/s] 75%|  | 602/805 [03:38<00:56,  3.59it/s] 75%|  | 603/805 [03:38<00:56,  3.59it/s] 75%|  | 604/805 [03:38<00:55,  3.60it/s] 75%|  | 605/805 [03:38<00:55,  3.61it/s] 75%|  | 606/805 [03:39<00:55,  3.61it/s] 75%|  | 607/805 [03:39<00:54,  3.61it/s] 76%|  | 608/805 [03:39<00:54,  3.61it/s] 76%|  | 609/805 [03:39<00:54,  3.61it/s] 76%|  | 610/805 [03:40<00:54,  3.61it/s] 76%|  | 611/805 [03:40<00:53,  3.61it/s] 76%|  | 612/805 [03:40<00:53,  3.61it/s] 76%|  | 613/805 [03:41<00:53,  3.61it/s] 76%|  | 614/805 [03:41<00:52,  3.61it/s] 76%|  | 615/805 [03:41<00:52,  3.61it/s] 77%|  | 616/805 [03:41<00:52,  3.61it/s] 77%|  | 617/805 [03:42<00:51,  3.62it/s] 77%|  | 618/805 [03:42<00:51,  3.62it/s] 77%|  | 619/805 [03:42<00:55,  3.37it/s] 77%|  | 620/805 [03:43<00:53,  3.44it/s] 77%|  | 621/805 [03:43<00:52,  3.49it/s] 77%|  | 622/805 [03:43<00:51,  3.53it/s] 77%|  | 623/805 [03:43<00:51,  3.55it/s] 78%|  | 624/805 [03:44<00:50,  3.57it/s] 78%|  | 625/805 [03:44<00:50,  3.59it/s] 78%|  | 626/805 [03:44<00:49,  3.60it/s] 78%|  | 627/805 [03:45<00:49,  3.60it/s] 78%|  | 628/805 [03:45<00:49,  3.60it/s] 78%|  | 629/805 [03:45<00:48,  3.61it/s] 78%|  | 630/805 [03:45<00:48,  3.61it/s] 78%|  | 631/805 [03:46<00:48,  3.62it/s] 79%|  | 632/805 [03:46<00:47,  3.61it/s] 79%|  | 633/805 [03:46<00:47,  3.61it/s] 79%|  | 634/805 [03:46<00:47,  3.61it/s] 79%|  | 635/805 [03:47<00:46,  3.62it/s] 79%|  | 636/805 [03:47<00:46,  3.62it/s] 79%|  | 637/805 [03:47<00:50,  3.31it/s] 79%|  | 638/805 [03:48<00:49,  3.40it/s] 79%|  | 639/805 [03:48<00:47,  3.46it/s] 80%|  | 640/805 [03:48<00:47,  3.50it/s] 80%|  | 641/805 [03:48<00:46,  3.53it/s] 80%|  | 642/805 [03:49<00:45,  3.55it/s] 80%|  | 643/805 [03:49<00:45,  3.57it/s] 80%|  | 644/805 [03:49<00:44,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 03:25:05,694 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:25:05,694 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 03:25:05,694 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8123, 'eval_samples_per_second': 354.861, 'eval_steps_per_second': 44.434, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.17it/s][A
  3%|         | 12/436 [00:00<00:08, 49.24it/s][A
  4%|         | 17/436 [00:00<00:08, 47.22it/s][A
  5%|         | 22/436 [00:00<00:08, 46.09it/s][A
  6%|         | 27/436 [00:00<00:09, 45.35it/s][A
  7%|         | 32/436 [00:00<00:08, 44.98it/s][A
  8%|         | 37/436 [00:00<00:10, 38.93it/s][A
 10%|         | 42/436 [00:00<00:09, 40.71it/s][A
 11%|         | 47/436 [00:01<00:09, 42.06it/s][A
 12%|        | 52/436 [00:01<00:08, 43.00it/s][A
 13%|        | 57/436 [00:01<00:08, 43.74it/s][A
 14%|        | 62/436 [00:01<00:08, 44.23it/s][A
 15%|        | 67/436 [00:01<00:08, 44.53it/s][A
 17%|        | 72/436 [00:01<00:08, 43.89it/s][A
 18%|        | 77/436 [00:01<00:08, 43.78it/s][A
 19%|        | 82/436 [00:01<00:08, 44.12it/s][A
 20%|        | 87/436 [00:01<00:07, 44.15it/s][A
 21%|        | 92/436 [00:02<00:07, 44.43it/s][A
 22%|       | 97/436 [00:02<00:07, 44.57it/s][A
 23%|       | 102/436 [00:02<00:07, 44.77it/s][A
 25%|       | 107/436 [00:02<00:07, 44.72it/s][A
 26%|       | 112/436 [00:02<00:07, 44.85it/s][A
 27%|       | 117/436 [00:02<00:07, 44.94it/s][A
 28%|       | 122/436 [00:02<00:07, 44.64it/s][A
 29%|       | 127/436 [00:02<00:06, 44.48it/s][A
 30%|       | 132/436 [00:02<00:06, 44.37it/s][A
 31%|      | 137/436 [00:03<00:06, 44.36it/s][A
 33%|      | 142/436 [00:03<00:06, 44.61it/s][A
 34%|      | 147/436 [00:03<00:06, 44.75it/s][A
 35%|      | 152/436 [00:03<00:06, 44.71it/s][A
 36%|      | 157/436 [00:03<00:06, 44.94it/s][A
 37%|      | 162/436 [00:03<00:06, 44.74it/s][A
 38%|      | 167/436 [00:03<00:06, 44.50it/s][A
 39%|      | 172/436 [00:03<00:06, 43.43it/s][A
 41%|      | 177/436 [00:03<00:05, 43.70it/s][A
 42%|     | 182/436 [00:04<00:05, 44.04it/s][A
 43%|     | 187/436 [00:04<00:05, 44.29it/s][A
 44%|     | 192/436 [00:04<00:05, 44.63it/s][A
 45%|     | 197/436 [00:04<00:05, 44.78it/s][A
 46%|     | 202/436 [00:04<00:05, 44.86it/s][A
 47%|     | 207/436 [00:04<00:05, 44.60it/s][A
 49%|     | 212/436 [00:04<00:05, 44.45it/s][A
 50%|     | 217/436 [00:04<00:04, 44.30it/s][A
 51%|     | 222/436 [00:05<00:04, 44.33it/s][A
 52%|    | 227/436 [00:05<00:04, 44.43it/s][A
 53%|    | 232/436 [00:05<00:04, 44.61it/s][A
 54%|    | 237/436 [00:05<00:04, 44.76it/s][A
 56%|    | 242/436 [00:05<00:04, 44.92it/s][A
 57%|    | 247/436 [00:05<00:04, 44.92it/s][A
 58%|    | 252/436 [00:05<00:04, 44.83it/s][A
 59%|    | 257/436 [00:05<00:04, 44.59it/s][A
 60%|    | 262/436 [00:05<00:03, 44.47it/s][A
 61%|    | 267/436 [00:06<00:03, 44.30it/s][A
 62%|   | 272/436 [00:06<00:03, 44.42it/s][A
 64%|   | 277/436 [00:06<00:03, 44.61it/s][A
 65%|   | 282/436 [00:06<00:03, 44.78it/s][A
 66%|   | 287/436 [00:06<00:03, 44.93it/s][A
 67%|   | 292/436 [00:06<00:03, 44.91it/s][A
 68%|   | 297/436 [00:06<00:03, 44.85it/s][A
 69%|   | 302/436 [00:06<00:03, 44.57it/s][A
 70%|   | 307/436 [00:06<00:03, 42.34it/s][A
 72%|  | 312/436 [00:07<00:02, 43.14it/s][A
 73%|  | 317/436 [00:07<00:02, 43.59it/s][A
 74%|  | 322/436 [00:07<00:02, 43.81it/s][A
 75%|  | 327/436 [00:07<00:02, 44.22it/s][A
 76%|  | 332/436 [00:07<00:02, 44.42it/s][A
 77%|  | 337/436 [00:07<00:02, 44.63it/s][A
 78%|  | 342/436 [00:07<00:02, 44.53it/s][A
 80%|  | 347/436 [00:07<00:02, 44.27it/s][A
 81%|  | 352/436 [00:07<00:01, 44.36it/s][A
 82%| | 357/436 [00:08<00:01, 44.42it/s][A
 83%| | 362/436 [00:08<00:01, 44.62it/s][A
 84%| | 367/436 [00:08<00:01, 44.72it/s][A
 85%| | 372/436 [00:08<00:01, 44.86it/s][A
 86%| | 377/436 [00:08<00:01, 44.93it/s][A
 88%| | 382/436 [00:08<00:01, 44.86it/s][A
 89%| | 387/436 [00:08<00:01, 44.67it/s][A
 90%| | 392/436 [00:08<00:00, 44.57it/s][A
 91%| | 397/436 [00:08<00:00, 44.46it/s][A
 92%|| 402/436 [00:09<00:00, 44.48it/s][A
 93%|| 407/436 [00:09<00:00, 44.57it/s][A
 94%|| 412/436 [00:09<00:00, 44.71it/s][A
 96%|| 417/436 [00:09<00:00, 44.73it/s][A
 97%|| 422/436 [00:09<00:00, 44.90it/s][A
 98%|| 427/436 [00:09<00:00, 44.78it/s][A
 99%|| 432/436 [00:09<00:00, 44.71it/s][A                                                 
                                                 [A 80%|  | 644/805 [03:59<00:44,  3.59it/s]
100%|| 436/436 [00:09<00:00, 44.71it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:25:15,841 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 03:25:16,004 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:25:21,329 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:25:21,475 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:25:21,549 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-644/special_tokens_map.json
 80%|  | 645/805 [04:06<14:16,  5.35s/it] 80%|  | 646/805 [04:07<10:09,  3.83s/it] 80%|  | 647/805 [04:07<07:17,  2.77s/it] 80%|  | 648/805 [04:07<05:17,  2.02s/it] 81%|  | 649/805 [04:08<03:53,  1.50s/it] 81%|  | 650/805 [04:08<02:55,  1.13s/it] 81%|  | 651/805 [04:08<02:14,  1.14it/s] 81%|  | 652/805 [04:08<01:48,  1.41it/s] 81%|  | 653/805 [04:09<01:28,  1.72it/s] 81%|  | 654/805 [04:09<01:13,  2.04it/s] 81%| | 655/805 [04:09<01:03,  2.35it/s] 81%| | 656/805 [04:10<00:56,  2.62it/s] 82%| | 657/805 [04:10<00:51,  2.85it/s] 82%| | 658/805 [04:10<00:48,  3.04it/s] 82%| | 659/805 [04:10<00:45,  3.20it/s] 82%| | 660/805 [04:11<00:43,  3.31it/s] 82%| | 661/805 [04:11<00:42,  3.39it/s] 82%| | 662/805 [04:11<00:41,  3.46it/s] 82%| | 663/805 [04:12<00:42,  3.32it/s] 82%| | 664/805 [04:12<00:41,  3.40it/s] 83%| | 665/805 [04:12<00:40,  3.46it/s] 83%| | 666/805 [04:12<00:39,  3.51it/s] 83%| | 667/805 [04:13<00:38,  3.54it/s] 83%| | 668/805 [04:13<00:38,  3.56it/s] 83%| | 669/805 [04:13<00:38,  3.57it/s] 83%| | 670/805 [04:14<00:37,  3.59it/s] 83%| | 671/805 [04:14<00:37,  3.60it/s] 83%| | 672/805 [04:14<00:36,  3.60it/s] 84%| | 673/805 [04:14<00:36,  3.60it/s] 84%| | 674/805 [04:15<00:38,  3.44it/s] 84%| | 675/805 [04:15<00:37,  3.49it/s] 84%| | 676/805 [04:15<00:36,  3.53it/s] 84%| | 677/805 [04:16<00:36,  3.56it/s] 84%| | 678/805 [04:16<00:35,  3.57it/s] 84%| | 679/805 [04:16<00:35,  3.59it/s] 84%| | 680/805 [04:16<00:34,  3.59it/s] 85%| | 681/805 [04:17<00:34,  3.60it/s] 85%| | 682/805 [04:17<00:34,  3.60it/s] 85%| | 683/805 [04:17<00:33,  3.61it/s] 85%| | 684/805 [04:17<00:33,  3.61it/s] 85%| | 685/805 [04:18<00:33,  3.61it/s] 85%| | 686/805 [04:18<00:33,  3.58it/s] 85%| | 687/805 [04:18<00:32,  3.58it/s] 85%| | 688/805 [04:19<00:32,  3.59it/s] 86%| | 689/805 [04:19<00:32,  3.59it/s] 86%| | 690/805 [04:19<00:32,  3.59it/s] 86%| | 691/805 [04:19<00:31,  3.60it/s] 86%| | 692/805 [04:20<00:31,  3.60it/s] 86%| | 693/805 [04:20<00:31,  3.61it/s] 86%| | 694/805 [04:20<00:30,  3.61it/s] 86%| | 695/805 [04:21<00:30,  3.61it/s] 86%| | 696/805 [04:21<00:30,  3.61it/s] 87%| | 697/805 [04:21<00:31,  3.47it/s] 87%| | 698/805 [04:21<00:30,  3.51it/s] 87%| | 699/805 [04:22<00:29,  3.54it/s] 87%| | 700/805 [04:22<00:29,  3.57it/s] 87%| | 701/805 [04:22<00:29,  3.58it/s] 87%| | 702/805 [04:22<00:28,  3.59it/s] 87%| | 703/805 [04:23<00:28,  3.59it/s] 87%| | 704/805 [04:23<00:28,  3.60it/s] 88%| | 705/805 [04:23<00:27,  3.60it/s] 88%| | 706/805 [04:24<00:27,  3.60it/s] 88%| | 707/805 [04:24<00:27,  3.61it/s] 88%| | 708/805 [04:24<00:28,  3.46it/s] 88%| | 709/805 [04:24<00:27,  3.50it/s] 88%| | 710/805 [04:25<00:26,  3.53it/s] 88%| | 711/805 [04:25<00:26,  3.55it/s] 88%| | 712/805 [04:25<00:26,  3.57it/s] 89%| | 713/805 [04:26<00:25,  3.59it/s] 89%| | 714/805 [04:26<00:25,  3.59it/s] 89%| | 715/805 [04:26<00:25,  3.60it/s] 89%| | 716/805 [04:26<00:24,  3.60it/s] 89%| | 717/805 [04:27<00:24,  3.60it/s] 89%| | 718/805 [04:27<00:24,  3.60it/s] 89%| | 719/805 [04:27<00:24,  3.51it/s] 89%| | 720/805 [04:28<00:25,  3.38it/s] 90%| | 721/805 [04:28<00:24,  3.43it/s] 90%| | 722/805 [04:28<00:23,  3.48it/s] 90%| | 723/805 [04:28<00:23,  3.52it/s] 90%| | 724/805 [04:29<00:22,  3.55it/s] 90%| | 725/805 [04:29<00:22,  3.57it/s] 90%| | 726/805 [04:29<00:22,  3.58it/s] 90%| | 727/805 [04:30<00:21,  3.59it/s] 90%| | 728/805 [04:30<00:21,  3.60it/s] 91%| | 729/805 [04:30<00:21,  3.60it/s] 91%| | 730/805 [04:30<00:21,  3.51it/s] 91%| | 731/805 [04:31<00:20,  3.54it/s] 91%| | 732/805 [04:31<00:20,  3.56it/s] 91%| | 733/805 [04:31<00:20,  3.58it/s] 91%| | 734/805 [04:31<00:19,  3.58it/s] 91%|| 735/805 [04:32<00:19,  3.59it/s] 91%|| 736/805 [04:32<00:19,  3.60it/s] 92%|| 737/805 [04:32<00:18,  3.59it/s] 92%|| 738/805 [04:33<00:18,  3.60it/s] 92%|| 739/805 [04:33<00:18,  3.60it/s] 92%|| 740/805 [04:33<00:18,  3.61it/s] 92%|| 741/805 [04:33<00:18,  3.47it/s] 92%|| 742/805 [04:34<00:17,  3.51it/s] 92%|| 743/805 [04:34<00:17,  3.54it/s] 92%|| 744/805 [04:34<00:17,  3.56it/s] 93%|| 745/805 [04:35<00:16,  3.58it/s] 93%|| 746/805 [04:35<00:16,  3.59it/s] 93%|| 747/805 [04:35<00:16,  3.59it/s] 93%|| 748/805 [04:35<00:15,  3.59it/s] 93%|| 749/805 [04:36<00:15,  3.60it/s] 93%|| 750/805 [04:36<00:15,  3.60it/s] 93%|| 751/805 [04:36<00:14,  3.60it/s] 93%|| 752/805 [04:37<00:15,  3.50it/s] 94%|| 753/805 [04:37<00:14,  3.53it/s] 94%|| 754/805 [04:37<00:14,  3.56it/s] 94%|| 755/805 [04:37<00:13,  3.57it/s] 94%|| 756/805 [04:38<00:13,  3.58it/s] 94%|| 757/805 [04:38<00:13,  3.59it/s] 94%|| 758/805 [04:38<00:13,  3.60it/s] 94%|| 759/805 [04:38<00:12,  3.60it/s] 94%|| 760/805 [04:39<00:12,  3.60it/s] 95%|| 761/805 [04:39<00:12,  3.60it/s] 95%|| 762/805 [04:39<00:11,  3.60it/s] 95%|| 763/805 [04:40<00:12,  3.46it/s] 95%|| 764/805 [04:40<00:11,  3.50it/s] 95%|| 765/805 [04:40<00:11,  3.53it/s] 95%|| 766/805 [04:40<00:10,  3.56it/s] 95%|| 767/805 [04:41<00:10,  3.57it/s] 95%|| 768/805 [04:41<00:10,  3.57it/s] 96%|| 769/805 [04:41<00:10,  3.58it/s] 96%|| 770/805 [04:42<00:09,  3.59it/s] 96%|| 771/805 [04:42<00:09,  3.60it/s] 96%|| 772/805 [04:42<00:09,  3.61it/s] 96%|| 773/805 [04:42<00:08,  3.60it/s] 96%|| 774/805 [04:43<00:09,  3.34it/s] 96%|| 775/805 [04:43<00:08,  3.42it/s] 96%|| 776/805 [04:43<00:08,  3.47it/s] 97%|| 777/805 [04:44<00:07,  3.51it/s] 97%|| 778/805 [04:44<00:07,  3.54it/s] 97%|| 779/805 [04:44<00:07,  3.56it/s] 97%|| 780/805 [04:44<00:06,  3.57it/s] 97%|| 781/805 [04:45<00:06,  3.58it/s] 97%|| 782/805 [04:45<00:06,  3.59it/s] 97%|| 783/805 [04:45<00:06,  3.60it/s] 97%|| 784/805 [04:46<00:05,  3.60it/s] 98%|| 785/805 [04:46<00:05,  3.61it/s] 98%|| 786/805 [04:46<00:05,  3.60it/s] 98%|| 787/805 [04:46<00:05,  3.60it/s] 98%|| 788/805 [04:47<00:04,  3.59it/s] 98%|| 789/805 [04:47<00:04,  3.59it/s] 98%|| 790/805 [04:47<00:04,  3.60it/s] 98%|| 791/805 [04:47<00:03,  3.60it/s] 98%|| 792/805 [04:48<00:03,  3.60it/s] 99%|| 793/805 [04:48<00:03,  3.61it/s] 99%|| 794/805 [04:48<00:03,  3.60it/s] 99%|| 795/805 [04:49<00:02,  3.52it/s] 99%|| 796/805 [04:49<00:02,  3.54it/s] 99%|| 797/805 [04:49<00:02,  3.56it/s] 99%|| 798/805 [04:49<00:01,  3.57it/s] 99%|| 799/805 [04:50<00:01,  3.58it/s] 99%|| 800/805 [04:50<00:01,  3.58it/s]100%|| 801/805 [04:50<00:01,  3.59it/s]100%|| 802/805 [04:51<00:00,  3.60it/s]100%|| 803/805 [04:51<00:00,  3.60it/s]100%|| 804/805 [04:51<00:00,  3.60it/s]100%|| 805/805 [04:51<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 03:26:07,750 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:26:07,751 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 03:26:07,751 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.861, 'eval_samples_per_second': 353.107, 'eval_steps_per_second': 44.214, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 57.11it/s][A
  3%|         | 12/436 [00:00<00:08, 49.47it/s][A
  4%|         | 18/436 [00:00<00:08, 47.30it/s][A
  5%|         | 23/436 [00:00<00:08, 46.62it/s][A
  6%|         | 28/436 [00:00<00:08, 46.09it/s][A
  8%|         | 33/436 [00:00<00:08, 45.67it/s][A
  9%|         | 38/436 [00:00<00:08, 45.43it/s][A
 10%|         | 43/436 [00:00<00:08, 44.87it/s][A
 11%|         | 48/436 [00:01<00:08, 44.62it/s][A
 12%|        | 53/436 [00:01<00:08, 44.60it/s][A
 13%|        | 58/436 [00:01<00:08, 44.74it/s][A
 14%|        | 63/436 [00:01<00:08, 44.89it/s][A
 16%|        | 68/436 [00:01<00:08, 44.85it/s][A
 17%|        | 73/436 [00:01<00:08, 44.86it/s][A
 18%|        | 78/436 [00:01<00:07, 44.81it/s][A
 19%|        | 83/436 [00:01<00:07, 44.77it/s][A
 20%|        | 88/436 [00:01<00:07, 44.64it/s][A
 21%|       | 93/436 [00:02<00:07, 44.54it/s][A
 22%|       | 98/436 [00:02<00:07, 44.39it/s][A
 24%|       | 103/436 [00:02<00:07, 44.59it/s][A
 25%|       | 108/436 [00:02<00:07, 44.67it/s][A
 26%|       | 113/436 [00:02<00:07, 44.90it/s][A
 27%|       | 118/436 [00:02<00:07, 44.89it/s][A
 28%|       | 123/436 [00:02<00:07, 44.66it/s][A
 29%|       | 128/436 [00:02<00:06, 44.77it/s][A
 31%|       | 133/436 [00:02<00:06, 44.57it/s][A
 32%|      | 138/436 [00:03<00:06, 44.54it/s][A
 33%|      | 143/436 [00:03<00:06, 44.32it/s][A
 34%|      | 148/436 [00:03<00:06, 44.40it/s][A
 35%|      | 153/436 [00:03<00:06, 44.62it/s][A
 36%|      | 158/436 [00:03<00:06, 44.67it/s][A
 37%|      | 163/436 [00:03<00:06, 44.76it/s][A
 39%|      | 168/436 [00:03<00:05, 44.76it/s][A
 40%|      | 173/436 [00:03<00:05, 44.64it/s][A
 41%|      | 178/436 [00:03<00:05, 44.50it/s][A
 42%|     | 183/436 [00:04<00:05, 44.46it/s][A
 43%|     | 188/436 [00:04<00:05, 44.36it/s][A
 44%|     | 193/436 [00:04<00:05, 44.52it/s][A
 45%|     | 198/436 [00:04<00:05, 44.73it/s][A
 47%|     | 203/436 [00:04<00:05, 44.76it/s][A
 48%|     | 208/436 [00:04<00:05, 44.81it/s][A
 49%|     | 213/436 [00:04<00:04, 44.82it/s][A
 50%|     | 218/436 [00:04<00:04, 44.78it/s][A
 51%|     | 223/436 [00:04<00:04, 44.63it/s][A
 52%|    | 228/436 [00:05<00:04, 44.45it/s][A
 53%|    | 233/436 [00:05<00:04, 44.41it/s][A
 55%|    | 238/436 [00:05<00:04, 44.43it/s][A
 56%|    | 243/436 [00:05<00:04, 44.75it/s][A
 57%|    | 248/436 [00:05<00:04, 44.82it/s][A
 58%|    | 253/436 [00:05<00:04, 44.87it/s][A
 59%|    | 258/436 [00:05<00:03, 44.90it/s][A
 60%|    | 263/436 [00:05<00:03, 44.79it/s][A
 61%|   | 268/436 [00:05<00:03, 44.64it/s][A
 63%|   | 273/436 [00:06<00:03, 44.42it/s][A
 64%|   | 278/436 [00:06<00:03, 43.42it/s][A
 65%|   | 283/436 [00:06<00:03, 43.84it/s][A
 66%|   | 288/436 [00:06<00:03, 44.21it/s][A
 67%|   | 293/436 [00:06<00:03, 44.40it/s][A
 68%|   | 298/436 [00:06<00:03, 44.63it/s][A
 69%|   | 303/436 [00:06<00:02, 44.77it/s][A
 71%|   | 308/436 [00:06<00:02, 44.66it/s][A
 72%|  | 313/436 [00:06<00:02, 44.47it/s][A
 73%|  | 318/436 [00:07<00:02, 44.26it/s][A
 74%|  | 323/436 [00:07<00:02, 44.39it/s][A
 75%|  | 328/436 [00:07<00:02, 44.50it/s][A
 76%|  | 333/436 [00:07<00:02, 44.73it/s][A
 78%|  | 338/436 [00:07<00:02, 44.76it/s][A
 79%|  | 343/436 [00:07<00:02, 44.83it/s][A
 80%|  | 348/436 [00:07<00:01, 44.89it/s][A
 81%|  | 353/436 [00:07<00:01, 44.65it/s][A
 82%| | 358/436 [00:07<00:01, 44.41it/s][A
 83%| | 363/436 [00:08<00:01, 44.32it/s][A
 84%| | 368/436 [00:08<00:01, 44.37it/s][A
 86%| | 373/436 [00:08<00:01, 44.56it/s][A
 87%| | 378/436 [00:08<00:01, 44.74it/s][A
 88%| | 383/436 [00:08<00:01, 44.86it/s][A
 89%| | 388/436 [00:08<00:01, 44.84it/s][A
 90%| | 393/436 [00:08<00:00, 44.86it/s][A
 91%|| 398/436 [00:08<00:00, 44.83it/s][A
 92%|| 403/436 [00:09<00:00, 44.64it/s][A
 94%|| 408/436 [00:09<00:00, 44.46it/s][A
 95%|| 413/436 [00:09<00:00, 40.81it/s][A
 96%|| 418/436 [00:09<00:00, 42.06it/s][A
 97%|| 423/436 [00:09<00:00, 43.03it/s][A
 98%|| 428/436 [00:09<00:00, 43.74it/s][A
 99%|| 433/436 [00:09<00:00, 44.15it/s][A                                                 
                                                 [A100%|| 805/805 [05:01<00:00,  3.60it/s]
100%|| 436/436 [00:09<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:26:17,725 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 03:26:17,850 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:26:20,532 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:26:20,720 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:26:20,746 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:26:21,625 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:26:21,625 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161 (score: 1.1192312240600586).
                                                 100%|| 805/805 [05:15<00:00,  3.60it/s]100%|| 805/805 [05:15<00:00,  2.55it/s]
[INFO|trainer.py:1894] 2023-08-28 03:26:31,762 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 03:26:31,933 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:26:35,095 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:26:35,225 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:26:35,308 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:26:35,842 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:35,871 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:35,871 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:35,871 >>   train_runtime            = 0:05:15.82
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:35,871 >>   train_samples            =      10320
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:35,871 >>   train_samples_per_second =    163.379
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:35,871 >>   train_steps_per_second   =      2.549
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7858, 'eval_samples_per_second': 355.821, 'eval_steps_per_second': 44.554, 'epoch': 5.0}
{'train_runtime': 315.8293, 'train_samples_per_second': 163.379, 'train_steps_per_second': 2.549, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:26:36 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:26:36,164 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:26:36,164 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 03:26:36,164 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.34it/s]  3%|         | 12/436 [00:00<00:08, 48.97it/s]  4%|         | 17/436 [00:00<00:08, 47.57it/s]  5%|         | 22/436 [00:00<00:08, 46.61it/s]  6%|         | 27/436 [00:00<00:08, 46.18it/s]  7%|         | 32/436 [00:00<00:08, 45.89it/s]  8%|         | 37/436 [00:00<00:08, 45.77it/s] 10%|         | 42/436 [00:00<00:08, 45.32it/s] 11%|         | 47/436 [00:01<00:08, 44.73it/s] 12%|        | 52/436 [00:01<00:08, 44.48it/s] 13%|        | 57/436 [00:01<00:08, 44.58it/s] 14%|        | 62/436 [00:01<00:08, 44.70it/s] 15%|        | 67/436 [00:01<00:08, 44.87it/s] 17%|        | 72/436 [00:01<00:08, 44.90it/s] 18%|        | 77/436 [00:01<00:07, 44.89it/s] 19%|        | 82/436 [00:01<00:07, 44.95it/s] 20%|        | 87/436 [00:01<00:07, 44.79it/s] 21%|        | 92/436 [00:02<00:07, 44.55it/s] 22%|       | 97/436 [00:02<00:07, 44.47it/s] 23%|       | 102/436 [00:02<00:07, 44.46it/s] 25%|       | 107/436 [00:02<00:07, 44.57it/s] 26%|       | 112/436 [00:02<00:07, 44.75it/s] 27%|       | 117/436 [00:02<00:07, 44.87it/s] 28%|       | 122/436 [00:02<00:06, 45.00it/s] 29%|       | 127/436 [00:02<00:07, 42.82it/s] 30%|       | 132/436 [00:02<00:07, 43.36it/s] 31%|      | 137/436 [00:03<00:06, 43.66it/s] 33%|      | 142/436 [00:03<00:06, 43.83it/s] 34%|      | 147/436 [00:03<00:06, 44.06it/s] 35%|      | 152/436 [00:03<00:06, 43.64it/s] 36%|      | 157/436 [00:03<00:06, 44.12it/s] 37%|      | 162/436 [00:03<00:06, 44.36it/s] 38%|      | 167/436 [00:03<00:06, 44.37it/s] 39%|      | 172/436 [00:03<00:05, 44.44it/s] 41%|      | 177/436 [00:03<00:05, 44.44it/s] 42%|     | 182/436 [00:04<00:05, 44.42it/s] 43%|     | 187/436 [00:04<00:05, 44.45it/s] 44%|     | 192/436 [00:04<00:05, 44.56it/s] 45%|     | 197/436 [00:04<00:05, 44.66it/s] 46%|     | 202/436 [00:04<00:05, 44.70it/s] 47%|     | 207/436 [00:04<00:05, 44.67it/s] 49%|     | 212/436 [00:04<00:05, 44.75it/s] 50%|     | 217/436 [00:04<00:04, 44.85it/s] 51%|     | 222/436 [00:04<00:04, 44.81it/s] 52%|    | 227/436 [00:05<00:04, 44.79it/s] 53%|    | 232/436 [00:05<00:04, 44.71it/s] 54%|    | 237/436 [00:05<00:04, 44.65it/s] 56%|    | 242/436 [00:05<00:04, 42.31it/s] 57%|    | 247/436 [00:05<00:04, 43.12it/s] 58%|    | 252/436 [00:05<00:04, 43.82it/s] 59%|    | 257/436 [00:05<00:04, 44.15it/s] 60%|    | 262/436 [00:05<00:04, 43.10it/s] 61%|    | 267/436 [00:05<00:03, 43.68it/s] 62%|   | 272/436 [00:06<00:03, 44.02it/s] 64%|   | 277/436 [00:06<00:03, 44.16it/s] 65%|   | 282/436 [00:06<00:03, 44.10it/s] 66%|   | 287/436 [00:06<00:03, 44.13it/s] 67%|   | 292/436 [00:06<00:03, 44.31it/s] 68%|   | 297/436 [00:06<00:03, 44.64it/s] 69%|   | 302/436 [00:06<00:03, 44.61it/s] 70%|   | 307/436 [00:06<00:02, 44.74it/s] 72%|  | 312/436 [00:06<00:02, 44.54it/s] 73%|  | 317/436 [00:07<00:02, 44.69it/s] 74%|  | 322/436 [00:07<00:02, 44.73it/s] 75%|  | 327/436 [00:07<00:02, 42.83it/s] 76%|  | 332/436 [00:07<00:02, 43.94it/s] 77%|  | 337/436 [00:07<00:02, 44.17it/s] 78%|  | 342/436 [00:07<00:02, 37.71it/s] 80%|  | 347/436 [00:07<00:02, 40.71it/s] 81%|  | 352/436 [00:08<00:02, 31.52it/s] 82%| | 357/436 [00:08<00:02, 35.05it/s] 83%| | 362/436 [00:08<00:01, 37.59it/s] 84%| | 367/436 [00:08<00:01, 39.60it/s] 85%| | 372/436 [00:08<00:01, 41.11it/s] 86%| | 377/436 [00:08<00:01, 42.25it/s] 88%| | 382/436 [00:08<00:01, 43.05it/s] 89%| | 387/436 [00:08<00:01, 43.68it/s] 90%| | 392/436 [00:08<00:01, 42.67it/s] 91%| | 397/436 [00:09<00:00, 42.99it/s] 92%|| 402/436 [00:09<00:00, 43.39it/s] 93%|| 407/436 [00:09<00:00, 43.81it/s] 94%|| 412/436 [00:09<00:00, 44.12it/s] 96%|| 417/436 [00:09<00:00, 44.51it/s] 97%|| 422/436 [00:09<00:00, 44.61it/s] 98%|| 427/436 [00:09<00:00, 44.78it/s] 99%|| 432/436 [00:09<00:00, 44.69it/s]100%|| 436/436 [00:09<00:00, 43.74it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:26:46,152 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   eval_loss               =     1.1192
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   eval_runtime            = 0:00:09.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   eval_samples_per_second =    348.638
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   eval_steps_per_second   =     43.655
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:46,152 >>   perplexity              =     3.0625
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:26:55,766 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:26:55,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:26:55,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:26:55,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:26:55,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:26:56,794 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:26:56,795 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:26:57,526 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:26:58,680 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:26:58,680 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:01,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:01,900 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:01,900 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:01,900 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:01,900 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:27:02,820 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:27:02,821 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:27:03,508 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:27:03,794 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:27:03,794 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-644
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-161
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-805
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-322
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/checkpoint-483
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.75it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.73it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:08,  1.67it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:13,  1.73it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:14,  1.71it/s]Extractor Predicting: 25it [00:14,  1.68it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:16,  1.60it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:17,  1.63it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:19,  1.60it/s]Extractor Predicting: 34it [00:20,  1.58it/s]Extractor Predicting: 35it [00:20,  1.63it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.62it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:24,  1.65it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:25,  1.64it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:27,  1.66it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:28,  1.62it/s]Extractor Predicting: 49it [00:29,  1.58it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:30,  1.47it/s]Extractor Predicting: 52it [00:31,  1.53it/s]Extractor Predicting: 53it [00:32,  1.55it/s]Extractor Predicting: 54it [00:32,  1.54it/s]Extractor Predicting: 55it [00:33,  1.57it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:34,  1.61it/s]Extractor Predicting: 58it [00:35,  1.60it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:39,  1.58it/s]Extractor Predicting: 65it [00:39,  1.63it/s]Extractor Predicting: 66it [00:40,  1.60it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.64it/s]Extractor Predicting: 69it [00:42,  1.65it/s]Extractor Predicting: 70it [00:42,  1.70it/s]Extractor Predicting: 71it [00:43,  1.70it/s]Extractor Predicting: 72it [00:43,  1.57it/s]Extractor Predicting: 73it [00:44,  1.66it/s]Extractor Predicting: 74it [00:45,  1.70it/s]Extractor Predicting: 75it [00:45,  1.67it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.64it/s]Extractor Predicting: 79it [00:48,  1.64it/s]Extractor Predicting: 80it [00:48,  1.64it/s]Extractor Predicting: 81it [00:49,  1.64it/s]Extractor Predicting: 82it [00:50,  1.59it/s]Extractor Predicting: 83it [00:50,  1.63it/s]Extractor Predicting: 84it [00:51,  1.60it/s]Extractor Predicting: 85it [00:51,  1.66it/s]Extractor Predicting: 86it [00:52,  1.62it/s]Extractor Predicting: 87it [00:53,  1.62it/s]Extractor Predicting: 88it [00:53,  1.60it/s]Extractor Predicting: 89it [00:54,  1.59it/s]Extractor Predicting: 90it [00:55,  1.58it/s]Extractor Predicting: 91it [00:55,  1.57it/s]Extractor Predicting: 92it [00:56,  1.56it/s]Extractor Predicting: 93it [00:56,  1.61it/s]Extractor Predicting: 94it [00:57,  1.58it/s]Extractor Predicting: 95it [00:58,  1.64it/s]Extractor Predicting: 96it [00:58,  1.65it/s]Extractor Predicting: 97it [00:59,  1.64it/s]Extractor Predicting: 98it [00:59,  1.62it/s]Extractor Predicting: 99it [01:00,  1.58it/s]Extractor Predicting: 100it [01:01,  1.56it/s]Extractor Predicting: 101it [01:01,  1.57it/s]Extractor Predicting: 102it [01:02,  1.56it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:03,  1.60it/s]Extractor Predicting: 105it [01:04,  1.61it/s]Extractor Predicting: 106it [01:05,  1.56it/s]Extractor Predicting: 107it [01:05,  1.58it/s]Extractor Predicting: 108it [01:06,  1.61it/s]Extractor Predicting: 109it [01:06,  1.66it/s]Extractor Predicting: 110it [01:07,  1.67it/s]Extractor Predicting: 111it [01:08,  1.69it/s]Extractor Predicting: 112it [01:08,  1.68it/s]Extractor Predicting: 113it [01:09,  1.67it/s]Extractor Predicting: 114it [01:09,  1.65it/s]Extractor Predicting: 115it [01:10,  1.63it/s]Extractor Predicting: 116it [01:11,  1.60it/s]Extractor Predicting: 117it [01:11,  1.60it/s]Extractor Predicting: 118it [01:12,  1.46it/s]Extractor Predicting: 119it [01:13,  1.51it/s]Extractor Predicting: 120it [01:13,  1.57it/s]Extractor Predicting: 121it [01:14,  1.56it/s]Extractor Predicting: 122it [01:15,  1.60it/s]Extractor Predicting: 123it [01:15,  1.58it/s]Extractor Predicting: 124it [01:16,  1.61it/s]Extractor Predicting: 125it [01:16,  1.62it/s]Extractor Predicting: 126it [01:17,  1.60it/s]Extractor Predicting: 127it [01:18,  1.61it/s]Extractor Predicting: 128it [01:18,  1.59it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.58it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.62it/s]Extractor Predicting: 133it [01:21,  1.58it/s]Extractor Predicting: 134it [01:22,  1.54it/s]Extractor Predicting: 135it [01:23,  1.56it/s]Extractor Predicting: 136it [01:23,  1.57it/s]Extractor Predicting: 137it [01:24,  1.59it/s]Extractor Predicting: 138it [01:25,  1.61it/s]Extractor Predicting: 139it [01:25,  1.59it/s]Extractor Predicting: 140it [01:26,  1.57it/s]Extractor Predicting: 141it [01:27,  1.55it/s]Extractor Predicting: 142it [01:27,  1.60it/s]Extractor Predicting: 143it [01:28,  1.66it/s]Extractor Predicting: 143it [01:28,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:44,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:44,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:44,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:44,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:44,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:28:44,940 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:28:44,941 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:28:45,494 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:28:46,567 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:28:46,568 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:49,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:49,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:49,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:49,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:49,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:28:50,549 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:28:50,550 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:28:51,141 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:28:51,293 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:28:51,294 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.78it/s]Extractor Predicting: 3it [00:01,  1.78it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.82it/s]Extractor Predicting: 6it [00:03,  1.74it/s]Extractor Predicting: 7it [00:03,  1.74it/s]Extractor Predicting: 8it [00:04,  1.77it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:06,  1.77it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:07,  1.75it/s]Extractor Predicting: 15it [00:08,  1.60it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:09,  1.71it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:10,  1.75it/s]Extractor Predicting: 20it [00:11,  1.73it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.68it/s]Extractor Predicting: 23it [00:13,  1.75it/s]Extractor Predicting: 24it [00:13,  1.75it/s]Extractor Predicting: 25it [00:14,  1.76it/s]Extractor Predicting: 26it [00:14,  1.83it/s]Extractor Predicting: 27it [00:15,  1.82it/s]Extractor Predicting: 28it [00:16,  1.83it/s]Extractor Predicting: 29it [00:16,  1.84it/s]Extractor Predicting: 30it [00:17,  1.75it/s]Extractor Predicting: 31it [00:17,  1.71it/s]Extractor Predicting: 32it [00:18,  1.68it/s]Extractor Predicting: 33it [00:19,  1.66it/s]Extractor Predicting: 34it [00:19,  1.65it/s]Extractor Predicting: 35it [00:20,  1.51it/s]Extractor Predicting: 36it [00:21,  1.55it/s]Extractor Predicting: 37it [00:21,  1.58it/s]Extractor Predicting: 38it [00:22,  1.58it/s]Extractor Predicting: 39it [00:22,  1.62it/s]Extractor Predicting: 40it [00:23,  1.59it/s]Extractor Predicting: 41it [00:24,  1.62it/s]Extractor Predicting: 42it [00:24,  1.65it/s]Extractor Predicting: 43it [00:25,  1.61it/s]Extractor Predicting: 44it [00:25,  1.64it/s]Extractor Predicting: 45it [00:26,  1.63it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:27,  1.64it/s]Extractor Predicting: 48it [00:28,  1.64it/s]Extractor Predicting: 49it [00:28,  1.63it/s]Extractor Predicting: 50it [00:29,  1.62it/s]Extractor Predicting: 51it [00:30,  1.63it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:31,  1.65it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:32,  1.58it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:33,  1.65it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:35,  1.63it/s]Extractor Predicting: 61it [00:36,  1.62it/s]Extractor Predicting: 62it [00:36,  1.65it/s]Extractor Predicting: 63it [00:37,  1.70it/s]Extractor Predicting: 64it [00:38,  1.72it/s]Extractor Predicting: 65it [00:38,  1.72it/s]Extractor Predicting: 66it [00:39,  1.62it/s]Extractor Predicting: 67it [00:39,  1.66it/s]Extractor Predicting: 68it [00:40,  1.65it/s]Extractor Predicting: 69it [00:41,  1.68it/s]Extractor Predicting: 70it [00:41,  1.68it/s]Extractor Predicting: 71it [00:42,  1.67it/s]Extractor Predicting: 72it [00:42,  1.66it/s]Extractor Predicting: 73it [00:43,  1.65it/s]Extractor Predicting: 74it [00:44,  1.68it/s]Extractor Predicting: 75it [00:44,  1.67it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:45,  1.71it/s]Extractor Predicting: 78it [00:46,  1.76it/s]Extractor Predicting: 79it [00:46,  1.75it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:48,  1.71it/s]Extractor Predicting: 83it [00:49,  1.69it/s]Extractor Predicting: 84it [00:49,  1.67it/s]Extractor Predicting: 85it [00:50,  1.66it/s]Extractor Predicting: 86it [00:51,  1.66it/s]Extractor Predicting: 87it [00:51,  1.69it/s]Extractor Predicting: 88it [00:52,  1.69it/s]Extractor Predicting: 89it [00:52,  1.75it/s]Extractor Predicting: 90it [00:53,  1.75it/s]Extractor Predicting: 91it [00:53,  1.74it/s]Extractor Predicting: 92it [00:54,  1.66it/s]Extractor Predicting: 93it [00:55,  1.67it/s]Extractor Predicting: 94it [00:55,  1.64it/s]Extractor Predicting: 95it [00:56,  1.64it/s]Extractor Predicting: 96it [00:57,  1.63it/s]Extractor Predicting: 97it [00:57,  1.63it/s]Extractor Predicting: 98it [00:58,  1.62it/s]Extractor Predicting: 99it [00:58,  1.64it/s]Extractor Predicting: 100it [00:59,  1.64it/s]Extractor Predicting: 101it [01:00,  1.63it/s]Extractor Predicting: 102it [01:00,  1.63it/s]Extractor Predicting: 103it [01:01,  1.65it/s]Extractor Predicting: 104it [01:01,  1.65it/s]Extractor Predicting: 105it [01:02,  1.65it/s]Extractor Predicting: 106it [01:03,  1.65it/s]Extractor Predicting: 107it [01:03,  1.61it/s]Extractor Predicting: 108it [01:04,  1.63it/s]Extractor Predicting: 109it [01:05,  1.64it/s]Extractor Predicting: 110it [01:05,  1.63it/s]Extractor Predicting: 111it [01:06,  1.63it/s]Extractor Predicting: 112it [01:07,  1.40it/s]Extractor Predicting: 113it [01:07,  1.48it/s]Extractor Predicting: 114it [01:08,  1.53it/s]Extractor Predicting: 115it [01:09,  1.55it/s]Extractor Predicting: 116it [01:09,  1.64it/s]Extractor Predicting: 117it [01:10,  1.56it/s]Extractor Predicting: 118it [01:10,  1.66it/s]Extractor Predicting: 119it [01:11,  1.70it/s]Extractor Predicting: 120it [01:11,  1.72it/s]Extractor Predicting: 121it [01:12,  1.71it/s]Extractor Predicting: 122it [01:13,  1.70it/s]Extractor Predicting: 123it [01:13,  1.70it/s]Extractor Predicting: 124it [01:14,  1.75it/s]Extractor Predicting: 125it [01:14,  1.82it/s]Extractor Predicting: 126it [01:15,  1.81it/s]Extractor Predicting: 127it [01:15,  1.81it/s]Extractor Predicting: 128it [01:16,  1.85it/s]Extractor Predicting: 129it [01:16,  1.83it/s]Extractor Predicting: 130it [01:17,  1.81it/s]Extractor Predicting: 131it [01:18,  1.82it/s]Extractor Predicting: 132it [01:18,  1.83it/s]Extractor Predicting: 133it [01:19,  1.81it/s]Extractor Predicting: 134it [01:19,  1.82it/s]Extractor Predicting: 135it [01:20,  1.85it/s]Extractor Predicting: 136it [01:20,  1.89it/s]Extractor Predicting: 137it [01:21,  1.90it/s]Extractor Predicting: 138it [01:21,  1.86it/s]Extractor Predicting: 139it [01:22,  1.79it/s]Extractor Predicting: 140it [01:23,  1.58it/s]Extractor Predicting: 141it [01:23,  1.61it/s]Extractor Predicting: 142it [01:24,  1.65it/s]Extractor Predicting: 143it [01:24,  1.72it/s]Extractor Predicting: 144it [01:25,  1.67it/s]Extractor Predicting: 145it [01:26,  1.70it/s]Extractor Predicting: 146it [01:26,  1.74it/s]Extractor Predicting: 147it [01:27,  1.76it/s]Extractor Predicting: 148it [01:27,  1.74it/s]Extractor Predicting: 149it [01:28,  1.81it/s]Extractor Predicting: 150it [01:28,  1.77it/s]Extractor Predicting: 151it [01:29,  1.75it/s]Extractor Predicting: 152it [01:29,  1.79it/s]Extractor Predicting: 153it [01:30,  1.84it/s]Extractor Predicting: 154it [01:30,  1.85it/s]Extractor Predicting: 155it [01:31,  1.86it/s]Extractor Predicting: 156it [01:32,  1.80it/s]Extractor Predicting: 157it [01:32,  1.80it/s]Extractor Predicting: 158it [01:33,  1.82it/s]Extractor Predicting: 159it [01:33,  1.77it/s]Extractor Predicting: 160it [01:34,  1.79it/s]Extractor Predicting: 161it [01:34,  1.79it/s]Extractor Predicting: 162it [01:35,  1.76it/s]Extractor Predicting: 163it [01:36,  1.77it/s]Extractor Predicting: 164it [01:36,  1.78it/s]Extractor Predicting: 165it [01:37,  1.81it/s]Extractor Predicting: 166it [01:37,  1.81it/s]Extractor Predicting: 167it [01:38,  1.81it/s]Extractor Predicting: 168it [01:38,  1.74it/s]Extractor Predicting: 169it [01:39,  1.76it/s]Extractor Predicting: 170it [01:40,  1.73it/s]Extractor Predicting: 171it [01:40,  1.72it/s]Extractor Predicting: 172it [01:41,  1.79it/s]Extractor Predicting: 173it [01:41,  1.72it/s]Extractor Predicting: 174it [01:42,  1.69it/s]Extractor Predicting: 175it [01:43,  1.67it/s]Extractor Predicting: 176it [01:43,  1.67it/s]Extractor Predicting: 177it [01:44,  1.66it/s]Extractor Predicting: 178it [01:44,  1.64it/s]Extractor Predicting: 179it [01:45,  1.65it/s]Extractor Predicting: 180it [01:45,  1.70it/s]Extractor Predicting: 181it [01:46,  1.66it/s]Extractor Predicting: 182it [01:47,  1.65it/s]Extractor Predicting: 183it [01:47,  1.65it/s]Extractor Predicting: 184it [01:48,  1.64it/s]Extractor Predicting: 185it [01:49,  1.63it/s]Extractor Predicting: 186it [01:49,  1.62it/s]Extractor Predicting: 187it [01:50,  1.61it/s]Extractor Predicting: 188it [01:50,  1.66it/s]Extractor Predicting: 189it [01:51,  1.67it/s]Extractor Predicting: 190it [01:52,  1.68it/s]Extractor Predicting: 191it [01:52,  1.64it/s]Extractor Predicting: 192it [01:53,  1.62it/s]Extractor Predicting: 193it [01:53,  1.60it/s]Extractor Predicting: 194it [01:54,  1.60it/s]Extractor Predicting: 195it [01:55,  1.60it/s]Extractor Predicting: 196it [01:55,  1.62it/s]Extractor Predicting: 197it [01:56,  1.59it/s]Extractor Predicting: 198it [01:57,  1.60it/s]Extractor Predicting: 199it [01:57,  1.61it/s]Extractor Predicting: 200it [01:58,  1.60it/s]Extractor Predicting: 201it [01:58,  1.62it/s]Extractor Predicting: 202it [01:59,  1.59it/s]Extractor Predicting: 203it [02:00,  1.62it/s]Extractor Predicting: 204it [02:00,  1.65it/s]Extractor Predicting: 205it [02:01,  1.71it/s]Extractor Predicting: 206it [02:01,  1.70it/s]Extractor Predicting: 207it [02:02,  1.69it/s]Extractor Predicting: 208it [02:03,  1.66it/s]Extractor Predicting: 209it [02:03,  1.66it/s]Extractor Predicting: 210it [02:04,  1.69it/s]Extractor Predicting: 211it [02:04,  1.74it/s]Extractor Predicting: 212it [02:05,  1.74it/s]Extractor Predicting: 213it [02:05,  1.77it/s]Extractor Predicting: 214it [02:06,  1.70it/s]Extractor Predicting: 215it [02:07,  1.67it/s]Extractor Predicting: 216it [02:07,  1.69it/s]Extractor Predicting: 217it [02:08,  1.70it/s]Extractor Predicting: 218it [02:08,  1.74it/s]Extractor Predicting: 219it [02:09,  1.77it/s]Extractor Predicting: 220it [02:10,  1.70it/s]Extractor Predicting: 221it [02:10,  1.74it/s]Extractor Predicting: 222it [02:11,  1.77it/s]Extractor Predicting: 223it [02:11,  1.75it/s]Extractor Predicting: 224it [02:12,  1.76it/s]Extractor Predicting: 225it [02:12,  1.75it/s]Extractor Predicting: 226it [02:13,  1.65it/s]Extractor Predicting: 227it [02:14,  1.66it/s]Extractor Predicting: 228it [02:14,  1.68it/s]Extractor Predicting: 229it [02:15,  1.66it/s]Extractor Predicting: 230it [02:16,  1.66it/s]Extractor Predicting: 231it [02:16,  1.68it/s]Extractor Predicting: 232it [02:17,  1.67it/s]Extractor Predicting: 233it [02:17,  1.75it/s]Extractor Predicting: 234it [02:18,  1.77it/s]Extractor Predicting: 235it [02:18,  1.79it/s]Extractor Predicting: 236it [02:19,  1.85it/s]Extractor Predicting: 237it [02:19,  1.86it/s]Extractor Predicting: 238it [02:20,  1.84it/s]Extractor Predicting: 239it [02:20,  1.83it/s]Extractor Predicting: 240it [02:21,  1.86it/s]Extractor Predicting: 241it [02:21,  1.86it/s]Extractor Predicting: 242it [02:22,  1.86it/s]Extractor Predicting: 243it [02:23,  1.92it/s]Extractor Predicting: 244it [02:23,  1.83it/s]Extractor Predicting: 245it [02:24,  1.91it/s]Extractor Predicting: 246it [02:24,  1.97it/s]Extractor Predicting: 247it [02:25,  1.91it/s]Extractor Predicting: 248it [02:25,  1.86it/s]Extractor Predicting: 249it [02:26,  1.88it/s]Extractor Predicting: 250it [02:26,  1.85it/s]Extractor Predicting: 251it [02:27,  1.89it/s]Extractor Predicting: 252it [02:27,  1.91it/s]Extractor Predicting: 253it [02:28,  1.90it/s]Extractor Predicting: 254it [02:28,  1.87it/s]Extractor Predicting: 255it [02:29,  1.90it/s]Extractor Predicting: 256it [02:29,  1.88it/s]Extractor Predicting: 257it [02:30,  1.91it/s]Extractor Predicting: 258it [02:30,  1.91it/s]Extractor Predicting: 259it [02:31,  1.94it/s]Extractor Predicting: 260it [02:31,  1.91it/s]Extractor Predicting: 261it [02:32,  1.81it/s]Extractor Predicting: 262it [02:33,  1.80it/s]Extractor Predicting: 263it [02:33,  1.74it/s]Extractor Predicting: 264it [02:34,  1.48it/s]Extractor Predicting: 265it [02:35,  1.53it/s]Extractor Predicting: 266it [02:35,  1.58it/s]Extractor Predicting: 267it [02:36,  1.62it/s]Extractor Predicting: 268it [02:37,  1.65it/s]Extractor Predicting: 269it [02:37,  1.63it/s]Extractor Predicting: 270it [02:38,  1.61it/s]Extractor Predicting: 271it [02:38,  1.61it/s]Extractor Predicting: 272it [02:39,  1.60it/s]Extractor Predicting: 273it [02:40,  1.61it/s]Extractor Predicting: 274it [02:40,  1.61it/s]Extractor Predicting: 275it [02:41,  1.63it/s]Extractor Predicting: 276it [02:42,  1.64it/s]Extractor Predicting: 277it [02:42,  1.60it/s]Extractor Predicting: 278it [02:43,  1.61it/s]Extractor Predicting: 279it [02:43,  1.65it/s]Extractor Predicting: 280it [02:44,  1.63it/s]Extractor Predicting: 281it [02:45,  1.62it/s]Extractor Predicting: 282it [02:45,  1.63it/s]Extractor Predicting: 283it [02:46,  1.64it/s]Extractor Predicting: 284it [02:46,  1.64it/s]Extractor Predicting: 285it [02:47,  1.64it/s]Extractor Predicting: 286it [02:48,  1.62it/s]Extractor Predicting: 287it [02:48,  1.61it/s]Extractor Predicting: 288it [02:49,  1.67it/s]Extractor Predicting: 289it [02:49,  1.67it/s]Extractor Predicting: 290it [02:50,  1.70it/s]Extractor Predicting: 291it [02:51,  1.72it/s]Extractor Predicting: 292it [02:51,  1.70it/s]Extractor Predicting: 293it [02:52,  1.70it/s]Extractor Predicting: 294it [02:52,  1.73it/s]Extractor Predicting: 295it [02:53,  1.74it/s]Extractor Predicting: 296it [02:53,  1.72it/s]Extractor Predicting: 297it [02:54,  1.73it/s]Extractor Predicting: 298it [02:55,  1.74it/s]Extractor Predicting: 299it [02:55,  1.71it/s]Extractor Predicting: 300it [02:56,  1.73it/s]Extractor Predicting: 301it [02:56,  1.72it/s]Extractor Predicting: 302it [02:57,  1.70it/s]Extractor Predicting: 303it [02:58,  1.70it/s]Extractor Predicting: 304it [02:58,  1.67it/s]Extractor Predicting: 305it [02:59,  1.67it/s]Extractor Predicting: 306it [02:59,  1.70it/s]Extractor Predicting: 307it [03:00,  1.71it/s]Extractor Predicting: 308it [03:01,  1.70it/s]Extractor Predicting: 309it [03:01,  1.66it/s]Extractor Predicting: 310it [03:02,  1.66it/s]Extractor Predicting: 311it [03:02,  1.67it/s]Extractor Predicting: 312it [03:03,  1.68it/s]Extractor Predicting: 313it [03:04,  1.68it/s]Extractor Predicting: 314it [03:04,  1.68it/s]Extractor Predicting: 315it [03:05,  1.71it/s]Extractor Predicting: 316it [03:05,  1.65it/s]Extractor Predicting: 317it [03:06,  1.69it/s]Extractor Predicting: 318it [03:06,  1.74it/s]Extractor Predicting: 319it [03:07,  1.73it/s]Extractor Predicting: 320it [03:08,  1.72it/s]Extractor Predicting: 321it [03:08,  1.71it/s]Extractor Predicting: 322it [03:09,  1.65it/s]Extractor Predicting: 323it [03:09,  1.69it/s]Extractor Predicting: 324it [03:10,  1.71it/s]Extractor Predicting: 325it [03:11,  1.70it/s]Extractor Predicting: 326it [03:11,  1.72it/s]Extractor Predicting: 327it [03:12,  1.73it/s]Extractor Predicting: 328it [03:12,  1.68it/s]Extractor Predicting: 329it [03:13,  1.70it/s]Extractor Predicting: 330it [03:14,  1.69it/s]Extractor Predicting: 331it [03:14,  1.69it/s]Extractor Predicting: 332it [03:15,  1.73it/s]Extractor Predicting: 333it [03:15,  1.69it/s]Extractor Predicting: 334it [03:16,  1.73it/s]Extractor Predicting: 335it [03:16,  1.72it/s]Extractor Predicting: 336it [03:17,  1.73it/s]Extractor Predicting: 337it [03:18,  1.72it/s]Extractor Predicting: 338it [03:18,  1.71it/s]Extractor Predicting: 339it [03:19,  1.72it/s]Extractor Predicting: 340it [03:19,  1.70it/s]Extractor Predicting: 341it [03:20,  1.73it/s]Extractor Predicting: 342it [03:20,  1.75it/s]Extractor Predicting: 343it [03:21,  1.74it/s]Extractor Predicting: 344it [03:22,  1.75it/s]Extractor Predicting: 345it [03:22,  1.72it/s]Extractor Predicting: 346it [03:23,  1.72it/s]Extractor Predicting: 347it [03:23,  1.73it/s]Extractor Predicting: 348it [03:24,  1.69it/s]Extractor Predicting: 349it [03:25,  1.71it/s]Extractor Predicting: 350it [03:25,  1.67it/s]Extractor Predicting: 351it [03:26,  1.70it/s]Extractor Predicting: 352it [03:26,  1.69it/s]Extractor Predicting: 353it [03:27,  1.69it/s]Extractor Predicting: 354it [03:28,  1.70it/s]Extractor Predicting: 355it [03:28,  1.68it/s]Extractor Predicting: 356it [03:29,  1.66it/s]Extractor Predicting: 357it [03:29,  1.65it/s]Extractor Predicting: 358it [03:30,  1.67it/s]Extractor Predicting: 359it [03:31,  1.69it/s]Extractor Predicting: 360it [03:31,  1.70it/s]Extractor Predicting: 361it [03:32,  1.70it/s]Extractor Predicting: 362it [03:32,  1.71it/s]Extractor Predicting: 363it [03:33,  1.71it/s]Extractor Predicting: 364it [03:34,  1.53it/s]Extractor Predicting: 365it [03:34,  1.57it/s]Extractor Predicting: 366it [03:35,  1.62it/s]Extractor Predicting: 367it [03:36,  1.57it/s]Extractor Predicting: 368it [03:36,  1.62it/s]Extractor Predicting: 369it [03:37,  1.61it/s]Extractor Predicting: 370it [03:37,  1.63it/s]Extractor Predicting: 371it [03:38,  1.65it/s]Extractor Predicting: 372it [03:39,  1.64it/s]Extractor Predicting: 373it [03:39,  1.65it/s]Extractor Predicting: 374it [03:40,  1.69it/s]Extractor Predicting: 375it [03:40,  1.69it/s]Extractor Predicting: 376it [03:41,  1.67it/s]Extractor Predicting: 377it [03:41,  1.74it/s]Extractor Predicting: 378it [03:42,  1.71it/s]Extractor Predicting: 379it [03:43,  1.75it/s]Extractor Predicting: 380it [03:43,  1.71it/s]Extractor Predicting: 381it [03:44,  1.72it/s]Extractor Predicting: 382it [03:44,  1.71it/s]Extractor Predicting: 383it [03:45,  1.69it/s]Extractor Predicting: 384it [03:46,  1.66it/s]Extractor Predicting: 385it [03:46,  1.68it/s]Extractor Predicting: 386it [03:47,  1.67it/s]Extractor Predicting: 387it [03:47,  1.66it/s]Extractor Predicting: 388it [03:48,  1.61it/s]Extractor Predicting: 389it [03:49,  1.63it/s]Extractor Predicting: 390it [03:49,  1.64it/s]Extractor Predicting: 391it [03:50,  1.67it/s]Extractor Predicting: 392it [03:50,  1.69it/s]Extractor Predicting: 393it [03:51,  1.68it/s]Extractor Predicting: 394it [03:52,  1.69it/s]Extractor Predicting: 395it [03:52,  1.70it/s]Extractor Predicting: 396it [03:53,  1.65it/s]Extractor Predicting: 397it [03:53,  1.67it/s]Extractor Predicting: 398it [03:54,  1.66it/s]Extractor Predicting: 399it [03:55,  1.70it/s]Extractor Predicting: 400it [03:55,  1.67it/s]Extractor Predicting: 401it [03:56,  1.65it/s]Extractor Predicting: 402it [03:56,  1.66it/s]Extractor Predicting: 403it [03:57,  1.65it/s]Extractor Predicting: 404it [03:58,  1.69it/s]Extractor Predicting: 405it [03:58,  1.69it/s]Extractor Predicting: 406it [03:59,  1.72it/s]Extractor Predicting: 407it [03:59,  1.70it/s]Extractor Predicting: 408it [04:00,  1.69it/s]Extractor Predicting: 409it [04:01,  1.71it/s]Extractor Predicting: 410it [04:01,  1.72it/s]Extractor Predicting: 411it [04:02,  1.71it/s]Extractor Predicting: 412it [04:02,  1.74it/s]Extractor Predicting: 413it [04:03,  1.73it/s]Extractor Predicting: 414it [04:03,  1.72it/s]Extractor Predicting: 415it [04:04,  1.69it/s]Extractor Predicting: 416it [04:05,  1.74it/s]Extractor Predicting: 417it [04:05,  1.73it/s]Extractor Predicting: 418it [04:06,  1.76it/s]Extractor Predicting: 419it [04:06,  1.73it/s]Extractor Predicting: 420it [04:07,  1.76it/s]Extractor Predicting: 421it [04:07,  1.72it/s]Extractor Predicting: 422it [04:08,  1.73it/s]Extractor Predicting: 423it [04:09,  1.74it/s]Extractor Predicting: 424it [04:09,  1.74it/s]Extractor Predicting: 425it [04:10,  1.67it/s]Extractor Predicting: 426it [04:10,  1.73it/s]Extractor Predicting: 427it [04:11,  1.72it/s]Extractor Predicting: 428it [04:12,  1.68it/s]Extractor Predicting: 429it [04:12,  1.96it/s]Extractor Predicting: 429it [04:12,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:17,306 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:17,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:17,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:17,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:17,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:33:18,225 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:33:18,226 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:33:18,832 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:33:19,974 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:33:19,974 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:22,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:22,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:22,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:22,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:22,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:33:23,682 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:33:23,683 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:33:24,288 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:33:24,515 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:33:24,515 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:02,  1.87it/s]Extractor Predicting: 5it [00:02,  1.72it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:33:29,058 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:33:29,059 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:33:29,092 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:33:29,093 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:33:29,116 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:33:41,096 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:33:41,122 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:33:41,309 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:33:41,310 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:33:41,376 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:41,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:41,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:41,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:41,422 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:41,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:41,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:33:41,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:42,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:43,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:43,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:44,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:44,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:46,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:46,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:48,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:48,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:50,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:51,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:51,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:52,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:53,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:53,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:55,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:56,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:57,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:57,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:15, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:58,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:58,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:59,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:00,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:00,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:02,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:03,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:03,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:04,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:07,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:08,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:08,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:09,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:10,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:10,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:33, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:12,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:14,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:15,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:15,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:16,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:18,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:19,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:20,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:20,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:21,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:22,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:23,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:23,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:24,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:25,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:26,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:27,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:27,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:28,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:30,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:31,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:31,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:50<04:56, 17.43s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:32,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:33,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:33,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:34,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:35,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:35,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:36,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:37,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:39,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:39,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:40,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:41,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:41,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:42,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:44,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:46,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:47,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:06<04:24, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:47,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:49,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:49,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:51,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:52,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:52,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:53,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:54,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:56,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:57,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:57,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:58,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:00,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:00,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:01,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:02,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:02,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:23<04:10, 16.73s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:04,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:05,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:06,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:07,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:09,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:11,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:12,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:12,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:13,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:15,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:18,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:20,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:39<03:52, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:21,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:21,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:22,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:23,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:24,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:24,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:25,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:26,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:26,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:27,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:28,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:28,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:30,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:30,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:31,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:32,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:33,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:33,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:35,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:36,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:36,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:37,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:38,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:38,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:57<03:42, 17.12s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:39,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:40,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:40,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:41,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:41,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:42,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:43,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:43,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:44,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:45,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:45,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:46,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:47,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:47,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:48,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:49,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:49,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:50,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:50,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:51,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:52,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:52,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:53,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:54,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:54,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:55,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:56,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:15<03:26, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:56,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:57,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:58,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:58,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:59,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:00,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:00,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:01,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:02,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:02,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:03,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:04,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:04,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:05,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:06,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:07,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:07,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:08,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:09,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:09,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:10,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:12,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:12,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:13,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:33<03:12, 17.47s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:14,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:15,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:16,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:16,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:17,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:19,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:20,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:21,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:22,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:22,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:23,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:24,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:24,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:25,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:26,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:26,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:28,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:28,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:29,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:30,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:30,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:49<02:52, 17.25s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:31,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:33,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:35,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:38,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:38,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:39,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:41,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:42,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:42,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:43,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:43,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:45,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:46,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:46,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:47,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:48,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:07<02:35, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:48,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:49,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:50,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:50,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:51,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:52,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:52,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:53,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:54,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:54,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:55,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:56,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:57,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:59,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:00,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:00,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:01,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:03,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:04,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:05,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:24<02:17, 17.17s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:05,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:06,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:08,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:09,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:11,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:13,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:15,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:16,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:17,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:19,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:20,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:20,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:21,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:40<01:58, 16.95s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:22,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:22,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:23,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:24,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:25,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:25,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:26,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:28,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:30,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:31,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:31,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:32,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:34,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:35,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:36,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:37,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:37,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:38,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:57<01:41, 16.97s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:39,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:39,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:40,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:41,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:42,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:42,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:44,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:45,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:45,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:46,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:48,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:48,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:49,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:50,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:51,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:52,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:52,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:53,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:54,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:54,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:55,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:14<01:24, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:55,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:56,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:57,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:57,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:58,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:59,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:59,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:00,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:01,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:01,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:02,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:03,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:03,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:04,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:05,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:06,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:07,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:07,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:08,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:09,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:10,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:10,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:11,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:12,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:13,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:13,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:32<01:09, 17.35s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:14,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:15,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:15,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:16,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:17,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:17,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:18,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:19,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:20,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:20,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:21,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:22,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:22,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:23,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:24,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:24,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:25,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:26,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:26,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:27,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:28,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:29,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:29,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:48<00:50, 16.98s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:30,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:31,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:31,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:32,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:33,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:33,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:34,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:34,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:35,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:36,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:37,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:37,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:38,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:39,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:40,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:40,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:41,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:41,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:42,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:43,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:43,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:44,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:45,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:46,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:05<00:33, 16.78s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:46,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:47,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:48,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:48,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:50,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:50,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:51,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:52,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:52,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:53,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:54,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:54,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:55,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:55,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:56,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:57,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:57,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:58,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:59,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:59,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:00,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:01,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:01,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:02,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:02,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:03,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:22<00:16, 16.95s/it][WARNING|generation_utils.py:914] 2023-08-28 03:39:04,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:04,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:05,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:06,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:06,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:07,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:08,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:09,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:09,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:10,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:11,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:11,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:12,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:13,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:13,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:14,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:15,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:15,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:16,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:17,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:18,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:18,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:19,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:38<00:00, 16.66s/it]Generating: 100%|| 20/20 [05:38<00:00, 16.92s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:29,317 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:29,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:29,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:29,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:29,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:39:30,011 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:39:30,012 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:39:30,614 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:39:31,759 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:39:31,759 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:34,757 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:34,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:34,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:34,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:34,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:39:35,548 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:39:35,549 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:39:36,192 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:39:36,410 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:39:36,410 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of Lpez 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of Lpez 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Dmontagne " , including Boulogne \'s " Montessemble des deux de Chteau des Gains " . Head Entity : Montessemble des deux de Chteau des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of Franois Renoul .\')', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.48it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:09,  1.57it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:10,  1.65it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:11,  1.65it/s]Extractor Estimating: 19it [00:11,  1.67it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:16,  1.67it/s]Extractor Estimating: 28it [00:17,  1.70it/s]Extractor Estimating: 29it [00:18,  1.64it/s]Extractor Estimating: 30it [00:18,  1.60it/s]Extractor Estimating: 31it [00:19,  1.61it/s]Extractor Estimating: 32it [00:20,  1.61it/s]Extractor Estimating: 33it [00:20,  1.56it/s]Extractor Estimating: 34it [00:21,  1.59it/s]Extractor Estimating: 35it [00:22,  1.54it/s]Extractor Estimating: 36it [00:22,  1.51it/s]Extractor Estimating: 37it [00:23,  1.49it/s]Extractor Estimating: 38it [00:23,  1.57it/s]Extractor Estimating: 39it [00:24,  1.53it/s]Extractor Estimating: 40it [00:25,  1.47it/s]Extractor Estimating: 41it [00:25,  1.53it/s]Extractor Estimating: 42it [00:26,  1.52it/s]Extractor Estimating: 43it [00:27,  1.53it/s]Extractor Estimating: 44it [00:27,  1.56it/s]Extractor Estimating: 45it [00:28,  1.53it/s]Extractor Estimating: 46it [00:29,  1.51it/s]Extractor Estimating: 47it [00:29,  1.57it/s]Extractor Estimating: 48it [00:30,  1.52it/s]Extractor Estimating: 49it [00:31,  1.57it/s]Extractor Estimating: 50it [00:31,  1.60it/s]Extractor Estimating: 51it [00:32,  1.57it/s]Extractor Estimating: 52it [00:33,  1.54it/s]Extractor Estimating: 53it [00:33,  1.48it/s]Extractor Estimating: 54it [00:34,  1.53it/s]Extractor Estimating: 55it [00:35,  1.53it/s]Extractor Estimating: 56it [00:35,  1.54it/s]Extractor Estimating: 57it [00:36,  1.49it/s]Extractor Estimating: 58it [00:37,  1.55it/s]Extractor Estimating: 59it [00:37,  1.56it/s]Extractor Estimating: 60it [00:38,  1.48it/s]Extractor Estimating: 61it [00:39,  1.49it/s]Extractor Estimating: 62it [00:39,  1.48it/s]Extractor Estimating: 63it [00:40,  1.53it/s]Extractor Estimating: 64it [00:41,  1.52it/s]Extractor Estimating: 65it [00:41,  1.46it/s]Extractor Estimating: 66it [00:42,  1.42it/s]Extractor Estimating: 67it [00:43,  1.44it/s]Extractor Estimating: 68it [00:43,  1.47it/s]Extractor Estimating: 69it [00:44,  1.45it/s]Extractor Estimating: 70it [00:45,  1.47it/s]Extractor Estimating: 71it [00:45,  1.51it/s]Extractor Estimating: 72it [00:46,  1.52it/s]Extractor Estimating: 73it [00:47,  1.52it/s]Extractor Estimating: 74it [00:47,  1.51it/s]Extractor Estimating: 75it [00:48,  1.52it/s]Extractor Estimating: 76it [00:49,  1.50it/s]Extractor Estimating: 77it [00:49,  1.40it/s]Extractor Estimating: 78it [00:50,  1.43it/s]Extractor Estimating: 79it [00:51,  1.48it/s]Extractor Estimating: 80it [00:51,  1.48it/s]Extractor Estimating: 81it [00:52,  1.47it/s]Extractor Estimating: 82it [00:53,  1.47it/s]Extractor Estimating: 83it [00:53,  1.52it/s]Extractor Estimating: 84it [00:54,  1.48it/s]Extractor Estimating: 85it [00:55,  1.48it/s]Extractor Estimating: 86it [00:55,  1.50it/s]Extractor Estimating: 87it [00:56,  1.52it/s]Extractor Estimating: 88it [00:57,  1.45it/s]Extractor Estimating: 89it [00:58,  1.48it/s]Extractor Estimating: 90it [00:58,  1.51it/s]Extractor Estimating: 91it [00:59,  1.55it/s]Extractor Estimating: 92it [00:59,  1.51it/s]Extractor Estimating: 93it [01:00,  1.46it/s]Extractor Estimating: 94it [01:01,  1.50it/s]Extractor Estimating: 95it [01:01,  1.51it/s]Extractor Estimating: 96it [01:02,  1.51it/s]Extractor Estimating: 97it [01:03,  1.44it/s]Extractor Estimating: 98it [01:04,  1.47it/s]Extractor Estimating: 99it [01:04,  1.37it/s]Extractor Estimating: 100it [01:05,  1.39it/s]Extractor Estimating: 101it [01:06,  1.40it/s]Extractor Estimating: 102it [01:06,  1.45it/s]Extractor Estimating: 103it [01:07,  1.54it/s]Extractor Estimating: 104it [01:08,  1.53it/s]Extractor Estimating: 105it [01:08,  1.56it/s]Extractor Estimating: 106it [01:09,  1.56it/s]Extractor Estimating: 107it [01:10,  1.58it/s]Extractor Estimating: 108it [01:10,  1.60it/s]Extractor Estimating: 109it [01:11,  1.62it/s]Extractor Estimating: 110it [01:11,  1.69it/s]Extractor Estimating: 111it [01:12,  1.58it/s]Extractor Estimating: 112it [01:13,  1.56it/s]Extractor Estimating: 113it [01:13,  1.60it/s]Extractor Estimating: 114it [01:14,  1.62it/s]Extractor Estimating: 115it [01:15,  1.54it/s]Extractor Estimating: 116it [01:15,  1.53it/s]Extractor Estimating: 117it [01:16,  1.48it/s]Extractor Estimating: 118it [01:17,  1.50it/s]Extractor Estimating: 119it [01:17,  1.53it/s]Extractor Estimating: 120it [01:18,  1.53it/s]Extractor Estimating: 121it [01:18,  1.59it/s]Extractor Estimating: 122it [01:19,  1.59it/s]Extractor Estimating: 123it [01:20,  1.59it/s]Extractor Estimating: 124it [01:20,  1.64it/s]Extractor Estimating: 125it [01:21,  1.61it/s]Extractor Estimating: 126it [01:21,  1.64it/s]Extractor Estimating: 127it [01:22,  1.52it/s]Extractor Estimating: 128it [01:23,  1.54it/s]Extractor Estimating: 129it [01:24,  1.52it/s]Extractor Estimating: 130it [01:24,  1.49it/s]Extractor Estimating: 131it [01:25,  1.52it/s]Extractor Estimating: 132it [01:26,  1.45it/s]Extractor Estimating: 133it [01:26,  1.48it/s]Extractor Estimating: 134it [01:27,  1.48it/s]Extractor Estimating: 135it [01:28,  1.47it/s]Extractor Estimating: 136it [01:28,  1.52it/s]Extractor Estimating: 137it [01:29,  1.52it/s]Extractor Estimating: 138it [01:30,  1.53it/s]Extractor Estimating: 139it [01:30,  1.58it/s]Extractor Estimating: 140it [01:31,  1.60it/s]Extractor Estimating: 141it [01:31,  1.54it/s]Extractor Estimating: 142it [01:32,  1.50it/s]Extractor Estimating: 143it [01:33,  1.50it/s]Extractor Estimating: 144it [01:34,  1.45it/s]Extractor Estimating: 145it [01:34,  1.49it/s]Extractor Estimating: 146it [01:35,  1.46it/s]Extractor Estimating: 147it [01:36,  1.46it/s]Extractor Estimating: 148it [01:36,  1.43it/s]Extractor Estimating: 149it [01:37,  1.48it/s]Extractor Estimating: 150it [01:38,  1.49it/s]Extractor Estimating: 151it [01:38,  1.52it/s]Extractor Estimating: 152it [01:39,  1.55it/s]Extractor Estimating: 153it [01:39,  1.59it/s]Extractor Estimating: 154it [01:40,  1.69it/s]Extractor Estimating: 155it [01:41,  1.70it/s]Extractor Estimating: 156it [01:41,  1.66it/s]Extractor Estimating: 157it [01:42,  1.67it/s]Extractor Estimating: 158it [01:42,  1.61it/s]Extractor Estimating: 159it [01:43,  1.60it/s]Extractor Estimating: 160it [01:44,  1.66it/s]Extractor Estimating: 161it [01:44,  1.70it/s]Extractor Estimating: 162it [01:45,  1.68it/s]Extractor Estimating: 163it [01:45,  1.71it/s]Extractor Estimating: 164it [01:46,  1.72it/s]Extractor Estimating: 165it [01:47,  1.68it/s]Extractor Estimating: 166it [01:47,  1.67it/s]Extractor Estimating: 167it [01:48,  1.64it/s]Extractor Estimating: 168it [01:48,  1.69it/s]Extractor Estimating: 169it [01:49,  1.74it/s]Extractor Estimating: 170it [01:50,  1.69it/s]Extractor Estimating: 171it [01:50,  1.45it/s]Extractor Estimating: 172it [01:51,  1.49it/s]Extractor Estimating: 173it [01:52,  1.55it/s]Extractor Estimating: 174it [01:52,  1.60it/s]Extractor Estimating: 175it [01:53,  1.62it/s]Extractor Estimating: 176it [01:53,  1.60it/s]Extractor Estimating: 177it [01:54,  1.65it/s]Extractor Estimating: 178it [01:55,  1.68it/s]Extractor Estimating: 179it [01:55,  1.64it/s]Extractor Estimating: 180it [01:56,  1.47it/s]Extractor Estimating: 181it [01:57,  1.50it/s]Extractor Estimating: 182it [01:57,  1.59it/s]Extractor Estimating: 183it [01:58,  1.59it/s]Extractor Estimating: 184it [01:59,  1.51it/s]Extractor Estimating: 185it [01:59,  1.58it/s]Extractor Estimating: 186it [02:00,  1.55it/s]Extractor Estimating: 187it [02:01,  1.52it/s]Extractor Estimating: 188it [02:01,  1.55it/s]Extractor Estimating: 189it [02:02,  1.54it/s]Extractor Estimating: 190it [02:02,  1.57it/s]Extractor Estimating: 191it [02:03,  1.57it/s]Extractor Estimating: 192it [02:04,  1.61it/s]Extractor Estimating: 193it [02:04,  1.64it/s]Extractor Estimating: 194it [02:05,  1.65it/s]Extractor Estimating: 195it [02:05,  1.61it/s]Extractor Estimating: 196it [02:06,  1.61it/s]Extractor Estimating: 197it [02:07,  1.66it/s]Extractor Estimating: 198it [02:07,  1.63it/s]Extractor Estimating: 199it [02:08,  1.65it/s]Extractor Estimating: 200it [02:09,  1.62it/s]Extractor Estimating: 201it [02:09,  1.58it/s]Extractor Estimating: 202it [02:10,  1.45it/s]Extractor Estimating: 203it [02:11,  1.51it/s]Extractor Estimating: 204it [02:11,  1.51it/s]Extractor Estimating: 205it [02:12,  1.51it/s]Extractor Estimating: 206it [02:13,  1.50it/s]Extractor Estimating: 207it [02:13,  1.39it/s]Extractor Estimating: 208it [02:14,  1.41it/s]Extractor Estimating: 209it [02:15,  1.45it/s]Extractor Estimating: 210it [02:15,  1.47it/s]Extractor Estimating: 211it [02:16,  1.46it/s]Extractor Estimating: 212it [02:17,  1.40it/s]Extractor Estimating: 213it [02:18,  1.43it/s]Extractor Estimating: 214it [02:18,  1.46it/s]Extractor Estimating: 215it [02:19,  1.44it/s]Extractor Estimating: 216it [02:20,  1.42it/s]Extractor Estimating: 217it [02:20,  1.43it/s]Extractor Estimating: 218it [02:21,  1.42it/s]Extractor Estimating: 219it [02:22,  1.41it/s]Extractor Estimating: 220it [02:22,  1.46it/s]Extractor Estimating: 221it [02:23,  1.51it/s]Extractor Estimating: 222it [02:24,  1.44it/s]Extractor Estimating: 223it [02:24,  1.49it/s]Extractor Estimating: 224it [02:25,  1.49it/s]Extractor Estimating: 225it [02:26,  1.47it/s]Extractor Estimating: 226it [02:27,  1.47it/s]Extractor Estimating: 227it [02:27,  1.47it/s]Extractor Estimating: 228it [02:28,  1.49it/s]Extractor Estimating: 229it [02:28,  1.53it/s]Extractor Estimating: 230it [02:29,  1.50it/s]Extractor Estimating: 231it [02:30,  1.51it/s]Extractor Estimating: 232it [02:30,  1.51it/s]Extractor Estimating: 233it [02:31,  1.49it/s]Extractor Estimating: 234it [02:32,  1.55it/s]Extractor Estimating: 235it [02:32,  1.56it/s]Extractor Estimating: 236it [02:33,  1.56it/s]Extractor Estimating: 237it [02:34,  1.55it/s]Extractor Estimating: 238it [02:34,  1.52it/s]Extractor Estimating: 239it [02:35,  1.53it/s]Extractor Estimating: 240it [02:36,  1.56it/s]Extractor Estimating: 241it [02:36,  1.53it/s]Extractor Estimating: 242it [02:37,  1.55it/s]Extractor Estimating: 243it [02:38,  1.53it/s]Extractor Estimating: 244it [02:38,  1.50it/s]Extractor Estimating: 245it [02:39,  1.51it/s]Extractor Estimating: 246it [02:40,  1.50it/s]Extractor Estimating: 247it [02:40,  1.55it/s]Extractor Estimating: 248it [02:41,  1.44it/s]Extractor Estimating: 249it [02:42,  1.44it/s]Extractor Estimating: 250it [02:42,  1.49it/s]Extractor Estimating: 251it [02:43,  1.50it/s]Extractor Estimating: 252it [02:44,  1.48it/s]Extractor Estimating: 253it [02:44,  1.47it/s]Extractor Estimating: 254it [02:45,  1.52it/s]Extractor Estimating: 255it [02:46,  1.50it/s]Extractor Estimating: 256it [02:46,  1.49it/s]Extractor Estimating: 257it [02:47,  1.51it/s]Extractor Estimating: 258it [02:48,  1.54it/s]Extractor Estimating: 259it [02:48,  1.41it/s]Extractor Estimating: 260it [02:49,  1.42it/s]Extractor Estimating: 261it [02:50,  1.43it/s]Extractor Estimating: 262it [02:50,  1.45it/s]Extractor Estimating: 263it [02:51,  1.48it/s]Extractor Estimating: 264it [02:52,  1.49it/s]Extractor Estimating: 265it [02:52,  1.49it/s]Extractor Estimating: 266it [02:53,  1.47it/s]Extractor Estimating: 267it [02:54,  1.53it/s]Extractor Estimating: 268it [02:54,  1.58it/s]Extractor Estimating: 269it [02:55,  1.63it/s]Extractor Estimating: 270it [02:56,  1.58it/s]Extractor Estimating: 271it [02:56,  1.59it/s]Extractor Estimating: 272it [02:57,  1.55it/s]Extractor Estimating: 273it [02:57,  1.59it/s]Extractor Estimating: 274it [02:58,  1.53it/s]Extractor Estimating: 275it [02:59,  1.51it/s]Extractor Estimating: 276it [03:00,  1.47it/s]Extractor Estimating: 277it [03:00,  1.52it/s]Extractor Estimating: 278it [03:01,  1.56it/s]Extractor Estimating: 279it [03:01,  1.55it/s]Extractor Estimating: 280it [03:02,  1.51it/s]Extractor Estimating: 281it [03:03,  1.55it/s]Extractor Estimating: 282it [03:03,  1.54it/s]Extractor Estimating: 283it [03:04,  1.57it/s]Extractor Estimating: 284it [03:05,  1.53it/s]Extractor Estimating: 285it [03:05,  1.51it/s]Extractor Estimating: 286it [03:06,  1.53it/s]Extractor Estimating: 287it [03:07,  1.51it/s]Extractor Estimating: 288it [03:07,  1.50it/s]Extractor Estimating: 289it [03:08,  1.49it/s]Extractor Estimating: 290it [03:09,  1.48it/s]Extractor Estimating: 291it [03:09,  1.47it/s]Extractor Estimating: 292it [03:10,  1.44it/s]Extractor Estimating: 293it [03:11,  1.48it/s]Extractor Estimating: 294it [03:12,  1.47it/s]Extractor Estimating: 295it [03:12,  1.49it/s]Extractor Estimating: 296it [03:13,  1.43it/s]Extractor Estimating: 297it [03:14,  1.39it/s]Extractor Estimating: 298it [03:14,  1.40it/s]Extractor Estimating: 299it [03:15,  1.39it/s]Extractor Estimating: 300it [03:16,  1.47it/s]Extractor Estimating: 301it [03:16,  1.52it/s]Extractor Estimating: 302it [03:17,  1.58it/s]Extractor Estimating: 303it [03:18,  1.58it/s]Extractor Estimating: 304it [03:18,  1.62it/s]Extractor Estimating: 305it [03:19,  1.59it/s]Extractor Estimating: 306it [03:19,  1.60it/s]Extractor Estimating: 307it [03:20,  1.59it/s]Extractor Estimating: 308it [03:21,  1.60it/s]Extractor Estimating: 309it [03:21,  1.66it/s]Extractor Estimating: 310it [03:22,  1.65it/s]Extractor Estimating: 311it [03:22,  1.65it/s]Extractor Estimating: 312it [03:23,  1.63it/s]Extractor Estimating: 313it [03:24,  1.64it/s]Extractor Estimating: 314it [03:24,  1.61it/s]Extractor Estimating: 315it [03:25,  1.62it/s]Extractor Estimating: 316it [03:25,  1.67it/s]Extractor Estimating: 317it [03:26,  1.67it/s]Extractor Estimating: 318it [03:27,  1.65it/s]Extractor Estimating: 319it [03:27,  1.62it/s]Extractor Estimating: 320it [03:28,  1.58it/s]Extractor Estimating: 321it [03:29,  1.59it/s]Extractor Estimating: 322it [03:29,  1.62it/s]Extractor Estimating: 323it [03:30,  1.66it/s]Extractor Estimating: 324it [03:30,  1.67it/s]Extractor Estimating: 325it [03:31,  1.65it/s]Extractor Estimating: 326it [03:32,  1.64it/s]Extractor Estimating: 327it [03:32,  1.60it/s]Extractor Estimating: 328it [03:33,  1.54it/s]Extractor Estimating: 329it [03:34,  1.51it/s]Extractor Estimating: 330it [03:34,  1.53it/s]Extractor Estimating: 331it [03:35,  1.61it/s]Extractor Estimating: 332it [03:35,  1.68it/s]Extractor Estimating: 333it [03:36,  1.63it/s]Extractor Estimating: 334it [03:37,  1.59it/s]Extractor Estimating: 335it [03:37,  1.56it/s]Extractor Estimating: 336it [03:38,  1.57it/s]Extractor Estimating: 337it [03:39,  1.55it/s]Extractor Estimating: 338it [03:39,  1.53it/s]Extractor Estimating: 339it [03:40,  1.60it/s]Extractor Estimating: 340it [03:41,  1.54it/s]Extractor Estimating: 341it [03:41,  1.55it/s]Extractor Estimating: 342it [03:42,  1.46it/s]Extractor Estimating: 343it [03:43,  1.51it/s]Extractor Estimating: 344it [03:43,  1.53it/s]Extractor Estimating: 345it [03:44,  1.51it/s]Extractor Estimating: 346it [03:45,  1.54it/s]Extractor Estimating: 347it [03:45,  1.62it/s]Extractor Estimating: 348it [03:46,  1.62it/s]Extractor Estimating: 349it [03:46,  1.57it/s]Extractor Estimating: 350it [03:47,  1.61it/s]Extractor Estimating: 351it [03:48,  1.60it/s]Extractor Estimating: 352it [03:48,  1.57it/s]Extractor Estimating: 353it [03:49,  1.55it/s]Extractor Estimating: 354it [03:50,  1.57it/s]Extractor Estimating: 355it [03:50,  1.60it/s]Extractor Estimating: 356it [03:51,  1.62it/s]Extractor Estimating: 357it [03:51,  1.61it/s]Extractor Estimating: 358it [03:52,  1.61it/s]Extractor Estimating: 359it [03:53,  1.60it/s]Extractor Estimating: 360it [03:53,  1.55it/s]Extractor Estimating: 361it [03:54,  1.52it/s]Extractor Estimating: 362it [03:55,  1.55it/s]Extractor Estimating: 363it [03:55,  1.57it/s]Extractor Estimating: 364it [03:56,  1.55it/s]Extractor Estimating: 365it [03:57,  1.54it/s]Extractor Estimating: 366it [03:57,  1.57it/s]Extractor Estimating: 367it [03:58,  1.51it/s]Extractor Estimating: 368it [03:59,  1.52it/s]Extractor Estimating: 369it [03:59,  1.35it/s]Extractor Estimating: 370it [04:00,  1.41it/s]Extractor Estimating: 371it [04:01,  1.45it/s]Extractor Estimating: 372it [04:01,  1.50it/s]Extractor Estimating: 373it [04:02,  1.53it/s]Extractor Estimating: 374it [04:03,  1.56it/s]Extractor Estimating: 375it [04:03,  1.56it/s]Extractor Estimating: 376it [04:04,  1.56it/s]Extractor Estimating: 377it [04:05,  1.55it/s]Extractor Estimating: 378it [04:05,  1.54it/s]Extractor Estimating: 379it [04:06,  1.55it/s]Extractor Estimating: 380it [04:06,  1.58it/s]Extractor Estimating: 381it [04:07,  1.54it/s]Extractor Estimating: 382it [04:08,  1.57it/s]Extractor Estimating: 383it [04:08,  1.57it/s]Extractor Estimating: 384it [04:09,  1.52it/s]Extractor Estimating: 385it [04:10,  1.50it/s]Extractor Estimating: 386it [04:10,  1.51it/s]Extractor Estimating: 387it [04:11,  1.50it/s]Extractor Estimating: 388it [04:12,  1.53it/s]Extractor Estimating: 389it [04:12,  1.48it/s]Extractor Estimating: 390it [04:13,  1.48it/s]Extractor Estimating: 391it [04:14,  1.48it/s]Extractor Estimating: 392it [04:14,  1.48it/s]Extractor Estimating: 393it [04:15,  1.46it/s]Extractor Estimating: 394it [04:16,  1.49it/s]Extractor Estimating: 395it [04:16,  1.52it/s]Extractor Estimating: 396it [04:17,  1.45it/s]Extractor Estimating: 397it [04:18,  1.48it/s]Extractor Estimating: 398it [04:19,  1.46it/s]Extractor Estimating: 399it [04:19,  1.48it/s]Extractor Estimating: 400it [04:20,  1.52it/s]Extractor Estimating: 401it [04:20,  1.52it/s]Extractor Estimating: 402it [04:21,  1.50it/s]Extractor Estimating: 403it [04:22,  1.57it/s]Extractor Estimating: 404it [04:22,  1.55it/s]Extractor Estimating: 405it [04:23,  1.59it/s]Extractor Estimating: 406it [04:24,  1.58it/s]Extractor Estimating: 407it [04:24,  1.56it/s]Extractor Estimating: 408it [04:25,  1.60it/s]Extractor Estimating: 409it [04:26,  1.60it/s]Extractor Estimating: 410it [04:26,  1.60it/s]Extractor Estimating: 411it [04:27,  1.57it/s]Extractor Estimating: 412it [04:27,  1.60it/s]Extractor Estimating: 413it [04:28,  1.59it/s]Extractor Estimating: 414it [04:29,  1.56it/s]Extractor Estimating: 415it [04:29,  1.55it/s]Extractor Estimating: 416it [04:30,  1.50it/s]Extractor Estimating: 417it [04:31,  1.57it/s]Extractor Estimating: 418it [04:31,  1.53it/s]Extractor Estimating: 419it [04:32,  1.56it/s]Extractor Estimating: 420it [04:33,  1.55it/s]Extractor Estimating: 421it [04:33,  1.53it/s]Extractor Estimating: 422it [04:34,  1.49it/s]Extractor Estimating: 423it [04:35,  1.48it/s]Extractor Estimating: 424it [04:35,  1.53it/s]Extractor Estimating: 425it [04:36,  1.52it/s]Extractor Estimating: 426it [04:37,  1.53it/s]Extractor Estimating: 427it [04:37,  1.56it/s]Extractor Estimating: 428it [04:38,  1.56it/s]Extractor Estimating: 429it [04:38,  1.60it/s]Extractor Estimating: 430it [04:39,  1.55it/s]Extractor Estimating: 431it [04:40,  1.49it/s]Extractor Estimating: 432it [04:41,  1.51it/s]Extractor Estimating: 433it [04:41,  1.47it/s]Extractor Estimating: 434it [04:42,  1.45it/s]Extractor Estimating: 435it [04:43,  1.46it/s]Extractor Estimating: 436it [04:43,  1.48it/s]Extractor Estimating: 437it [04:44,  1.37it/s]Extractor Estimating: 438it [04:45,  1.41it/s]Extractor Estimating: 439it [04:46,  1.41it/s]Extractor Estimating: 440it [04:46,  1.43it/s]Extractor Estimating: 441it [04:47,  1.40it/s]Extractor Estimating: 442it [04:48,  1.44it/s]Extractor Estimating: 443it [04:48,  1.46it/s]Extractor Estimating: 444it [04:49,  1.46it/s]Extractor Estimating: 445it [04:50,  1.44it/s]Extractor Estimating: 446it [04:50,  1.44it/s]Extractor Estimating: 447it [04:51,  1.42it/s]Extractor Estimating: 448it [04:52,  1.41it/s]Extractor Estimating: 449it [04:53,  1.39it/s]Extractor Estimating: 450it [04:53,  1.46it/s]Extractor Estimating: 451it [04:54,  1.55it/s]Extractor Estimating: 452it [04:54,  1.53it/s]Extractor Estimating: 453it [04:55,  1.50it/s]Extractor Estimating: 454it [04:56,  1.54it/s]Extractor Estimating: 455it [04:56,  1.57it/s]Extractor Estimating: 456it [04:57,  1.60it/s]Extractor Estimating: 457it [04:58,  1.56it/s]Extractor Estimating: 458it [04:58,  1.54it/s]Extractor Estimating: 459it [04:59,  1.59it/s]Extractor Estimating: 460it [04:59,  1.63it/s]Extractor Estimating: 461it [05:00,  1.60it/s]Extractor Estimating: 462it [05:01,  1.56it/s]Extractor Estimating: 463it [05:01,  1.58it/s]Extractor Estimating: 464it [05:02,  1.59it/s]Extractor Estimating: 465it [05:03,  1.54it/s]Extractor Estimating: 466it [05:03,  1.62it/s]Extractor Estimating: 467it [05:04,  1.58it/s]Extractor Estimating: 468it [05:04,  1.60it/s]Extractor Estimating: 469it [05:05,  1.62it/s]Extractor Estimating: 470it [05:06,  1.62it/s]Extractor Estimating: 471it [05:06,  1.62it/s]Extractor Estimating: 472it [05:07,  1.52it/s]Extractor Estimating: 473it [05:08,  1.53it/s]Extractor Estimating: 474it [05:08,  1.56it/s]Extractor Estimating: 475it [05:09,  1.60it/s]Extractor Estimating: 476it [05:10,  1.56it/s]Extractor Estimating: 477it [05:10,  1.52it/s]Extractor Estimating: 478it [05:11,  1.55it/s]Extractor Estimating: 479it [05:12,  1.53it/s]Extractor Estimating: 480it [05:12,  1.52it/s]Extractor Estimating: 481it [05:13,  1.51it/s]Extractor Estimating: 482it [05:14,  1.50it/s]Extractor Estimating: 483it [05:14,  1.52it/s]Extractor Estimating: 484it [05:15,  1.56it/s]Extractor Estimating: 485it [05:15,  1.54it/s]Extractor Estimating: 486it [05:16,  1.56it/s]Extractor Estimating: 487it [05:17,  1.51it/s]Extractor Estimating: 488it [05:17,  1.50it/s]Extractor Estimating: 489it [05:18,  1.52it/s]Extractor Estimating: 490it [05:19,  1.56it/s]Extractor Estimating: 491it [05:19,  1.57it/s]Extractor Estimating: 492it [05:20,  1.55it/s]Extractor Estimating: 493it [05:21,  1.56it/s]Extractor Estimating: 494it [05:21,  1.55it/s]Extractor Estimating: 495it [05:22,  1.51it/s]Extractor Estimating: 496it [05:23,  1.52it/s]Extractor Estimating: 497it [05:23,  1.43it/s]Extractor Estimating: 498it [05:24,  1.47it/s]Extractor Estimating: 499it [05:25,  1.49it/s]Extractor Estimating: 500it [05:25,  1.57it/s]Extractor Estimating: 500it [05:25,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:20,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:20,545 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:20,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:20,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:20,546 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:45:21,513 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:45:21,514 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:45:22,437 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:45:23,633 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:45:23,680 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:28,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:45:29,519 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:45:29,521 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:45:30,438 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:45:30,701 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:45:30,701 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:49:14,899 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:49:14,920 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 10485 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 29547
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29647, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29647, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.045, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.047, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.038, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.058, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 63, avg_time 1.059, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 163, avg_time 2.130, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 263, avg_time 1.043, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 363, avg_time 1.053, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 26, avg_time 1.045, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 126, avg_time 1.049, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 226, avg_time 2.143, loss:nan
g_step 1200, step 326, avg_time 1.050, loss:nan
g_step 1300, step 426, avg_time 1.046, loss:nan
g_step 1400, step 89, avg_time 1.058, loss:nan
g_step 1500, step 189, avg_time 1.058, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 289, avg_time 2.138, loss:nan
g_step 1700, step 389, avg_time 1.036, loss:nan
g_step 1800, step 52, avg_time 1.052, loss:nan
g_step 1900, step 152, avg_time 1.042, loss:nan
g_step 2000, step 252, avg_time 1.050, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 352, avg_time 2.136, loss:nan
g_step 2200, step 15, avg_time 1.048, loss:nan
g_step 2300, step 115, avg_time 1.048, loss:nan
g_step 2400, step 215, avg_time 1.053, loss:nan
g_step 2500, step 315, avg_time 1.032, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 415, avg_time 2.146, loss:nan
g_step 2700, step 78, avg_time 1.071, loss:nan
g_step 2800, step 178, avg_time 1.044, loss:nan
g_step 2900, step 278, avg_time 1.031, loss:nan
g_step 3000, step 378, avg_time 1.047, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 41, avg_time 2.147, loss:nan
g_step 3200, step 141, avg_time 1.033, loss:nan
g_step 3300, step 241, avg_time 1.056, loss:nan
g_step 3400, step 341, avg_time 1.055, loss:nan
g_step 3500, step 4, avg_time 1.044, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 104, avg_time 2.140, loss:nan
g_step 3700, step 204, avg_time 1.040, loss:nan
g_step 3800, step 304, avg_time 1.057, loss:nan
g_step 3900, step 404, avg_time 1.054, loss:nan
g_step 4000, step 67, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 167, avg_time 2.139, loss:nan
g_step 4200, step 267, avg_time 1.042, loss:nan
g_step 4300, step 367, avg_time 1.069, loss:nan
g_step 4400, step 30, avg_time 1.044, loss:nan
g_step 4500, step 130, avg_time 1.045, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 230, avg_time 2.142, loss:nan
g_step 4700, step 330, avg_time 1.047, loss:nan
g_step 4800, step 430, avg_time 1.039, loss:nan
g_step 4900, step 93, avg_time 1.062, loss:nan
g_step 5000, step 193, avg_time 1.043, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 293, avg_time 2.143, loss:nan
g_step 5200, step 393, avg_time 1.046, loss:nan
g_step 5300, step 56, avg_time 1.051, loss:nan
g_step 5400, step 156, avg_time 1.050, loss:nan
g_step 5500, step 256, avg_time 1.042, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 356, avg_time 2.139, loss:nan
g_step 5700, step 19, avg_time 1.046, loss:nan
g_step 5800, step 119, avg_time 1.043, loss:nan
g_step 5900, step 219, avg_time 1.049, loss:nan
g_step 6000, step 319, avg_time 1.056, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 419, avg_time 2.143, loss:nan
g_step 6200, step 82, avg_time 1.064, loss:nan
g_step 6300, step 182, avg_time 1.044, loss:nan
g_step 6400, step 282, avg_time 1.040, loss:nan
g_step 6500, step 382, avg_time 1.056, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 45, avg_time 2.123, loss:nan
g_step 6700, step 145, avg_time 1.048, loss:nan
g_step 6800, step 245, avg_time 1.057, loss:nan
g_step 6900, step 345, avg_time 1.040, loss:nan
g_step 7000, step 8, avg_time 1.050, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 108, avg_time 2.139, loss:nan
g_step 7200, step 208, avg_time 1.043, loss:nan
g_step 7300, step 308, avg_time 1.051, loss:nan
g_step 7400, step 408, avg_time 1.053, loss:nan
g_step 7500, step 71, avg_time 1.056, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 171, avg_time 2.139, loss:nan
g_step 7700, step 271, avg_time 1.054, loss:nan
g_step 7800, step 371, avg_time 1.036, loss:nan
g_step 7900, step 34, avg_time 1.065, loss:nan
g_step 8000, step 134, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 234, avg_time 2.152, loss:nan
g_step 8200, step 334, avg_time 1.042, loss:nan
g_step 8300, step 434, avg_time 1.044, loss:nan
g_step 8400, step 97, avg_time 1.057, loss:nan
g_step 8500, step 197, avg_time 1.052, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 297, avg_time 2.125, loss:nan
g_step 8700, step 397, avg_time 1.043, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:49:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:49:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-49-14_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:49:16 - WARNING - datasets.builder -   Using custom data configuration default-8fbd6ec855e68d5d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8fbd6ec855e68d5d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:49:17,693 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:49:17,695 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:49:17,695 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:49:17,696 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:49:17,744 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:17,771 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:17,771 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:17,771 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:17,771 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:17,771 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:17,772 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:49:18,042 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:49:21,190 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:49:21,200 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8fbd6ec855e68d5d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.95ba/s] 18%|        | 2/11 [00:00<00:02,  3.77ba/s] 27%|       | 3/11 [00:00<00:01,  4.08ba/s] 36%|      | 4/11 [00:00<00:01,  4.25ba/s] 45%|     | 5/11 [00:01<00:01,  4.34ba/s] 55%|    | 6/11 [00:01<00:01,  4.41ba/s] 64%|   | 7/11 [00:01<00:01,  3.66ba/s] 73%|  | 8/11 [00:02<00:00,  3.91ba/s] 82%| | 9/11 [00:02<00:00,  4.08ba/s] 91%| | 10/11 [00:02<00:00,  4.22ba/s]100%|| 11/11 [00:02<00:00,  4.99ba/s]100%|| 11/11 [00:02<00:00,  4.26ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.32ba/s] 50%|     | 2/4 [00:00<00:00,  3.74ba/s] 75%|  | 3/4 [00:00<00:00,  4.05ba/s]100%|| 4/4 [00:00<00:00,  5.12ba/s]100%|| 4/4 [00:00<00:00,  4.52ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.69ba/s] 27%|       | 3/11 [00:00<00:00,  8.47ba/s] 45%|     | 5/11 [00:00<00:00,  9.33ba/s] 64%|   | 7/11 [00:00<00:00,  9.75ba/s] 82%| | 9/11 [00:00<00:00,  9.94ba/s]100%|| 11/11 [00:01<00:00, 10.93ba/s]100%|| 11/11 [00:01<00:00,  9.98ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.03ba/s] 75%|  | 3/4 [00:00<00:00,  8.17ba/s]100%|| 4/4 [00:00<00:00,  9.14ba/s]
[INFO|trainer.py:414] 2023-08-28 06:49:27,195 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:49:27,251 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:49:27,251 >>   Num examples = 10520
[INFO|trainer.py:1149] 2023-08-28 06:49:27,251 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:49:27,251 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:49:27,251 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:49:27,251 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:49:27,251 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:53,  3.51it/s]  0%|          | 2/820 [00:00<04:02,  3.37it/s]  0%|          | 3/820 [00:00<03:55,  3.48it/s]  0%|          | 4/820 [00:01<03:51,  3.53it/s]  1%|          | 5/820 [00:01<03:50,  3.54it/s]  1%|          | 6/820 [00:01<03:49,  3.55it/s]  1%|          | 7/820 [00:01<03:48,  3.55it/s]  1%|          | 8/820 [00:02<03:47,  3.57it/s]  1%|          | 9/820 [00:02<03:46,  3.58it/s]  1%|          | 10/820 [00:02<03:45,  3.59it/s]  1%|         | 11/820 [00:03<03:45,  3.60it/s]  1%|         | 12/820 [00:03<03:44,  3.60it/s]  2%|         | 13/820 [00:03<03:45,  3.58it/s]  2%|         | 14/820 [00:03<03:44,  3.58it/s]  2%|         | 15/820 [00:04<03:44,  3.59it/s]  2%|         | 16/820 [00:04<03:43,  3.59it/s]  2%|         | 17/820 [00:04<03:43,  3.59it/s]  2%|         | 18/820 [00:05<03:42,  3.60it/s]  2%|         | 19/820 [00:05<03:42,  3.60it/s]  2%|         | 20/820 [00:05<03:41,  3.61it/s]  3%|         | 21/820 [00:05<03:41,  3.61it/s]  3%|         | 22/820 [00:06<03:41,  3.60it/s]  3%|         | 23/820 [00:06<03:41,  3.60it/s]  3%|         | 24/820 [00:06<03:48,  3.49it/s]  3%|         | 25/820 [00:07<03:45,  3.52it/s]  3%|         | 26/820 [00:07<03:43,  3.55it/s]  3%|         | 27/820 [00:07<03:42,  3.56it/s]  3%|         | 28/820 [00:07<03:41,  3.57it/s]  4%|         | 29/820 [00:08<03:40,  3.58it/s]  4%|         | 30/820 [00:08<03:40,  3.59it/s]  4%|         | 31/820 [00:08<03:39,  3.59it/s]  4%|         | 32/820 [00:08<03:38,  3.60it/s]  4%|         | 33/820 [00:09<03:38,  3.60it/s]  4%|         | 34/820 [00:09<03:38,  3.60it/s]  4%|         | 35/820 [00:09<03:42,  3.54it/s]  4%|         | 36/820 [00:10<03:40,  3.55it/s]  5%|         | 37/820 [00:10<03:39,  3.56it/s]  5%|         | 38/820 [00:10<03:38,  3.57it/s]  5%|         | 39/820 [00:10<03:37,  3.58it/s]  5%|         | 40/820 [00:11<03:37,  3.59it/s]  5%|         | 41/820 [00:11<03:36,  3.59it/s]  5%|         | 42/820 [00:11<03:36,  3.60it/s]  5%|         | 43/820 [00:12<03:36,  3.60it/s]  5%|         | 44/820 [00:12<03:35,  3.60it/s]  5%|         | 45/820 [00:12<04:12,  3.06it/s]  6%|         | 46/820 [00:13<04:48,  2.69it/s]  6%|         | 47/820 [00:13<04:26,  2.90it/s]  6%|         | 48/820 [00:13<04:10,  3.08it/s]  6%|         | 49/820 [00:14<03:59,  3.22it/s]  6%|         | 50/820 [00:14<03:51,  3.33it/s]  6%|         | 51/820 [00:14<03:45,  3.41it/s]  6%|         | 52/820 [00:14<03:41,  3.47it/s]  6%|         | 53/820 [00:15<03:38,  3.51it/s]  7%|         | 54/820 [00:15<03:36,  3.53it/s]  7%|         | 55/820 [00:15<03:35,  3.56it/s]  7%|         | 56/820 [00:16<03:33,  3.57it/s]  7%|         | 57/820 [00:16<03:43,  3.42it/s]  7%|         | 58/820 [00:16<03:39,  3.47it/s]  7%|         | 59/820 [00:16<03:36,  3.51it/s]  7%|         | 60/820 [00:17<03:34,  3.54it/s]  7%|         | 61/820 [00:17<03:33,  3.56it/s]  8%|         | 62/820 [00:17<03:32,  3.57it/s]  8%|         | 63/820 [00:17<03:31,  3.58it/s]  8%|         | 64/820 [00:18<03:30,  3.59it/s]  8%|         | 65/820 [00:18<03:29,  3.60it/s]  8%|         | 66/820 [00:18<03:29,  3.60it/s]  8%|         | 67/820 [00:19<03:28,  3.60it/s]  8%|         | 68/820 [00:19<03:39,  3.43it/s]  8%|         | 69/820 [00:19<03:35,  3.49it/s]  9%|         | 70/820 [00:19<03:32,  3.52it/s]  9%|         | 71/820 [00:20<03:31,  3.55it/s]  9%|         | 72/820 [00:20<03:29,  3.56it/s]  9%|         | 73/820 [00:20<03:28,  3.57it/s]  9%|         | 74/820 [00:21<03:28,  3.58it/s]  9%|         | 75/820 [00:21<03:27,  3.59it/s]  9%|         | 76/820 [00:21<03:26,  3.60it/s]  9%|         | 77/820 [00:21<03:26,  3.60it/s] 10%|         | 78/820 [00:22<03:25,  3.60it/s] 10%|         | 79/820 [00:22<03:31,  3.50it/s] 10%|         | 80/820 [00:22<03:46,  3.27it/s] 10%|         | 81/820 [00:23<03:40,  3.35it/s] 10%|         | 82/820 [00:23<03:35,  3.42it/s] 10%|         | 83/820 [00:23<03:32,  3.47it/s] 10%|         | 84/820 [00:23<03:29,  3.51it/s] 10%|         | 85/820 [00:24<03:27,  3.54it/s] 10%|         | 86/820 [00:24<03:26,  3.56it/s] 11%|         | 87/820 [00:24<03:24,  3.58it/s] 11%|         | 88/820 [00:25<03:24,  3.58it/s] 11%|         | 89/820 [00:25<03:23,  3.59it/s] 11%|         | 90/820 [00:25<03:23,  3.59it/s] 11%|         | 91/820 [00:25<03:22,  3.60it/s] 11%|         | 92/820 [00:26<03:32,  3.43it/s] 11%|        | 93/820 [00:26<03:28,  3.48it/s] 11%|        | 94/820 [00:26<03:26,  3.51it/s] 12%|        | 95/820 [00:27<03:24,  3.54it/s] 12%|        | 96/820 [00:27<03:23,  3.56it/s] 12%|        | 97/820 [00:27<03:22,  3.57it/s] 12%|        | 98/820 [00:27<03:21,  3.58it/s] 12%|        | 99/820 [00:28<03:20,  3.59it/s] 12%|        | 100/820 [00:28<03:20,  3.59it/s] 12%|        | 101/820 [00:28<03:19,  3.60it/s] 12%|        | 102/820 [00:29<03:19,  3.60it/s] 13%|        | 103/820 [00:29<03:22,  3.54it/s] 13%|        | 104/820 [00:29<03:21,  3.56it/s] 13%|        | 105/820 [00:29<03:20,  3.57it/s] 13%|        | 106/820 [00:30<03:19,  3.58it/s] 13%|        | 107/820 [00:30<03:18,  3.59it/s] 13%|        | 108/820 [00:30<03:18,  3.59it/s] 13%|        | 109/820 [00:30<03:17,  3.60it/s] 13%|        | 110/820 [00:31<03:17,  3.60it/s] 14%|        | 111/820 [00:31<03:17,  3.60it/s] 14%|        | 112/820 [00:31<03:16,  3.60it/s] 14%|        | 113/820 [00:32<03:16,  3.60it/s] 14%|        | 114/820 [00:32<03:27,  3.41it/s] 14%|        | 115/820 [00:32<03:23,  3.46it/s] 14%|        | 116/820 [00:32<03:21,  3.50it/s] 14%|        | 117/820 [00:33<03:19,  3.53it/s] 14%|        | 118/820 [00:33<03:17,  3.55it/s] 15%|        | 119/820 [00:33<03:16,  3.57it/s] 15%|        | 120/820 [00:34<03:15,  3.58it/s] 15%|        | 121/820 [00:34<03:14,  3.59it/s] 15%|        | 122/820 [00:34<03:14,  3.59it/s] 15%|        | 123/820 [00:34<03:13,  3.60it/s] 15%|        | 124/820 [00:35<03:13,  3.60it/s] 15%|        | 125/820 [00:35<03:18,  3.50it/s] 15%|        | 126/820 [00:35<03:16,  3.53it/s] 15%|        | 127/820 [00:36<03:15,  3.55it/s] 16%|        | 128/820 [00:36<03:13,  3.57it/s] 16%|        | 129/820 [00:36<03:12,  3.58it/s] 16%|        | 130/820 [00:36<03:12,  3.59it/s] 16%|        | 131/820 [00:37<03:11,  3.59it/s] 16%|        | 132/820 [00:37<03:11,  3.60it/s] 16%|        | 133/820 [00:37<03:10,  3.60it/s] 16%|        | 134/820 [00:37<03:10,  3.60it/s] 16%|        | 135/820 [00:38<03:10,  3.60it/s] 17%|        | 136/820 [00:38<03:28,  3.28it/s] 17%|        | 137/820 [00:38<03:22,  3.37it/s] 17%|        | 138/820 [00:39<03:18,  3.43it/s] 17%|        | 139/820 [00:39<03:15,  3.48it/s] 17%|        | 140/820 [00:39<03:13,  3.52it/s] 17%|        | 141/820 [00:40<03:11,  3.54it/s] 17%|        | 142/820 [00:40<03:10,  3.56it/s] 17%|        | 143/820 [00:40<03:09,  3.57it/s] 18%|        | 144/820 [00:40<03:08,  3.58it/s] 18%|        | 145/820 [00:41<03:08,  3.59it/s] 18%|        | 146/820 [00:41<03:07,  3.59it/s] 18%|        | 147/820 [00:41<03:12,  3.49it/s] 18%|        | 148/820 [00:41<03:10,  3.53it/s] 18%|        | 149/820 [00:42<03:09,  3.55it/s] 18%|        | 150/820 [00:42<03:08,  3.56it/s] 18%|        | 151/820 [00:42<03:07,  3.57it/s] 19%|        | 152/820 [00:43<03:06,  3.58it/s] 19%|        | 153/820 [00:43<03:05,  3.59it/s] 19%|        | 154/820 [00:43<03:05,  3.60it/s] 19%|        | 155/820 [00:43<03:08,  3.53it/s] 19%|        | 156/820 [00:44<03:07,  3.54it/s] 19%|        | 157/820 [00:44<03:06,  3.56it/s] 19%|        | 158/820 [00:44<03:08,  3.51it/s] 19%|        | 159/820 [00:45<03:06,  3.54it/s] 20%|        | 160/820 [00:45<03:05,  3.56it/s] 20%|        | 161/820 [00:45<03:04,  3.57it/s] 20%|        | 162/820 [00:45<03:03,  3.58it/s] 20%|        | 163/820 [00:46<03:03,  3.59it/s] 20%|        | 164/820 [00:46<03:02,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:50:13,760 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:50:13,761 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 06:50:13,761 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.74it/s][A
  3%|         | 12/436 [00:00<00:08, 48.88it/s][A
  4%|         | 17/436 [00:00<00:08, 47.20it/s][A
  5%|         | 22/436 [00:00<00:08, 46.25it/s][A
  6%|         | 27/436 [00:00<00:08, 45.52it/s][A
  7%|         | 32/436 [00:00<00:08, 44.99it/s][A
  8%|         | 37/436 [00:00<00:08, 44.50it/s][A
 10%|         | 42/436 [00:00<00:08, 44.27it/s][A
 11%|         | 47/436 [00:01<00:08, 44.48it/s][A
 12%|        | 52/436 [00:01<00:09, 42.65it/s][A
 13%|        | 57/436 [00:01<00:08, 43.32it/s][A
 14%|        | 62/436 [00:01<00:08, 43.83it/s][A
 15%|        | 67/436 [00:01<00:08, 44.28it/s][A
 17%|        | 72/436 [00:01<00:08, 44.31it/s][A
 18%|        | 77/436 [00:01<00:08, 44.19it/s][A
 19%|        | 82/436 [00:01<00:08, 43.98it/s][A
 20%|        | 87/436 [00:01<00:07, 43.91it/s][A
 21%|        | 92/436 [00:02<00:07, 44.00it/s][A
 22%|       | 97/436 [00:02<00:07, 44.22it/s][A
 23%|       | 102/436 [00:02<00:07, 44.51it/s][A
 25%|       | 107/436 [00:02<00:07, 44.61it/s][A
 26%|       | 112/436 [00:02<00:07, 44.74it/s][A
 27%|       | 117/436 [00:02<00:07, 44.71it/s][A
 28%|       | 122/436 [00:02<00:07, 44.42it/s][A
 29%|       | 127/436 [00:02<00:06, 44.22it/s][A
 30%|       | 132/436 [00:02<00:06, 44.08it/s][A
 31%|      | 137/436 [00:03<00:06, 44.15it/s][A
 33%|      | 142/436 [00:03<00:06, 44.23it/s][A
 34%|      | 147/436 [00:03<00:06, 44.53it/s][A
 35%|      | 152/436 [00:03<00:06, 44.65it/s][A
 36%|      | 157/436 [00:03<00:06, 44.83it/s][A
 37%|      | 162/436 [00:03<00:06, 44.77it/s][A
 38%|      | 167/436 [00:03<00:06, 44.53it/s][A
 39%|      | 172/436 [00:03<00:05, 44.42it/s][A
 41%|      | 177/436 [00:03<00:05, 44.27it/s][A
 42%|     | 182/436 [00:04<00:05, 44.21it/s][A
 43%|     | 187/436 [00:04<00:05, 42.84it/s][A
 44%|     | 192/436 [00:04<00:05, 43.43it/s][A
 45%|     | 197/436 [00:04<00:05, 43.99it/s][A
 46%|     | 202/436 [00:04<00:05, 44.26it/s][A
 47%|     | 207/436 [00:04<00:05, 44.50it/s][A
 49%|     | 212/436 [00:04<00:05, 44.30it/s][A
 50%|     | 217/436 [00:04<00:04, 44.10it/s][A
 51%|     | 222/436 [00:04<00:04, 44.05it/s][A
 52%|    | 227/436 [00:05<00:04, 43.98it/s][A
 53%|    | 232/436 [00:05<00:04, 44.19it/s][A
 54%|    | 237/436 [00:05<00:04, 44.31it/s][A
 56%|    | 242/436 [00:05<00:04, 44.39it/s][A
 57%|    | 247/436 [00:05<00:04, 44.68it/s][A
 58%|    | 252/436 [00:05<00:04, 44.69it/s][A
 59%|    | 257/436 [00:05<00:04, 44.68it/s][A
 60%|    | 262/436 [00:05<00:03, 44.56it/s][A
 61%|    | 267/436 [00:06<00:03, 44.31it/s][A
 62%|   | 272/436 [00:06<00:03, 44.30it/s][A
 64%|   | 277/436 [00:06<00:03, 44.35it/s][A
 65%|   | 282/436 [00:06<00:03, 44.36it/s][A
 66%|   | 287/436 [00:06<00:03, 44.57it/s][A
 67%|   | 292/436 [00:06<00:03, 44.49it/s][A
 68%|   | 297/436 [00:06<00:03, 44.59it/s][A
 69%|   | 302/436 [00:06<00:03, 44.58it/s][A
 70%|   | 307/436 [00:06<00:02, 44.61it/s][A
 72%|  | 312/436 [00:07<00:02, 44.51it/s][A
 73%|  | 317/436 [00:07<00:02, 44.39it/s][A
 74%|  | 322/436 [00:07<00:02, 44.28it/s][A
 75%|  | 327/436 [00:07<00:02, 44.34it/s][A
 76%|  | 332/436 [00:07<00:02, 44.46it/s][A
 77%|  | 337/436 [00:07<00:02, 44.59it/s][A
 78%|  | 342/436 [00:07<00:02, 44.69it/s][A
 80%|  | 347/436 [00:07<00:01, 44.56it/s][A
 81%|  | 352/436 [00:07<00:01, 44.50it/s][A
 82%| | 357/436 [00:08<00:01, 44.41it/s][A
 83%| | 362/436 [00:08<00:01, 44.23it/s][A
 84%| | 367/436 [00:08<00:01, 44.17it/s][A
 85%| | 372/436 [00:08<00:01, 44.13it/s][A
 86%| | 377/436 [00:08<00:01, 44.22it/s][A
 88%| | 382/436 [00:08<00:01, 44.56it/s][A
 89%| | 387/436 [00:08<00:01, 44.53it/s][A
 90%| | 392/436 [00:08<00:00, 44.59it/s][A
 91%| | 397/436 [00:08<00:00, 44.49it/s][A
 92%|| 402/436 [00:09<00:00, 44.33it/s][A
 93%|| 407/436 [00:09<00:00, 44.30it/s][A
 94%|| 412/436 [00:09<00:00, 44.32it/s][A
 96%|| 417/436 [00:09<00:00, 44.33it/s][A
 97%|| 422/436 [00:09<00:00, 44.47it/s][A
 98%|| 427/436 [00:09<00:00, 44.59it/s][A
 99%|| 432/436 [00:09<00:00, 44.60it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.60it/s][A 20%|        | 164/820 [00:56<03:02,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:50:23,927 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:50:24,226 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:50:28,247 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:50:28,364 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:50:28,415 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164/special_tokens_map.json
 20%|        | 165/820 [01:02<53:59,  4.95s/it] 20%|        | 166/820 [01:02<38:39,  3.55s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/820 [01:02<27:56,  2.57s/it] 20%|        | 168/820 [01:03<20:26,  1.88s/it] 21%|        | 169/820 [01:03<15:11,  1.40s/it] 21%|        | 170/820 [01:03<11:32,  1.06s/it] 21%|        | 171/820 [01:03<08:58,  1.21it/s] 21%|        | 172/820 [01:04<07:10,  1.50it/s] 21%|        | 173/820 [01:04<06:02,  1.79it/s] 21%|        | 174/820 [01:04<05:07,  2.10it/s] 21%|       | 175/820 [01:05<04:29,  2.39it/s] 21%|       | 176/820 [01:05<04:02,  2.65it/s] 22%|       | 177/820 [01:05<03:43,  2.87it/s] 22%|       | 178/820 [01:05<03:30,  3.05it/s] 22%|       | 179/820 [01:06<03:21,  3.19it/s] 22%|       | 180/820 [01:06<03:14,  3.29it/s] 22%|       | 181/820 [01:06<03:09,  3.37it/s] 22%|       | 182/820 [01:07<03:06,  3.42it/s] 22%|       | 183/820 [01:07<03:04,  3.46it/s] 22%|       | 184/820 [01:07<03:03,  3.47it/s] 23%|       | 185/820 [01:07<03:01,  3.49it/s] 23%|       | 186/820 [01:08<03:00,  3.51it/s] 23%|       | 187/820 [01:08<02:59,  3.52it/s] 23%|       | 188/820 [01:08<02:58,  3.54it/s] 23%|       | 189/820 [01:09<02:58,  3.54it/s] 23%|       | 190/820 [01:09<02:57,  3.55it/s] 23%|       | 191/820 [01:09<02:57,  3.55it/s] 23%|       | 192/820 [01:09<02:56,  3.55it/s] 24%|       | 193/820 [01:10<02:56,  3.55it/s] 24%|       | 194/820 [01:10<02:56,  3.55it/s] 24%|       | 195/820 [01:10<03:00,  3.46it/s] 24%|       | 196/820 [01:11<02:59,  3.49it/s] 24%|       | 197/820 [01:11<02:57,  3.51it/s] 24%|       | 198/820 [01:11<02:56,  3.52it/s] 24%|       | 199/820 [01:11<02:55,  3.53it/s] 24%|       | 200/820 [01:12<02:55,  3.54it/s] 25%|       | 201/820 [01:12<02:53,  3.56it/s] 25%|       | 202/820 [01:12<03:18,  3.12it/s] 25%|       | 203/820 [01:13<03:47,  2.72it/s] 25%|       | 204/820 [01:13<03:30,  2.92it/s] 25%|       | 205/820 [01:13<03:30,  2.93it/s] 25%|       | 206/820 [01:14<03:18,  3.10it/s] 25%|       | 207/820 [01:14<03:09,  3.24it/s] 25%|       | 208/820 [01:14<03:03,  3.34it/s] 25%|       | 209/820 [01:15<02:58,  3.42it/s] 26%|       | 210/820 [01:15<02:55,  3.47it/s] 26%|       | 211/820 [01:15<02:53,  3.51it/s] 26%|       | 212/820 [01:15<02:51,  3.54it/s] 26%|       | 213/820 [01:16<02:50,  3.56it/s] 26%|       | 214/820 [01:16<02:49,  3.57it/s] 26%|       | 215/820 [01:16<02:49,  3.58it/s] 26%|       | 216/820 [01:17<02:59,  3.37it/s] 26%|       | 217/820 [01:17<02:55,  3.44it/s] 27%|       | 218/820 [01:17<02:52,  3.49it/s] 27%|       | 219/820 [01:17<02:50,  3.52it/s] 27%|       | 220/820 [01:18<02:49,  3.55it/s] 27%|       | 221/820 [01:18<02:48,  3.56it/s] 27%|       | 222/820 [01:18<02:47,  3.58it/s] 27%|       | 223/820 [01:19<02:46,  3.58it/s] 27%|       | 224/820 [01:19<02:45,  3.59it/s] 27%|       | 225/820 [01:19<02:45,  3.60it/s] 28%|       | 226/820 [01:19<02:44,  3.60it/s] 28%|       | 227/820 [01:20<02:54,  3.40it/s] 28%|       | 228/820 [01:20<02:51,  3.46it/s] 28%|       | 229/820 [01:20<02:48,  3.50it/s] 28%|       | 230/820 [01:21<02:47,  3.53it/s] 28%|       | 231/820 [01:21<02:45,  3.56it/s] 28%|       | 232/820 [01:21<02:44,  3.57it/s] 28%|       | 233/820 [01:21<02:43,  3.58it/s] 29%|       | 234/820 [01:22<02:43,  3.59it/s] 29%|       | 235/820 [01:22<02:42,  3.59it/s] 29%|       | 236/820 [01:22<02:42,  3.60it/s] 29%|       | 237/820 [01:22<02:41,  3.60it/s] 29%|       | 238/820 [01:23<02:51,  3.39it/s] 29%|       | 239/820 [01:23<02:48,  3.45it/s] 29%|       | 240/820 [01:23<02:46,  3.49it/s] 29%|       | 241/820 [01:24<02:44,  3.52it/s] 30%|       | 242/820 [01:24<02:52,  3.35it/s] 30%|       | 243/820 [01:24<02:48,  3.42it/s] 30%|       | 244/820 [01:25<02:45,  3.47it/s] 30%|       | 245/820 [01:25<02:43,  3.51it/s] 30%|       | 246/820 [01:25<02:41,  3.54it/s] 30%|       | 247/820 [01:25<02:40,  3.56it/s] 30%|       | 248/820 [01:26<02:40,  3.57it/s] 30%|       | 249/820 [01:26<02:39,  3.58it/s] 30%|       | 250/820 [01:26<02:38,  3.59it/s] 31%|       | 251/820 [01:26<02:38,  3.59it/s] 31%|       | 252/820 [01:27<02:38,  3.59it/s] 31%|       | 253/820 [01:27<02:41,  3.50it/s] 31%|       | 254/820 [01:27<02:40,  3.53it/s] 31%|       | 255/820 [01:28<02:38,  3.56it/s] 31%|       | 256/820 [01:28<02:37,  3.57it/s] 31%|      | 257/820 [01:28<02:37,  3.58it/s] 31%|      | 258/820 [01:28<02:36,  3.59it/s] 32%|      | 259/820 [01:29<02:36,  3.59it/s] 32%|      | 260/820 [01:29<02:35,  3.60it/s] 32%|      | 261/820 [01:29<02:35,  3.60it/s] 32%|      | 262/820 [01:30<02:34,  3.60it/s] 32%|      | 263/820 [01:30<02:34,  3.60it/s] 32%|      | 264/820 [01:30<02:37,  3.53it/s] 32%|      | 265/820 [01:30<02:36,  3.55it/s] 32%|      | 266/820 [01:31<02:35,  3.57it/s] 33%|      | 267/820 [01:31<02:34,  3.58it/s] 33%|      | 268/820 [01:31<02:33,  3.59it/s] 33%|      | 269/820 [01:32<02:33,  3.60it/s] 33%|      | 270/820 [01:32<02:32,  3.60it/s] 33%|      | 271/820 [01:32<02:32,  3.60it/s] 33%|      | 272/820 [01:32<02:32,  3.60it/s] 33%|      | 273/820 [01:33<02:31,  3.60it/s] 33%|      | 274/820 [01:33<02:31,  3.60it/s] 34%|      | 275/820 [01:33<02:35,  3.50it/s] 34%|      | 276/820 [01:33<02:34,  3.53it/s] 34%|      | 277/820 [01:34<02:33,  3.55it/s] 34%|      | 278/820 [01:34<02:32,  3.56it/s] 34%|      | 279/820 [01:34<02:31,  3.58it/s] 34%|      | 280/820 [01:35<02:30,  3.59it/s] 34%|      | 281/820 [01:35<02:29,  3.59it/s] 34%|      | 282/820 [01:35<02:29,  3.60it/s] 35%|      | 283/820 [01:35<02:29,  3.60it/s] 35%|      | 284/820 [01:36<02:28,  3.60it/s] 35%|      | 285/820 [01:36<02:28,  3.60it/s] 35%|      | 286/820 [01:36<02:29,  3.58it/s] 35%|      | 287/820 [01:37<02:28,  3.59it/s] 35%|      | 288/820 [01:37<02:28,  3.59it/s] 35%|      | 289/820 [01:37<02:27,  3.59it/s] 35%|      | 290/820 [01:37<02:27,  3.60it/s] 35%|      | 291/820 [01:38<02:26,  3.60it/s] 36%|      | 292/820 [01:38<02:26,  3.60it/s] 36%|      | 293/820 [01:38<02:26,  3.60it/s] 36%|      | 294/820 [01:38<02:25,  3.60it/s] 36%|      | 295/820 [01:39<02:25,  3.60it/s] 36%|      | 296/820 [01:39<02:25,  3.60it/s] 36%|      | 297/820 [01:39<02:28,  3.51it/s] 36%|      | 298/820 [01:40<02:27,  3.54it/s] 36%|      | 299/820 [01:40<02:26,  3.56it/s] 37%|      | 300/820 [01:40<02:25,  3.57it/s] 37%|      | 301/820 [01:40<02:24,  3.58it/s] 37%|      | 302/820 [01:41<02:24,  3.59it/s] 37%|      | 303/820 [01:41<02:23,  3.60it/s] 37%|      | 304/820 [01:41<02:23,  3.60it/s] 37%|      | 305/820 [01:42<02:23,  3.60it/s] 37%|      | 306/820 [01:42<02:22,  3.60it/s] 37%|      | 307/820 [01:42<02:22,  3.60it/s] 38%|      | 308/820 [01:42<02:25,  3.53it/s] 38%|      | 309/820 [01:43<02:23,  3.55it/s] 38%|      | 310/820 [01:43<02:22,  3.57it/s] 38%|      | 311/820 [01:43<02:22,  3.58it/s] 38%|      | 312/820 [01:44<02:24,  3.52it/s] 38%|      | 313/820 [01:44<02:23,  3.53it/s] 38%|      | 314/820 [01:44<02:22,  3.55it/s] 38%|      | 315/820 [01:44<02:21,  3.57it/s] 39%|      | 316/820 [01:45<02:20,  3.58it/s] 39%|      | 317/820 [01:45<02:20,  3.59it/s] 39%|      | 318/820 [01:45<02:19,  3.59it/s] 39%|      | 319/820 [01:46<02:21,  3.53it/s] 39%|      | 320/820 [01:46<02:20,  3.55it/s] 39%|      | 321/820 [01:46<02:19,  3.57it/s] 39%|      | 322/820 [01:46<02:19,  3.58it/s] 39%|      | 323/820 [01:47<02:18,  3.58it/s] 40%|      | 324/820 [01:47<02:18,  3.59it/s] 40%|      | 325/820 [01:47<02:17,  3.60it/s] 40%|      | 326/820 [01:47<02:17,  3.60it/s] 40%|      | 327/820 [01:48<02:16,  3.60it/s] 40%|      | 328/820 [01:48<02:16,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 06:51:15,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:51:15,797 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 06:51:15,797 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8772, 'eval_samples_per_second': 352.53, 'eval_steps_per_second': 44.142, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.83it/s][A
  3%|         | 12/436 [00:00<00:09, 44.93it/s][A
  4%|         | 17/436 [00:00<00:09, 44.95it/s][A
  5%|         | 22/436 [00:00<00:09, 44.98it/s][A
  6%|         | 27/436 [00:00<00:09, 44.97it/s][A
  7%|         | 32/436 [00:00<00:09, 44.87it/s][A
  8%|         | 37/436 [00:00<00:08, 44.61it/s][A
 10%|         | 42/436 [00:00<00:08, 44.48it/s][A
 11%|         | 47/436 [00:01<00:08, 44.62it/s][A
 12%|        | 52/436 [00:01<00:08, 44.52it/s][A
 13%|        | 57/436 [00:01<00:08, 44.71it/s][A
 14%|        | 62/436 [00:01<00:08, 44.83it/s][A
 15%|        | 67/436 [00:01<00:08, 44.88it/s][A
 17%|        | 72/436 [00:01<00:08, 44.88it/s][A
 18%|        | 77/436 [00:01<00:08, 44.67it/s][A
 19%|        | 82/436 [00:01<00:07, 44.68it/s][A
 20%|        | 87/436 [00:01<00:07, 44.62it/s][A
 21%|        | 92/436 [00:02<00:07, 44.59it/s][A
 22%|       | 97/436 [00:02<00:07, 44.65it/s][A
 23%|       | 102/436 [00:02<00:07, 44.66it/s][A
 25%|       | 107/436 [00:02<00:07, 44.86it/s][A
 26%|       | 112/436 [00:02<00:07, 44.89it/s][A
 27%|       | 117/436 [00:02<00:07, 44.76it/s][A
 28%|       | 122/436 [00:02<00:07, 44.79it/s][A
 29%|       | 127/436 [00:02<00:06, 44.61it/s][A
 30%|       | 132/436 [00:02<00:06, 44.53it/s][A
 31%|      | 137/436 [00:03<00:06, 44.59it/s][A
 33%|      | 142/436 [00:03<00:06, 44.67it/s][A
 34%|      | 147/436 [00:03<00:06, 44.35it/s][A
 35%|      | 152/436 [00:03<00:06, 44.64it/s][A
 36%|      | 157/436 [00:03<00:06, 44.70it/s][A
 37%|      | 162/436 [00:03<00:06, 44.73it/s][A
 38%|      | 167/436 [00:03<00:06, 44.68it/s][A
 39%|      | 172/436 [00:03<00:05, 44.54it/s][A
 41%|      | 177/436 [00:03<00:05, 44.54it/s][A
 42%|     | 182/436 [00:04<00:05, 44.57it/s][A
 43%|     | 187/436 [00:04<00:05, 44.64it/s][A
 44%|     | 192/436 [00:04<00:05, 44.64it/s][A
 45%|     | 197/436 [00:04<00:05, 44.81it/s][A
 46%|     | 202/436 [00:04<00:05, 44.78it/s][A
 47%|     | 207/436 [00:04<00:05, 44.85it/s][A
 49%|     | 212/436 [00:04<00:05, 44.75it/s][A
 50%|     | 217/436 [00:04<00:04, 44.29it/s][A
 51%|     | 222/436 [00:04<00:04, 44.58it/s][A
 52%|    | 227/436 [00:05<00:04, 44.53it/s][A
 53%|    | 232/436 [00:05<00:04, 44.56it/s][A
 54%|    | 237/436 [00:05<00:04, 44.68it/s][A
 56%|    | 242/436 [00:05<00:04, 44.77it/s][A
 57%|    | 247/436 [00:05<00:04, 44.78it/s][A
 58%|    | 252/436 [00:05<00:04, 44.71it/s][A
 59%|    | 257/436 [00:05<00:04, 44.68it/s][A
 60%|    | 262/436 [00:05<00:03, 44.60it/s][A
 61%|    | 267/436 [00:05<00:03, 44.55it/s][A
 62%|   | 272/436 [00:06<00:03, 44.64it/s][A
 64%|   | 277/436 [00:06<00:03, 44.67it/s][A
 65%|   | 282/436 [00:06<00:03, 44.38it/s][A
 66%|   | 287/436 [00:06<00:03, 44.64it/s][A
 67%|   | 292/436 [00:06<00:03, 44.62it/s][A
 68%|   | 297/436 [00:06<00:03, 44.53it/s][A
 69%|   | 302/436 [00:06<00:03, 44.53it/s][A
 70%|   | 307/436 [00:06<00:02, 44.45it/s][A
 72%|  | 312/436 [00:06<00:02, 44.50it/s][A
 73%|  | 317/436 [00:07<00:02, 44.51it/s][A
 74%|  | 322/436 [00:07<00:02, 44.62it/s][A
 75%|  | 327/436 [00:07<00:02, 44.72it/s][A
 76%|  | 332/436 [00:07<00:02, 44.72it/s][A
 77%|  | 337/436 [00:07<00:02, 44.71it/s][A
 78%|  | 342/436 [00:07<00:02, 44.58it/s][A
 80%|  | 347/436 [00:07<00:02, 44.49it/s][A
 81%|  | 352/436 [00:07<00:01, 44.53it/s][A
 82%| | 357/436 [00:07<00:01, 44.53it/s][A
 83%| | 362/436 [00:08<00:01, 44.54it/s][A
 84%| | 367/436 [00:08<00:01, 44.53it/s][A
 85%| | 372/436 [00:08<00:01, 44.60it/s][A
 86%| | 377/436 [00:08<00:01, 44.70it/s][A
 88%| | 382/436 [00:08<00:01, 44.81it/s][A
 89%| | 387/436 [00:08<00:01, 44.70it/s][A
 90%| | 392/436 [00:08<00:00, 44.63it/s][A
 91%| | 397/436 [00:08<00:00, 44.74it/s][A
 92%|| 402/436 [00:08<00:00, 44.68it/s][A
 93%|| 407/436 [00:09<00:00, 44.68it/s][A
 94%|| 412/436 [00:09<00:00, 44.67it/s][A
 96%|| 417/436 [00:09<00:00, 44.62it/s][A
 97%|| 422/436 [00:09<00:00, 41.64it/s][A
 98%|| 427/436 [00:09<00:00, 42.63it/s][A
 99%|| 432/436 [00:09<00:00, 43.45it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 43.45it/s][A 40%|      | 328/820 [01:58<02:16,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:51:25,875 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:51:26,067 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:51:29,317 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:51:29,517 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:51:29,602 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-328/special_tokens_map.json
 40%|      | 329/820 [02:03<38:28,  4.70s/it] 40%|      | 330/820 [02:03<27:33,  3.37s/it] 40%|      | 331/820 [02:04<19:55,  2.45s/it] 40%|      | 332/820 [02:04<14:35,  1.79s/it] 41%|      | 333/820 [02:04<10:52,  1.34s/it] 41%|      | 334/820 [02:04<08:16,  1.02s/it] 41%|      | 335/820 [02:05<06:26,  1.25it/s] 41%|      | 336/820 [02:05<05:11,  1.55it/s] 41%|      | 337/820 [02:05<04:18,  1.87it/s] 41%|      | 338/820 [02:06<03:40,  2.19it/s] 41%|     | 339/820 [02:06<03:13,  2.48it/s] 41%|     | 340/820 [02:06<02:55,  2.74it/s] 42%|     | 341/820 [02:06<02:42,  2.96it/s] 42%|     | 342/820 [02:07<02:32,  3.13it/s] 42%|     | 343/820 [02:07<02:26,  3.26it/s] 42%|     | 344/820 [02:07<02:21,  3.35it/s] 42%|     | 345/820 [02:07<02:18,  3.43it/s] 42%|     | 346/820 [02:08<02:16,  3.48it/s] 42%|     | 347/820 [02:08<02:18,  3.41it/s] 42%|     | 348/820 [02:08<02:16,  3.47it/s] 43%|     | 349/820 [02:09<02:14,  3.51it/s] 43%|     | 350/820 [02:09<02:12,  3.54it/s] 43%|     | 351/820 [02:09<02:11,  3.56it/s] 43%|     | 352/820 [02:09<02:10,  3.57it/s] 43%|     | 353/820 [02:10<02:10,  3.58it/s] 43%|     | 354/820 [02:10<02:09,  3.59it/s] 43%|     | 355/820 [02:10<02:09,  3.60it/s] 43%|     | 356/820 [02:11<02:08,  3.60it/s] 44%|     | 357/820 [02:11<02:08,  3.60it/s] 44%|     | 358/820 [02:11<02:12,  3.50it/s] 44%|     | 359/820 [02:11<02:10,  3.53it/s] 44%|     | 360/820 [02:12<02:09,  3.55it/s] 44%|     | 361/820 [02:12<02:08,  3.57it/s] 44%|     | 362/820 [02:12<02:07,  3.58it/s] 44%|     | 363/820 [02:13<02:07,  3.59it/s] 44%|     | 364/820 [02:13<02:19,  3.27it/s] 45%|     | 365/820 [02:13<02:16,  3.34it/s] 45%|     | 366/820 [02:13<02:12,  3.42it/s] 45%|     | 367/820 [02:14<02:10,  3.47it/s] 45%|     | 368/820 [02:14<02:08,  3.51it/s] 45%|     | 369/820 [02:14<02:17,  3.28it/s] 45%|     | 370/820 [02:15<02:13,  3.37it/s] 45%|     | 371/820 [02:15<02:10,  3.44it/s] 45%|     | 372/820 [02:15<02:08,  3.49it/s] 45%|     | 373/820 [02:15<02:06,  3.53it/s] 46%|     | 374/820 [02:16<02:05,  3.55it/s] 46%|     | 375/820 [02:16<02:04,  3.57it/s] 46%|     | 376/820 [02:16<02:04,  3.58it/s] 46%|     | 377/820 [02:17<02:03,  3.59it/s] 46%|     | 378/820 [02:17<02:02,  3.60it/s] 46%|     | 379/820 [02:17<02:02,  3.60it/s] 46%|     | 380/820 [02:17<02:06,  3.48it/s] 46%|     | 381/820 [02:18<02:04,  3.51it/s] 47%|     | 382/820 [02:18<02:03,  3.54it/s] 47%|     | 383/820 [02:18<02:02,  3.56it/s] 47%|     | 384/820 [02:19<02:01,  3.58it/s] 47%|     | 385/820 [02:19<02:01,  3.59it/s] 47%|     | 386/820 [02:19<02:00,  3.59it/s] 47%|     | 387/820 [02:19<02:00,  3.60it/s] 47%|     | 388/820 [02:20<02:04,  3.47it/s] 47%|     | 389/820 [02:20<02:02,  3.51it/s] 48%|     | 390/820 [02:20<02:01,  3.54it/s] 48%|     | 391/820 [02:21<02:05,  3.42it/s] 48%|     | 392/820 [02:21<02:03,  3.47it/s] 48%|     | 393/820 [02:21<02:01,  3.51it/s] 48%|     | 394/820 [02:21<02:00,  3.54it/s] 48%|     | 395/820 [02:22<01:59,  3.56it/s] 48%|     | 396/820 [02:22<01:58,  3.58it/s] 48%|     | 397/820 [02:22<01:57,  3.59it/s] 49%|     | 398/820 [02:22<01:57,  3.58it/s] 49%|     | 399/820 [02:23<01:57,  3.59it/s] 49%|     | 400/820 [02:23<01:57,  3.59it/s] 49%|     | 401/820 [02:23<01:56,  3.59it/s] 49%|     | 402/820 [02:24<02:00,  3.46it/s] 49%|     | 403/820 [02:24<01:59,  3.50it/s] 49%|     | 404/820 [02:24<01:57,  3.53it/s] 49%|     | 405/820 [02:24<01:56,  3.56it/s] 50%|     | 406/820 [02:25<01:56,  3.57it/s] 50%|     | 407/820 [02:25<01:55,  3.58it/s] 50%|     | 408/820 [02:25<01:54,  3.59it/s] 50%|     | 409/820 [02:26<01:54,  3.59it/s] 50%|     | 410/820 [02:26<01:54,  3.59it/s] 50%|     | 411/820 [02:26<01:53,  3.60it/s] 50%|     | 412/820 [02:26<01:53,  3.60it/s] 50%|     | 413/820 [02:27<01:52,  3.60it/s] 50%|     | 414/820 [02:27<01:52,  3.60it/s] 51%|     | 415/820 [02:27<01:52,  3.60it/s] 51%|     | 416/820 [02:28<01:52,  3.60it/s] 51%|     | 417/820 [02:28<01:51,  3.60it/s] 51%|     | 418/820 [02:28<01:51,  3.60it/s] 51%|     | 419/820 [02:28<01:53,  3.55it/s] 51%|     | 420/820 [02:29<01:52,  3.57it/s] 51%|    | 421/820 [02:29<01:51,  3.58it/s] 51%|    | 422/820 [02:29<01:51,  3.58it/s] 52%|    | 423/820 [02:29<01:50,  3.59it/s] 52%|    | 424/820 [02:30<01:50,  3.59it/s] 52%|    | 425/820 [02:30<01:49,  3.60it/s] 52%|    | 426/820 [02:30<01:49,  3.60it/s] 52%|    | 427/820 [02:31<01:49,  3.60it/s] 52%|    | 428/820 [02:31<01:48,  3.60it/s] 52%|    | 429/820 [02:31<01:48,  3.60it/s] 52%|    | 430/820 [02:31<01:52,  3.48it/s] 53%|    | 431/820 [02:32<01:50,  3.52it/s] 53%|    | 432/820 [02:32<01:49,  3.54it/s] 53%|    | 433/820 [02:32<01:48,  3.56it/s] 53%|    | 434/820 [02:33<01:47,  3.58it/s] 53%|    | 435/820 [02:33<01:47,  3.58it/s] 53%|    | 436/820 [02:33<01:46,  3.59it/s] 53%|    | 437/820 [02:33<01:46,  3.60it/s] 53%|    | 438/820 [02:34<01:46,  3.60it/s] 54%|    | 439/820 [02:34<01:45,  3.60it/s] 54%|    | 440/820 [02:34<01:45,  3.60it/s] 54%|    | 441/820 [02:35<01:49,  3.46it/s] 54%|    | 442/820 [02:35<01:47,  3.50it/s] 54%|    | 443/820 [02:35<01:46,  3.53it/s] 54%|    | 444/820 [02:35<01:45,  3.55it/s] 54%|    | 445/820 [02:36<01:45,  3.57it/s] 54%|    | 446/820 [02:36<01:44,  3.58it/s] 55%|    | 447/820 [02:36<01:43,  3.59it/s] 55%|    | 448/820 [02:36<01:43,  3.59it/s] 55%|    | 449/820 [02:37<01:43,  3.60it/s] 55%|    | 450/820 [02:37<01:42,  3.60it/s] 55%|    | 451/820 [02:37<01:42,  3.60it/s] 55%|    | 452/820 [02:38<01:45,  3.50it/s] 55%|    | 453/820 [02:38<01:43,  3.53it/s] 55%|    | 454/820 [02:38<01:43,  3.55it/s] 55%|    | 455/820 [02:38<01:42,  3.57it/s] 56%|    | 456/820 [02:39<01:41,  3.58it/s] 56%|    | 457/820 [02:39<01:41,  3.59it/s] 56%|    | 458/820 [02:39<01:40,  3.59it/s] 56%|    | 459/820 [02:40<01:40,  3.60it/s] 56%|    | 460/820 [02:40<01:40,  3.60it/s] 56%|    | 461/820 [02:40<01:39,  3.60it/s] 56%|    | 462/820 [02:40<01:39,  3.60it/s] 56%|    | 463/820 [02:41<01:42,  3.50it/s] 57%|    | 464/820 [02:41<01:40,  3.53it/s] 57%|    | 465/820 [02:41<01:40,  3.55it/s] 57%|    | 466/820 [02:42<01:39,  3.56it/s] 57%|    | 467/820 [02:42<01:38,  3.57it/s] 57%|    | 468/820 [02:42<01:38,  3.58it/s] 57%|    | 469/820 [02:42<01:37,  3.59it/s] 57%|    | 470/820 [02:43<01:40,  3.47it/s] 57%|    | 471/820 [02:43<01:39,  3.50it/s] 58%|    | 472/820 [02:43<01:38,  3.52it/s] 58%|    | 473/820 [02:44<01:41,  3.41it/s] 58%|    | 474/820 [02:44<01:43,  3.33it/s] 58%|    | 475/820 [02:44<01:41,  3.41it/s] 58%|    | 476/820 [02:44<01:39,  3.47it/s] 58%|    | 477/820 [02:45<01:37,  3.51it/s] 58%|    | 478/820 [02:45<01:36,  3.53it/s] 58%|    | 479/820 [02:45<01:35,  3.55it/s] 59%|    | 480/820 [02:46<01:35,  3.57it/s] 59%|    | 481/820 [02:46<01:34,  3.58it/s] 59%|    | 482/820 [02:46<01:34,  3.58it/s] 59%|    | 483/820 [02:46<01:34,  3.58it/s] 59%|    | 484/820 [02:47<01:33,  3.58it/s] 59%|    | 485/820 [02:47<01:38,  3.40it/s] 59%|    | 486/820 [02:47<01:36,  3.46it/s] 59%|    | 487/820 [02:48<01:35,  3.50it/s] 60%|    | 488/820 [02:48<01:34,  3.53it/s] 60%|    | 489/820 [02:48<01:33,  3.55it/s] 60%|    | 490/820 [02:48<01:32,  3.56it/s] 60%|    | 491/820 [02:49<01:31,  3.58it/s] 60%|    | 492/820 [02:49<01:31,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:52:16,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:52:16,727 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 06:52:16,727 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8488, 'eval_samples_per_second': 353.547, 'eval_steps_per_second': 44.269, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.22it/s][A
  3%|         | 12/436 [00:00<00:08, 48.78it/s][A
  4%|         | 17/436 [00:00<00:08, 47.27it/s][A
  5%|         | 22/436 [00:00<00:08, 46.29it/s][A
  6%|         | 27/436 [00:00<00:08, 45.49it/s][A
  7%|         | 32/436 [00:00<00:08, 45.01it/s][A
  8%|         | 37/436 [00:00<00:09, 42.85it/s][A
 10%|         | 42/436 [00:00<00:09, 43.49it/s][A
 11%|         | 47/436 [00:01<00:08, 43.94it/s][A
 12%|        | 52/436 [00:01<00:08, 44.37it/s][A
 13%|        | 57/436 [00:01<00:08, 44.70it/s][A
 14%|        | 62/436 [00:01<00:08, 44.82it/s][A
 15%|        | 67/436 [00:01<00:08, 44.71it/s][A
 17%|        | 72/436 [00:01<00:08, 44.58it/s][A
 18%|        | 77/436 [00:01<00:08, 44.38it/s][A
 19%|        | 82/436 [00:01<00:08, 44.24it/s][A
 20%|        | 87/436 [00:01<00:07, 44.32it/s][A
 21%|        | 92/436 [00:02<00:07, 44.46it/s][A
 22%|       | 97/436 [00:02<00:07, 44.64it/s][A
 23%|       | 102/436 [00:02<00:07, 44.93it/s][A
 25%|       | 107/436 [00:02<00:07, 44.93it/s][A
 26%|       | 112/436 [00:02<00:07, 44.96it/s][A
 27%|       | 117/436 [00:02<00:07, 44.68it/s][A
 28%|       | 122/436 [00:02<00:07, 44.40it/s][A
 29%|       | 127/436 [00:02<00:06, 44.40it/s][A
 30%|       | 132/436 [00:02<00:06, 44.43it/s][A
 31%|      | 137/436 [00:03<00:06, 44.54it/s][A
 33%|      | 142/436 [00:03<00:06, 44.82it/s][A
 34%|      | 147/436 [00:03<00:06, 44.88it/s][A
 35%|      | 152/436 [00:03<00:06, 44.98it/s][A
 36%|      | 157/436 [00:03<00:06, 44.80it/s][A
 37%|      | 162/436 [00:03<00:06, 44.70it/s][A
 38%|      | 167/436 [00:03<00:06, 44.56it/s][A
 39%|      | 172/436 [00:03<00:05, 44.08it/s][A
 41%|      | 177/436 [00:03<00:05, 44.21it/s][A
 42%|     | 182/436 [00:04<00:05, 44.44it/s][A
 43%|     | 187/436 [00:04<00:05, 44.68it/s][A
 44%|     | 192/436 [00:04<00:05, 44.82it/s][A
 45%|     | 197/436 [00:04<00:05, 44.79it/s][A
 46%|     | 202/436 [00:04<00:05, 44.81it/s][A
 47%|     | 207/436 [00:04<00:05, 44.67it/s][A
 49%|     | 212/436 [00:04<00:05, 44.56it/s][A
 50%|     | 217/436 [00:04<00:04, 44.37it/s][A
 51%|     | 222/436 [00:04<00:04, 44.36it/s][A
 52%|    | 227/436 [00:05<00:04, 44.52it/s][A
 53%|    | 232/436 [00:05<00:04, 44.70it/s][A
 54%|    | 237/436 [00:05<00:04, 44.83it/s][A
 56%|    | 242/436 [00:05<00:04, 44.86it/s][A
 57%|    | 247/436 [00:05<00:04, 44.83it/s][A
 58%|    | 252/436 [00:05<00:04, 44.77it/s][A
 59%|    | 257/436 [00:05<00:04, 44.55it/s][A
 60%|    | 262/436 [00:05<00:03, 44.38it/s][A
 61%|    | 267/436 [00:05<00:03, 44.50it/s][A
 62%|   | 272/436 [00:06<00:03, 44.56it/s][A
 64%|   | 277/436 [00:06<00:03, 44.69it/s][A
 65%|   | 282/436 [00:06<00:03, 44.82it/s][A
 66%|   | 287/436 [00:06<00:03, 44.85it/s][A
 67%|   | 292/436 [00:06<00:03, 44.90it/s][A
 68%|   | 297/436 [00:06<00:03, 44.72it/s][A
 69%|   | 302/436 [00:06<00:03, 44.58it/s][A
 70%|   | 307/436 [00:07<00:04, 32.09it/s][A
 72%|  | 312/436 [00:07<00:03, 35.19it/s][A
 73%|  | 317/436 [00:07<00:03, 37.75it/s][A
 74%|  | 322/436 [00:07<00:02, 39.71it/s][A
 75%|  | 327/436 [00:07<00:02, 41.21it/s][A
 76%|  | 332/436 [00:07<00:02, 42.31it/s][A
 77%|  | 337/436 [00:07<00:02, 43.23it/s][A
 78%|  | 342/436 [00:07<00:02, 43.67it/s][A
 80%|  | 347/436 [00:07<00:02, 43.53it/s][A
 81%|  | 352/436 [00:08<00:01, 43.52it/s][A
 82%| | 357/436 [00:08<00:01, 43.71it/s][A
 83%| | 362/436 [00:08<00:01, 44.05it/s][A
 84%| | 367/436 [00:08<00:01, 44.45it/s][A
 85%| | 372/436 [00:08<00:01, 44.62it/s][A
 86%| | 377/436 [00:08<00:01, 44.88it/s][A
 88%| | 382/436 [00:08<00:01, 44.91it/s][A
 89%| | 387/436 [00:08<00:01, 44.79it/s][A
 90%| | 392/436 [00:08<00:00, 44.44it/s][A
 91%| | 397/436 [00:09<00:00, 44.16it/s][A
 92%|| 402/436 [00:09<00:00, 44.11it/s][A
 93%|| 407/436 [00:09<00:00, 44.34it/s][A
 94%|| 412/436 [00:09<00:00, 44.43it/s][A
 96%|| 417/436 [00:09<00:00, 44.75it/s][A
 97%|| 422/436 [00:09<00:00, 44.88it/s][A
 98%|| 427/436 [00:09<00:00, 44.96it/s][A
 99%|| 432/436 [00:09<00:00, 44.95it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.95it/s][A 60%|    | 492/820 [02:59<01:31,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:52:26,895 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 06:52:27,107 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:52:29,923 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:52:30,042 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:52:30,234 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-492/special_tokens_map.json
 60%|    | 493/820 [03:04<25:09,  4.62s/it] 60%|    | 494/820 [03:04<18:00,  3.32s/it] 60%|    | 495/820 [03:04<13:01,  2.41s/it] 60%|    | 496/820 [03:05<09:32,  1.77s/it] 61%|    | 497/820 [03:05<07:07,  1.32s/it] 61%|    | 498/820 [03:05<05:25,  1.01s/it] 61%|    | 499/820 [03:05<04:14,  1.26it/s] 61%|    | 500/820 [03:06<03:27,  1.54it/s]                                                  61%|    | 500/820 [03:06<03:27,  1.54it/s] 61%|    | 501/820 [03:06<02:51,  1.86it/s] 61%|    | 502/820 [03:06<02:26,  2.17it/s] 61%|   | 503/820 [03:07<02:09,  2.46it/s] 61%|   | 504/820 [03:07<01:56,  2.71it/s] 62%|   | 505/820 [03:07<01:48,  2.91it/s] 62%|   | 506/820 [03:07<01:41,  3.08it/s] 62%|   | 507/820 [03:08<01:37,  3.21it/s] 62%|   | 508/820 [03:08<01:34,  3.30it/s] 62%|   | 509/820 [03:08<01:32,  3.37it/s] 62%|   | 510/820 [03:08<01:30,  3.43it/s] 62%|   | 511/820 [03:09<01:32,  3.34it/s] 62%|   | 512/820 [03:09<01:30,  3.40it/s] 63%|   | 513/820 [03:09<01:29,  3.45it/s] 63%|   | 514/820 [03:10<01:28,  3.48it/s] 63%|   | 515/820 [03:10<01:27,  3.50it/s] 63%|   | 516/820 [03:10<01:26,  3.52it/s] 63%|   | 517/820 [03:10<01:25,  3.53it/s] 63%|   | 518/820 [03:11<01:25,  3.53it/s] 63%|   | 519/820 [03:11<01:25,  3.54it/s] 63%|   | 520/820 [03:11<01:24,  3.54it/s] 64%|   | 521/820 [03:12<01:24,  3.54it/s] 64%|   | 522/820 [03:12<01:26,  3.44it/s] 64%|   | 523/820 [03:12<01:25,  3.47it/s] 64%|   | 524/820 [03:12<01:24,  3.49it/s] 64%|   | 525/820 [03:13<01:37,  3.02it/s] 64%|   | 526/820 [03:13<01:33,  3.15it/s] 64%|   | 527/820 [03:13<01:29,  3.26it/s] 64%|   | 528/820 [03:14<01:27,  3.34it/s] 65%|   | 529/820 [03:14<01:25,  3.40it/s] 65%|   | 530/820 [03:14<01:24,  3.44it/s] 65%|   | 531/820 [03:15<01:23,  3.47it/s] 65%|   | 532/820 [03:15<01:25,  3.38it/s] 65%|   | 533/820 [03:15<01:23,  3.43it/s] 65%|   | 534/820 [03:15<01:22,  3.46it/s] 65%|   | 535/820 [03:16<01:21,  3.49it/s] 65%|   | 536/820 [03:16<01:20,  3.51it/s] 65%|   | 537/820 [03:16<01:20,  3.52it/s] 66%|   | 538/820 [03:17<01:19,  3.53it/s] 66%|   | 539/820 [03:17<01:19,  3.54it/s] 66%|   | 540/820 [03:17<01:19,  3.54it/s] 66%|   | 541/820 [03:17<01:18,  3.54it/s] 66%|   | 542/820 [03:18<01:18,  3.54it/s] 66%|   | 543/820 [03:18<01:21,  3.38it/s] 66%|   | 544/820 [03:18<01:20,  3.43it/s] 66%|   | 545/820 [03:19<01:19,  3.47it/s] 67%|   | 546/820 [03:19<01:18,  3.49it/s] 67%|   | 547/820 [03:19<01:17,  3.51it/s] 67%|   | 548/820 [03:19<01:17,  3.52it/s] 67%|   | 549/820 [03:20<01:16,  3.53it/s] 67%|   | 550/820 [03:20<01:16,  3.53it/s] 67%|   | 551/820 [03:20<01:16,  3.53it/s] 67%|   | 552/820 [03:21<01:15,  3.54it/s] 67%|   | 553/820 [03:21<01:15,  3.54it/s] 68%|   | 554/820 [03:21<01:18,  3.37it/s] 68%|   | 555/820 [03:22<01:17,  3.43it/s] 68%|   | 556/820 [03:22<01:15,  3.48it/s] 68%|   | 557/820 [03:22<01:15,  3.51it/s] 68%|   | 558/820 [03:22<01:25,  3.05it/s] 68%|   | 559/820 [03:23<01:21,  3.19it/s] 68%|   | 560/820 [03:23<01:18,  3.30it/s] 68%|   | 561/820 [03:23<01:16,  3.39it/s] 69%|   | 562/820 [03:24<01:14,  3.45it/s] 69%|   | 563/820 [03:24<01:13,  3.49it/s] 69%|   | 564/820 [03:24<01:12,  3.53it/s] 69%|   | 565/820 [03:24<01:14,  3.43it/s] 69%|   | 566/820 [03:25<01:12,  3.48it/s] 69%|   | 567/820 [03:25<01:11,  3.52it/s] 69%|   | 568/820 [03:25<01:11,  3.54it/s] 69%|   | 569/820 [03:26<01:10,  3.55it/s] 70%|   | 570/820 [03:26<01:10,  3.57it/s] 70%|   | 571/820 [03:26<01:09,  3.58it/s] 70%|   | 572/820 [03:26<01:09,  3.59it/s] 70%|   | 573/820 [03:27<01:08,  3.59it/s] 70%|   | 574/820 [03:27<01:08,  3.60it/s] 70%|   | 575/820 [03:27<01:08,  3.60it/s] 70%|   | 576/820 [03:28<01:07,  3.60it/s] 70%|   | 577/820 [03:28<01:07,  3.60it/s] 70%|   | 578/820 [03:28<01:07,  3.60it/s] 71%|   | 579/820 [03:28<01:06,  3.60it/s] 71%|   | 580/820 [03:29<01:06,  3.60it/s] 71%|   | 581/820 [03:29<01:06,  3.60it/s] 71%|   | 582/820 [03:29<01:06,  3.60it/s] 71%|   | 583/820 [03:29<01:05,  3.60it/s] 71%|   | 584/820 [03:30<01:05,  3.60it/s] 71%|  | 585/820 [03:30<01:06,  3.51it/s] 71%|  | 586/820 [03:30<01:06,  3.54it/s] 72%|  | 587/820 [03:31<01:05,  3.55it/s] 72%|  | 588/820 [03:31<01:05,  3.56it/s] 72%|  | 589/820 [03:31<01:04,  3.58it/s] 72%|  | 590/820 [03:31<01:04,  3.58it/s] 72%|  | 591/820 [03:32<01:03,  3.59it/s] 72%|  | 592/820 [03:32<01:03,  3.59it/s] 72%|  | 593/820 [03:32<01:03,  3.59it/s] 72%|  | 594/820 [03:33<01:02,  3.60it/s] 73%|  | 595/820 [03:33<01:02,  3.60it/s] 73%|  | 596/820 [03:33<01:02,  3.60it/s] 73%|  | 597/820 [03:33<01:02,  3.59it/s] 73%|  | 598/820 [03:34<01:01,  3.59it/s] 73%|  | 599/820 [03:34<01:01,  3.60it/s] 73%|  | 600/820 [03:34<01:01,  3.60it/s] 73%|  | 601/820 [03:34<01:00,  3.60it/s] 73%|  | 602/820 [03:35<01:00,  3.60it/s] 74%|  | 603/820 [03:35<01:01,  3.54it/s] 74%|  | 604/820 [03:35<01:00,  3.56it/s] 74%|  | 605/820 [03:36<01:00,  3.57it/s] 74%|  | 606/820 [03:36<00:59,  3.58it/s] 74%|  | 607/820 [03:36<00:59,  3.58it/s] 74%|  | 608/820 [03:36<00:59,  3.59it/s] 74%|  | 609/820 [03:37<00:58,  3.59it/s] 74%|  | 610/820 [03:37<00:58,  3.59it/s] 75%|  | 611/820 [03:37<00:58,  3.60it/s] 75%|  | 612/820 [03:38<00:57,  3.60it/s] 75%|  | 613/820 [03:38<00:57,  3.60it/s] 75%|  | 614/820 [03:38<00:58,  3.51it/s] 75%|  | 615/820 [03:38<00:57,  3.54it/s] 75%|  | 616/820 [03:39<00:57,  3.55it/s] 75%|  | 617/820 [03:39<00:56,  3.57it/s] 75%|  | 618/820 [03:39<00:56,  3.57it/s] 75%|  | 619/820 [03:40<00:56,  3.58it/s] 76%|  | 620/820 [03:40<00:55,  3.59it/s] 76%|  | 621/820 [03:40<00:55,  3.59it/s] 76%|  | 622/820 [03:40<00:55,  3.59it/s] 76%|  | 623/820 [03:41<00:54,  3.60it/s] 76%|  | 624/820 [03:41<00:54,  3.60it/s] 76%|  | 625/820 [03:41<00:55,  3.52it/s] 76%|  | 626/820 [03:42<00:54,  3.54it/s] 76%|  | 627/820 [03:42<00:54,  3.56it/s] 77%|  | 628/820 [03:42<00:53,  3.57it/s] 77%|  | 629/820 [03:42<00:53,  3.58it/s] 77%|  | 630/820 [03:43<00:53,  3.58it/s] 77%|  | 631/820 [03:43<00:52,  3.59it/s] 77%|  | 632/820 [03:43<00:52,  3.59it/s] 77%|  | 633/820 [03:43<00:51,  3.60it/s] 77%|  | 634/820 [03:44<00:53,  3.48it/s] 77%|  | 635/820 [03:44<00:52,  3.50it/s] 78%|  | 636/820 [03:44<00:54,  3.40it/s] 78%|  | 637/820 [03:45<00:52,  3.45it/s] 78%|  | 638/820 [03:45<00:52,  3.50it/s] 78%|  | 639/820 [03:45<00:51,  3.53it/s] 78%|  | 640/820 [03:45<00:50,  3.55it/s] 78%|  | 641/820 [03:46<00:50,  3.56it/s] 78%|  | 642/820 [03:46<00:49,  3.57it/s] 78%|  | 643/820 [03:46<00:49,  3.58it/s] 79%|  | 644/820 [03:47<00:49,  3.58it/s] 79%|  | 645/820 [03:47<00:48,  3.59it/s] 79%|  | 646/820 [03:47<00:48,  3.59it/s] 79%|  | 647/820 [03:47<00:50,  3.42it/s] 79%|  | 648/820 [03:48<00:49,  3.47it/s] 79%|  | 649/820 [03:48<00:48,  3.51it/s] 79%|  | 650/820 [03:48<00:48,  3.54it/s] 79%|  | 651/820 [03:49<00:47,  3.56it/s] 80%|  | 652/820 [03:49<00:47,  3.57it/s] 80%|  | 653/820 [03:49<00:46,  3.58it/s] 80%|  | 654/820 [03:49<00:46,  3.58it/s] 80%|  | 655/820 [03:50<00:45,  3.59it/s] 80%|  | 656/820 [03:50<00:45,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:53:17,757 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:53:17,757 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 06:53:17,757 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.9676, 'eval_samples_per_second': 349.333, 'eval_steps_per_second': 43.742, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.29it/s][A
  3%|         | 12/436 [00:00<00:09, 46.15it/s][A
  4%|         | 17/436 [00:00<00:09, 45.68it/s][A
  5%|         | 22/436 [00:00<00:09, 45.40it/s][A
  6%|         | 27/436 [00:00<00:09, 45.20it/s][A
  7%|         | 32/436 [00:00<00:08, 44.99it/s][A
  8%|         | 37/436 [00:00<00:08, 44.60it/s][A
 10%|         | 42/436 [00:00<00:08, 44.46it/s][A
 11%|         | 47/436 [00:01<00:08, 44.55it/s][A
 12%|        | 52/436 [00:01<00:08, 44.59it/s][A
 13%|        | 57/436 [00:01<00:08, 44.86it/s][A
 14%|        | 62/436 [00:01<00:08, 44.83it/s][A
 15%|        | 67/436 [00:01<00:08, 44.88it/s][A
 17%|        | 72/436 [00:01<00:08, 44.74it/s][A
 18%|        | 77/436 [00:01<00:08, 44.64it/s][A
 19%|        | 82/436 [00:01<00:07, 44.56it/s][A
 20%|        | 87/436 [00:01<00:07, 44.46it/s][A
 21%|        | 92/436 [00:02<00:07, 44.45it/s][A
 22%|       | 97/436 [00:02<00:07, 44.63it/s][A
 23%|       | 102/436 [00:02<00:07, 44.72it/s][A
 25%|       | 107/436 [00:02<00:07, 44.85it/s][A
 26%|       | 112/436 [00:02<00:07, 44.88it/s][A
 27%|       | 117/436 [00:02<00:07, 44.85it/s][A
 28%|       | 122/436 [00:02<00:07, 44.72it/s][A
 29%|       | 127/436 [00:02<00:06, 44.55it/s][A
 30%|       | 132/436 [00:02<00:06, 44.45it/s][A
 31%|      | 137/436 [00:03<00:06, 44.43it/s][A
 33%|      | 142/436 [00:03<00:06, 44.61it/s][A
 34%|      | 147/436 [00:03<00:06, 43.88it/s][A
 35%|      | 152/436 [00:03<00:06, 44.22it/s][A
 36%|      | 157/436 [00:03<00:06, 44.50it/s][A
 37%|      | 162/436 [00:03<00:06, 44.62it/s][A
 38%|      | 167/436 [00:03<00:06, 44.60it/s][A
 39%|      | 172/436 [00:03<00:05, 44.57it/s][A
 41%|      | 177/436 [00:03<00:05, 44.34it/s][A
 42%|     | 182/436 [00:04<00:05, 44.32it/s][A
 43%|     | 187/436 [00:04<00:05, 44.45it/s][A
 44%|     | 192/436 [00:04<00:05, 44.54it/s][A
 45%|     | 197/436 [00:04<00:05, 44.84it/s][A
 46%|     | 202/436 [00:04<00:05, 44.83it/s][A
 47%|     | 207/436 [00:04<00:05, 44.91it/s][A
 49%|     | 212/436 [00:04<00:04, 44.81it/s][A
 50%|     | 217/436 [00:04<00:04, 44.64it/s][A
 51%|     | 222/436 [00:04<00:04, 44.53it/s][A
 52%|    | 227/436 [00:05<00:04, 44.44it/s][A
 53%|    | 232/436 [00:05<00:04, 44.45it/s][A
 54%|    | 237/436 [00:05<00:04, 44.66it/s][A
 56%|    | 242/436 [00:05<00:04, 44.73it/s][A
 57%|    | 247/436 [00:05<00:04, 44.84it/s][A
 58%|    | 252/436 [00:05<00:04, 44.82it/s][A
 59%|    | 257/436 [00:05<00:03, 44.75it/s][A
 60%|    | 262/436 [00:05<00:03, 44.65it/s][A
 61%|    | 267/436 [00:05<00:03, 44.48it/s][A
 62%|   | 272/436 [00:06<00:03, 44.53it/s][A
 64%|   | 277/436 [00:06<00:03, 44.55it/s][A
 65%|   | 282/436 [00:06<00:05, 29.21it/s][A
 66%|   | 287/436 [00:06<00:04, 32.69it/s][A
 67%|   | 292/436 [00:06<00:04, 35.72it/s][A
 68%|   | 297/436 [00:06<00:03, 38.11it/s][A
 69%|   | 302/436 [00:06<00:03, 40.05it/s][A
 70%|   | 307/436 [00:07<00:03, 41.50it/s][A
 72%|  | 312/436 [00:07<00:02, 42.59it/s][A
 73%|  | 317/436 [00:07<00:02, 43.23it/s][A
 74%|  | 322/436 [00:07<00:02, 43.25it/s][A
 75%|  | 327/436 [00:07<00:02, 43.33it/s][A
 76%|  | 332/436 [00:07<00:02, 43.60it/s][A
 77%|  | 337/436 [00:07<00:02, 43.98it/s][A
 78%|  | 342/436 [00:07<00:02, 44.43it/s][A
 80%|  | 347/436 [00:07<00:01, 44.66it/s][A
 81%|  | 352/436 [00:08<00:01, 44.77it/s][A
 82%| | 357/436 [00:08<00:01, 44.83it/s][A
 83%| | 362/436 [00:08<00:01, 44.68it/s][A
 84%| | 367/436 [00:08<00:01, 44.45it/s][A
 85%| | 372/436 [00:08<00:01, 44.30it/s][A
 86%| | 377/436 [00:08<00:01, 44.26it/s][A
 88%| | 382/436 [00:08<00:01, 44.51it/s][A
 89%| | 387/436 [00:08<00:01, 44.65it/s][A
 90%| | 392/436 [00:08<00:00, 44.89it/s][A
 91%| | 397/436 [00:09<00:00, 44.92it/s][A
 92%|| 402/436 [00:09<00:00, 44.94it/s][A
 93%|| 407/436 [00:09<00:00, 44.67it/s][A
 94%|| 412/436 [00:09<00:00, 42.65it/s][A
 96%|| 417/436 [00:09<00:00, 43.25it/s][A
 97%|| 422/436 [00:09<00:00, 43.57it/s][A
 98%|| 427/436 [00:09<00:00, 44.02it/s][A
 99%|| 432/436 [00:09<00:00, 44.33it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.33it/s][A 80%|  | 656/820 [04:00<00:45,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:53:27,904 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 06:53:27,974 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:53:30,768 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:53:30,918 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:53:31,001 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-656/special_tokens_map.json
 80%|  | 657/820 [04:04<12:11,  4.49s/it] 80%|  | 658/820 [04:05<08:42,  3.23s/it] 80%|  | 659/820 [04:05<06:17,  2.34s/it] 80%|  | 660/820 [04:05<04:35,  1.72s/it] 81%|  | 661/820 [04:05<03:25,  1.29s/it] 81%|  | 662/820 [04:06<02:37,  1.01it/s] 81%|  | 663/820 [04:06<02:02,  1.28it/s] 81%|  | 664/820 [04:06<01:38,  1.59it/s] 81%|  | 665/820 [04:07<01:21,  1.90it/s] 81%|  | 666/820 [04:07<01:09,  2.21it/s] 81%| | 667/820 [04:07<01:01,  2.50it/s] 81%| | 668/820 [04:07<00:55,  2.74it/s] 82%| | 669/820 [04:08<00:51,  2.95it/s] 82%| | 670/820 [04:08<00:48,  3.11it/s] 82%| | 671/820 [04:08<00:46,  3.23it/s] 82%| | 672/820 [04:09<00:44,  3.32it/s] 82%| | 673/820 [04:09<00:44,  3.29it/s] 82%| | 674/820 [04:09<00:43,  3.36it/s] 82%| | 675/820 [04:09<00:42,  3.42it/s] 82%| | 676/820 [04:10<00:41,  3.46it/s] 83%| | 677/820 [04:10<00:40,  3.49it/s] 83%| | 678/820 [04:10<00:40,  3.51it/s] 83%| | 679/820 [04:11<00:39,  3.53it/s] 83%| | 680/820 [04:11<00:39,  3.54it/s] 83%| | 681/820 [04:11<00:39,  3.54it/s] 83%| | 682/820 [04:11<00:38,  3.55it/s] 83%| | 683/820 [04:12<00:38,  3.55it/s] 83%| | 684/820 [04:12<00:43,  3.11it/s] 84%| | 685/820 [04:12<00:41,  3.25it/s] 84%| | 686/820 [04:13<00:40,  3.35it/s] 84%| | 687/820 [04:13<00:42,  3.11it/s] 84%| | 688/820 [04:13<00:40,  3.23it/s] 84%| | 689/820 [04:14<00:39,  3.34it/s] 84%| | 690/820 [04:14<00:38,  3.41it/s] 84%| | 691/820 [04:14<00:37,  3.47it/s] 84%| | 692/820 [04:14<00:36,  3.52it/s] 85%| | 693/820 [04:15<00:35,  3.54it/s] 85%| | 694/820 [04:15<00:36,  3.48it/s] 85%| | 695/820 [04:15<00:35,  3.52it/s] 85%| | 696/820 [04:15<00:35,  3.54it/s] 85%| | 697/820 [04:16<00:34,  3.56it/s] 85%| | 698/820 [04:16<00:34,  3.58it/s] 85%| | 699/820 [04:16<00:33,  3.59it/s] 85%| | 700/820 [04:17<00:33,  3.59it/s] 85%| | 701/820 [04:17<00:33,  3.60it/s] 86%| | 702/820 [04:17<00:32,  3.60it/s] 86%| | 703/820 [04:17<00:32,  3.60it/s] 86%| | 704/820 [04:18<00:32,  3.59it/s] 86%| | 705/820 [04:18<00:33,  3.43it/s] 86%| | 706/820 [04:18<00:32,  3.48it/s] 86%| | 707/820 [04:19<00:32,  3.52it/s] 86%| | 708/820 [04:19<00:31,  3.55it/s] 86%| | 709/820 [04:19<00:31,  3.56it/s] 87%| | 710/820 [04:19<00:30,  3.58it/s] 87%| | 711/820 [04:20<00:31,  3.50it/s] 87%| | 712/820 [04:20<00:30,  3.52it/s] 87%| | 713/820 [04:20<00:30,  3.55it/s] 87%| | 714/820 [04:21<00:29,  3.56it/s] 87%| | 715/820 [04:21<00:29,  3.58it/s] 87%| | 716/820 [04:21<00:30,  3.39it/s] 87%| | 717/820 [04:21<00:29,  3.46it/s] 88%| | 718/820 [04:22<00:29,  3.50it/s] 88%| | 719/820 [04:22<00:28,  3.53it/s] 88%| | 720/820 [04:22<00:28,  3.55it/s] 88%| | 721/820 [04:23<00:27,  3.56it/s] 88%| | 722/820 [04:23<00:27,  3.57it/s] 88%| | 723/820 [04:23<00:27,  3.59it/s] 88%| | 724/820 [04:23<00:26,  3.60it/s] 88%| | 725/820 [04:24<00:26,  3.60it/s] 89%| | 726/820 [04:24<00:26,  3.61it/s] 89%| | 727/820 [04:24<00:27,  3.42it/s] 89%| | 728/820 [04:25<00:26,  3.47it/s] 89%| | 729/820 [04:25<00:25,  3.52it/s] 89%| | 730/820 [04:25<00:25,  3.54it/s] 89%| | 731/820 [04:25<00:24,  3.56it/s] 89%| | 732/820 [04:26<00:24,  3.57it/s] 89%| | 733/820 [04:26<00:24,  3.59it/s] 90%| | 734/820 [04:26<00:23,  3.59it/s] 90%| | 735/820 [04:26<00:23,  3.60it/s] 90%| | 736/820 [04:27<00:23,  3.60it/s] 90%| | 737/820 [04:27<00:23,  3.60it/s] 90%| | 738/820 [04:27<00:23,  3.55it/s] 90%| | 739/820 [04:28<00:22,  3.56it/s] 90%| | 740/820 [04:28<00:22,  3.58it/s] 90%| | 741/820 [04:28<00:22,  3.59it/s] 90%| | 742/820 [04:28<00:21,  3.59it/s] 91%| | 743/820 [04:29<00:21,  3.60it/s] 91%| | 744/820 [04:29<00:21,  3.60it/s] 91%| | 745/820 [04:29<00:20,  3.60it/s] 91%| | 746/820 [04:30<00:20,  3.60it/s] 91%| | 747/820 [04:30<00:20,  3.61it/s] 91%| | 748/820 [04:30<00:19,  3.60it/s] 91%|| 749/820 [04:30<00:19,  3.58it/s] 91%|| 750/820 [04:31<00:19,  3.59it/s] 92%|| 751/820 [04:31<00:19,  3.59it/s] 92%|| 752/820 [04:31<00:18,  3.59it/s] 92%|| 753/820 [04:31<00:18,  3.60it/s] 92%|| 754/820 [04:32<00:18,  3.60it/s] 92%|| 755/820 [04:32<00:18,  3.60it/s] 92%|| 756/820 [04:32<00:17,  3.60it/s] 92%|| 757/820 [04:33<00:17,  3.61it/s] 92%|| 758/820 [04:33<00:17,  3.60it/s] 93%|| 759/820 [04:33<00:16,  3.61it/s] 93%|| 760/820 [04:33<00:16,  3.60it/s] 93%|| 761/820 [04:34<00:16,  3.60it/s] 93%|| 762/820 [04:34<00:16,  3.60it/s] 93%|| 763/820 [04:34<00:15,  3.60it/s] 93%|| 764/820 [04:35<00:15,  3.60it/s] 93%|| 765/820 [04:35<00:15,  3.60it/s] 93%|| 766/820 [04:35<00:15,  3.60it/s] 94%|| 767/820 [04:35<00:14,  3.60it/s] 94%|| 768/820 [04:36<00:14,  3.60it/s] 94%|| 769/820 [04:36<00:14,  3.60it/s] 94%|| 770/820 [04:36<00:13,  3.60it/s] 94%|| 771/820 [04:37<00:13,  3.53it/s] 94%|| 772/820 [04:37<00:13,  3.55it/s] 94%|| 773/820 [04:37<00:13,  3.57it/s] 94%|| 774/820 [04:37<00:12,  3.58it/s] 95%|| 775/820 [04:38<00:12,  3.59it/s] 95%|| 776/820 [04:38<00:12,  3.59it/s] 95%|| 777/820 [04:38<00:11,  3.60it/s] 95%|| 778/820 [04:38<00:11,  3.60it/s] 95%|| 779/820 [04:39<00:11,  3.60it/s] 95%|| 780/820 [04:39<00:11,  3.60it/s] 95%|| 781/820 [04:39<00:10,  3.60it/s] 95%|| 782/820 [04:40<00:10,  3.52it/s] 95%|| 783/820 [04:40<00:10,  3.54it/s] 96%|| 784/820 [04:40<00:10,  3.55it/s] 96%|| 785/820 [04:40<00:09,  3.57it/s] 96%|| 786/820 [04:41<00:09,  3.58it/s] 96%|| 787/820 [04:41<00:09,  3.59it/s] 96%|| 788/820 [04:41<00:08,  3.59it/s] 96%|| 789/820 [04:42<00:08,  3.60it/s] 96%|| 790/820 [04:42<00:08,  3.60it/s] 96%|| 791/820 [04:42<00:08,  3.60it/s] 97%|| 792/820 [04:42<00:07,  3.60it/s] 97%|| 793/820 [04:43<00:07,  3.49it/s] 97%|| 794/820 [04:43<00:07,  3.52it/s] 97%|| 795/820 [04:43<00:07,  3.54it/s] 97%|| 796/820 [04:43<00:06,  3.56it/s] 97%|| 797/820 [04:44<00:06,  3.57it/s] 97%|| 798/820 [04:44<00:06,  3.58it/s] 97%|| 799/820 [04:44<00:05,  3.59it/s] 98%|| 800/820 [04:45<00:05,  3.59it/s] 98%|| 801/820 [04:45<00:05,  3.60it/s] 98%|| 802/820 [04:45<00:05,  3.60it/s] 98%|| 803/820 [04:45<00:04,  3.60it/s] 98%|| 804/820 [04:46<00:04,  3.52it/s] 98%|| 805/820 [04:46<00:04,  3.54it/s] 98%|| 806/820 [04:46<00:03,  3.56it/s] 98%|| 807/820 [04:47<00:03,  3.57it/s] 99%|| 808/820 [04:47<00:03,  3.58it/s] 99%|| 809/820 [04:47<00:03,  3.59it/s] 99%|| 810/820 [04:47<00:02,  3.59it/s] 99%|| 811/820 [04:48<00:02,  3.60it/s] 99%|| 812/820 [04:48<00:02,  3.60it/s] 99%|| 813/820 [04:48<00:01,  3.60it/s] 99%|| 814/820 [04:49<00:01,  3.60it/s] 99%|| 815/820 [04:49<00:01,  3.51it/s]100%|| 816/820 [04:49<00:01,  3.54it/s]100%|| 817/820 [04:49<00:00,  3.56it/s]100%|| 818/820 [04:50<00:00,  3.57it/s]100%|| 819/820 [04:50<00:00,  3.58it/s]100%|| 820/820 [04:50<00:00,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:54:17,962 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:54:17,962 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 06:54:17,962 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 10.0282, 'eval_samples_per_second': 347.222, 'eval_steps_per_second': 43.478, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.37it/s][A
  3%|         | 12/436 [00:00<00:08, 49.03it/s][A
  4%|         | 17/436 [00:00<00:08, 47.40it/s][A
  5%|         | 22/436 [00:00<00:08, 46.27it/s][A
  6%|         | 27/436 [00:00<00:08, 45.83it/s][A
  7%|         | 32/436 [00:00<00:08, 45.17it/s][A
  8%|         | 37/436 [00:00<00:08, 44.62it/s][A
 10%|         | 42/436 [00:00<00:08, 44.60it/s][A
 11%|         | 47/436 [00:01<00:08, 44.69it/s][A
 12%|        | 52/436 [00:01<00:08, 44.81it/s][A
 13%|        | 57/436 [00:01<00:08, 44.95it/s][A
 14%|        | 62/436 [00:01<00:08, 44.86it/s][A
 15%|        | 67/436 [00:01<00:08, 44.08it/s][A
 17%|        | 72/436 [00:01<00:08, 44.26it/s][A
 18%|        | 77/436 [00:01<00:08, 44.10it/s][A
 19%|        | 82/436 [00:01<00:08, 44.13it/s][A
 20%|        | 87/436 [00:01<00:07, 44.18it/s][A
 21%|        | 92/436 [00:02<00:07, 44.35it/s][A
 22%|       | 97/436 [00:02<00:07, 44.56it/s][A
 23%|       | 102/436 [00:02<00:07, 44.77it/s][A
 25%|       | 107/436 [00:02<00:07, 44.79it/s][A
 26%|       | 112/436 [00:02<00:07, 44.75it/s][A
 27%|       | 117/436 [00:02<00:07, 44.71it/s][A
 28%|       | 122/436 [00:02<00:07, 44.63it/s][A
 29%|       | 127/436 [00:02<00:06, 44.40it/s][A
 30%|       | 132/436 [00:02<00:06, 44.41it/s][A
 31%|      | 137/436 [00:03<00:06, 44.48it/s][A
 33%|      | 142/436 [00:03<00:06, 44.58it/s][A
 34%|      | 147/436 [00:03<00:06, 44.81it/s][A
 35%|      | 152/436 [00:03<00:06, 44.81it/s][A
 36%|      | 157/436 [00:03<00:06, 44.92it/s][A
 37%|      | 162/436 [00:03<00:06, 44.73it/s][A
 38%|      | 167/436 [00:03<00:06, 44.51it/s][A
 39%|      | 172/436 [00:03<00:05, 44.49it/s][A
 41%|      | 177/436 [00:03<00:05, 44.31it/s][A
 42%|     | 182/436 [00:04<00:05, 44.43it/s][A
 43%|     | 187/436 [00:04<00:05, 44.68it/s][A
 44%|     | 192/436 [00:04<00:05, 44.64it/s][A
 45%|     | 197/436 [00:04<00:05, 44.84it/s][A
 46%|     | 202/436 [00:04<00:05, 43.60it/s][A
 47%|     | 207/436 [00:04<00:05, 44.01it/s][A
 49%|     | 212/436 [00:04<00:05, 44.09it/s][A
 50%|     | 217/436 [00:04<00:04, 44.09it/s][A
 51%|     | 222/436 [00:04<00:04, 44.16it/s][A
 52%|    | 227/436 [00:05<00:04, 44.33it/s][A
 53%|    | 232/436 [00:05<00:04, 44.54it/s][A
 54%|    | 237/436 [00:05<00:04, 44.69it/s][A
 56%|    | 242/436 [00:05<00:04, 44.57it/s][A
 57%|    | 247/436 [00:05<00:04, 44.66it/s][A
 58%|    | 252/436 [00:05<00:04, 44.80it/s][A
 59%|    | 257/436 [00:05<00:04, 44.70it/s][A
 60%|    | 262/436 [00:05<00:03, 44.54it/s][A
 61%|    | 267/436 [00:05<00:03, 44.36it/s][A
 62%|   | 272/436 [00:06<00:03, 44.41it/s][A
 64%|   | 277/436 [00:06<00:03, 44.44it/s][A
 65%|   | 282/436 [00:06<00:03, 44.66it/s][A
 66%|   | 287/436 [00:06<00:03, 44.72it/s][A
 67%|   | 292/436 [00:06<00:03, 44.68it/s][A
 68%|   | 297/436 [00:06<00:03, 44.69it/s][A
 69%|   | 302/436 [00:06<00:02, 44.70it/s][A
 70%|   | 307/436 [00:06<00:02, 44.68it/s][A
 72%|  | 312/436 [00:06<00:02, 44.53it/s][A
 73%|  | 317/436 [00:07<00:02, 44.61it/s][A
 74%|  | 322/436 [00:07<00:02, 44.45it/s][A
 75%|  | 327/436 [00:07<00:02, 44.53it/s][A
 76%|  | 332/436 [00:07<00:02, 44.71it/s][A
 77%|  | 337/436 [00:07<00:02, 42.79it/s][A
 78%|  | 342/436 [00:07<00:02, 43.61it/s][A
 80%|  | 347/436 [00:07<00:02, 43.86it/s][A
 81%|  | 352/436 [00:07<00:01, 44.18it/s][A
 82%| | 357/436 [00:08<00:01, 44.25it/s][A
 83%| | 362/436 [00:08<00:01, 44.36it/s][A
 84%| | 367/436 [00:08<00:01, 44.49it/s][A
 85%| | 372/436 [00:08<00:01, 44.39it/s][A
 86%| | 377/436 [00:08<00:01, 44.31it/s][A
 88%| | 382/436 [00:08<00:01, 44.31it/s][A
 89%| | 387/436 [00:08<00:01, 44.64it/s][A
 90%| | 392/436 [00:08<00:00, 44.78it/s][A
 91%| | 397/436 [00:08<00:00, 44.69it/s][A
 92%|| 402/436 [00:09<00:00, 44.62it/s][A
 93%|| 407/436 [00:09<00:00, 44.56it/s][A
 94%|| 412/436 [00:09<00:00, 44.73it/s][A
 96%|| 417/436 [00:09<00:00, 44.63it/s][A
 97%|| 422/436 [00:09<00:00, 44.39it/s][A
 98%|| 427/436 [00:09<00:00, 44.55it/s][A
 99%|| 432/436 [00:09<00:00, 44.58it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.58it/s][A100%|| 820/820 [05:00<00:00,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:54:27,924 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 06:54:28,004 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:54:30,762 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:54:30,927 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:54:30,995 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:54:31,637 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:54:31,638 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164 (score: 1.1192312240600586).
                                                 100%|| 820/820 [05:11<00:00,  3.59it/s]100%|| 820/820 [05:11<00:00,  2.63it/s]
[INFO|trainer.py:1894] 2023-08-28 06:54:38,886 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 06:54:39,008 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:54:41,931 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:54:42,049 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:54:42,106 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:54:42,569 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:42,569 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:42,569 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:42,569 >>   train_runtime            = 0:05:11.60
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:42,570 >>   train_samples            =      10520
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:42,570 >>   train_samples_per_second =    168.805
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:42,570 >>   train_steps_per_second   =      2.632
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7869, 'eval_samples_per_second': 355.782, 'eval_steps_per_second': 44.549, 'epoch': 5.0}
{'train_runtime': 311.6018, 'train_samples_per_second': 168.805, 'train_steps_per_second': 2.632, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:54:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:54:42,902 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:54:42,902 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 06:54:42,902 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 54.99it/s]  3%|         | 12/436 [00:00<00:08, 49.15it/s]  4%|         | 17/436 [00:00<00:08, 47.37it/s]  5%|         | 22/436 [00:00<00:08, 46.57it/s]  6%|         | 27/436 [00:00<00:08, 46.05it/s]  7%|         | 32/436 [00:00<00:08, 45.83it/s]  8%|         | 37/436 [00:00<00:08, 45.64it/s] 10%|         | 42/436 [00:00<00:08, 45.19it/s] 11%|         | 47/436 [00:01<00:08, 44.46it/s] 12%|        | 52/436 [00:01<00:08, 44.42it/s] 13%|        | 57/436 [00:01<00:08, 44.43it/s] 14%|        | 62/436 [00:01<00:08, 44.58it/s] 15%|        | 67/436 [00:01<00:08, 44.76it/s] 17%|        | 72/436 [00:01<00:08, 44.93it/s] 18%|        | 77/436 [00:01<00:07, 44.94it/s] 19%|        | 82/436 [00:01<00:07, 45.04it/s] 20%|        | 87/436 [00:01<00:07, 44.78it/s] 21%|        | 92/436 [00:02<00:07, 44.46it/s] 22%|       | 97/436 [00:02<00:07, 44.42it/s] 23%|       | 102/436 [00:02<00:07, 44.48it/s] 25%|       | 107/436 [00:02<00:07, 44.51it/s] 26%|       | 112/436 [00:02<00:07, 44.66it/s] 27%|       | 117/436 [00:02<00:07, 44.79it/s] 28%|       | 122/436 [00:02<00:06, 44.94it/s] 29%|       | 127/436 [00:02<00:06, 44.92it/s] 30%|       | 132/436 [00:02<00:06, 44.78it/s] 31%|      | 137/436 [00:03<00:06, 44.53it/s] 33%|      | 142/436 [00:03<00:06, 44.41it/s] 34%|      | 147/436 [00:03<00:06, 44.58it/s] 35%|      | 152/436 [00:03<00:06, 44.63it/s] 36%|      | 157/436 [00:03<00:06, 44.64it/s] 37%|      | 162/436 [00:03<00:06, 44.70it/s] 38%|      | 167/436 [00:03<00:06, 44.79it/s] 39%|      | 172/436 [00:03<00:05, 44.90it/s] 41%|      | 177/436 [00:03<00:05, 44.71it/s] 42%|     | 182/436 [00:04<00:05, 44.65it/s] 43%|     | 187/436 [00:04<00:05, 44.50it/s] 44%|     | 192/436 [00:04<00:05, 44.46it/s] 45%|     | 197/436 [00:04<00:05, 44.67it/s] 46%|     | 202/436 [00:04<00:05, 44.67it/s] 47%|     | 207/436 [00:04<00:05, 42.81it/s] 49%|     | 212/436 [00:04<00:05, 43.58it/s] 50%|     | 217/436 [00:04<00:04, 44.07it/s] 51%|     | 222/436 [00:04<00:04, 44.24it/s] 52%|    | 227/436 [00:05<00:04, 44.34it/s] 53%|    | 232/436 [00:05<00:04, 41.72it/s] 54%|    | 237/436 [00:05<00:04, 42.70it/s] 56%|    | 242/436 [00:05<00:04, 43.42it/s] 57%|    | 247/436 [00:05<00:04, 43.66it/s] 58%|    | 252/436 [00:05<00:04, 43.89it/s] 59%|    | 257/436 [00:05<00:04, 44.30it/s] 60%|    | 262/436 [00:05<00:03, 44.41it/s] 61%|    | 267/436 [00:05<00:03, 44.57it/s] 62%|   | 272/436 [00:06<00:03, 44.30it/s] 64%|   | 277/436 [00:06<00:03, 44.27it/s] 65%|   | 282/436 [00:06<00:03, 44.65it/s] 66%|   | 287/436 [00:06<00:03, 44.70it/s] 67%|   | 292/436 [00:06<00:03, 44.62it/s] 68%|   | 297/436 [00:06<00:03, 44.58it/s] 69%|   | 302/436 [00:06<00:03, 44.58it/s] 70%|   | 307/436 [00:06<00:02, 44.71it/s] 72%|  | 312/436 [00:06<00:02, 44.59it/s] 73%|  | 317/436 [00:07<00:02, 44.48it/s] 74%|  | 322/436 [00:07<00:02, 41.73it/s] 75%|  | 327/436 [00:07<00:02, 40.79it/s] 76%|  | 332/436 [00:07<00:02, 42.09it/s] 77%|  | 337/436 [00:07<00:02, 43.01it/s] 78%|  | 342/436 [00:07<00:02, 43.61it/s] 80%|  | 347/436 [00:07<00:02, 44.06it/s] 81%|  | 352/436 [00:07<00:01, 44.34it/s] 82%| | 357/436 [00:08<00:01, 44.43it/s] 83%| | 362/436 [00:08<00:01, 44.29it/s] 84%| | 367/436 [00:08<00:01, 41.76it/s] 85%| | 372/436 [00:08<00:01, 42.78it/s] 86%| | 377/436 [00:08<00:01, 43.45it/s] 88%| | 382/436 [00:08<00:01, 44.03it/s] 89%| | 387/436 [00:08<00:01, 44.24it/s] 90%| | 392/436 [00:08<00:00, 44.55it/s] 91%| | 397/436 [00:08<00:00, 44.53it/s] 92%|| 402/436 [00:09<00:00, 44.50it/s] 93%|| 407/436 [00:09<00:00, 44.26it/s] 94%|| 412/436 [00:09<00:00, 44.02it/s] 96%|| 417/436 [00:09<00:00, 44.27it/s] 97%|| 422/436 [00:09<00:00, 44.62it/s] 98%|| 427/436 [00:09<00:00, 44.61it/s] 99%|| 432/436 [00:09<00:00, 44.84it/s]100%|| 436/436 [00:09<00:00, 44.37it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:54:52,747 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   eval_loss               =     1.1192
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   eval_runtime            = 0:00:09.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   eval_samples_per_second =    353.679
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   eval_steps_per_second   =     44.286
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:52,747 >>   perplexity              =     3.0625
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:02,365 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:02,367 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:02,367 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:02,367 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:02,367 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:55:02,828 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:55:02,829 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:55:03,100 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:55:04,181 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:55:04,181 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:06,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:06,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:06,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:06,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:06,671 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:55:07,037 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:55:07,038 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:55:07,349 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:55:07,526 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:55:07,526 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-656
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/checkpoint-820
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:07,  1.61it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.70it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:16,  1.60it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:20,  1.60it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:24,  1.66it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:27,  1.67it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:29,  1.58it/s]Extractor Predicting: 50it [00:30,  1.58it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:31,  1.62it/s]Extractor Predicting: 53it [00:32,  1.63it/s]Extractor Predicting: 54it [00:32,  1.58it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:35,  1.61it/s]Extractor Predicting: 59it [00:36,  1.59it/s]Extractor Predicting: 60it [00:36,  1.59it/s]Extractor Predicting: 61it [00:37,  1.62it/s]Extractor Predicting: 62it [00:37,  1.64it/s]Extractor Predicting: 63it [00:38,  1.59it/s]Extractor Predicting: 64it [00:39,  1.54it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:40,  1.60it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.64it/s]Extractor Predicting: 69it [00:42,  1.61it/s]Extractor Predicting: 70it [00:42,  1.67it/s]Extractor Predicting: 71it [00:43,  1.68it/s]Extractor Predicting: 72it [00:43,  1.68it/s]Extractor Predicting: 73it [00:44,  1.75it/s]Extractor Predicting: 74it [00:45,  1.75it/s]Extractor Predicting: 75it [00:45,  1.67it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:46,  1.65it/s]Extractor Predicting: 78it [00:47,  1.66it/s]Extractor Predicting: 79it [00:48,  1.64it/s]Extractor Predicting: 80it [00:48,  1.61it/s]Extractor Predicting: 81it [00:49,  1.61it/s]Extractor Predicting: 82it [00:50,  1.62it/s]Extractor Predicting: 83it [00:50,  1.64it/s]Extractor Predicting: 84it [00:51,  1.61it/s]Extractor Predicting: 85it [00:51,  1.64it/s]Extractor Predicting: 86it [00:52,  1.61it/s]Extractor Predicting: 87it [00:53,  1.65it/s]Extractor Predicting: 88it [00:53,  1.63it/s]Extractor Predicting: 89it [00:54,  1.61it/s]Extractor Predicting: 90it [00:54,  1.59it/s]Extractor Predicting: 91it [00:55,  1.57it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:56,  1.59it/s]Extractor Predicting: 94it [00:57,  1.57it/s]Extractor Predicting: 95it [00:58,  1.47it/s]Extractor Predicting: 96it [00:58,  1.52it/s]Extractor Predicting: 97it [00:59,  1.54it/s]Extractor Predicting: 98it [01:00,  1.52it/s]Extractor Predicting: 99it [01:00,  1.50it/s]Extractor Predicting: 100it [01:01,  1.50it/s]Extractor Predicting: 101it [01:02,  1.54it/s]Extractor Predicting: 102it [01:02,  1.54it/s]Extractor Predicting: 103it [01:03,  1.51it/s]Extractor Predicting: 104it [01:04,  1.56it/s]Extractor Predicting: 105it [01:04,  1.58it/s]Extractor Predicting: 106it [01:05,  1.54it/s]Extractor Predicting: 107it [01:06,  1.57it/s]Extractor Predicting: 108it [01:06,  1.59it/s]Extractor Predicting: 109it [01:07,  1.64it/s]Extractor Predicting: 110it [01:07,  1.65it/s]Extractor Predicting: 111it [01:08,  1.71it/s]Extractor Predicting: 112it [01:08,  1.69it/s]Extractor Predicting: 113it [01:09,  1.68it/s]Extractor Predicting: 114it [01:10,  1.63it/s]Extractor Predicting: 115it [01:10,  1.61it/s]Extractor Predicting: 116it [01:11,  1.62it/s]Extractor Predicting: 117it [01:12,  1.61it/s]Extractor Predicting: 118it [01:12,  1.61it/s]Extractor Predicting: 119it [01:13,  1.60it/s]Extractor Predicting: 120it [01:13,  1.64it/s]Extractor Predicting: 121it [01:14,  1.62it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:15,  1.61it/s]Extractor Predicting: 124it [01:16,  1.62it/s]Extractor Predicting: 125it [01:17,  1.62it/s]Extractor Predicting: 126it [01:17,  1.64it/s]Extractor Predicting: 127it [01:18,  1.63it/s]Extractor Predicting: 128it [01:18,  1.62it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.58it/s]Extractor Predicting: 131it [01:20,  1.61it/s]Extractor Predicting: 132it [01:21,  1.64it/s]Extractor Predicting: 133it [01:22,  1.58it/s]Extractor Predicting: 134it [01:22,  1.51it/s]Extractor Predicting: 135it [01:23,  1.55it/s]Extractor Predicting: 136it [01:23,  1.57it/s]Extractor Predicting: 137it [01:24,  1.58it/s]Extractor Predicting: 138it [01:25,  1.60it/s]Extractor Predicting: 139it [01:25,  1.58it/s]Extractor Predicting: 140it [01:26,  1.56it/s]Extractor Predicting: 141it [01:27,  1.56it/s]Extractor Predicting: 142it [01:27,  1.59it/s]Extractor Predicting: 143it [01:28,  1.64it/s]Extractor Predicting: 143it [01:28,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:48,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:48,064 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:48,064 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:48,064 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:48,064 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:56:48,703 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:56:48,704 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:56:49,308 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:56:50,355 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:56:50,355 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:53,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:53,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:53,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:53,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:53,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:56:54,264 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:56:54,265 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:56:54,853 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:56:55,024 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:56:55,024 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.79it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.68it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:06,  1.75it/s]Extractor Predicting: 13it [00:07,  1.77it/s]Extractor Predicting: 14it [00:08,  1.74it/s]Extractor Predicting: 15it [00:08,  1.73it/s]Extractor Predicting: 16it [00:09,  1.76it/s]Extractor Predicting: 17it [00:09,  1.77it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.67it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.78it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:14,  1.81it/s]Extractor Predicting: 27it [00:15,  1.80it/s]Extractor Predicting: 28it [00:15,  1.82it/s]Extractor Predicting: 29it [00:16,  1.83it/s]Extractor Predicting: 30it [00:17,  1.76it/s]Extractor Predicting: 31it [00:17,  1.71it/s]Extractor Predicting: 32it [00:18,  1.63it/s]Extractor Predicting: 33it [00:19,  1.63it/s]Extractor Predicting: 34it [00:19,  1.63it/s]Extractor Predicting: 35it [00:20,  1.62it/s]Extractor Predicting: 36it [00:20,  1.63it/s]Extractor Predicting: 37it [00:21,  1.45it/s]Extractor Predicting: 38it [00:22,  1.50it/s]Extractor Predicting: 39it [00:22,  1.55it/s]Extractor Predicting: 40it [00:23,  1.56it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:24,  1.61it/s]Extractor Predicting: 43it [00:25,  1.59it/s]Extractor Predicting: 44it [00:26,  1.61it/s]Extractor Predicting: 45it [00:26,  1.63it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:27,  1.64it/s]Extractor Predicting: 48it [00:28,  1.62it/s]Extractor Predicting: 49it [00:29,  1.62it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:30,  1.64it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:31,  1.65it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:32,  1.64it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:33,  1.66it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:35,  1.66it/s]Extractor Predicting: 61it [00:36,  1.64it/s]Extractor Predicting: 62it [00:36,  1.66it/s]Extractor Predicting: 63it [00:37,  1.71it/s]Extractor Predicting: 64it [00:38,  1.71it/s]Extractor Predicting: 65it [00:38,  1.68it/s]Extractor Predicting: 66it [00:39,  1.63it/s]Extractor Predicting: 67it [00:39,  1.67it/s]Extractor Predicting: 68it [00:40,  1.65it/s]Extractor Predicting: 69it [00:41,  1.68it/s]Extractor Predicting: 70it [00:41,  1.69it/s]Extractor Predicting: 71it [00:42,  1.67it/s]Extractor Predicting: 72it [00:42,  1.66it/s]Extractor Predicting: 73it [00:43,  1.62it/s]Extractor Predicting: 74it [00:44,  1.65it/s]Extractor Predicting: 75it [00:44,  1.68it/s]Extractor Predicting: 76it [00:45,  1.69it/s]Extractor Predicting: 77it [00:45,  1.72it/s]Extractor Predicting: 78it [00:46,  1.76it/s]Extractor Predicting: 79it [00:46,  1.75it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:48,  1.69it/s]Extractor Predicting: 82it [00:48,  1.70it/s]Extractor Predicting: 83it [00:49,  1.68it/s]Extractor Predicting: 84it [00:49,  1.67it/s]Extractor Predicting: 85it [00:50,  1.66it/s]Extractor Predicting: 86it [00:51,  1.68it/s]Extractor Predicting: 87it [00:51,  1.70it/s]Extractor Predicting: 88it [00:52,  1.70it/s]Extractor Predicting: 89it [00:52,  1.75it/s]Extractor Predicting: 90it [00:53,  1.74it/s]Extractor Predicting: 91it [00:54,  1.70it/s]Extractor Predicting: 92it [00:54,  1.66it/s]Extractor Predicting: 93it [00:55,  1.67it/s]Extractor Predicting: 94it [00:55,  1.64it/s]Extractor Predicting: 95it [00:56,  1.64it/s]Extractor Predicting: 96it [00:57,  1.63it/s]Extractor Predicting: 97it [00:57,  1.65it/s]Extractor Predicting: 98it [00:58,  1.63it/s]Extractor Predicting: 99it [00:58,  1.62it/s]Extractor Predicting: 100it [00:59,  1.44it/s]Extractor Predicting: 101it [01:00,  1.48it/s]Extractor Predicting: 102it [01:01,  1.54it/s]Extractor Predicting: 103it [01:01,  1.58it/s]Extractor Predicting: 104it [01:02,  1.59it/s]Extractor Predicting: 105it [01:02,  1.59it/s]Extractor Predicting: 106it [01:03,  1.61it/s]Extractor Predicting: 107it [01:04,  1.60it/s]Extractor Predicting: 108it [01:04,  1.63it/s]Extractor Predicting: 109it [01:05,  1.61it/s]Extractor Predicting: 110it [01:05,  1.61it/s]Extractor Predicting: 111it [01:06,  1.62it/s]Extractor Predicting: 112it [01:07,  1.61it/s]Extractor Predicting: 113it [01:07,  1.63it/s]Extractor Predicting: 114it [01:08,  1.61it/s]Extractor Predicting: 115it [01:09,  1.61it/s]Extractor Predicting: 116it [01:09,  1.69it/s]Extractor Predicting: 117it [01:10,  1.79it/s]Extractor Predicting: 118it [01:10,  1.83it/s]Extractor Predicting: 119it [01:11,  1.82it/s]Extractor Predicting: 120it [01:11,  1.79it/s]Extractor Predicting: 121it [01:12,  1.77it/s]Extractor Predicting: 122it [01:12,  1.75it/s]Extractor Predicting: 123it [01:13,  1.74it/s]Extractor Predicting: 124it [01:14,  1.78it/s]Extractor Predicting: 125it [01:14,  1.85it/s]Extractor Predicting: 126it [01:15,  1.81it/s]Extractor Predicting: 127it [01:15,  1.83it/s]Extractor Predicting: 128it [01:16,  1.86it/s]Extractor Predicting: 129it [01:16,  1.83it/s]Extractor Predicting: 130it [01:17,  1.82it/s]Extractor Predicting: 131it [01:17,  1.84it/s]Extractor Predicting: 132it [01:18,  1.84it/s]Extractor Predicting: 133it [01:18,  1.82it/s]Extractor Predicting: 134it [01:19,  1.82it/s]Extractor Predicting: 135it [01:19,  1.85it/s]Extractor Predicting: 136it [01:20,  1.89it/s]Extractor Predicting: 137it [01:21,  1.89it/s]Extractor Predicting: 138it [01:21,  1.85it/s]Extractor Predicting: 139it [01:22,  1.81it/s]Extractor Predicting: 140it [01:22,  1.83it/s]Extractor Predicting: 141it [01:23,  1.78it/s]Extractor Predicting: 142it [01:23,  1.78it/s]Extractor Predicting: 143it [01:24,  1.79it/s]Extractor Predicting: 144it [01:24,  1.77it/s]Extractor Predicting: 145it [01:25,  1.77it/s]Extractor Predicting: 146it [01:26,  1.78it/s]Extractor Predicting: 147it [01:26,  1.79it/s]Extractor Predicting: 148it [01:27,  1.76it/s]Extractor Predicting: 149it [01:27,  1.83it/s]Extractor Predicting: 150it [01:28,  1.80it/s]Extractor Predicting: 151it [01:28,  1.78it/s]Extractor Predicting: 152it [01:29,  1.81it/s]Extractor Predicting: 153it [01:29,  1.85it/s]Extractor Predicting: 154it [01:30,  1.86it/s]Extractor Predicting: 155it [01:30,  1.86it/s]Extractor Predicting: 156it [01:31,  1.84it/s]Extractor Predicting: 157it [01:32,  1.83it/s]Extractor Predicting: 158it [01:32,  1.84it/s]Extractor Predicting: 159it [01:33,  1.79it/s]Extractor Predicting: 160it [01:33,  1.80it/s]Extractor Predicting: 161it [01:34,  1.80it/s]Extractor Predicting: 162it [01:34,  1.77it/s]Extractor Predicting: 163it [01:35,  1.78it/s]Extractor Predicting: 164it [01:36,  1.78it/s]Extractor Predicting: 165it [01:36,  1.81it/s]Extractor Predicting: 166it [01:37,  1.81it/s]Extractor Predicting: 167it [01:37,  1.82it/s]Extractor Predicting: 168it [01:38,  1.77it/s]Extractor Predicting: 169it [01:38,  1.78it/s]Extractor Predicting: 170it [01:39,  1.74it/s]Extractor Predicting: 171it [01:40,  1.70it/s]Extractor Predicting: 172it [01:40,  1.78it/s]Extractor Predicting: 173it [01:41,  1.72it/s]Extractor Predicting: 174it [01:41,  1.71it/s]Extractor Predicting: 175it [01:42,  1.68it/s]Extractor Predicting: 176it [01:42,  1.69it/s]Extractor Predicting: 177it [01:43,  1.67it/s]Extractor Predicting: 178it [01:44,  1.65it/s]Extractor Predicting: 179it [01:44,  1.66it/s]Extractor Predicting: 180it [01:45,  1.68it/s]Extractor Predicting: 181it [01:45,  1.68it/s]Extractor Predicting: 182it [01:46,  1.66it/s]Extractor Predicting: 183it [01:47,  1.66it/s]Extractor Predicting: 184it [01:47,  1.64it/s]Extractor Predicting: 185it [01:48,  1.63it/s]Extractor Predicting: 186it [01:49,  1.63it/s]Extractor Predicting: 187it [01:49,  1.62it/s]Extractor Predicting: 188it [01:50,  1.65it/s]Extractor Predicting: 189it [01:50,  1.66it/s]Extractor Predicting: 190it [01:51,  1.67it/s]Extractor Predicting: 191it [01:52,  1.63it/s]Extractor Predicting: 192it [01:52,  1.64it/s]Extractor Predicting: 193it [01:53,  1.61it/s]Extractor Predicting: 194it [01:53,  1.61it/s]Extractor Predicting: 195it [01:54,  1.61it/s]Extractor Predicting: 196it [01:55,  1.58it/s]Extractor Predicting: 197it [01:55,  1.59it/s]Extractor Predicting: 198it [01:56,  1.60it/s]Extractor Predicting: 199it [01:57,  1.61it/s]Extractor Predicting: 200it [01:57,  1.60it/s]Extractor Predicting: 201it [01:58,  1.62it/s]Extractor Predicting: 202it [01:58,  1.63it/s]Extractor Predicting: 203it [01:59,  1.65it/s]Extractor Predicting: 204it [02:00,  1.67it/s]Extractor Predicting: 205it [02:00,  1.70it/s]Extractor Predicting: 206it [02:01,  1.69it/s]Extractor Predicting: 207it [02:01,  1.68it/s]Extractor Predicting: 208it [02:02,  1.68it/s]Extractor Predicting: 209it [02:03,  1.68it/s]Extractor Predicting: 210it [02:03,  1.70it/s]Extractor Predicting: 211it [02:04,  1.75it/s]Extractor Predicting: 212it [02:04,  1.75it/s]Extractor Predicting: 213it [02:05,  1.76it/s]Extractor Predicting: 214it [02:05,  1.74it/s]Extractor Predicting: 215it [02:06,  1.69it/s]Extractor Predicting: 216it [02:07,  1.71it/s]Extractor Predicting: 217it [02:07,  1.72it/s]Extractor Predicting: 218it [02:08,  1.75it/s]Extractor Predicting: 219it [02:08,  1.78it/s]Extractor Predicting: 220it [02:09,  1.75it/s]Extractor Predicting: 221it [02:09,  1.77it/s]Extractor Predicting: 222it [02:10,  1.74it/s]Extractor Predicting: 223it [02:11,  1.50it/s]Extractor Predicting: 224it [02:11,  1.57it/s]Extractor Predicting: 225it [02:12,  1.61it/s]Extractor Predicting: 226it [02:13,  1.60it/s]Extractor Predicting: 227it [02:13,  1.61it/s]Extractor Predicting: 228it [02:14,  1.63it/s]Extractor Predicting: 229it [02:14,  1.64it/s]Extractor Predicting: 230it [02:15,  1.62it/s]Extractor Predicting: 231it [02:16,  1.64it/s]Extractor Predicting: 232it [02:16,  1.68it/s]Extractor Predicting: 233it [02:17,  1.76it/s]Extractor Predicting: 234it [02:17,  1.78it/s]Extractor Predicting: 235it [02:18,  1.79it/s]Extractor Predicting: 236it [02:18,  1.84it/s]Extractor Predicting: 237it [02:19,  1.84it/s]Extractor Predicting: 238it [02:19,  1.84it/s]Extractor Predicting: 239it [02:20,  1.84it/s]Extractor Predicting: 240it [02:21,  1.84it/s]Extractor Predicting: 241it [02:21,  1.83it/s]Extractor Predicting: 242it [02:22,  1.84it/s]Extractor Predicting: 243it [02:22,  1.91it/s]Extractor Predicting: 244it [02:23,  1.88it/s]Extractor Predicting: 245it [02:23,  1.95it/s]Extractor Predicting: 246it [02:24,  1.99it/s]Extractor Predicting: 247it [02:24,  1.92it/s]Extractor Predicting: 248it [02:25,  1.86it/s]Extractor Predicting: 249it [02:25,  1.85it/s]Extractor Predicting: 250it [02:26,  1.86it/s]Extractor Predicting: 251it [02:26,  1.90it/s]Extractor Predicting: 252it [02:27,  1.92it/s]Extractor Predicting: 253it [02:27,  1.86it/s]Extractor Predicting: 254it [02:28,  1.85it/s]Extractor Predicting: 255it [02:28,  1.89it/s]Extractor Predicting: 256it [02:29,  1.89it/s]Extractor Predicting: 257it [02:30,  1.92it/s]Extractor Predicting: 258it [02:30,  1.92it/s]Extractor Predicting: 259it [02:31,  1.91it/s]Extractor Predicting: 260it [02:31,  1.89it/s]Extractor Predicting: 261it [02:32,  1.79it/s]Extractor Predicting: 262it [02:32,  1.80it/s]Extractor Predicting: 263it [02:33,  1.74it/s]Extractor Predicting: 264it [02:34,  1.69it/s]Extractor Predicting: 265it [02:34,  1.68it/s]Extractor Predicting: 266it [02:35,  1.69it/s]Extractor Predicting: 267it [02:35,  1.72it/s]Extractor Predicting: 268it [02:36,  1.70it/s]Extractor Predicting: 269it [02:37,  1.67it/s]Extractor Predicting: 270it [02:37,  1.64it/s]Extractor Predicting: 271it [02:38,  1.63it/s]Extractor Predicting: 272it [02:38,  1.66it/s]Extractor Predicting: 273it [02:39,  1.66it/s]Extractor Predicting: 274it [02:40,  1.65it/s]Extractor Predicting: 275it [02:40,  1.66it/s]Extractor Predicting: 276it [02:41,  1.64it/s]Extractor Predicting: 277it [02:41,  1.62it/s]Extractor Predicting: 278it [02:42,  1.63it/s]Extractor Predicting: 279it [02:43,  1.67it/s]Extractor Predicting: 280it [02:43,  1.65it/s]Extractor Predicting: 281it [02:44,  1.63it/s]Extractor Predicting: 282it [02:44,  1.63it/s]Extractor Predicting: 283it [02:45,  1.65it/s]Extractor Predicting: 284it [02:46,  1.64it/s]Extractor Predicting: 285it [02:46,  1.63it/s]Extractor Predicting: 286it [02:47,  1.63it/s]Extractor Predicting: 287it [02:48,  1.62it/s]Extractor Predicting: 288it [02:48,  1.68it/s]Extractor Predicting: 289it [02:49,  1.68it/s]Extractor Predicting: 290it [02:49,  1.70it/s]Extractor Predicting: 291it [02:50,  1.72it/s]Extractor Predicting: 292it [02:50,  1.72it/s]Extractor Predicting: 293it [02:51,  1.69it/s]Extractor Predicting: 294it [02:52,  1.72it/s]Extractor Predicting: 295it [02:52,  1.73it/s]Extractor Predicting: 296it [02:53,  1.71it/s]Extractor Predicting: 297it [02:53,  1.72it/s]Extractor Predicting: 298it [02:54,  1.77it/s]Extractor Predicting: 299it [02:54,  1.73it/s]Extractor Predicting: 300it [02:55,  1.73it/s]Extractor Predicting: 301it [02:56,  1.72it/s]Extractor Predicting: 302it [02:56,  1.67it/s]Extractor Predicting: 303it [02:57,  1.68it/s]Extractor Predicting: 304it [02:57,  1.67it/s]Extractor Predicting: 305it [02:58,  1.67it/s]Extractor Predicting: 306it [02:59,  1.70it/s]Extractor Predicting: 307it [02:59,  1.70it/s]Extractor Predicting: 308it [03:00,  1.69it/s]Extractor Predicting: 309it [03:00,  1.66it/s]Extractor Predicting: 310it [03:01,  1.68it/s]Extractor Predicting: 311it [03:02,  1.64it/s]Extractor Predicting: 312it [03:02,  1.67it/s]Extractor Predicting: 313it [03:03,  1.68it/s]Extractor Predicting: 314it [03:03,  1.68it/s]Extractor Predicting: 315it [03:04,  1.70it/s]Extractor Predicting: 316it [03:05,  1.70it/s]Extractor Predicting: 317it [03:05,  1.72it/s]Extractor Predicting: 318it [03:06,  1.76it/s]Extractor Predicting: 319it [03:06,  1.75it/s]Extractor Predicting: 320it [03:07,  1.70it/s]Extractor Predicting: 321it [03:07,  1.69it/s]Extractor Predicting: 322it [03:08,  1.69it/s]Extractor Predicting: 323it [03:09,  1.50it/s]Extractor Predicting: 324it [03:09,  1.56it/s]Extractor Predicting: 325it [03:10,  1.59it/s]Extractor Predicting: 326it [03:11,  1.64it/s]Extractor Predicting: 327it [03:11,  1.66it/s]Extractor Predicting: 328it [03:12,  1.64it/s]Extractor Predicting: 329it [03:12,  1.66it/s]Extractor Predicting: 330it [03:13,  1.66it/s]Extractor Predicting: 331it [03:14,  1.66it/s]Extractor Predicting: 332it [03:14,  1.70it/s]Extractor Predicting: 333it [03:15,  1.67it/s]Extractor Predicting: 334it [03:15,  1.72it/s]Extractor Predicting: 335it [03:16,  1.71it/s]Extractor Predicting: 336it [03:17,  1.71it/s]Extractor Predicting: 337it [03:17,  1.70it/s]Extractor Predicting: 338it [03:18,  1.71it/s]Extractor Predicting: 339it [03:18,  1.71it/s]Extractor Predicting: 340it [03:19,  1.70it/s]Extractor Predicting: 341it [03:19,  1.72it/s]Extractor Predicting: 342it [03:20,  1.74it/s]Extractor Predicting: 343it [03:21,  1.74it/s]Extractor Predicting: 344it [03:21,  1.78it/s]Extractor Predicting: 345it [03:22,  1.70it/s]Extractor Predicting: 346it [03:22,  1.71it/s]Extractor Predicting: 347it [03:23,  1.72it/s]Extractor Predicting: 348it [03:24,  1.69it/s]Extractor Predicting: 349it [03:24,  1.70it/s]Extractor Predicting: 350it [03:25,  1.68it/s]Extractor Predicting: 351it [03:25,  1.70it/s]Extractor Predicting: 352it [03:26,  1.69it/s]Extractor Predicting: 353it [03:27,  1.69it/s]Extractor Predicting: 354it [03:27,  1.66it/s]Extractor Predicting: 355it [03:28,  1.66it/s]Extractor Predicting: 356it [03:28,  1.68it/s]Extractor Predicting: 357it [03:29,  1.65it/s]Extractor Predicting: 358it [03:30,  1.68it/s]Extractor Predicting: 359it [03:30,  1.69it/s]Extractor Predicting: 360it [03:31,  1.70it/s]Extractor Predicting: 361it [03:31,  1.70it/s]Extractor Predicting: 362it [03:32,  1.73it/s]Extractor Predicting: 363it [03:32,  1.71it/s]Extractor Predicting: 364it [03:33,  1.74it/s]Extractor Predicting: 365it [03:34,  1.72it/s]Extractor Predicting: 366it [03:34,  1.73it/s]Extractor Predicting: 367it [03:35,  1.73it/s]Extractor Predicting: 368it [03:35,  1.74it/s]Extractor Predicting: 369it [03:36,  1.69it/s]Extractor Predicting: 370it [03:37,  1.69it/s]Extractor Predicting: 371it [03:37,  1.67it/s]Extractor Predicting: 372it [03:38,  1.69it/s]Extractor Predicting: 373it [03:38,  1.68it/s]Extractor Predicting: 374it [03:39,  1.72it/s]Extractor Predicting: 375it [03:39,  1.71it/s]Extractor Predicting: 376it [03:40,  1.68it/s]Extractor Predicting: 377it [03:41,  1.75it/s]Extractor Predicting: 378it [03:41,  1.75it/s]Extractor Predicting: 379it [03:42,  1.78it/s]Extractor Predicting: 380it [03:42,  1.70it/s]Extractor Predicting: 381it [03:43,  1.72it/s]Extractor Predicting: 382it [03:44,  1.71it/s]Extractor Predicting: 383it [03:44,  1.69it/s]Extractor Predicting: 384it [03:45,  1.68it/s]Extractor Predicting: 385it [03:45,  1.70it/s]Extractor Predicting: 386it [03:46,  1.68it/s]Extractor Predicting: 387it [03:46,  1.67it/s]Extractor Predicting: 388it [03:47,  1.61it/s]Extractor Predicting: 389it [03:48,  1.62it/s]Extractor Predicting: 390it [03:48,  1.64it/s]Extractor Predicting: 391it [03:49,  1.67it/s]Extractor Predicting: 392it [03:50,  1.69it/s]Extractor Predicting: 393it [03:50,  1.68it/s]Extractor Predicting: 394it [03:51,  1.69it/s]Extractor Predicting: 395it [03:51,  1.70it/s]Extractor Predicting: 396it [03:52,  1.67it/s]Extractor Predicting: 397it [03:53,  1.66it/s]Extractor Predicting: 398it [03:53,  1.65it/s]Extractor Predicting: 399it [03:54,  1.70it/s]Extractor Predicting: 400it [03:54,  1.67it/s]Extractor Predicting: 401it [03:55,  1.68it/s]Extractor Predicting: 402it [03:55,  1.68it/s]Extractor Predicting: 403it [03:56,  1.66it/s]Extractor Predicting: 404it [03:57,  1.70it/s]Extractor Predicting: 405it [03:57,  1.71it/s]Extractor Predicting: 406it [03:58,  1.69it/s]Extractor Predicting: 407it [03:58,  1.69it/s]Extractor Predicting: 408it [03:59,  1.68it/s]Extractor Predicting: 409it [04:00,  1.68it/s]Extractor Predicting: 410it [04:00,  1.70it/s]Extractor Predicting: 411it [04:01,  1.69it/s]Extractor Predicting: 412it [04:01,  1.73it/s]Extractor Predicting: 413it [04:02,  1.73it/s]Extractor Predicting: 414it [04:03,  1.72it/s]Extractor Predicting: 415it [04:03,  1.65it/s]Extractor Predicting: 416it [04:04,  1.70it/s]Extractor Predicting: 417it [04:04,  1.71it/s]Extractor Predicting: 418it [04:05,  1.75it/s]Extractor Predicting: 419it [04:05,  1.76it/s]Extractor Predicting: 420it [04:06,  1.78it/s]Extractor Predicting: 421it [04:07,  1.74it/s]Extractor Predicting: 422it [04:07,  1.75it/s]Extractor Predicting: 423it [04:08,  1.72it/s]Extractor Predicting: 424it [04:08,  1.73it/s]Extractor Predicting: 425it [04:09,  1.49it/s]Extractor Predicting: 426it [04:10,  1.58it/s]Extractor Predicting: 427it [04:10,  1.61it/s]Extractor Predicting: 428it [04:11,  1.60it/s]Extractor Predicting: 429it [04:11,  1.88it/s]Extractor Predicting: 429it [04:11,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:21,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:21,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:21,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:21,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:21,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:01:22,674 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:01:22,676 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:01:23,281 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:01:24,431 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:01:24,431 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:27,452 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:27,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:27,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:27,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:27,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:01:28,292 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:01:28,293 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:01:28,894 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:01:29,104 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:01:29,104 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:02,  1.83it/s]Extractor Predicting: 5it [00:02,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:01:33,445 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:33,447 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:01:33,501 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:33,502 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:01:33,520 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:01:41,573 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:01:41,598 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:01:41,706 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:41,707 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:01:41,757 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:41,799 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:41,799 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:41,799 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:41,799 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:41,799 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:41,799 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:01:42,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:42,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:43,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:44,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:44,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:45,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:45,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:46,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:47,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:47,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:48,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:49,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:49,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:50,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:50,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:51,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:52,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:52,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:53,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:54,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:54,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:55,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:56,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:56,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:57,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:58,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:15, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:58,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:59,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:59,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:00,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:01,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:01,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:02,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:03,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:03,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:04,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:04,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:05,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:06,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:06,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:08,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:10,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:12,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:33, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:12,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:13,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:15,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:16,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:19,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:21,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:21,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:22,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:23,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:24,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:25,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:27,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:28,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:28,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:30,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:30,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:31,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:32,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:50<04:56, 17.42s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:33,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:33,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:34,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:34,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:35,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:37,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:38,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:39,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:39,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:40,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:41,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:41,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:42,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:42,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:43,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:44,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:45,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:45,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:47,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:06<04:24, 16.54s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:48,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:48,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:49,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:52,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:54,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:55,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:56,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:56,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:57,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:58,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:58,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:59,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:00,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:02,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:04,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:23<04:10, 16.72s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:05,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:06,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:08,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:10,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:12,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:14,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:15,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:15,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:16,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:17,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:19,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:39<03:52, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:21,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:23,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:24,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:27,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:28,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:32,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:33,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:35,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:37,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:37,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:38,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:39,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:57<03:43, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:39,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:41,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:41,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:45,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:46,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:46,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:47,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:48,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:48,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:49,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:50,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:50,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:52,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:52,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:53,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:55,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:15<03:26, 17.22s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:57,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:57,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:58,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:59,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:59,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:01,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:02,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:02,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:03,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:04,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:04,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:05,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:06,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:07,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:07,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:08,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:09,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:09,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:10,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:11,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:11,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:12,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:13,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:14,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:14,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:33<03:13, 17.55s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:15,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:16,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:16,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:17,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:18,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:18,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:19,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:20,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:21,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:22,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:22,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:23,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:24,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:24,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:25,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:26,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:26,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:27,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:28,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:28,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:29,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:30,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:31,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:31,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:50<02:53, 17.35s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:32,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:33,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:33,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:34,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:35,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:35,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:38,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:38,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:39,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:40,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:41,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:42,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:42,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:43,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:44,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:44,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:45,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:45,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:46,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:47,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:48,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:48,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:49,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:08<02:38, 17.65s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:50,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:51,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:52,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:52,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:53,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:54,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:54,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:55,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:56,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:57,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:57,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:58,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:59,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:00,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:00,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:01,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:02,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:02,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:03,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:04,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:05,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:05,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:06,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:07,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:25<02:20, 17.52s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:08,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:08,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:09,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:10,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:10,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:11,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:12,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:12,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:13,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:14,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:15,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:15,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:16,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:17,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:17,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:18,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:19,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:20,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:21,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:21,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:22,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:23,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:23,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:42<02:00, 17.23s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:24,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:25,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:25,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:26,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:27,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:28,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:28,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:29,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:30,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:30,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:31,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:32,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:33,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:33,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:34,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:35,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:35,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:36,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:37,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:38,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:38,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:39,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:40,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:41,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:59<01:43, 17.20s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:41,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:42,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:43,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:43,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:44,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:45,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:45,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:46,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:46,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:47,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:48,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:48,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:49,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:50,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:50,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:51,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:52,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:52,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:53,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:54,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:55,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:55,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:56,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:57,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:57,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:16<01:24, 16.98s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:58,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:58,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:59,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:00,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:00,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:01,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:02,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:02,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:03,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:04,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:04,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:05,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:06,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:06,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:07,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:08,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:09,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:10,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:10,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:11,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:12,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:12,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:13,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:14,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:15,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:15,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:34<01:09, 17.42s/it][WARNING|generation_utils.py:914] 2023-08-28 07:06:16,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:17,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:18,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:18,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:19,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:20,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:20,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:21,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:22,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:22,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:23,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:24,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:25,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:25,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:26,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:27,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:27,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:28,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:29,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:29,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:30,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:31,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:31,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:50<00:50, 16.99s/it][WARNING|generation_utils.py:914] 2023-08-28 07:06:32,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:33,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:33,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:34,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:35,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:35,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:36,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:36,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:38,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:39,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:40,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:40,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:41,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:41,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:42,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:43,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:43,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:44,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:45,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:45,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:46,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:47,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:47,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:48,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:07<00:33, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-28 07:06:49,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:49,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:50,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:51,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:51,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:52,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:53,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:53,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:54,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:55,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:55,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:56,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:57,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:57,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:58,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:59,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:59,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:00,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:01,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:01,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:02,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:03,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:03,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:04,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:05,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:05,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:06,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:24<00:17, 17.08s/it][WARNING|generation_utils.py:914] 2023-08-28 07:07:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:07,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:08,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:08,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:09,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:10,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:10,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:11,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:12,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:13,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:13,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:14,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:15,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:15,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:16,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:17,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:17,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:18,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:19,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:19,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:20,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:21,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:22,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:40<00:00, 16.75s/it]Generating: 100%|| 20/20 [05:40<00:00, 17.04s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:30,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:31,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:31,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:31,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:31,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:07:31,420 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:07:31,421 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:07:31,706 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:07:32,891 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:07:32,891 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:34,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:34,762 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:34,762 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:34,762 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:07:34,762 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:07:35,512 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:07:35,513 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:07:36,113 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:07:36,348 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:07:36,349 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of Lpez 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of Lpez 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Dmontagne " , including Boulogne \'s " Montessemble des deux de Chteau des Gains " . Head Entity : Montessemble des deux de Chteau des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of Franois Renoul .\')', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.64it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:07,  1.54it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.62it/s]Extractor Estimating: 16it [00:10,  1.67it/s]Extractor Estimating: 17it [00:10,  1.70it/s]Extractor Estimating: 18it [00:11,  1.66it/s]Extractor Estimating: 19it [00:11,  1.67it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:13,  1.61it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:16,  1.68it/s]Extractor Estimating: 28it [00:17,  1.70it/s]Extractor Estimating: 29it [00:17,  1.64it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:19,  1.61it/s]Extractor Estimating: 33it [00:20,  1.57it/s]Extractor Estimating: 34it [00:21,  1.60it/s]Extractor Estimating: 35it [00:21,  1.57it/s]Extractor Estimating: 36it [00:22,  1.54it/s]Extractor Estimating: 37it [00:23,  1.51it/s]Extractor Estimating: 38it [00:23,  1.59it/s]Extractor Estimating: 39it [00:24,  1.54it/s]Extractor Estimating: 40it [00:25,  1.49it/s]Extractor Estimating: 41it [00:25,  1.56it/s]Extractor Estimating: 42it [00:26,  1.54it/s]Extractor Estimating: 43it [00:27,  1.54it/s]Extractor Estimating: 44it [00:27,  1.57it/s]Extractor Estimating: 45it [00:28,  1.56it/s]Extractor Estimating: 46it [00:28,  1.54it/s]Extractor Estimating: 47it [00:29,  1.59it/s]Extractor Estimating: 48it [00:30,  1.51it/s]Extractor Estimating: 49it [00:30,  1.56it/s]Extractor Estimating: 50it [00:31,  1.52it/s]Extractor Estimating: 51it [00:32,  1.52it/s]Extractor Estimating: 52it [00:32,  1.50it/s]Extractor Estimating: 53it [00:33,  1.41it/s]Extractor Estimating: 54it [00:34,  1.48it/s]Extractor Estimating: 55it [00:34,  1.53it/s]Extractor Estimating: 56it [00:35,  1.55it/s]Extractor Estimating: 57it [00:36,  1.50it/s]Extractor Estimating: 58it [00:36,  1.53it/s]Extractor Estimating: 59it [00:37,  1.55it/s]Extractor Estimating: 60it [00:38,  1.52it/s]Extractor Estimating: 61it [00:38,  1.52it/s]Extractor Estimating: 62it [00:39,  1.51it/s]Extractor Estimating: 63it [00:40,  1.54it/s]Extractor Estimating: 64it [00:40,  1.53it/s]Extractor Estimating: 65it [00:41,  1.49it/s]Extractor Estimating: 66it [00:42,  1.47it/s]Extractor Estimating: 67it [00:42,  1.47it/s]Extractor Estimating: 68it [00:43,  1.49it/s]Extractor Estimating: 69it [00:44,  1.47it/s]Extractor Estimating: 70it [00:44,  1.49it/s]Extractor Estimating: 71it [00:45,  1.53it/s]Extractor Estimating: 72it [00:46,  1.54it/s]Extractor Estimating: 73it [00:46,  1.52it/s]Extractor Estimating: 74it [00:47,  1.52it/s]Extractor Estimating: 75it [00:48,  1.53it/s]Extractor Estimating: 76it [00:48,  1.53it/s]Extractor Estimating: 77it [00:49,  1.42it/s]Extractor Estimating: 78it [00:50,  1.42it/s]Extractor Estimating: 79it [00:50,  1.48it/s]Extractor Estimating: 80it [00:51,  1.48it/s]Extractor Estimating: 81it [00:52,  1.49it/s]Extractor Estimating: 82it [00:52,  1.50it/s]Extractor Estimating: 83it [00:53,  1.54it/s]Extractor Estimating: 84it [00:54,  1.49it/s]Extractor Estimating: 85it [00:54,  1.50it/s]Extractor Estimating: 86it [00:55,  1.53it/s]Extractor Estimating: 87it [00:56,  1.54it/s]Extractor Estimating: 88it [00:56,  1.46it/s]Extractor Estimating: 89it [00:57,  1.49it/s]Extractor Estimating: 90it [00:58,  1.52it/s]Extractor Estimating: 91it [00:58,  1.60it/s]Extractor Estimating: 92it [00:59,  1.55it/s]Extractor Estimating: 93it [01:00,  1.49it/s]Extractor Estimating: 94it [01:00,  1.50it/s]Extractor Estimating: 95it [01:01,  1.52it/s]Extractor Estimating: 96it [01:02,  1.54it/s]Extractor Estimating: 97it [01:02,  1.47it/s]Extractor Estimating: 98it [01:03,  1.50it/s]Extractor Estimating: 99it [01:04,  1.52it/s]Extractor Estimating: 100it [01:04,  1.50it/s]Extractor Estimating: 101it [01:05,  1.50it/s]Extractor Estimating: 102it [01:06,  1.52it/s]Extractor Estimating: 103it [01:06,  1.60it/s]Extractor Estimating: 104it [01:07,  1.57it/s]Extractor Estimating: 105it [01:07,  1.60it/s]Extractor Estimating: 106it [01:08,  1.60it/s]Extractor Estimating: 107it [01:09,  1.61it/s]Extractor Estimating: 108it [01:09,  1.62it/s]Extractor Estimating: 109it [01:10,  1.64it/s]Extractor Estimating: 110it [01:10,  1.68it/s]Extractor Estimating: 111it [01:11,  1.57it/s]Extractor Estimating: 112it [01:12,  1.56it/s]Extractor Estimating: 113it [01:12,  1.60it/s]Extractor Estimating: 114it [01:13,  1.63it/s]Extractor Estimating: 115it [01:14,  1.54it/s]Extractor Estimating: 116it [01:14,  1.53it/s]Extractor Estimating: 117it [01:15,  1.50it/s]Extractor Estimating: 118it [01:16,  1.52it/s]Extractor Estimating: 119it [01:16,  1.54it/s]Extractor Estimating: 120it [01:17,  1.55it/s]Extractor Estimating: 121it [01:18,  1.60it/s]Extractor Estimating: 122it [01:18,  1.64it/s]Extractor Estimating: 123it [01:19,  1.62it/s]Extractor Estimating: 124it [01:19,  1.66it/s]Extractor Estimating: 125it [01:20,  1.63it/s]Extractor Estimating: 126it [01:21,  1.63it/s]Extractor Estimating: 127it [01:21,  1.52it/s]Extractor Estimating: 128it [01:22,  1.54it/s]Extractor Estimating: 129it [01:23,  1.52it/s]Extractor Estimating: 130it [01:23,  1.50it/s]Extractor Estimating: 131it [01:24,  1.53it/s]Extractor Estimating: 132it [01:25,  1.48it/s]Extractor Estimating: 133it [01:25,  1.50it/s]Extractor Estimating: 134it [01:26,  1.48it/s]Extractor Estimating: 135it [01:27,  1.47it/s]Extractor Estimating: 136it [01:28,  1.39it/s]Extractor Estimating: 137it [01:28,  1.44it/s]Extractor Estimating: 138it [01:29,  1.47it/s]Extractor Estimating: 139it [01:29,  1.53it/s]Extractor Estimating: 140it [01:30,  1.56it/s]Extractor Estimating: 141it [01:31,  1.48it/s]Extractor Estimating: 142it [01:31,  1.48it/s]Extractor Estimating: 143it [01:32,  1.49it/s]Extractor Estimating: 144it [01:33,  1.45it/s]Extractor Estimating: 145it [01:33,  1.48it/s]Extractor Estimating: 146it [01:34,  1.44it/s]Extractor Estimating: 147it [01:35,  1.47it/s]Extractor Estimating: 148it [01:36,  1.44it/s]Extractor Estimating: 149it [01:36,  1.49it/s]Extractor Estimating: 150it [01:37,  1.50it/s]Extractor Estimating: 151it [01:38,  1.49it/s]Extractor Estimating: 152it [01:38,  1.56it/s]Extractor Estimating: 153it [01:39,  1.61it/s]Extractor Estimating: 154it [01:39,  1.71it/s]Extractor Estimating: 155it [01:40,  1.72it/s]Extractor Estimating: 156it [01:40,  1.67it/s]Extractor Estimating: 157it [01:41,  1.67it/s]Extractor Estimating: 158it [01:42,  1.66it/s]Extractor Estimating: 159it [01:42,  1.64it/s]Extractor Estimating: 160it [01:43,  1.66it/s]Extractor Estimating: 161it [01:43,  1.70it/s]Extractor Estimating: 162it [01:44,  1.68it/s]Extractor Estimating: 163it [01:45,  1.71it/s]Extractor Estimating: 164it [01:45,  1.73it/s]Extractor Estimating: 165it [01:46,  1.70it/s]Extractor Estimating: 166it [01:46,  1.68it/s]Extractor Estimating: 167it [01:47,  1.65it/s]Extractor Estimating: 168it [01:48,  1.71it/s]Extractor Estimating: 169it [01:48,  1.66it/s]Extractor Estimating: 170it [01:49,  1.64it/s]Extractor Estimating: 171it [01:50,  1.44it/s]Extractor Estimating: 172it [01:50,  1.49it/s]Extractor Estimating: 173it [01:51,  1.55it/s]Extractor Estimating: 174it [01:51,  1.59it/s]Extractor Estimating: 175it [01:52,  1.62it/s]Extractor Estimating: 176it [01:53,  1.62it/s]Extractor Estimating: 177it [01:53,  1.67it/s]Extractor Estimating: 178it [01:54,  1.70it/s]Extractor Estimating: 179it [01:54,  1.65it/s]Extractor Estimating: 180it [01:55,  1.63it/s]Extractor Estimating: 181it [01:56,  1.64it/s]Extractor Estimating: 182it [01:56,  1.69it/s]Extractor Estimating: 183it [01:57,  1.63it/s]Extractor Estimating: 184it [01:58,  1.55it/s]Extractor Estimating: 185it [01:58,  1.61it/s]Extractor Estimating: 186it [01:59,  1.59it/s]Extractor Estimating: 187it [02:00,  1.55it/s]Extractor Estimating: 188it [02:00,  1.58it/s]Extractor Estimating: 189it [02:01,  1.56it/s]Extractor Estimating: 190it [02:01,  1.59it/s]Extractor Estimating: 191it [02:02,  1.60it/s]Extractor Estimating: 192it [02:03,  1.64it/s]Extractor Estimating: 193it [02:03,  1.66it/s]Extractor Estimating: 194it [02:04,  1.67it/s]Extractor Estimating: 195it [02:04,  1.62it/s]Extractor Estimating: 196it [02:05,  1.63it/s]Extractor Estimating: 197it [02:06,  1.68it/s]Extractor Estimating: 198it [02:06,  1.65it/s]Extractor Estimating: 199it [02:07,  1.65it/s]Extractor Estimating: 200it [02:07,  1.62it/s]Extractor Estimating: 201it [02:08,  1.60it/s]Extractor Estimating: 202it [02:09,  1.47it/s]Extractor Estimating: 203it [02:09,  1.52it/s]Extractor Estimating: 204it [02:10,  1.52it/s]Extractor Estimating: 205it [02:11,  1.52it/s]Extractor Estimating: 206it [02:11,  1.53it/s]Extractor Estimating: 207it [02:13,  1.28it/s]Extractor Estimating: 208it [02:13,  1.33it/s]Extractor Estimating: 209it [02:14,  1.40it/s]Extractor Estimating: 210it [02:15,  1.43it/s]Extractor Estimating: 211it [02:15,  1.41it/s]Extractor Estimating: 212it [02:16,  1.36it/s]Extractor Estimating: 213it [02:17,  1.43it/s]Extractor Estimating: 214it [02:17,  1.47it/s]Extractor Estimating: 215it [02:18,  1.45it/s]Extractor Estimating: 216it [02:19,  1.42it/s]Extractor Estimating: 217it [02:19,  1.43it/s]Extractor Estimating: 218it [02:20,  1.44it/s]Extractor Estimating: 219it [02:21,  1.43it/s]Extractor Estimating: 220it [02:21,  1.48it/s]Extractor Estimating: 221it [02:22,  1.51it/s]Extractor Estimating: 222it [02:23,  1.45it/s]Extractor Estimating: 223it [02:23,  1.52it/s]Extractor Estimating: 224it [02:24,  1.52it/s]Extractor Estimating: 225it [02:25,  1.49it/s]Extractor Estimating: 226it [02:25,  1.46it/s]Extractor Estimating: 227it [02:26,  1.47it/s]Extractor Estimating: 228it [02:27,  1.52it/s]Extractor Estimating: 229it [02:27,  1.55it/s]Extractor Estimating: 230it [02:28,  1.52it/s]Extractor Estimating: 231it [02:29,  1.49it/s]Extractor Estimating: 232it [02:29,  1.50it/s]Extractor Estimating: 233it [02:30,  1.49it/s]Extractor Estimating: 234it [02:31,  1.56it/s]Extractor Estimating: 235it [02:31,  1.57it/s]Extractor Estimating: 236it [02:32,  1.54it/s]Extractor Estimating: 237it [02:33,  1.54it/s]Extractor Estimating: 238it [02:33,  1.54it/s]Extractor Estimating: 239it [02:34,  1.55it/s]Extractor Estimating: 240it [02:35,  1.57it/s]Extractor Estimating: 241it [02:35,  1.51it/s]Extractor Estimating: 242it [02:36,  1.54it/s]Extractor Estimating: 243it [02:36,  1.57it/s]Extractor Estimating: 244it [02:37,  1.51it/s]Extractor Estimating: 245it [02:38,  1.52it/s]Extractor Estimating: 246it [02:39,  1.49it/s]Extractor Estimating: 247it [02:39,  1.55it/s]Extractor Estimating: 248it [02:40,  1.48it/s]Extractor Estimating: 249it [02:41,  1.47it/s]Extractor Estimating: 250it [02:41,  1.52it/s]Extractor Estimating: 251it [02:42,  1.53it/s]Extractor Estimating: 252it [02:43,  1.49it/s]Extractor Estimating: 253it [02:43,  1.50it/s]Extractor Estimating: 254it [02:44,  1.55it/s]Extractor Estimating: 255it [02:44,  1.52it/s]Extractor Estimating: 256it [02:45,  1.51it/s]Extractor Estimating: 257it [02:46,  1.53it/s]Extractor Estimating: 258it [02:46,  1.56it/s]Extractor Estimating: 259it [02:47,  1.58it/s]Extractor Estimating: 260it [02:48,  1.52it/s]Extractor Estimating: 261it [02:48,  1.50it/s]Extractor Estimating: 262it [02:49,  1.51it/s]Extractor Estimating: 263it [02:50,  1.48it/s]Extractor Estimating: 264it [02:50,  1.51it/s]Extractor Estimating: 265it [02:51,  1.50it/s]Extractor Estimating: 266it [02:52,  1.49it/s]Extractor Estimating: 267it [02:52,  1.55it/s]Extractor Estimating: 268it [02:53,  1.58it/s]Extractor Estimating: 269it [02:53,  1.66it/s]Extractor Estimating: 270it [02:54,  1.59it/s]Extractor Estimating: 271it [02:55,  1.60it/s]Extractor Estimating: 272it [02:55,  1.57it/s]Extractor Estimating: 273it [02:56,  1.61it/s]Extractor Estimating: 274it [02:57,  1.57it/s]Extractor Estimating: 275it [02:57,  1.54it/s]Extractor Estimating: 276it [02:58,  1.48it/s]Extractor Estimating: 277it [02:59,  1.53it/s]Extractor Estimating: 278it [02:59,  1.56it/s]Extractor Estimating: 279it [03:00,  1.57it/s]Extractor Estimating: 280it [03:01,  1.37it/s]Extractor Estimating: 281it [03:02,  1.45it/s]Extractor Estimating: 282it [03:02,  1.46it/s]Extractor Estimating: 283it [03:03,  1.51it/s]Extractor Estimating: 284it [03:03,  1.49it/s]Extractor Estimating: 285it [03:04,  1.49it/s]Extractor Estimating: 286it [03:05,  1.51it/s]Extractor Estimating: 287it [03:06,  1.49it/s]Extractor Estimating: 288it [03:06,  1.49it/s]Extractor Estimating: 289it [03:07,  1.49it/s]Extractor Estimating: 290it [03:08,  1.48it/s]Extractor Estimating: 291it [03:08,  1.46it/s]Extractor Estimating: 292it [03:09,  1.45it/s]Extractor Estimating: 293it [03:10,  1.48it/s]Extractor Estimating: 294it [03:10,  1.48it/s]Extractor Estimating: 295it [03:11,  1.50it/s]Extractor Estimating: 296it [03:12,  1.44it/s]Extractor Estimating: 297it [03:12,  1.40it/s]Extractor Estimating: 298it [03:13,  1.40it/s]Extractor Estimating: 299it [03:14,  1.41it/s]Extractor Estimating: 300it [03:14,  1.47it/s]Extractor Estimating: 301it [03:15,  1.53it/s]Extractor Estimating: 302it [03:16,  1.58it/s]Extractor Estimating: 303it [03:16,  1.59it/s]Extractor Estimating: 304it [03:17,  1.62it/s]Extractor Estimating: 305it [03:17,  1.62it/s]Extractor Estimating: 306it [03:18,  1.62it/s]Extractor Estimating: 307it [03:19,  1.59it/s]Extractor Estimating: 308it [03:19,  1.60it/s]Extractor Estimating: 309it [03:20,  1.67it/s]Extractor Estimating: 310it [03:20,  1.68it/s]Extractor Estimating: 311it [03:21,  1.67it/s]Extractor Estimating: 312it [03:22,  1.65it/s]Extractor Estimating: 313it [03:22,  1.66it/s]Extractor Estimating: 314it [03:23,  1.64it/s]Extractor Estimating: 315it [03:24,  1.64it/s]Extractor Estimating: 316it [03:24,  1.70it/s]Extractor Estimating: 317it [03:25,  1.70it/s]Extractor Estimating: 318it [03:25,  1.67it/s]Extractor Estimating: 319it [03:26,  1.64it/s]Extractor Estimating: 320it [03:27,  1.63it/s]Extractor Estimating: 321it [03:27,  1.64it/s]Extractor Estimating: 322it [03:28,  1.65it/s]Extractor Estimating: 323it [03:28,  1.69it/s]Extractor Estimating: 324it [03:29,  1.67it/s]Extractor Estimating: 325it [03:29,  1.67it/s]Extractor Estimating: 326it [03:30,  1.66it/s]Extractor Estimating: 327it [03:31,  1.60it/s]Extractor Estimating: 328it [03:31,  1.54it/s]Extractor Estimating: 329it [03:32,  1.51it/s]Extractor Estimating: 330it [03:33,  1.55it/s]Extractor Estimating: 331it [03:33,  1.63it/s]Extractor Estimating: 332it [03:34,  1.66it/s]Extractor Estimating: 333it [03:35,  1.63it/s]Extractor Estimating: 334it [03:35,  1.59it/s]Extractor Estimating: 335it [03:36,  1.53it/s]Extractor Estimating: 336it [03:37,  1.56it/s]Extractor Estimating: 337it [03:37,  1.54it/s]Extractor Estimating: 338it [03:38,  1.51it/s]Extractor Estimating: 339it [03:38,  1.59it/s]Extractor Estimating: 340it [03:39,  1.54it/s]Extractor Estimating: 341it [03:40,  1.57it/s]Extractor Estimating: 342it [03:40,  1.63it/s]Extractor Estimating: 343it [03:41,  1.59it/s]Extractor Estimating: 344it [03:42,  1.59it/s]Extractor Estimating: 345it [03:42,  1.58it/s]Extractor Estimating: 346it [03:43,  1.60it/s]Extractor Estimating: 347it [03:43,  1.66it/s]Extractor Estimating: 348it [03:44,  1.63it/s]Extractor Estimating: 349it [03:45,  1.59it/s]Extractor Estimating: 350it [03:45,  1.62it/s]Extractor Estimating: 351it [03:46,  1.62it/s]Extractor Estimating: 352it [03:47,  1.58it/s]Extractor Estimating: 353it [03:47,  1.56it/s]Extractor Estimating: 354it [03:48,  1.61it/s]Extractor Estimating: 355it [03:48,  1.63it/s]Extractor Estimating: 356it [03:49,  1.61it/s]Extractor Estimating: 357it [03:50,  1.61it/s]Extractor Estimating: 358it [03:50,  1.61it/s]Extractor Estimating: 359it [03:51,  1.63it/s]Extractor Estimating: 360it [03:52,  1.57it/s]Extractor Estimating: 361it [03:52,  1.54it/s]Extractor Estimating: 362it [03:53,  1.57it/s]Extractor Estimating: 363it [03:53,  1.58it/s]Extractor Estimating: 364it [03:54,  1.58it/s]Extractor Estimating: 365it [03:55,  1.54it/s]Extractor Estimating: 366it [03:55,  1.57it/s]Extractor Estimating: 367it [03:56,  1.51it/s]Extractor Estimating: 368it [03:57,  1.52it/s]Extractor Estimating: 369it [03:58,  1.37it/s]Extractor Estimating: 370it [03:58,  1.43it/s]Extractor Estimating: 371it [03:59,  1.46it/s]Extractor Estimating: 372it [04:00,  1.35it/s]Extractor Estimating: 373it [04:00,  1.42it/s]Extractor Estimating: 374it [04:01,  1.50it/s]Extractor Estimating: 375it [04:02,  1.51it/s]Extractor Estimating: 376it [04:02,  1.53it/s]Extractor Estimating: 377it [04:03,  1.52it/s]Extractor Estimating: 378it [04:04,  1.53it/s]Extractor Estimating: 379it [04:04,  1.57it/s]Extractor Estimating: 380it [04:05,  1.55it/s]Extractor Estimating: 381it [04:06,  1.52it/s]Extractor Estimating: 382it [04:06,  1.55it/s]Extractor Estimating: 383it [04:07,  1.57it/s]Extractor Estimating: 384it [04:07,  1.53it/s]Extractor Estimating: 385it [04:08,  1.51it/s]Extractor Estimating: 386it [04:09,  1.52it/s]Extractor Estimating: 387it [04:09,  1.51it/s]Extractor Estimating: 388it [04:10,  1.52it/s]Extractor Estimating: 389it [04:11,  1.48it/s]Extractor Estimating: 390it [04:12,  1.48it/s]Extractor Estimating: 391it [04:12,  1.49it/s]Extractor Estimating: 392it [04:13,  1.49it/s]Extractor Estimating: 393it [04:14,  1.46it/s]Extractor Estimating: 394it [04:14,  1.50it/s]Extractor Estimating: 395it [04:15,  1.51it/s]Extractor Estimating: 396it [04:16,  1.44it/s]Extractor Estimating: 397it [04:16,  1.47it/s]Extractor Estimating: 398it [04:17,  1.46it/s]Extractor Estimating: 399it [04:18,  1.48it/s]Extractor Estimating: 400it [04:18,  1.52it/s]Extractor Estimating: 401it [04:19,  1.54it/s]Extractor Estimating: 402it [04:20,  1.52it/s]Extractor Estimating: 403it [04:20,  1.57it/s]Extractor Estimating: 404it [04:21,  1.55it/s]Extractor Estimating: 405it [04:21,  1.59it/s]Extractor Estimating: 406it [04:22,  1.61it/s]Extractor Estimating: 407it [04:23,  1.59it/s]Extractor Estimating: 408it [04:23,  1.62it/s]Extractor Estimating: 409it [04:24,  1.62it/s]Extractor Estimating: 410it [04:24,  1.61it/s]Extractor Estimating: 411it [04:25,  1.57it/s]Extractor Estimating: 412it [04:26,  1.61it/s]Extractor Estimating: 413it [04:26,  1.60it/s]Extractor Estimating: 414it [04:27,  1.52it/s]Extractor Estimating: 415it [04:28,  1.53it/s]Extractor Estimating: 416it [04:28,  1.50it/s]Extractor Estimating: 417it [04:29,  1.56it/s]Extractor Estimating: 418it [04:30,  1.52it/s]Extractor Estimating: 419it [04:30,  1.54it/s]Extractor Estimating: 420it [04:31,  1.54it/s]Extractor Estimating: 421it [04:32,  1.53it/s]Extractor Estimating: 422it [04:32,  1.49it/s]Extractor Estimating: 423it [04:33,  1.48it/s]Extractor Estimating: 424it [04:34,  1.54it/s]Extractor Estimating: 425it [04:34,  1.53it/s]Extractor Estimating: 426it [04:35,  1.55it/s]Extractor Estimating: 427it [04:36,  1.54it/s]Extractor Estimating: 428it [04:36,  1.55it/s]Extractor Estimating: 429it [04:37,  1.59it/s]Extractor Estimating: 430it [04:38,  1.55it/s]Extractor Estimating: 431it [04:38,  1.53it/s]Extractor Estimating: 432it [04:39,  1.54it/s]Extractor Estimating: 433it [04:40,  1.49it/s]Extractor Estimating: 434it [04:40,  1.47it/s]Extractor Estimating: 435it [04:41,  1.43it/s]Extractor Estimating: 436it [04:42,  1.49it/s]Extractor Estimating: 437it [04:42,  1.52it/s]Extractor Estimating: 438it [04:43,  1.52it/s]Extractor Estimating: 439it [04:44,  1.49it/s]Extractor Estimating: 440it [04:44,  1.48it/s]Extractor Estimating: 441it [04:45,  1.48it/s]Extractor Estimating: 442it [04:46,  1.47it/s]Extractor Estimating: 443it [04:46,  1.49it/s]Extractor Estimating: 444it [04:47,  1.48it/s]Extractor Estimating: 445it [04:48,  1.46it/s]Extractor Estimating: 446it [04:48,  1.46it/s]Extractor Estimating: 447it [04:49,  1.46it/s]Extractor Estimating: 448it [04:50,  1.43it/s]Extractor Estimating: 449it [04:51,  1.41it/s]Extractor Estimating: 450it [04:51,  1.45it/s]Extractor Estimating: 451it [04:52,  1.55it/s]Extractor Estimating: 452it [04:52,  1.55it/s]Extractor Estimating: 453it [04:53,  1.52it/s]Extractor Estimating: 454it [04:54,  1.56it/s]Extractor Estimating: 455it [04:54,  1.59it/s]Extractor Estimating: 456it [04:55,  1.44it/s]Extractor Estimating: 457it [04:56,  1.47it/s]Extractor Estimating: 458it [04:56,  1.47it/s]Extractor Estimating: 459it [04:57,  1.53it/s]Extractor Estimating: 460it [04:58,  1.59it/s]Extractor Estimating: 461it [04:58,  1.57it/s]Extractor Estimating: 462it [04:59,  1.57it/s]Extractor Estimating: 463it [04:59,  1.58it/s]Extractor Estimating: 464it [05:00,  1.59it/s]Extractor Estimating: 465it [05:01,  1.53it/s]Extractor Estimating: 466it [05:01,  1.61it/s]Extractor Estimating: 467it [05:02,  1.59it/s]Extractor Estimating: 468it [05:03,  1.62it/s]Extractor Estimating: 469it [05:03,  1.63it/s]Extractor Estimating: 470it [05:04,  1.63it/s]Extractor Estimating: 471it [05:04,  1.63it/s]Extractor Estimating: 472it [05:05,  1.54it/s]Extractor Estimating: 473it [05:06,  1.55it/s]Extractor Estimating: 474it [05:06,  1.54it/s]Extractor Estimating: 475it [05:07,  1.58it/s]Extractor Estimating: 476it [05:08,  1.56it/s]Extractor Estimating: 477it [05:08,  1.54it/s]Extractor Estimating: 478it [05:09,  1.56it/s]Extractor Estimating: 479it [05:10,  1.54it/s]Extractor Estimating: 480it [05:10,  1.53it/s]Extractor Estimating: 481it [05:11,  1.52it/s]Extractor Estimating: 482it [05:12,  1.53it/s]Extractor Estimating: 483it [05:12,  1.55it/s]Extractor Estimating: 484it [05:13,  1.59it/s]Extractor Estimating: 485it [05:14,  1.56it/s]Extractor Estimating: 486it [05:14,  1.57it/s]Extractor Estimating: 487it [05:15,  1.54it/s]Extractor Estimating: 488it [05:15,  1.53it/s]Extractor Estimating: 489it [05:16,  1.54it/s]Extractor Estimating: 490it [05:17,  1.57it/s]Extractor Estimating: 491it [05:17,  1.57it/s]Extractor Estimating: 492it [05:18,  1.56it/s]Extractor Estimating: 493it [05:19,  1.57it/s]Extractor Estimating: 494it [05:19,  1.55it/s]Extractor Estimating: 495it [05:20,  1.56it/s]Extractor Estimating: 496it [05:21,  1.55it/s]Extractor Estimating: 497it [05:21,  1.45it/s]Extractor Estimating: 498it [05:22,  1.48it/s]Extractor Estimating: 499it [05:23,  1.49it/s]Extractor Estimating: 500it [05:23,  1.58it/s]Extractor Estimating: 500it [05:23,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:17,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:17,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:17,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:17,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:17,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:13:18,613 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:13:18,614 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:13:19,250 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:13:20,351 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:13:20,351 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:23,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:23,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:23,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:23,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:23,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:13:23,889 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:13:23,891 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:13:24,464 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:13:24,640 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:13:24,641 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:23:11,369 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:23:11,391 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 10580 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 27386
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27486, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27486, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.056, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.144, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.062, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.086, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 59, avg_time 1.096, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 159, avg_time 2.192, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 259, avg_time 1.082, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 359, avg_time 1.097, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 18, avg_time 1.053, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 118, avg_time 1.069, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 218, avg_time 2.188, loss:nan
g_step 1200, step 318, avg_time 1.082, loss:nan
g_step 1300, step 418, avg_time 1.078, loss:nan
g_step 1400, step 77, avg_time 1.079, loss:nan
g_step 1500, step 177, avg_time 1.092, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 277, avg_time 2.179, loss:nan
g_step 1700, step 377, avg_time 1.071, loss:nan
g_step 1800, step 36, avg_time 1.099, loss:nan
g_step 1900, step 136, avg_time 1.097, loss:nan
g_step 2000, step 236, avg_time 1.078, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 336, avg_time 2.164, loss:nan
g_step 2200, step 436, avg_time 1.071, loss:nan
g_step 2300, step 95, avg_time 1.069, loss:nan
g_step 2400, step 195, avg_time 1.091, loss:nan
g_step 2500, step 295, avg_time 1.085, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 395, avg_time 2.174, loss:nan
g_step 2700, step 54, avg_time 1.074, loss:nan
g_step 2800, step 154, avg_time 1.100, loss:nan
g_step 2900, step 254, avg_time 1.080, loss:nan
g_step 3000, step 354, avg_time 1.069, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 13, avg_time 2.176, loss:nan
g_step 3200, step 113, avg_time 1.064, loss:nan
g_step 3300, step 213, avg_time 1.068, loss:nan
g_step 3400, step 313, avg_time 1.088, loss:nan
g_step 3500, step 413, avg_time 1.091, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 72, avg_time 2.175, loss:nan
g_step 3700, step 172, avg_time 1.079, loss:nan
g_step 3800, step 272, avg_time 1.084, loss:nan
g_step 3900, step 372, avg_time 1.068, loss:nan
g_step 4000, step 31, avg_time 1.086, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 131, avg_time 2.161, loss:nan
g_step 4200, step 231, avg_time 1.079, loss:nan
g_step 4300, step 331, avg_time 1.089, loss:nan
g_step 4400, step 431, avg_time 1.066, loss:nan
g_step 4500, step 90, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 190, avg_time 2.178, loss:nan
g_step 4700, step 290, avg_time 1.088, loss:nan
g_step 4800, step 390, avg_time 1.062, loss:nan
g_step 4900, step 49, avg_time 1.075, loss:nan
g_step 5000, step 149, avg_time 1.077, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 249, avg_time 2.181, loss:nan
g_step 5200, step 349, avg_time 1.077, loss:nan
g_step 5300, step 8, avg_time 1.084, loss:nan
g_step 5400, step 108, avg_time 1.070, loss:nan
g_step 5500, step 208, avg_time 1.072, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 308, avg_time 2.193, loss:nan
g_step 5700, step 408, avg_time 1.085, loss:nan
g_step 5800, step 67, avg_time 1.073, loss:nan
g_step 5900, step 167, avg_time 1.076, loss:nan
g_step 6000, step 267, avg_time 1.092, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 367, avg_time 2.186, loss:nan
g_step 6200, step 26, avg_time 1.054, loss:nan
g_step 6300, step 126, avg_time 1.109, loss:nan
g_step 6400, step 226, avg_time 1.064, loss:nan
g_step 6500, step 326, avg_time 1.068, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 426, avg_time 2.169, loss:nan
g_step 6700, step 85, avg_time 1.087, loss:nan
g_step 6800, step 185, avg_time 1.064, loss:nan
g_step 6900, step 285, avg_time 1.090, loss:nan
g_step 7000, step 385, avg_time 1.098, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 44, avg_time 2.174, loss:nan
g_step 7200, step 144, avg_time 1.067, loss:nan
g_step 7300, step 244, avg_time 1.071, loss:nan
g_step 7400, step 344, avg_time 1.083, loss:nan
g_step 7500, step 3, avg_time 1.069, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 103, avg_time 2.150, loss:nan
g_step 7700, step 203, avg_time 1.080, loss:nan
g_step 7800, step 303, avg_time 1.096, loss:nan
g_step 7900, step 403, avg_time 1.074, loss:nan
g_step 8000, step 62, avg_time 1.080, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 162, avg_time 2.163, loss:nan
g_step 8200, step 262, avg_time 1.061, loss:nan
g_step 8300, step 362, avg_time 1.073, loss:nan
g_step 8400, step 21, avg_time 1.087, loss:nan
g_step 8500, step 121, avg_time 1.087, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 221, avg_time 2.163, loss:nan
g_step 8700, step 321, avg_time 1.071, loss:nan
g_step 8800, step 421, avg_time 1.070, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:23:11 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:23:11 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-23-11_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:23:12 - WARNING - datasets.builder -   Using custom data configuration default-1ea432f989115a62
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1ea432f989115a62/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:23:14,251 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:23:14,253 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:23:14,253 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:23:14,254 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:23:14,320 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:23:14,358 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:23:14,358 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:23:14,358 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:23:14,358 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:23:14,358 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:23:14,358 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:23:14,692 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:23:17,818 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:23:17,835 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1ea432f989115a62/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.64ba/s] 18%|        | 2/11 [00:00<00:02,  3.57ba/s] 27%|       | 3/11 [00:00<00:02,  3.98ba/s] 36%|      | 4/11 [00:01<00:01,  4.18ba/s] 45%|     | 5/11 [00:01<00:01,  4.30ba/s] 55%|    | 6/11 [00:01<00:01,  4.39ba/s] 64%|   | 7/11 [00:01<00:00,  4.42ba/s] 73%|  | 8/11 [00:01<00:00,  4.47ba/s] 82%| | 9/11 [00:02<00:00,  4.51ba/s] 91%| | 10/11 [00:02<00:00,  4.51ba/s]100%|| 11/11 [00:02<00:00,  5.13ba/s]100%|| 11/11 [00:02<00:00,  4.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.37ba/s] 50%|     | 2/4 [00:00<00:00,  2.83ba/s] 75%|  | 3/4 [00:00<00:00,  3.39ba/s]100%|| 4/4 [00:01<00:00,  4.48ba/s]100%|| 4/4 [00:01<00:00,  3.89ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.77ba/s] 27%|       | 3/11 [00:00<00:00,  8.62ba/s] 45%|     | 5/11 [00:00<00:00,  9.45ba/s] 64%|   | 7/11 [00:00<00:00,  9.83ba/s] 82%| | 9/11 [00:00<00:00, 10.01ba/s]100%|| 11/11 [00:01<00:00, 10.86ba/s]100%|| 11/11 [00:01<00:00, 10.01ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.55ba/s] 75%|  | 3/4 [00:00<00:00,  8.48ba/s]100%|| 4/4 [00:00<00:00,  9.46ba/s]
[INFO|trainer.py:414] 2023-08-28 10:23:24,590 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:23:24,724 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:23:24,724 >>   Num examples = 10600
[INFO|trainer.py:1149] 2023-08-28 10:23:24,724 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:23:24,724 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:23:24,724 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:23:24,724 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:23:24,724 >>   Total optimization steps = 830
  0%|          | 0/830 [00:00<?, ?it/s]  0%|          | 1/830 [00:01<16:44,  1.21s/it]  0%|          | 2/830 [00:01<12:11,  1.13it/s]  0%|          | 3/830 [00:02<10:08,  1.36it/s]  0%|          | 4/830 [00:02<08:53,  1.55it/s]  1%|          | 5/830 [00:03<07:49,  1.76it/s]  1%|          | 6/830 [00:03<06:42,  2.05it/s]  1%|          | 7/830 [00:04<05:53,  2.33it/s]  1%|          | 8/830 [00:04<05:25,  2.53it/s]  1%|          | 9/830 [00:04<05:11,  2.64it/s]  1%|          | 10/830 [00:04<04:45,  2.88it/s]  1%|         | 11/830 [00:05<04:26,  3.07it/s]  1%|         | 12/830 [00:05<04:19,  3.15it/s]  2%|         | 13/830 [00:05<04:19,  3.15it/s]  2%|         | 14/830 [00:06<04:09,  3.27it/s]  2%|         | 15/830 [00:06<04:03,  3.35it/s]  2%|         | 16/830 [00:06<03:58,  3.42it/s]  2%|         | 17/830 [00:06<03:54,  3.47it/s]  2%|         | 18/830 [00:07<03:51,  3.51it/s]  2%|         | 19/830 [00:07<03:49,  3.53it/s]  2%|         | 20/830 [00:07<03:57,  3.42it/s]  3%|         | 21/830 [00:08<03:53,  3.47it/s]  3%|         | 22/830 [00:08<03:50,  3.51it/s]  3%|         | 23/830 [00:08<03:48,  3.54it/s]  3%|         | 24/830 [00:08<03:54,  3.44it/s]  3%|         | 25/830 [00:09<03:50,  3.49it/s]  3%|         | 26/830 [00:09<03:48,  3.52it/s]  3%|         | 27/830 [00:09<03:46,  3.54it/s]  3%|         | 28/830 [00:10<03:45,  3.56it/s]  3%|         | 29/830 [00:10<03:44,  3.57it/s]  4%|         | 30/830 [00:10<03:43,  3.58it/s]  4%|         | 31/830 [00:10<03:42,  3.59it/s]  4%|         | 32/830 [00:11<03:42,  3.59it/s]  4%|         | 33/830 [00:11<03:41,  3.59it/s]  4%|         | 34/830 [00:11<03:41,  3.60it/s]  4%|         | 35/830 [00:12<03:50,  3.45it/s]  4%|         | 36/830 [00:12<03:47,  3.50it/s]  4%|         | 37/830 [00:12<03:44,  3.53it/s]  5%|         | 38/830 [00:12<03:43,  3.55it/s]  5%|         | 39/830 [00:13<03:41,  3.56it/s]  5%|         | 40/830 [00:13<03:41,  3.57it/s]  5%|         | 41/830 [00:13<03:40,  3.58it/s]  5%|         | 42/830 [00:14<03:39,  3.59it/s]  5%|         | 43/830 [00:14<03:39,  3.59it/s]  5%|         | 44/830 [00:14<03:38,  3.60it/s]  5%|         | 45/830 [00:14<03:38,  3.60it/s]  6%|         | 46/830 [00:15<03:52,  3.37it/s]  6%|         | 47/830 [00:15<03:47,  3.44it/s]  6%|         | 48/830 [00:15<03:44,  3.49it/s]  6%|         | 49/830 [00:16<03:41,  3.52it/s]  6%|         | 50/830 [00:16<03:40,  3.54it/s]  6%|         | 51/830 [00:16<03:38,  3.56it/s]  6%|         | 52/830 [00:16<03:37,  3.57it/s]  6%|         | 53/830 [00:17<03:37,  3.58it/s]  7%|         | 54/830 [00:17<03:36,  3.59it/s]  7%|         | 55/830 [00:17<03:35,  3.59it/s]  7%|         | 56/830 [00:17<03:35,  3.60it/s]  7%|         | 57/830 [00:18<03:34,  3.60it/s]  7%|         | 58/830 [00:18<03:34,  3.59it/s]  7%|         | 59/830 [00:18<03:34,  3.59it/s]  7%|         | 60/830 [00:19<03:34,  3.59it/s]  7%|         | 61/830 [00:19<03:33,  3.59it/s]  7%|         | 62/830 [00:19<03:33,  3.59it/s]  8%|         | 63/830 [00:19<03:33,  3.60it/s]  8%|         | 64/830 [00:20<03:32,  3.60it/s]  8%|         | 65/830 [00:20<03:32,  3.60it/s]  8%|         | 66/830 [00:20<03:32,  3.60it/s]  8%|         | 67/830 [00:21<03:41,  3.44it/s]  8%|         | 68/830 [00:21<03:38,  3.49it/s]  8%|         | 69/830 [00:21<03:36,  3.52it/s]  8%|         | 70/830 [00:21<03:34,  3.54it/s]  9%|         | 71/830 [00:22<03:33,  3.56it/s]  9%|         | 72/830 [00:22<03:32,  3.57it/s]  9%|         | 73/830 [00:22<03:31,  3.58it/s]  9%|         | 74/830 [00:23<03:31,  3.58it/s]  9%|         | 75/830 [00:23<03:30,  3.58it/s]  9%|         | 76/830 [00:23<03:30,  3.59it/s]  9%|         | 77/830 [00:23<03:29,  3.59it/s]  9%|         | 78/830 [00:24<03:37,  3.46it/s] 10%|         | 79/830 [00:24<03:34,  3.50it/s] 10%|         | 80/830 [00:24<03:32,  3.53it/s] 10%|         | 81/830 [00:24<03:31,  3.55it/s] 10%|         | 82/830 [00:25<03:30,  3.56it/s] 10%|         | 83/830 [00:25<03:29,  3.57it/s] 10%|         | 84/830 [00:25<03:28,  3.58it/s] 10%|         | 85/830 [00:26<03:27,  3.59it/s] 10%|         | 86/830 [00:26<03:27,  3.59it/s] 10%|         | 87/830 [00:27<05:08,  2.41it/s] 11%|         | 88/830 [00:27<04:52,  2.53it/s] 11%|         | 89/830 [00:27<04:26,  2.78it/s] 11%|         | 90/830 [00:28<04:07,  2.99it/s] 11%|         | 91/830 [00:28<03:54,  3.15it/s] 11%|         | 92/830 [00:28<03:45,  3.27it/s] 11%|         | 93/830 [00:28<03:39,  3.36it/s] 11%|        | 94/830 [00:29<03:34,  3.43it/s] 11%|        | 95/830 [00:29<03:31,  3.48it/s] 12%|        | 96/830 [00:29<03:28,  3.52it/s] 12%|        | 97/830 [00:29<03:26,  3.54it/s] 12%|        | 98/830 [00:30<03:25,  3.57it/s] 12%|        | 99/830 [00:30<03:28,  3.51it/s] 12%|        | 100/830 [00:30<03:26,  3.53it/s] 12%|        | 101/830 [00:31<03:25,  3.55it/s] 12%|        | 102/830 [00:31<03:23,  3.57it/s] 12%|        | 103/830 [00:31<03:23,  3.58it/s] 13%|        | 104/830 [00:31<03:22,  3.59it/s] 13%|        | 105/830 [00:32<03:21,  3.60it/s] 13%|        | 106/830 [00:32<03:21,  3.60it/s] 13%|        | 107/830 [00:32<03:20,  3.60it/s] 13%|        | 108/830 [00:33<03:20,  3.60it/s] 13%|        | 109/830 [00:33<03:20,  3.60it/s] 13%|        | 110/830 [00:33<03:28,  3.45it/s] 13%|        | 111/830 [00:33<03:25,  3.50it/s] 13%|        | 112/830 [00:34<03:23,  3.53it/s] 14%|        | 113/830 [00:34<03:21,  3.55it/s] 14%|        | 114/830 [00:34<03:20,  3.57it/s] 14%|        | 115/830 [00:35<03:19,  3.58it/s] 14%|        | 116/830 [00:35<03:19,  3.58it/s] 14%|        | 117/830 [00:35<03:18,  3.59it/s] 14%|        | 118/830 [00:35<03:18,  3.59it/s] 14%|        | 119/830 [00:36<03:17,  3.60it/s] 14%|        | 120/830 [00:36<03:17,  3.59it/s] 15%|        | 121/830 [00:36<03:24,  3.47it/s] 15%|        | 122/830 [00:36<03:21,  3.51it/s] 15%|        | 123/830 [00:37<03:20,  3.53it/s] 15%|        | 124/830 [00:37<03:18,  3.55it/s] 15%|        | 125/830 [00:37<03:17,  3.57it/s] 15%|        | 126/830 [00:38<03:16,  3.58it/s] 15%|        | 127/830 [00:38<03:16,  3.59it/s] 15%|        | 128/830 [00:38<03:15,  3.59it/s] 16%|        | 129/830 [00:38<03:14,  3.60it/s] 16%|        | 130/830 [00:39<03:14,  3.60it/s] 16%|        | 131/830 [00:39<03:14,  3.60it/s] 16%|        | 132/830 [00:39<03:18,  3.53it/s] 16%|        | 133/830 [00:40<03:16,  3.55it/s] 16%|        | 134/830 [00:40<03:15,  3.56it/s] 16%|        | 135/830 [00:40<03:14,  3.58it/s] 16%|        | 136/830 [00:40<03:13,  3.58it/s] 17%|        | 137/830 [00:41<03:12,  3.59it/s] 17%|        | 138/830 [00:41<03:12,  3.60it/s] 17%|        | 139/830 [00:41<03:12,  3.60it/s] 17%|        | 140/830 [00:42<03:11,  3.60it/s] 17%|        | 141/830 [00:42<03:11,  3.60it/s] 17%|        | 142/830 [00:42<03:11,  3.60it/s] 17%|        | 143/830 [00:42<03:19,  3.44it/s] 17%|        | 144/830 [00:43<03:16,  3.49it/s] 17%|        | 145/830 [00:43<03:14,  3.52it/s] 18%|        | 146/830 [00:43<03:12,  3.55it/s] 18%|        | 147/830 [00:43<03:11,  3.57it/s] 18%|        | 148/830 [00:44<03:10,  3.58it/s] 18%|        | 149/830 [00:44<03:10,  3.58it/s] 18%|        | 150/830 [00:44<03:09,  3.58it/s] 18%|        | 151/830 [00:45<03:09,  3.59it/s] 18%|        | 152/830 [00:45<03:08,  3.59it/s] 18%|        | 153/830 [00:45<03:08,  3.59it/s] 19%|        | 154/830 [00:45<03:16,  3.44it/s] 19%|        | 155/830 [00:46<03:13,  3.48it/s] 19%|        | 156/830 [00:46<03:11,  3.52it/s] 19%|        | 157/830 [00:46<03:09,  3.54it/s] 19%|        | 158/830 [00:47<03:08,  3.56it/s] 19%|        | 159/830 [00:47<03:07,  3.57it/s] 19%|        | 160/830 [00:47<03:07,  3.58it/s] 19%|        | 161/830 [00:47<03:06,  3.59it/s] 20%|        | 162/830 [00:48<03:06,  3.59it/s] 20%|        | 163/830 [00:48<03:14,  3.43it/s] 20%|        | 164/830 [00:48<03:11,  3.47it/s] 20%|        | 165/830 [00:49<03:09,  3.51it/s] 20%|        | 166/830 [00:49<02:49,  3.91it/s][INFO|trainer.py:2140] 2023-08-28 10:24:13,998 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:24:13,998 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 10:24:13,998 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.41it/s][A
  3%|         | 12/436 [00:00<00:08, 49.04it/s][A
  4%|         | 17/436 [00:00<00:08, 47.29it/s][A
  5%|         | 22/436 [00:00<00:08, 46.41it/s][A
  6%|         | 27/436 [00:00<00:08, 45.93it/s][A
  7%|         | 32/436 [00:00<00:08, 45.38it/s][A
  8%|         | 37/436 [00:00<00:08, 44.86it/s][A
 10%|         | 42/436 [00:00<00:08, 44.35it/s][A
 11%|         | 47/436 [00:01<00:08, 44.44it/s][A
 12%|        | 52/436 [00:01<00:08, 43.53it/s][A
 13%|        | 57/436 [00:01<00:08, 43.98it/s][A
 14%|        | 62/436 [00:01<00:08, 44.41it/s][A
 15%|        | 67/436 [00:01<00:08, 44.55it/s][A
 17%|        | 72/436 [00:01<00:08, 44.65it/s][A
 18%|        | 77/436 [00:01<00:08, 44.50it/s][A
 19%|        | 82/436 [00:01<00:08, 44.25it/s][A
 20%|        | 87/436 [00:01<00:07, 44.18it/s][A
 21%|        | 92/436 [00:02<00:08, 42.74it/s][A
 22%|       | 97/436 [00:02<00:07, 43.47it/s][A
 23%|       | 102/436 [00:02<00:07, 43.96it/s][A
 25%|       | 107/436 [00:02<00:07, 44.27it/s][A
 26%|       | 112/436 [00:02<00:07, 44.55it/s][A
 27%|       | 117/436 [00:02<00:07, 44.65it/s][A
 28%|       | 122/436 [00:02<00:07, 44.45it/s][A
 29%|       | 127/436 [00:02<00:06, 44.34it/s][A
 30%|       | 132/436 [00:02<00:06, 44.16it/s][A
 31%|      | 137/436 [00:03<00:06, 44.09it/s][A
 33%|      | 142/436 [00:03<00:06, 44.31it/s][A
 34%|      | 147/436 [00:03<00:06, 44.52it/s][A
 35%|      | 152/436 [00:03<00:06, 44.67it/s][A
 36%|      | 157/436 [00:03<00:06, 44.82it/s][A
 37%|      | 162/436 [00:03<00:06, 44.76it/s][A
 38%|      | 167/436 [00:03<00:06, 44.57it/s][A
 39%|      | 172/436 [00:03<00:05, 44.33it/s][A
 41%|      | 177/436 [00:03<00:05, 44.10it/s][A
 42%|     | 182/436 [00:04<00:05, 44.11it/s][A
 43%|     | 187/436 [00:04<00:05, 44.23it/s][A
 44%|     | 192/436 [00:04<00:05, 44.44it/s][A
 45%|     | 197/436 [00:04<00:05, 44.59it/s][A
 46%|     | 202/436 [00:04<00:05, 44.79it/s][A
 47%|     | 207/436 [00:04<00:05, 44.57it/s][A
 49%|     | 212/436 [00:04<00:05, 44.77it/s][A
 50%|     | 217/436 [00:04<00:04, 44.59it/s][A
 51%|     | 222/436 [00:04<00:04, 44.37it/s][A
 52%|    | 227/436 [00:05<00:04, 43.44it/s][A
 53%|    | 232/436 [00:05<00:04, 43.91it/s][A
 54%|    | 237/436 [00:05<00:04, 44.16it/s][A
 56%|    | 242/436 [00:05<00:04, 44.42it/s][A
 57%|    | 247/436 [00:05<00:04, 44.53it/s][A
 58%|    | 252/436 [00:05<00:04, 44.63it/s][A
 59%|    | 257/436 [00:05<00:04, 44.62it/s][A
 60%|    | 262/436 [00:05<00:03, 44.46it/s][A
 61%|    | 267/436 [00:05<00:03, 44.30it/s][A
 62%|   | 272/436 [00:06<00:03, 44.11it/s][A
 64%|   | 277/436 [00:06<00:03, 44.21it/s][A
 65%|   | 282/436 [00:06<00:03, 44.45it/s][A
 66%|   | 287/436 [00:06<00:03, 44.59it/s][A
 67%|   | 292/436 [00:06<00:03, 44.74it/s][A
 68%|   | 297/436 [00:06<00:03, 44.79it/s][A
 69%|   | 302/436 [00:06<00:03, 44.40it/s][A
 70%|   | 307/436 [00:06<00:02, 44.35it/s][A
 72%|  | 312/436 [00:07<00:02, 44.21it/s][A
 73%|  | 317/436 [00:07<00:02, 44.01it/s][A
 74%|  | 322/436 [00:07<00:02, 44.11it/s][A
 75%|  | 327/436 [00:07<00:02, 44.23it/s][A
 76%|  | 332/436 [00:07<00:02, 44.56it/s][A
 77%|  | 337/436 [00:07<00:02, 44.68it/s][A
 78%|  | 342/436 [00:07<00:02, 44.62it/s][A
 80%|  | 347/436 [00:07<00:02, 44.48it/s][A
 81%|  | 352/436 [00:07<00:01, 44.42it/s][A
 82%| | 357/436 [00:08<00:01, 44.33it/s][A
 83%| | 362/436 [00:08<00:01, 39.18it/s][A
 84%| | 367/436 [00:08<00:01, 40.84it/s][A
 85%| | 372/436 [00:08<00:01, 42.02it/s][A
 86%| | 377/436 [00:08<00:01, 42.97it/s][A
 88%| | 382/436 [00:08<00:01, 43.61it/s][A
 89%| | 387/436 [00:08<00:01, 44.03it/s][A
 90%| | 392/436 [00:08<00:00, 44.35it/s][A
 91%| | 397/436 [00:08<00:00, 44.29it/s][A
 92%|| 402/436 [00:09<00:00, 44.03it/s][A
 93%|| 407/436 [00:09<00:00, 43.94it/s][A
 94%|| 412/436 [00:09<00:00, 44.06it/s][A
 96%|| 417/436 [00:09<00:00, 44.35it/s][A
 97%|| 422/436 [00:09<00:00, 44.55it/s][A
 98%|| 427/436 [00:09<00:00, 44.68it/s][A
 99%|| 432/436 [00:09<00:00, 44.51it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.51it/s][A 20%|        | 166/830 [00:59<02:49,  3.91it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:24:23,971 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 10:24:24,055 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:24:26,823 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:24:26,983 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:24:27,067 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/830 [01:03<49:19,  4.46s/it] 20%|        | 168/830 [01:03<35:24,  3.21s/it] 20%|        | 169/830 [01:04<25:40,  2.33s/it] 20%|        | 170/830 [01:04<18:52,  1.72s/it] 21%|        | 171/830 [01:04<14:07,  1.29s/it] 21%|        | 172/830 [01:04<10:47,  1.02it/s] 21%|        | 173/830 [01:05<08:28,  1.29it/s] 21%|        | 174/830 [01:05<06:58,  1.57it/s] 21%|        | 175/830 [01:05<05:47,  1.88it/s] 21%|        | 176/830 [01:06<04:58,  2.19it/s] 21%|       | 177/830 [01:06<04:23,  2.48it/s] 21%|       | 178/830 [01:06<03:59,  2.72it/s] 22%|       | 179/830 [01:06<03:42,  2.93it/s] 22%|       | 180/830 [01:07<03:30,  3.09it/s] 22%|       | 181/830 [01:07<03:21,  3.21it/s] 22%|       | 182/830 [01:07<03:15,  3.31it/s] 22%|       | 183/830 [01:08<03:11,  3.37it/s] 22%|       | 184/830 [01:08<03:08,  3.42it/s] 22%|       | 185/830 [01:08<03:12,  3.34it/s] 22%|       | 186/830 [01:08<03:09,  3.40it/s] 23%|       | 187/830 [01:09<03:06,  3.44it/s] 23%|       | 188/830 [01:09<03:04,  3.47it/s] 23%|       | 189/830 [01:09<03:03,  3.49it/s] 23%|       | 190/830 [01:10<03:02,  3.51it/s] 23%|       | 191/830 [01:10<03:01,  3.52it/s] 23%|       | 192/830 [01:10<03:00,  3.53it/s] 23%|       | 193/830 [01:10<02:59,  3.54it/s] 23%|       | 194/830 [01:11<02:59,  3.54it/s] 23%|       | 195/830 [01:11<02:59,  3.54it/s] 24%|       | 196/830 [01:11<03:01,  3.49it/s] 24%|       | 197/830 [01:12<03:00,  3.51it/s] 24%|       | 198/830 [01:12<02:59,  3.52it/s] 24%|       | 199/830 [01:12<02:58,  3.53it/s] 24%|       | 200/830 [01:12<02:58,  3.53it/s] 24%|       | 201/830 [01:13<02:57,  3.54it/s] 24%|       | 202/830 [01:13<02:57,  3.54it/s] 24%|       | 203/830 [01:13<02:56,  3.54it/s] 25%|       | 204/830 [01:14<02:56,  3.55it/s] 25%|       | 205/830 [01:14<02:56,  3.55it/s] 25%|       | 206/830 [01:14<02:55,  3.55it/s] 25%|       | 207/830 [01:14<03:00,  3.45it/s] 25%|       | 208/830 [01:15<02:58,  3.48it/s] 25%|       | 209/830 [01:15<02:57,  3.50it/s] 25%|       | 210/830 [01:15<02:56,  3.51it/s] 25%|       | 211/830 [01:16<02:55,  3.53it/s] 26%|       | 212/830 [01:16<02:54,  3.53it/s] 26%|       | 213/830 [01:16<02:54,  3.54it/s] 26%|       | 214/830 [01:16<02:53,  3.54it/s] 26%|       | 215/830 [01:17<02:53,  3.55it/s] 26%|       | 216/830 [01:17<02:53,  3.55it/s] 26%|       | 217/830 [01:17<02:52,  3.54it/s] 26%|       | 218/830 [01:18<03:00,  3.39it/s] 26%|       | 219/830 [01:18<02:57,  3.44it/s] 27%|       | 220/830 [01:18<02:55,  3.47it/s] 27%|       | 221/830 [01:18<02:54,  3.49it/s] 27%|       | 222/830 [01:19<03:12,  3.15it/s] 27%|       | 223/830 [01:19<03:06,  3.26it/s] 27%|       | 224/830 [01:19<03:01,  3.34it/s] 27%|       | 225/830 [01:20<02:58,  3.40it/s] 27%|       | 226/830 [01:20<02:55,  3.44it/s] 27%|       | 227/830 [01:20<02:53,  3.48it/s] 27%|       | 228/830 [01:21<02:51,  3.50it/s] 28%|       | 229/830 [01:21<02:51,  3.51it/s] 28%|       | 230/830 [01:21<02:50,  3.52it/s] 28%|       | 231/830 [01:21<02:49,  3.53it/s] 28%|       | 232/830 [01:22<02:49,  3.52it/s] 28%|       | 233/830 [01:22<02:59,  3.33it/s] 28%|       | 234/830 [01:22<02:55,  3.39it/s] 28%|       | 235/830 [01:23<02:53,  3.44it/s] 28%|       | 236/830 [01:23<02:51,  3.47it/s] 29%|       | 237/830 [01:23<02:49,  3.50it/s] 29%|       | 238/830 [01:23<02:48,  3.51it/s] 29%|       | 239/830 [01:24<02:47,  3.52it/s] 29%|       | 240/830 [01:24<02:47,  3.53it/s] 29%|       | 241/830 [01:24<02:46,  3.54it/s] 29%|       | 242/830 [01:25<02:45,  3.54it/s] 29%|       | 243/830 [01:25<02:44,  3.56it/s] 29%|       | 244/830 [01:25<02:49,  3.45it/s] 30%|       | 245/830 [01:25<02:47,  3.49it/s] 30%|       | 246/830 [01:26<02:45,  3.52it/s] 30%|       | 247/830 [01:26<02:44,  3.54it/s] 30%|       | 248/830 [01:26<03:05,  3.13it/s] 30%|       | 249/830 [01:27<03:48,  2.54it/s] 30%|       | 250/830 [01:27<03:28,  2.78it/s] 30%|       | 251/830 [01:27<03:14,  2.98it/s] 30%|       | 252/830 [01:28<03:03,  3.14it/s] 30%|       | 253/830 [01:28<02:56,  3.27it/s] 31%|       | 254/830 [01:28<02:59,  3.21it/s] 31%|       | 255/830 [01:29<02:53,  3.32it/s] 31%|       | 256/830 [01:29<02:48,  3.40it/s] 31%|       | 257/830 [01:29<02:45,  3.46it/s] 31%|       | 258/830 [01:29<02:43,  3.49it/s] 31%|       | 259/830 [01:30<02:42,  3.52it/s] 31%|      | 260/830 [01:30<02:40,  3.54it/s] 31%|      | 261/830 [01:30<02:39,  3.56it/s] 32%|      | 262/830 [01:31<02:39,  3.57it/s] 32%|      | 263/830 [01:31<02:38,  3.57it/s] 32%|      | 264/830 [01:31<02:38,  3.58it/s] 32%|      | 265/830 [01:31<02:40,  3.52it/s] 32%|      | 266/830 [01:32<02:39,  3.54it/s] 32%|      | 267/830 [01:32<02:38,  3.56it/s] 32%|      | 268/830 [01:32<02:37,  3.57it/s] 32%|      | 269/830 [01:33<02:36,  3.58it/s] 33%|      | 270/830 [01:33<02:36,  3.58it/s] 33%|      | 271/830 [01:33<02:35,  3.59it/s] 33%|      | 272/830 [01:33<02:35,  3.59it/s] 33%|      | 273/830 [01:34<02:35,  3.59it/s] 33%|      | 274/830 [01:34<02:34,  3.59it/s] 33%|      | 275/830 [01:34<02:34,  3.59it/s] 33%|      | 276/830 [01:34<02:36,  3.55it/s] 33%|      | 277/830 [01:35<02:35,  3.56it/s] 33%|      | 278/830 [01:35<02:34,  3.57it/s] 34%|      | 279/830 [01:35<02:34,  3.58it/s] 34%|      | 280/830 [01:36<02:33,  3.58it/s] 34%|      | 281/830 [01:36<02:33,  3.58it/s] 34%|      | 282/830 [01:36<02:32,  3.59it/s] 34%|      | 283/830 [01:36<02:32,  3.59it/s] 34%|      | 284/830 [01:37<02:31,  3.59it/s] 34%|      | 285/830 [01:37<02:31,  3.60it/s] 34%|      | 286/830 [01:37<02:31,  3.60it/s] 35%|      | 287/830 [01:38<02:38,  3.43it/s] 35%|      | 288/830 [01:38<02:35,  3.48it/s] 35%|      | 289/830 [01:38<02:33,  3.52it/s] 35%|      | 290/830 [01:38<02:32,  3.53it/s] 35%|      | 291/830 [01:39<02:31,  3.55it/s] 35%|      | 292/830 [01:39<02:30,  3.56it/s] 35%|      | 293/830 [01:39<02:30,  3.57it/s] 35%|      | 294/830 [01:40<02:29,  3.58it/s] 36%|      | 295/830 [01:40<02:29,  3.58it/s] 36%|      | 296/830 [01:40<02:28,  3.59it/s] 36%|      | 297/830 [01:40<02:28,  3.59it/s] 36%|      | 298/830 [01:41<02:30,  3.54it/s] 36%|      | 299/830 [01:41<02:29,  3.56it/s] 36%|      | 300/830 [01:41<02:28,  3.57it/s] 36%|      | 301/830 [01:42<02:28,  3.57it/s] 36%|      | 302/830 [01:42<02:27,  3.58it/s] 37%|      | 303/830 [01:42<02:27,  3.58it/s] 37%|      | 304/830 [01:42<02:26,  3.59it/s] 37%|      | 305/830 [01:43<02:26,  3.59it/s] 37%|      | 306/830 [01:43<02:26,  3.59it/s] 37%|      | 307/830 [01:43<02:25,  3.59it/s] 37%|      | 308/830 [01:43<02:25,  3.59it/s] 37%|      | 309/830 [01:44<02:27,  3.52it/s] 37%|      | 310/830 [01:44<02:26,  3.55it/s] 37%|      | 311/830 [01:44<02:25,  3.56it/s] 38%|      | 312/830 [01:45<02:25,  3.57it/s] 38%|      | 313/830 [01:45<02:24,  3.58it/s] 38%|      | 314/830 [01:45<02:24,  3.58it/s] 38%|      | 315/830 [01:45<02:23,  3.59it/s] 38%|      | 316/830 [01:46<02:23,  3.59it/s] 38%|      | 317/830 [01:46<02:22,  3.59it/s] 38%|      | 318/830 [01:46<02:22,  3.60it/s] 38%|      | 319/830 [01:47<02:22,  3.60it/s] 39%|      | 320/830 [01:47<02:25,  3.50it/s] 39%|      | 321/830 [01:47<02:24,  3.53it/s] 39%|      | 322/830 [01:47<02:23,  3.55it/s] 39%|      | 323/830 [01:48<02:22,  3.57it/s] 39%|      | 324/830 [01:48<02:21,  3.58it/s] 39%|      | 325/830 [01:48<02:21,  3.58it/s] 39%|      | 326/830 [01:49<02:20,  3.58it/s] 39%|      | 327/830 [01:49<02:20,  3.58it/s] 40%|      | 328/830 [01:49<02:20,  3.59it/s] 40%|      | 329/830 [01:49<02:19,  3.59it/s] 40%|      | 330/830 [01:50<02:19,  3.59it/s] 40%|      | 331/830 [01:50<02:19,  3.59it/s] 40%|      | 332/830 [01:50<02:05,  3.98it/s][INFO|trainer.py:2140] 2023-08-28 10:25:15,315 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:25:15,315 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 10:25:15,315 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8513, 'eval_samples_per_second': 353.455, 'eval_steps_per_second': 44.258, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.17it/s][A
  3%|         | 12/436 [00:00<00:08, 49.19it/s][A
  4%|         | 17/436 [00:00<00:08, 47.41it/s][A
  5%|         | 22/436 [00:00<00:08, 46.44it/s][A
  6%|         | 27/436 [00:00<00:08, 45.92it/s][A
  7%|         | 32/436 [00:00<00:08, 45.28it/s][A
  8%|         | 37/436 [00:00<00:08, 44.94it/s][A
 10%|         | 42/436 [00:00<00:08, 44.55it/s][A
 11%|         | 47/436 [00:01<00:08, 44.52it/s][A
 12%|        | 52/436 [00:01<00:08, 44.84it/s][A
 13%|        | 57/436 [00:01<00:08, 44.86it/s][A
 14%|        | 62/436 [00:01<00:08, 44.94it/s][A
 15%|        | 67/436 [00:01<00:08, 44.95it/s][A
 17%|        | 72/436 [00:01<00:08, 44.75it/s][A
 18%|        | 77/436 [00:01<00:08, 44.69it/s][A
 19%|        | 82/436 [00:01<00:08, 44.03it/s][A
 20%|        | 87/436 [00:01<00:07, 44.01it/s][A
 21%|        | 92/436 [00:02<00:07, 44.29it/s][A
 22%|       | 97/436 [00:02<00:07, 42.45it/s][A
 23%|       | 102/436 [00:02<00:07, 43.32it/s][A
 25%|       | 107/436 [00:02<00:07, 43.88it/s][A
 26%|       | 112/436 [00:02<00:07, 44.27it/s][A
 27%|       | 117/436 [00:02<00:07, 44.53it/s][A
 28%|       | 122/436 [00:02<00:07, 44.55it/s][A
 29%|       | 127/436 [00:02<00:06, 44.35it/s][A
 30%|       | 132/436 [00:02<00:06, 44.34it/s][A
 31%|      | 137/436 [00:03<00:06, 44.18it/s][A
 33%|      | 142/436 [00:03<00:06, 44.35it/s][A
 34%|      | 147/436 [00:03<00:06, 44.48it/s][A
 35%|      | 152/436 [00:03<00:06, 44.72it/s][A
 36%|      | 157/436 [00:03<00:06, 44.93it/s][A
 37%|      | 162/436 [00:03<00:06, 44.95it/s][A
 38%|      | 167/436 [00:03<00:06, 44.82it/s][A
 39%|      | 172/436 [00:03<00:05, 44.64it/s][A
 41%|      | 177/436 [00:03<00:05, 44.50it/s][A
 42%|     | 182/436 [00:04<00:05, 44.40it/s][A
 43%|     | 187/436 [00:04<00:05, 44.44it/s][A
 44%|     | 192/436 [00:04<00:05, 44.46it/s][A
 45%|     | 197/436 [00:04<00:05, 44.78it/s][A
 46%|     | 202/436 [00:04<00:05, 44.84it/s][A
 47%|     | 207/436 [00:04<00:05, 44.92it/s][A
 49%|     | 212/436 [00:04<00:04, 44.92it/s][A
 50%|     | 217/436 [00:04<00:04, 44.68it/s][A
 51%|     | 222/436 [00:04<00:04, 44.53it/s][A
 52%|    | 227/436 [00:05<00:04, 44.48it/s][A
 53%|    | 232/436 [00:05<00:04, 42.88it/s][A
 54%|    | 237/436 [00:05<00:04, 43.53it/s][A
 56%|    | 242/436 [00:05<00:04, 44.03it/s][A
 57%|    | 247/436 [00:05<00:04, 44.46it/s][A
 58%|    | 252/436 [00:05<00:04, 44.64it/s][A
 59%|    | 257/436 [00:05<00:04, 44.67it/s][A
 60%|    | 262/436 [00:05<00:03, 44.55it/s][A
 61%|    | 267/436 [00:05<00:03, 44.31it/s][A
 62%|   | 272/436 [00:06<00:03, 44.24it/s][A
 64%|   | 277/436 [00:06<00:03, 44.37it/s][A
 65%|   | 282/436 [00:06<00:03, 44.43it/s][A
 66%|   | 287/436 [00:06<00:03, 44.67it/s][A
 67%|   | 292/436 [00:06<00:03, 44.84it/s][A
 68%|   | 297/436 [00:06<00:03, 44.97it/s][A
 69%|   | 302/436 [00:06<00:02, 44.95it/s][A
 70%|   | 307/436 [00:06<00:02, 44.81it/s][A
 72%|  | 312/436 [00:06<00:02, 44.42it/s][A
 73%|  | 317/436 [00:07<00:02, 44.33it/s][A
 74%|  | 322/436 [00:07<00:02, 44.37it/s][A
 75%|  | 327/436 [00:07<00:02, 44.53it/s][A
 76%|  | 332/436 [00:07<00:02, 44.70it/s][A
 77%|  | 337/436 [00:07<00:02, 44.88it/s][A
 78%|  | 342/436 [00:07<00:02, 45.05it/s][A
 80%|  | 347/436 [00:07<00:01, 45.05it/s][A
 81%|  | 352/436 [00:07<00:01, 44.72it/s][A
 82%| | 357/436 [00:07<00:01, 44.57it/s][A
 83%| | 362/436 [00:08<00:01, 44.50it/s][A
 84%| | 367/436 [00:08<00:01, 43.12it/s][A
 85%| | 372/436 [00:08<00:01, 43.77it/s][A
 86%| | 377/436 [00:08<00:01, 44.17it/s][A
 88%| | 382/436 [00:08<00:01, 44.41it/s][A
 89%| | 387/436 [00:08<00:01, 44.67it/s][A
 90%| | 392/436 [00:08<00:00, 44.73it/s][A
 91%| | 397/436 [00:08<00:00, 44.58it/s][A
 92%|| 402/436 [00:09<00:00, 44.39it/s][A
 93%|| 407/436 [00:09<00:00, 44.28it/s][A
 94%|| 412/436 [00:09<00:00, 44.37it/s][A
 96%|| 417/436 [00:09<00:00, 44.63it/s][A
 97%|| 422/436 [00:09<00:00, 44.81it/s][A
 98%|| 427/436 [00:09<00:00, 44.86it/s][A
 99%|| 432/436 [00:09<00:00, 44.88it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.88it/s][A 40%|      | 332/830 [02:00<02:05,  3.98it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:25:25,342 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 10:25:25,589 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:25:28,038 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:25:28,154 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:25:28,221 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-332/special_tokens_map.json
 40%|      | 333/830 [02:04<36:15,  4.38s/it] 40%|      | 334/830 [02:04<26:01,  3.15s/it] 40%|      | 335/830 [02:05<18:52,  2.29s/it] 40%|      | 336/830 [02:05<13:53,  1.69s/it] 41%|      | 337/830 [02:05<10:23,  1.27s/it] 41%|      | 338/830 [02:06<07:57,  1.03it/s] 41%|      | 339/830 [02:06<06:15,  1.31it/s] 41%|      | 340/830 [02:06<05:03,  1.62it/s] 41%|      | 341/830 [02:06<04:14,  1.92it/s] 41%|      | 342/830 [02:07<03:38,  2.23it/s] 41%|     | 343/830 [02:07<03:14,  2.51it/s] 41%|     | 344/830 [02:07<02:56,  2.75it/s] 42%|     | 345/830 [02:07<02:44,  2.95it/s] 42%|     | 346/830 [02:08<02:35,  3.11it/s] 42%|     | 347/830 [02:08<02:29,  3.23it/s] 42%|     | 348/830 [02:08<02:25,  3.32it/s] 42%|     | 349/830 [02:09<02:21,  3.39it/s] 42%|     | 350/830 [02:09<02:19,  3.44it/s] 42%|     | 351/830 [02:09<02:17,  3.48it/s] 42%|     | 352/830 [02:09<02:21,  3.37it/s] 43%|     | 353/830 [02:10<02:19,  3.42it/s] 43%|     | 354/830 [02:10<02:17,  3.46it/s] 43%|     | 355/830 [02:10<02:16,  3.49it/s] 43%|     | 356/830 [02:11<02:15,  3.51it/s] 43%|     | 357/830 [02:11<02:14,  3.52it/s] 43%|     | 358/830 [02:11<02:13,  3.52it/s] 43%|     | 359/830 [02:11<02:13,  3.53it/s] 43%|     | 360/830 [02:12<02:12,  3.54it/s] 43%|     | 361/830 [02:12<02:12,  3.54it/s] 44%|     | 362/830 [02:12<02:12,  3.54it/s] 44%|     | 363/830 [02:13<02:14,  3.47it/s] 44%|     | 364/830 [02:13<02:13,  3.50it/s] 44%|     | 365/830 [02:13<02:12,  3.51it/s] 44%|     | 366/830 [02:13<02:11,  3.52it/s] 44%|     | 367/830 [02:14<02:11,  3.53it/s] 44%|     | 368/830 [02:14<02:10,  3.53it/s] 44%|     | 369/830 [02:14<02:10,  3.53it/s] 45%|     | 370/830 [02:15<02:10,  3.54it/s] 45%|     | 371/830 [02:15<02:09,  3.54it/s] 45%|     | 372/830 [02:15<02:09,  3.54it/s] 45%|     | 373/830 [02:15<02:08,  3.55it/s] 45%|     | 374/830 [02:16<02:10,  3.50it/s] 45%|     | 375/830 [02:16<02:09,  3.52it/s] 45%|     | 376/830 [02:16<02:08,  3.53it/s] 45%|     | 377/830 [02:17<02:08,  3.54it/s] 46%|     | 378/830 [02:17<02:07,  3.55it/s] 46%|     | 379/830 [02:17<02:06,  3.57it/s] 46%|     | 380/830 [02:17<02:05,  3.58it/s] 46%|     | 381/830 [02:18<02:05,  3.58it/s] 46%|     | 382/830 [02:18<02:04,  3.59it/s] 46%|     | 383/830 [02:18<02:04,  3.59it/s] 46%|     | 384/830 [02:19<02:04,  3.59it/s] 46%|     | 385/830 [02:19<02:10,  3.40it/s] 47%|     | 386/830 [02:19<02:08,  3.46it/s] 47%|     | 387/830 [02:19<02:06,  3.50it/s] 47%|     | 388/830 [02:20<02:05,  3.53it/s] 47%|     | 389/830 [02:20<02:04,  3.55it/s] 47%|     | 390/830 [02:20<02:03,  3.56it/s] 47%|     | 391/830 [02:21<02:02,  3.57it/s] 47%|     | 392/830 [02:21<02:02,  3.58it/s] 47%|     | 393/830 [02:21<02:01,  3.59it/s] 47%|     | 394/830 [02:21<02:01,  3.59it/s] 48%|     | 395/830 [02:22<02:00,  3.60it/s] 48%|     | 396/830 [02:22<02:00,  3.60it/s] 48%|     | 397/830 [02:22<02:00,  3.60it/s] 48%|     | 398/830 [02:22<02:00,  3.60it/s] 48%|     | 399/830 [02:23<01:59,  3.60it/s] 48%|     | 400/830 [02:23<01:59,  3.60it/s] 48%|     | 401/830 [02:23<02:04,  3.46it/s] 48%|     | 402/830 [02:24<02:02,  3.50it/s] 49%|     | 403/830 [02:24<02:00,  3.53it/s] 49%|     | 404/830 [02:24<01:59,  3.56it/s] 49%|     | 405/830 [02:24<01:59,  3.57it/s] 49%|     | 406/830 [02:25<01:58,  3.58it/s] 49%|     | 407/830 [02:25<01:58,  3.58it/s] 49%|     | 408/830 [02:25<01:57,  3.59it/s] 49%|     | 409/830 [02:26<01:57,  3.59it/s] 49%|     | 410/830 [02:26<01:56,  3.59it/s] 50%|     | 411/830 [02:26<01:56,  3.60it/s] 50%|     | 412/830 [02:26<02:00,  3.47it/s] 50%|     | 413/830 [02:27<02:30,  2.76it/s] 50%|     | 414/830 [02:27<02:20,  2.96it/s] 50%|     | 415/830 [02:28<02:12,  3.13it/s] 50%|     | 416/830 [02:28<02:06,  3.26it/s] 50%|     | 417/830 [02:28<02:03,  3.36it/s] 50%|     | 418/830 [02:28<02:00,  3.43it/s] 50%|     | 419/830 [02:29<01:58,  3.48it/s] 51%|     | 420/830 [02:29<01:56,  3.51it/s] 51%|     | 421/830 [02:29<01:55,  3.54it/s] 51%|     | 422/830 [02:29<01:58,  3.45it/s] 51%|     | 423/830 [02:30<01:56,  3.49it/s] 51%|     | 424/830 [02:30<01:55,  3.52it/s] 51%|     | 425/830 [02:30<01:54,  3.55it/s] 51%|    | 426/830 [02:31<01:53,  3.56it/s] 51%|    | 427/830 [02:31<01:52,  3.58it/s] 52%|    | 428/830 [02:31<01:52,  3.58it/s] 52%|    | 429/830 [02:31<01:51,  3.59it/s] 52%|    | 430/830 [02:32<01:51,  3.59it/s] 52%|    | 431/830 [02:32<01:50,  3.60it/s] 52%|    | 432/830 [02:32<01:50,  3.60it/s] 52%|    | 433/830 [02:33<01:54,  3.48it/s] 52%|    | 434/830 [02:33<01:52,  3.51it/s] 52%|    | 435/830 [02:33<01:51,  3.54it/s] 53%|    | 436/830 [02:33<01:50,  3.56it/s] 53%|    | 437/830 [02:34<01:49,  3.58it/s] 53%|    | 438/830 [02:34<01:49,  3.58it/s] 53%|    | 439/830 [02:34<01:48,  3.59it/s] 53%|    | 440/830 [02:35<01:48,  3.59it/s] 53%|    | 441/830 [02:35<01:48,  3.59it/s] 53%|    | 442/830 [02:35<01:47,  3.60it/s] 53%|    | 443/830 [02:35<01:47,  3.60it/s] 53%|    | 444/830 [02:36<01:50,  3.50it/s] 54%|    | 445/830 [02:36<01:49,  3.53it/s] 54%|    | 446/830 [02:36<01:48,  3.55it/s] 54%|    | 447/830 [02:36<01:47,  3.57it/s] 54%|    | 448/830 [02:37<01:46,  3.58it/s] 54%|    | 449/830 [02:37<01:46,  3.58it/s] 54%|    | 450/830 [02:37<01:45,  3.59it/s] 54%|    | 451/830 [02:38<01:45,  3.59it/s] 54%|    | 452/830 [02:38<01:45,  3.59it/s] 55%|    | 453/830 [02:38<01:44,  3.59it/s] 55%|    | 454/830 [02:38<01:44,  3.60it/s] 55%|    | 455/830 [02:39<01:46,  3.52it/s] 55%|    | 456/830 [02:39<01:45,  3.54it/s] 55%|    | 457/830 [02:39<01:44,  3.56it/s] 55%|    | 458/830 [02:40<01:44,  3.57it/s] 55%|    | 459/830 [02:40<01:43,  3.58it/s] 55%|    | 460/830 [02:40<01:43,  3.58it/s] 56%|    | 461/830 [02:40<01:43,  3.58it/s] 56%|    | 462/830 [02:41<01:42,  3.58it/s] 56%|    | 463/830 [02:41<01:42,  3.59it/s] 56%|    | 464/830 [02:41<01:41,  3.59it/s] 56%|    | 465/830 [02:42<01:41,  3.59it/s] 56%|    | 466/830 [02:42<01:48,  3.34it/s] 56%|    | 467/830 [02:42<01:46,  3.41it/s] 56%|    | 468/830 [02:42<01:44,  3.47it/s] 57%|    | 469/830 [02:43<01:42,  3.51it/s] 57%|    | 470/830 [02:43<01:41,  3.54it/s] 57%|    | 471/830 [02:43<01:40,  3.56it/s] 57%|    | 472/830 [02:44<01:40,  3.57it/s] 57%|    | 473/830 [02:44<01:39,  3.58it/s] 57%|    | 474/830 [02:44<01:39,  3.58it/s] 57%|    | 475/830 [02:44<01:39,  3.58it/s] 57%|    | 476/830 [02:45<01:38,  3.58it/s] 57%|    | 477/830 [02:45<01:40,  3.50it/s] 58%|    | 478/830 [02:45<01:39,  3.53it/s] 58%|    | 479/830 [02:45<01:38,  3.55it/s] 58%|    | 480/830 [02:46<01:38,  3.56it/s] 58%|    | 481/830 [02:46<01:37,  3.58it/s] 58%|    | 482/830 [02:46<01:37,  3.58it/s] 58%|    | 483/830 [02:47<01:36,  3.59it/s] 58%|    | 484/830 [02:47<01:36,  3.59it/s] 58%|    | 485/830 [02:47<01:35,  3.60it/s] 59%|    | 486/830 [02:47<01:35,  3.60it/s] 59%|    | 487/830 [02:48<01:35,  3.60it/s] 59%|    | 488/830 [02:48<01:38,  3.46it/s] 59%|    | 489/830 [02:48<01:37,  3.50it/s] 59%|    | 490/830 [02:49<01:36,  3.53it/s] 59%|    | 491/830 [02:49<01:35,  3.55it/s] 59%|    | 492/830 [02:49<01:34,  3.57it/s] 59%|    | 493/830 [02:49<01:34,  3.58it/s] 60%|    | 494/830 [02:50<01:33,  3.59it/s] 60%|    | 495/830 [02:50<01:33,  3.59it/s] 60%|    | 496/830 [02:50<01:32,  3.59it/s] 60%|    | 497/830 [02:51<01:32,  3.59it/s] 60%|    | 498/830 [02:51<01:23,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 10:26:15,937 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:26:15,937 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 10:26:15,938 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7865, 'eval_samples_per_second': 355.796, 'eval_steps_per_second': 44.551, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.56it/s][A
  3%|         | 12/436 [00:00<00:08, 49.28it/s][A
  4%|         | 17/436 [00:00<00:08, 47.64it/s][A
  5%|         | 22/436 [00:00<00:08, 46.44it/s][A
  6%|         | 27/436 [00:00<00:08, 45.79it/s][A
  7%|         | 32/436 [00:00<00:08, 45.20it/s][A
  8%|         | 37/436 [00:00<00:08, 44.83it/s][A
 10%|         | 42/436 [00:00<00:08, 44.52it/s][A
 11%|         | 47/436 [00:01<00:08, 44.58it/s][A
 12%|        | 52/436 [00:01<00:08, 44.83it/s][A
 13%|        | 57/436 [00:01<00:08, 44.95it/s][A
 14%|        | 62/436 [00:01<00:08, 44.95it/s][A
 15%|        | 67/436 [00:01<00:08, 45.00it/s][A
 17%|        | 72/436 [00:01<00:08, 44.86it/s][A
 18%|        | 77/436 [00:01<00:08, 44.77it/s][A
 19%|        | 82/436 [00:01<00:08, 44.06it/s][A
 20%|        | 87/436 [00:01<00:07, 44.09it/s][A
 21%|        | 92/436 [00:02<00:07, 44.38it/s][A
 22%|       | 97/436 [00:02<00:07, 44.54it/s][A
 23%|       | 102/436 [00:02<00:07, 44.79it/s][A
 25%|       | 107/436 [00:02<00:07, 44.91it/s][A
 26%|       | 112/436 [00:02<00:07, 44.87it/s][A
 27%|       | 117/436 [00:02<00:07, 44.77it/s][A
 28%|       | 122/436 [00:02<00:07, 44.56it/s][A
 29%|       | 127/436 [00:02<00:06, 44.48it/s][A
 30%|       | 132/436 [00:02<00:06, 44.47it/s][A
 31%|      | 137/436 [00:03<00:06, 43.68it/s][A
 33%|      | 142/436 [00:03<00:06, 44.12it/s][A
 34%|      | 147/436 [00:03<00:06, 44.46it/s][A
 35%|      | 152/436 [00:03<00:06, 44.62it/s][A
 36%|      | 157/436 [00:03<00:06, 44.82it/s][A
 37%|      | 162/436 [00:03<00:06, 44.62it/s][A
 38%|      | 167/436 [00:03<00:06, 44.63it/s][A
 39%|      | 172/436 [00:03<00:05, 44.51it/s][A
 41%|      | 177/436 [00:03<00:05, 44.30it/s][A
 42%|     | 182/436 [00:04<00:05, 44.52it/s][A
 43%|     | 187/436 [00:04<00:05, 44.61it/s][A
 44%|     | 192/436 [00:04<00:05, 44.81it/s][A
 45%|     | 197/436 [00:04<00:05, 44.91it/s][A
 46%|     | 202/436 [00:04<00:05, 44.91it/s][A
 47%|     | 207/436 [00:04<00:05, 44.86it/s][A
 49%|     | 212/436 [00:04<00:05, 44.67it/s][A
 50%|     | 217/436 [00:04<00:04, 44.51it/s][A
 51%|     | 222/436 [00:04<00:04, 44.45it/s][A
 52%|    | 227/436 [00:05<00:04, 44.47it/s][A
 53%|    | 232/436 [00:05<00:04, 44.59it/s][A
 54%|    | 237/436 [00:05<00:04, 44.76it/s][A
 56%|    | 242/436 [00:05<00:04, 44.95it/s][A
 57%|    | 247/436 [00:05<00:04, 44.86it/s][A
 58%|    | 252/436 [00:05<00:04, 44.69it/s][A
 59%|    | 257/436 [00:05<00:04, 44.69it/s][A
 60%|    | 262/436 [00:05<00:03, 44.54it/s][A
 61%|    | 267/436 [00:05<00:03, 44.46it/s][A
 62%|   | 272/436 [00:06<00:03, 44.09it/s][A
 64%|   | 277/436 [00:06<00:03, 44.36it/s][A
 65%|   | 282/436 [00:06<00:03, 44.62it/s][A
 66%|   | 287/436 [00:06<00:03, 44.78it/s][A
 67%|   | 292/436 [00:06<00:03, 44.89it/s][A
 68%|   | 297/436 [00:06<00:03, 44.84it/s][A
 69%|   | 302/436 [00:06<00:02, 44.71it/s][A
 70%|   | 307/436 [00:06<00:02, 44.60it/s][A
 72%|  | 312/436 [00:06<00:02, 44.31it/s][A
 73%|  | 317/436 [00:07<00:02, 44.31it/s][A
 74%|  | 322/436 [00:07<00:02, 44.62it/s][A
 75%|  | 327/436 [00:07<00:02, 44.76it/s][A
 76%|  | 332/436 [00:07<00:02, 44.87it/s][A
 77%|  | 337/436 [00:07<00:02, 44.81it/s][A
 78%|  | 342/436 [00:07<00:02, 44.83it/s][A
 80%|  | 347/436 [00:07<00:01, 44.57it/s][A
 81%|  | 352/436 [00:07<00:01, 44.40it/s][A
 82%| | 357/436 [00:07<00:01, 44.43it/s][A
 83%| | 362/436 [00:08<00:01, 44.44it/s][A
 84%| | 367/436 [00:08<00:01, 44.58it/s][A
 85%| | 372/436 [00:08<00:01, 44.69it/s][A
 86%| | 377/436 [00:08<00:01, 44.87it/s][A
 88%| | 382/436 [00:08<00:01, 44.99it/s][A
 89%| | 387/436 [00:08<00:01, 44.82it/s][A
 90%| | 392/436 [00:08<00:00, 44.50it/s][A
 91%| | 397/436 [00:08<00:00, 44.59it/s][A
 92%|| 402/436 [00:08<00:00, 44.48it/s][A
 93%|| 407/436 [00:09<00:00, 44.03it/s][A
 94%|| 412/436 [00:09<00:00, 44.27it/s][A
 96%|| 417/436 [00:09<00:00, 44.46it/s][A
 97%|| 422/436 [00:09<00:00, 44.83it/s][A
 98%|| 427/436 [00:09<00:00, 44.89it/s][A
 99%|| 432/436 [00:09<00:00, 44.91it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.91it/s][A 60%|    | 498/830 [03:00<01:23,  3.99it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:26:25,823 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-498
[INFO|configuration_utils.py:351] 2023-08-28 10:26:25,944 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-498/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:26:28,687 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-498/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:26:28,835 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-498/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:26:28,917 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-498/special_tokens_map.json
 60%|    | 499/830 [03:05<24:27,  4.43s/it] 60%|    | 500/830 [03:05<17:32,  3.19s/it]                                                  60%|    | 500/830 [03:05<17:32,  3.19s/it] 60%|    | 501/830 [03:05<12:41,  2.32s/it] 60%|    | 502/830 [03:06<09:19,  1.71s/it] 61%|    | 503/830 [03:06<06:58,  1.28s/it] 61%|    | 504/830 [03:06<05:19,  1.02it/s] 61%|    | 505/830 [03:07<04:10,  1.30it/s] 61%|    | 506/830 [03:07<03:24,  1.58it/s] 61%|    | 507/830 [03:07<02:50,  1.90it/s] 61%|    | 508/830 [03:07<02:26,  2.21it/s] 61%|   | 509/830 [03:08<02:08,  2.49it/s] 61%|   | 510/830 [03:08<01:56,  2.74it/s] 62%|   | 511/830 [03:08<01:48,  2.94it/s] 62%|   | 512/830 [03:09<01:42,  3.10it/s] 62%|   | 513/830 [03:09<01:38,  3.23it/s] 62%|   | 514/830 [03:09<01:35,  3.32it/s] 62%|   | 515/830 [03:09<01:33,  3.39it/s] 62%|   | 516/830 [03:10<01:31,  3.43it/s] 62%|   | 517/830 [03:10<01:31,  3.43it/s] 62%|   | 518/830 [03:10<01:29,  3.47it/s] 63%|   | 519/830 [03:11<01:28,  3.50it/s] 63%|   | 520/830 [03:11<01:28,  3.51it/s] 63%|   | 521/830 [03:11<01:27,  3.52it/s] 63%|   | 522/830 [03:11<01:27,  3.53it/s] 63%|   | 523/830 [03:12<01:26,  3.54it/s] 63%|   | 524/830 [03:12<01:26,  3.54it/s] 63%|   | 525/830 [03:12<01:26,  3.54it/s] 63%|   | 526/830 [03:13<01:25,  3.55it/s] 63%|   | 527/830 [03:13<01:25,  3.54it/s] 64%|   | 528/830 [03:13<01:27,  3.46it/s] 64%|   | 529/830 [03:13<01:26,  3.49it/s] 64%|   | 530/830 [03:14<01:25,  3.51it/s] 64%|   | 531/830 [03:14<01:24,  3.52it/s] 64%|   | 532/830 [03:14<01:24,  3.53it/s] 64%|   | 533/830 [03:15<01:23,  3.54it/s] 64%|   | 534/830 [03:15<01:23,  3.54it/s] 64%|   | 535/830 [03:15<01:23,  3.53it/s] 65%|   | 536/830 [03:15<01:23,  3.54it/s] 65%|   | 537/830 [03:16<01:22,  3.54it/s] 65%|   | 538/830 [03:16<01:22,  3.54it/s] 65%|   | 539/830 [03:16<01:24,  3.44it/s] 65%|   | 540/830 [03:17<01:23,  3.47it/s] 65%|   | 541/830 [03:17<01:22,  3.49it/s] 65%|   | 542/830 [03:17<01:22,  3.51it/s] 65%|   | 543/830 [03:17<01:21,  3.52it/s] 66%|   | 544/830 [03:18<01:21,  3.53it/s] 66%|   | 545/830 [03:18<01:20,  3.54it/s] 66%|   | 546/830 [03:18<01:20,  3.54it/s] 66%|   | 547/830 [03:19<01:19,  3.55it/s] 66%|   | 548/830 [03:19<01:19,  3.57it/s] 66%|   | 549/830 [03:19<01:18,  3.58it/s] 66%|   | 550/830 [03:19<01:22,  3.38it/s] 66%|   | 551/830 [03:20<01:21,  3.44it/s] 67%|   | 552/830 [03:20<01:19,  3.49it/s] 67%|   | 553/830 [03:20<01:18,  3.52it/s] 67%|   | 554/830 [03:21<01:17,  3.55it/s] 67%|   | 555/830 [03:21<01:17,  3.56it/s] 67%|   | 556/830 [03:21<01:16,  3.57it/s] 67%|   | 557/830 [03:21<01:16,  3.58it/s] 67%|   | 558/830 [03:22<01:21,  3.34it/s] 67%|   | 559/830 [03:22<01:19,  3.42it/s] 67%|   | 560/830 [03:22<01:17,  3.47it/s] 68%|   | 561/830 [03:23<01:16,  3.51it/s] 68%|   | 562/830 [03:23<01:15,  3.54it/s] 68%|   | 563/830 [03:23<01:15,  3.55it/s] 68%|   | 564/830 [03:23<01:14,  3.57it/s] 68%|   | 565/830 [03:24<01:14,  3.58it/s] 68%|   | 566/830 [03:24<01:13,  3.58it/s] 68%|   | 567/830 [03:24<01:13,  3.59it/s] 68%|   | 568/830 [03:24<01:12,  3.60it/s] 69%|   | 569/830 [03:25<01:16,  3.41it/s] 69%|   | 570/830 [03:25<01:15,  3.46it/s] 69%|   | 571/830 [03:25<01:13,  3.51it/s] 69%|   | 572/830 [03:26<01:13,  3.53it/s] 69%|   | 573/830 [03:26<01:12,  3.55it/s] 69%|   | 574/830 [03:26<01:11,  3.57it/s] 69%|   | 575/830 [03:26<01:11,  3.58it/s] 69%|   | 576/830 [03:27<01:28,  2.88it/s] 70%|   | 577/830 [03:27<01:22,  3.05it/s] 70%|   | 578/830 [03:28<01:18,  3.20it/s] 70%|   | 579/830 [03:28<01:18,  3.19it/s] 70%|   | 580/830 [03:28<01:15,  3.30it/s] 70%|   | 581/830 [03:28<01:13,  3.39it/s] 70%|   | 582/830 [03:29<01:11,  3.45it/s] 70%|   | 583/830 [03:29<01:10,  3.49it/s] 70%|   | 584/830 [03:29<01:09,  3.53it/s] 70%|   | 585/830 [03:30<01:09,  3.55it/s] 71%|   | 586/830 [03:30<01:08,  3.57it/s] 71%|   | 587/830 [03:30<01:07,  3.58it/s] 71%|   | 588/830 [03:30<01:07,  3.59it/s] 71%|   | 589/830 [03:31<01:07,  3.59it/s] 71%|   | 590/830 [03:31<01:08,  3.53it/s] 71%|   | 591/830 [03:31<01:07,  3.55it/s] 71%|  | 592/830 [03:31<01:06,  3.57it/s] 71%|  | 593/830 [03:32<01:06,  3.58it/s] 72%|  | 594/830 [03:32<01:05,  3.59it/s] 72%|  | 595/830 [03:32<01:05,  3.59it/s] 72%|  | 596/830 [03:33<01:05,  3.59it/s] 72%|  | 597/830 [03:33<01:04,  3.60it/s] 72%|  | 598/830 [03:33<01:04,  3.60it/s] 72%|  | 599/830 [03:33<01:04,  3.60it/s] 72%|  | 600/830 [03:34<01:03,  3.60it/s] 72%|  | 601/830 [03:34<01:07,  3.42it/s] 73%|  | 602/830 [03:34<01:05,  3.47it/s] 73%|  | 603/830 [03:35<01:04,  3.51it/s] 73%|  | 604/830 [03:35<01:03,  3.53it/s] 73%|  | 605/830 [03:35<01:03,  3.56it/s] 73%|  | 606/830 [03:35<01:02,  3.57it/s] 73%|  | 607/830 [03:36<01:02,  3.58it/s] 73%|  | 608/830 [03:36<01:01,  3.59it/s] 73%|  | 609/830 [03:36<01:01,  3.59it/s] 73%|  | 610/830 [03:37<01:01,  3.59it/s] 74%|  | 611/830 [03:37<01:00,  3.60it/s] 74%|  | 612/830 [03:37<01:02,  3.51it/s] 74%|  | 613/830 [03:37<01:01,  3.53it/s] 74%|  | 614/830 [03:38<01:00,  3.55it/s] 74%|  | 615/830 [03:38<01:00,  3.57it/s] 74%|  | 616/830 [03:38<00:59,  3.58it/s] 74%|  | 617/830 [03:38<00:59,  3.58it/s] 74%|  | 618/830 [03:39<00:59,  3.59it/s] 75%|  | 619/830 [03:39<00:58,  3.59it/s] 75%|  | 620/830 [03:39<00:58,  3.60it/s] 75%|  | 621/830 [03:40<00:58,  3.60it/s] 75%|  | 622/830 [03:40<00:57,  3.60it/s] 75%|  | 623/830 [03:40<01:00,  3.44it/s] 75%|  | 624/830 [03:40<00:59,  3.48it/s] 75%|  | 625/830 [03:41<00:58,  3.52it/s] 75%|  | 626/830 [03:41<00:57,  3.55it/s] 76%|  | 627/830 [03:41<00:56,  3.56it/s] 76%|  | 628/830 [03:42<00:56,  3.58it/s] 76%|  | 629/830 [03:42<00:56,  3.58it/s] 76%|  | 630/830 [03:42<00:55,  3.58it/s] 76%|  | 631/830 [03:42<00:55,  3.59it/s] 76%|  | 632/830 [03:43<00:55,  3.59it/s] 76%|  | 633/830 [03:43<00:54,  3.59it/s] 76%|  | 634/830 [03:43<00:55,  3.51it/s] 77%|  | 635/830 [03:44<00:55,  3.53it/s] 77%|  | 636/830 [03:44<00:54,  3.56it/s] 77%|  | 637/830 [03:44<00:54,  3.57it/s] 77%|  | 638/830 [03:44<00:53,  3.58it/s] 77%|  | 639/830 [03:45<00:53,  3.59it/s] 77%|  | 640/830 [03:45<00:52,  3.59it/s] 77%|  | 641/830 [03:45<00:52,  3.60it/s] 77%|  | 642/830 [03:45<00:52,  3.59it/s] 77%|  | 643/830 [03:46<00:52,  3.59it/s] 78%|  | 644/830 [03:46<00:51,  3.60it/s] 78%|  | 645/830 [03:46<00:52,  3.51it/s] 78%|  | 646/830 [03:47<00:52,  3.54it/s] 78%|  | 647/830 [03:47<00:51,  3.56it/s] 78%|  | 648/830 [03:47<00:50,  3.57it/s] 78%|  | 649/830 [03:47<00:50,  3.58it/s] 78%|  | 650/830 [03:48<00:50,  3.59it/s] 78%|  | 651/830 [03:48<00:49,  3.59it/s] 79%|  | 652/830 [03:48<00:49,  3.60it/s] 79%|  | 653/830 [03:49<00:49,  3.60it/s] 79%|  | 654/830 [03:49<00:48,  3.60it/s] 79%|  | 655/830 [03:49<00:48,  3.60it/s] 79%|  | 656/830 [03:49<00:49,  3.50it/s] 79%|  | 657/830 [03:50<00:49,  3.53it/s] 79%|  | 658/830 [03:50<00:48,  3.55it/s] 79%|  | 659/830 [03:50<00:47,  3.57it/s] 80%|  | 660/830 [03:51<00:47,  3.58it/s] 80%|  | 661/830 [03:51<00:47,  3.59it/s] 80%|  | 662/830 [03:51<00:46,  3.59it/s] 80%|  | 663/830 [03:51<00:46,  3.59it/s] 80%|  | 664/830 [03:52<00:41,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 10:27:16,780 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:27:16,780 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 10:27:16,780 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7593, 'eval_samples_per_second': 356.789, 'eval_steps_per_second': 44.675, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.2409638554216866e-05, 'epoch': 3.01}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.52it/s][A
  3%|         | 12/436 [00:00<00:08, 49.30it/s][A
  4%|         | 17/436 [00:00<00:08, 47.61it/s][A
  5%|         | 22/436 [00:00<00:08, 46.41it/s][A
  6%|         | 27/436 [00:00<00:09, 42.69it/s][A
  7%|         | 32/436 [00:00<00:09, 43.41it/s][A
  8%|         | 37/436 [00:00<00:09, 43.70it/s][A
 10%|         | 42/436 [00:00<00:08, 43.81it/s][A
 11%|         | 47/436 [00:01<00:08, 44.18it/s][A
 12%|        | 52/436 [00:01<00:08, 44.47it/s][A
 13%|        | 57/436 [00:01<00:08, 44.69it/s][A
 14%|        | 62/436 [00:01<00:08, 44.90it/s][A
 15%|        | 67/436 [00:01<00:08, 44.68it/s][A
 17%|        | 72/436 [00:01<00:08, 44.62it/s][A
 18%|        | 77/436 [00:01<00:08, 44.61it/s][A
 19%|        | 82/436 [00:01<00:07, 44.59it/s][A
 20%|        | 87/436 [00:01<00:07, 44.51it/s][A
 21%|        | 92/436 [00:02<00:07, 44.66it/s][A
 22%|       | 97/436 [00:02<00:07, 44.73it/s][A
 23%|       | 102/436 [00:02<00:07, 44.94it/s][A
 25%|       | 107/436 [00:02<00:07, 44.86it/s][A
 26%|       | 112/436 [00:02<00:07, 44.78it/s][A
 27%|       | 117/436 [00:02<00:07, 44.79it/s][A
 28%|       | 122/436 [00:02<00:07, 44.65it/s][A
 29%|       | 127/436 [00:02<00:06, 44.65it/s][A
 30%|       | 132/436 [00:02<00:06, 44.51it/s][A
 31%|      | 137/436 [00:03<00:06, 44.60it/s][A
 33%|      | 142/436 [00:03<00:06, 44.76it/s][A
 34%|      | 147/436 [00:03<00:06, 44.88it/s][A
 35%|      | 152/436 [00:03<00:06, 44.83it/s][A
 36%|      | 157/436 [00:03<00:06, 44.74it/s][A
 37%|      | 162/436 [00:03<00:06, 42.98it/s][A
 38%|      | 167/436 [00:03<00:06, 43.54it/s][A
 39%|      | 172/436 [00:03<00:06, 43.96it/s][A
 41%|      | 177/436 [00:03<00:05, 44.15it/s][A
 42%|     | 182/436 [00:04<00:05, 44.34it/s][A
 43%|     | 187/436 [00:04<00:05, 44.45it/s][A
 44%|     | 192/436 [00:04<00:05, 44.61it/s][A
 45%|     | 197/436 [00:04<00:05, 44.09it/s][A
 46%|     | 202/436 [00:04<00:05, 44.45it/s][A
 47%|     | 207/436 [00:04<00:05, 44.52it/s][A
 49%|     | 212/436 [00:04<00:05, 44.60it/s][A
 50%|     | 217/436 [00:04<00:04, 44.64it/s][A
 51%|     | 222/436 [00:04<00:04, 44.76it/s][A
 52%|    | 227/436 [00:05<00:04, 44.74it/s][A
 53%|    | 232/436 [00:05<00:04, 44.81it/s][A
 54%|    | 237/436 [00:05<00:04, 44.72it/s][A
 56%|    | 242/436 [00:05<00:04, 44.60it/s][A
 57%|    | 247/436 [00:05<00:04, 44.65it/s][A
 58%|    | 252/436 [00:05<00:04, 44.68it/s][A
 59%|    | 257/436 [00:05<00:03, 44.75it/s][A
 60%|    | 262/436 [00:05<00:03, 44.78it/s][A
 61%|    | 267/436 [00:05<00:03, 44.76it/s][A
 62%|   | 272/436 [00:06<00:03, 44.81it/s][A
 64%|   | 277/436 [00:06<00:03, 44.62it/s][A
 65%|   | 282/436 [00:06<00:03, 44.67it/s][A
 66%|   | 287/436 [00:06<00:03, 44.64it/s][A
 67%|   | 292/436 [00:06<00:03, 44.53it/s][A
 68%|   | 297/436 [00:06<00:03, 43.62it/s][A
 69%|   | 302/436 [00:06<00:03, 44.14it/s][A
 70%|   | 307/436 [00:06<00:02, 44.41it/s][A
 72%|  | 312/436 [00:06<00:02, 44.70it/s][A
 73%|  | 317/436 [00:07<00:02, 44.59it/s][A
 74%|  | 322/436 [00:07<00:02, 44.61it/s][A
 75%|  | 327/436 [00:07<00:02, 44.58it/s][A
 76%|  | 332/436 [00:07<00:02, 44.52it/s][A
 77%|  | 337/436 [00:07<00:02, 44.44it/s][A
 78%|  | 342/436 [00:07<00:02, 44.40it/s][A
 80%|  | 347/436 [00:07<00:01, 44.59it/s][A
 81%|  | 352/436 [00:07<00:01, 44.84it/s][A
 82%| | 357/436 [00:07<00:01, 44.89it/s][A
 83%| | 362/436 [00:08<00:01, 44.94it/s][A
 84%| | 367/436 [00:08<00:01, 44.76it/s][A
 85%| | 372/436 [00:08<00:01, 44.72it/s][A
 86%| | 377/436 [00:08<00:01, 44.77it/s][A
 88%| | 382/436 [00:08<00:01, 44.57it/s][A
 89%| | 387/436 [00:08<00:01, 44.61it/s][A
 90%| | 392/436 [00:08<00:00, 44.60it/s][A
 91%| | 397/436 [00:08<00:00, 44.81it/s][A
 92%|| 402/436 [00:09<00:00, 44.95it/s][A
 93%|| 407/436 [00:09<00:00, 44.81it/s][A
 94%|| 412/436 [00:09<00:00, 44.77it/s][A
 96%|| 417/436 [00:09<00:00, 44.58it/s][A
 97%|| 422/436 [00:09<00:00, 44.57it/s][A
 98%|| 427/436 [00:09<00:00, 44.61it/s][A
 99%|| 432/436 [00:09<00:00, 43.83it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 43.83it/s][A 80%|  | 664/830 [04:01<00:41,  3.99it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:27:26,676 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-664
[INFO|configuration_utils.py:351] 2023-08-28 10:27:26,763 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-664/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:27:29,285 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-664/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:27:29,379 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-664/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:27:29,471 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-664/special_tokens_map.json
 80%|  | 665/830 [04:06<12:10,  4.42s/it] 80%|  | 666/830 [04:06<08:41,  3.18s/it] 80%|  | 667/830 [04:06<06:16,  2.31s/it] 80%|  | 668/830 [04:07<04:35,  1.70s/it] 81%|  | 669/830 [04:07<03:25,  1.28s/it] 81%|  | 670/830 [04:07<02:36,  1.02it/s] 81%|  | 671/830 [04:07<02:02,  1.30it/s] 81%|  | 672/830 [04:08<01:39,  1.59it/s] 81%|  | 673/830 [04:08<01:22,  1.90it/s] 81%|  | 674/830 [04:08<01:10,  2.21it/s] 81%| | 675/830 [04:09<01:02,  2.49it/s] 81%| | 676/830 [04:09<00:56,  2.74it/s] 82%| | 677/830 [04:09<00:52,  2.94it/s] 82%| | 678/830 [04:09<00:49,  3.10it/s] 82%| | 679/830 [04:10<00:46,  3.22it/s] 82%| | 680/830 [04:10<00:45,  3.31it/s] 82%| | 681/830 [04:10<00:44,  3.38it/s] 82%| | 682/830 [04:11<00:43,  3.43it/s] 82%| | 683/830 [04:11<00:43,  3.42it/s] 82%| | 684/830 [04:11<00:42,  3.45it/s] 83%| | 685/830 [04:11<00:41,  3.49it/s] 83%| | 686/830 [04:12<00:41,  3.51it/s] 83%| | 687/830 [04:12<00:40,  3.52it/s] 83%| | 688/830 [04:12<00:40,  3.53it/s] 83%| | 689/830 [04:13<00:39,  3.55it/s] 83%| | 690/830 [04:13<00:39,  3.57it/s] 83%| | 691/830 [04:13<00:38,  3.58it/s] 83%| | 692/830 [04:13<00:38,  3.59it/s] 83%| | 693/830 [04:14<00:38,  3.59it/s] 84%| | 694/830 [04:14<00:38,  3.49it/s] 84%| | 695/830 [04:14<00:38,  3.52it/s] 84%| | 696/830 [04:14<00:37,  3.55it/s] 84%| | 697/830 [04:15<00:37,  3.56it/s] 84%| | 698/830 [04:15<00:36,  3.58it/s] 84%| | 699/830 [04:15<00:36,  3.58it/s] 84%| | 700/830 [04:16<00:36,  3.59it/s] 84%| | 701/830 [04:16<00:35,  3.60it/s] 85%| | 702/830 [04:16<00:35,  3.60it/s] 85%| | 703/830 [04:16<00:35,  3.60it/s] 85%| | 704/830 [04:17<00:35,  3.60it/s] 85%| | 705/830 [04:17<00:35,  3.49it/s] 85%| | 706/830 [04:17<00:35,  3.52it/s] 85%| | 707/830 [04:18<00:34,  3.55it/s] 85%| | 708/830 [04:18<00:34,  3.56it/s] 85%| | 709/830 [04:18<00:33,  3.58it/s] 86%| | 710/830 [04:18<00:33,  3.59it/s] 86%| | 711/830 [04:19<00:33,  3.59it/s] 86%| | 712/830 [04:19<00:32,  3.60it/s] 86%| | 713/830 [04:19<00:32,  3.60it/s] 86%| | 714/830 [04:19<00:32,  3.60it/s] 86%| | 715/830 [04:20<00:31,  3.60it/s] 86%| | 716/830 [04:20<00:32,  3.54it/s] 86%| | 717/830 [04:20<00:31,  3.57it/s] 87%| | 718/830 [04:21<00:31,  3.58it/s] 87%| | 719/830 [04:21<00:30,  3.59it/s] 87%| | 720/830 [04:21<00:30,  3.59it/s] 87%| | 721/830 [04:21<00:30,  3.59it/s] 87%| | 722/830 [04:22<00:30,  3.60it/s] 87%| | 723/830 [04:22<00:29,  3.60it/s] 87%| | 724/830 [04:22<00:29,  3.60it/s] 87%| | 725/830 [04:23<00:29,  3.61it/s] 87%| | 726/830 [04:23<00:28,  3.61it/s] 88%| | 727/830 [04:23<00:28,  3.61it/s] 88%| | 728/830 [04:23<00:28,  3.60it/s] 88%| | 729/830 [04:24<00:28,  3.60it/s] 88%| | 730/830 [04:24<00:29,  3.43it/s] 88%| | 731/830 [04:24<00:28,  3.46it/s] 88%| | 732/830 [04:25<00:27,  3.50it/s] 88%| | 733/830 [04:25<00:27,  3.53it/s] 88%| | 734/830 [04:25<00:27,  3.55it/s] 89%| | 735/830 [04:25<00:26,  3.57it/s] 89%| | 736/830 [04:26<00:26,  3.58it/s] 89%| | 737/830 [04:26<00:25,  3.59it/s] 89%| | 738/830 [04:26<00:32,  2.81it/s] 89%| | 739/830 [04:27<00:38,  2.38it/s] 89%| | 740/830 [04:27<00:34,  2.64it/s] 89%| | 741/830 [04:28<00:31,  2.87it/s] 89%| | 742/830 [04:28<00:28,  3.06it/s] 90%| | 743/830 [04:28<00:29,  2.93it/s] 90%| | 744/830 [04:29<00:27,  3.11it/s] 90%| | 745/830 [04:29<00:26,  3.24it/s] 90%| | 746/830 [04:29<00:25,  3.34it/s] 90%| | 747/830 [04:29<00:24,  3.42it/s] 90%| | 748/830 [04:30<00:23,  3.47it/s] 90%| | 749/830 [04:30<00:23,  3.51it/s] 90%| | 750/830 [04:30<00:22,  3.54it/s] 90%| | 751/830 [04:30<00:22,  3.56it/s] 91%| | 752/830 [04:31<00:21,  3.57it/s] 91%| | 753/830 [04:31<00:21,  3.58it/s] 91%| | 754/830 [04:31<00:22,  3.38it/s] 91%| | 755/830 [04:32<00:21,  3.45it/s] 91%| | 756/830 [04:32<00:21,  3.49it/s] 91%| | 757/830 [04:32<00:20,  3.53it/s] 91%|| 758/830 [04:32<00:20,  3.55it/s] 91%|| 759/830 [04:33<00:19,  3.56it/s] 92%|| 760/830 [04:33<00:19,  3.58it/s] 92%|| 761/830 [04:33<00:19,  3.58it/s] 92%|| 762/830 [04:34<00:18,  3.59it/s] 92%|| 763/830 [04:34<00:18,  3.59it/s] 92%|| 764/830 [04:34<00:18,  3.59it/s] 92%|| 765/830 [04:34<00:19,  3.36it/s] 92%|| 766/830 [04:35<00:18,  3.43it/s] 92%|| 767/830 [04:35<00:18,  3.48it/s] 93%|| 768/830 [04:35<00:17,  3.52it/s] 93%|| 769/830 [04:36<00:17,  3.55it/s] 93%|| 770/830 [04:36<00:16,  3.56it/s] 93%|| 771/830 [04:36<00:16,  3.57it/s] 93%|| 772/830 [04:36<00:16,  3.58it/s] 93%|| 773/830 [04:37<00:15,  3.59it/s] 93%|| 774/830 [04:37<00:15,  3.59it/s] 93%|| 775/830 [04:37<00:15,  3.60it/s] 93%|| 776/830 [04:38<00:16,  3.36it/s] 94%|| 777/830 [04:38<00:15,  3.43it/s] 94%|| 778/830 [04:38<00:14,  3.47it/s] 94%|| 779/830 [04:38<00:14,  3.51it/s] 94%|| 780/830 [04:39<00:14,  3.54it/s] 94%|| 781/830 [04:39<00:13,  3.56it/s] 94%|| 782/830 [04:39<00:13,  3.57it/s] 94%|| 783/830 [04:40<00:13,  3.57it/s] 94%|| 784/830 [04:40<00:12,  3.58it/s] 95%|| 785/830 [04:40<00:12,  3.59it/s] 95%|| 786/830 [04:40<00:12,  3.59it/s] 95%|| 787/830 [04:41<00:12,  3.32it/s] 95%|| 788/830 [04:41<00:12,  3.40it/s] 95%|| 789/830 [04:41<00:11,  3.46it/s] 95%|| 790/830 [04:42<00:11,  3.50it/s] 95%|| 791/830 [04:42<00:11,  3.53it/s] 95%|| 792/830 [04:42<00:10,  3.55it/s] 96%|| 793/830 [04:42<00:10,  3.56it/s] 96%|| 794/830 [04:43<00:10,  3.58it/s] 96%|| 795/830 [04:43<00:09,  3.58it/s] 96%|| 796/830 [04:43<00:09,  3.59it/s] 96%|| 797/830 [04:44<00:09,  3.60it/s] 96%|| 798/830 [04:44<00:09,  3.50it/s] 96%|| 799/830 [04:44<00:08,  3.53it/s] 96%|| 800/830 [04:44<00:08,  3.54it/s] 97%|| 801/830 [04:45<00:08,  3.56it/s] 97%|| 802/830 [04:45<00:07,  3.57it/s] 97%|| 803/830 [04:45<00:07,  3.58it/s] 97%|| 804/830 [04:45<00:07,  3.58it/s] 97%|| 805/830 [04:46<00:06,  3.59it/s] 97%|| 806/830 [04:46<00:06,  3.59it/s] 97%|| 807/830 [04:46<00:06,  3.60it/s] 97%|| 808/830 [04:47<00:06,  3.60it/s] 97%|| 809/830 [04:47<00:05,  3.53it/s] 98%|| 810/830 [04:47<00:05,  3.55it/s] 98%|| 811/830 [04:47<00:05,  3.57it/s] 98%|| 812/830 [04:48<00:05,  3.57it/s] 98%|| 813/830 [04:48<00:04,  3.58it/s] 98%|| 814/830 [04:48<00:04,  3.58it/s] 98%|| 815/830 [04:49<00:04,  3.59it/s] 98%|| 816/830 [04:49<00:03,  3.59it/s] 98%|| 817/830 [04:49<00:03,  3.60it/s] 99%|| 818/830 [04:49<00:03,  3.60it/s] 99%|| 819/830 [04:50<00:03,  3.60it/s] 99%|| 820/830 [04:50<00:02,  3.50it/s] 99%|| 821/830 [04:50<00:02,  3.53it/s] 99%|| 822/830 [04:51<00:02,  3.55it/s] 99%|| 823/830 [04:51<00:01,  3.56it/s] 99%|| 824/830 [04:51<00:01,  3.57it/s] 99%|| 825/830 [04:51<00:01,  3.58it/s]100%|| 826/830 [04:52<00:01,  3.59it/s]100%|| 827/830 [04:52<00:00,  3.59it/s]100%|| 828/830 [04:52<00:00,  3.59it/s]100%|| 829/830 [04:52<00:00,  3.60it/s]100%|| 830/830 [04:53<00:00,  3.99it/s][INFO|trainer.py:2140] 2023-08-28 10:28:17,891 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:28:17,891 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 10:28:17,891 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7874, 'eval_samples_per_second': 355.765, 'eval_steps_per_second': 44.547, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  2%|         | 7/436 [00:00<00:07, 58.95it/s][A
  3%|         | 13/436 [00:00<00:08, 50.52it/s][A
  4%|         | 19/436 [00:00<00:08, 47.92it/s][A
  6%|         | 24/436 [00:00<00:08, 46.53it/s][A
  7%|         | 29/436 [00:00<00:08, 45.86it/s][A
  8%|         | 34/436 [00:00<00:08, 45.28it/s][A
  9%|         | 39/436 [00:00<00:08, 44.85it/s][A
 10%|         | 44/436 [00:00<00:08, 44.61it/s][A
 11%|         | 49/436 [00:01<00:08, 44.77it/s][A
 12%|        | 54/436 [00:01<00:08, 44.94it/s][A
 14%|        | 59/436 [00:01<00:08, 45.03it/s][A
 15%|        | 64/436 [00:01<00:08, 45.08it/s][A
 16%|        | 69/436 [00:01<00:08, 44.97it/s][A
 17%|        | 74/436 [00:01<00:08, 44.90it/s][A
 18%|        | 79/436 [00:01<00:07, 44.69it/s][A
 19%|        | 84/436 [00:01<00:07, 44.42it/s][A
 20%|        | 89/436 [00:01<00:07, 44.32it/s][A
 22%|       | 94/436 [00:02<00:07, 44.39it/s][A
 23%|       | 99/436 [00:02<00:07, 44.65it/s][A
 24%|       | 104/436 [00:02<00:07, 44.87it/s][A
 25%|       | 109/436 [00:02<00:07, 44.96it/s][A
 26%|       | 114/436 [00:02<00:07, 44.90it/s][A
 27%|       | 119/436 [00:02<00:07, 44.80it/s][A
 28%|       | 124/436 [00:02<00:06, 44.59it/s][A
 30%|       | 129/436 [00:02<00:06, 44.45it/s][A
 31%|       | 134/436 [00:02<00:06, 44.28it/s][A
 32%|      | 139/436 [00:03<00:06, 44.44it/s][A
 33%|      | 144/436 [00:03<00:06, 42.83it/s][A
 34%|      | 149/436 [00:03<00:06, 43.57it/s][A
 35%|      | 154/436 [00:03<00:06, 44.19it/s][A
 36%|      | 159/436 [00:03<00:06, 44.41it/s][A
 38%|      | 164/436 [00:03<00:06, 44.56it/s][A
 39%|      | 169/436 [00:03<00:06, 44.46it/s][A
 40%|      | 174/436 [00:03<00:05, 44.36it/s][A
 41%|      | 179/436 [00:03<00:05, 44.41it/s][A
 42%|     | 184/436 [00:04<00:05, 44.18it/s][A
 43%|     | 189/436 [00:04<00:05, 44.48it/s][A
 44%|     | 194/436 [00:04<00:05, 44.68it/s][A
 46%|     | 199/436 [00:04<00:05, 44.87it/s][A
 47%|     | 204/436 [00:04<00:05, 44.99it/s][A
 48%|     | 209/436 [00:04<00:05, 44.78it/s][A
 49%|     | 214/436 [00:04<00:04, 44.64it/s][A
 50%|     | 219/436 [00:04<00:04, 44.49it/s][A
 51%|    | 224/436 [00:04<00:04, 44.39it/s][A
 53%|    | 229/436 [00:05<00:04, 44.43it/s][A
 54%|    | 234/436 [00:05<00:04, 44.55it/s][A
 55%|    | 239/436 [00:05<00:04, 44.70it/s][A
 56%|    | 244/436 [00:05<00:04, 44.87it/s][A
 57%|    | 249/436 [00:05<00:04, 44.95it/s][A
 58%|    | 254/436 [00:05<00:04, 44.95it/s][A
 59%|    | 259/436 [00:05<00:03, 44.73it/s][A
 61%|    | 264/436 [00:05<00:03, 44.66it/s][A
 62%|   | 269/436 [00:06<00:03, 42.27it/s][A
 63%|   | 274/436 [00:06<00:03, 43.03it/s][A
 64%|   | 279/436 [00:06<00:03, 43.63it/s][A
 65%|   | 284/436 [00:06<00:03, 44.03it/s][A
 66%|   | 289/436 [00:06<00:03, 44.38it/s][A
 67%|   | 294/436 [00:06<00:03, 44.49it/s][A
 69%|   | 299/436 [00:06<00:03, 44.58it/s][A
 70%|   | 304/436 [00:06<00:02, 44.60it/s][A
 71%|   | 309/436 [00:06<00:02, 44.22it/s][A
 72%|  | 314/436 [00:07<00:02, 44.39it/s][A
 73%|  | 319/436 [00:07<00:02, 44.48it/s][A
 74%|  | 324/436 [00:07<00:02, 44.65it/s][A
 75%|  | 329/436 [00:07<00:02, 44.81it/s][A
 77%|  | 334/436 [00:07<00:02, 44.94it/s][A
 78%|  | 339/436 [00:07<00:02, 45.01it/s][A
 79%|  | 344/436 [00:07<00:02, 44.79it/s][A
 80%|  | 349/436 [00:07<00:01, 44.68it/s][A
 81%|  | 354/436 [00:07<00:01, 44.53it/s][A
 82%| | 359/436 [00:08<00:01, 44.40it/s][A
 83%| | 364/436 [00:08<00:01, 44.60it/s][A
 85%| | 369/436 [00:08<00:01, 44.66it/s][A
 86%| | 374/436 [00:08<00:01, 44.81it/s][A
 87%| | 379/436 [00:08<00:01, 44.98it/s][A
 88%| | 384/436 [00:08<00:01, 45.00it/s][A
 89%| | 389/436 [00:08<00:01, 44.93it/s][A
 90%| | 394/436 [00:08<00:00, 44.61it/s][A
 92%|| 399/436 [00:08<00:00, 44.57it/s][A
 93%|| 404/436 [00:09<00:00, 43.70it/s][A
 94%|| 409/436 [00:09<00:00, 44.14it/s][A
 95%|| 414/436 [00:09<00:00, 44.49it/s][A
 96%|| 419/436 [00:09<00:00, 44.60it/s][A
 97%|| 424/436 [00:09<00:00, 44.82it/s][A
 98%|| 429/436 [00:09<00:00, 44.87it/s][A
100%|| 434/436 [00:09<00:00, 44.70it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.70it/s][A100%|| 830/830 [05:02<00:00,  3.99it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:28:27,955 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-830
[INFO|configuration_utils.py:351] 2023-08-28 10:28:28,162 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-830/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:28:30,670 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-830/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:28:30,764 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-830/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:28:30,815 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-830/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:28:31,790 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:28:31,790 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166 (score: 1.1192312240600586).
                                                 100%|| 830/830 [05:14<00:00,  3.99it/s]100%|| 830/830 [05:14<00:00,  2.64it/s]
[INFO|trainer.py:1894] 2023-08-28 10:28:38,963 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 10:28:39,105 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:28:41,638 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:28:41,775 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:28:41,849 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:28:42,323 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:42,332 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:42,332 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:42,332 >>   train_runtime            = 0:05:14.20
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:42,332 >>   train_samples            =      10600
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:42,332 >>   train_samples_per_second =     168.68
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:42,332 >>   train_steps_per_second   =      2.642
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7874, 'eval_samples_per_second': 355.764, 'eval_steps_per_second': 44.547, 'epoch': 5.0}
{'train_runtime': 314.2039, 'train_samples_per_second': 168.68, 'train_steps_per_second': 2.642, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 10:28:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:28:42,498 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:28:42,498 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 10:28:42,498 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 56.49it/s]  3%|         | 12/436 [00:00<00:08, 49.28it/s]  4%|         | 17/436 [00:00<00:08, 47.71it/s]  5%|         | 22/436 [00:00<00:08, 46.70it/s]  6%|         | 27/436 [00:00<00:08, 46.33it/s]  7%|         | 32/436 [00:00<00:08, 45.97it/s]  8%|         | 37/436 [00:00<00:08, 45.82it/s] 10%|         | 42/436 [00:00<00:08, 45.40it/s] 11%|         | 47/436 [00:01<00:08, 44.74it/s] 12%|        | 52/436 [00:01<00:08, 44.49it/s] 13%|        | 57/436 [00:01<00:08, 44.67it/s] 14%|        | 62/436 [00:01<00:08, 44.76it/s] 15%|        | 67/436 [00:01<00:08, 44.94it/s] 17%|        | 72/436 [00:01<00:08, 45.01it/s] 18%|        | 77/436 [00:01<00:07, 45.17it/s] 19%|        | 82/436 [00:01<00:07, 45.15it/s] 20%|        | 87/436 [00:01<00:07, 44.93it/s] 21%|        | 92/436 [00:02<00:07, 44.63it/s] 22%|       | 97/436 [00:02<00:07, 44.41it/s] 23%|       | 102/436 [00:02<00:07, 44.56it/s] 25%|       | 107/436 [00:02<00:07, 44.74it/s] 26%|       | 112/436 [00:02<00:07, 44.88it/s] 27%|       | 117/436 [00:02<00:07, 44.95it/s] 28%|       | 122/436 [00:02<00:06, 45.02it/s] 29%|       | 127/436 [00:02<00:06, 44.97it/s] 30%|       | 132/436 [00:02<00:06, 44.66it/s] 31%|      | 137/436 [00:03<00:07, 41.66it/s] 33%|      | 142/436 [00:03<00:06, 42.70it/s] 34%|      | 147/436 [00:03<00:06, 43.31it/s] 35%|      | 152/436 [00:03<00:06, 43.87it/s] 36%|      | 157/436 [00:03<00:06, 44.32it/s] 37%|      | 162/436 [00:03<00:06, 44.66it/s] 38%|      | 167/436 [00:03<00:06, 44.72it/s] 39%|      | 172/436 [00:03<00:05, 44.66it/s] 41%|      | 177/436 [00:03<00:05, 44.38it/s] 42%|     | 182/436 [00:04<00:05, 44.32it/s] 43%|     | 187/436 [00:04<00:05, 44.44it/s] 44%|     | 192/436 [00:04<00:05, 44.48it/s] 45%|     | 197/436 [00:04<00:05, 44.69it/s] 46%|     | 202/436 [00:04<00:05, 44.82it/s] 47%|     | 207/436 [00:04<00:05, 44.91it/s] 49%|     | 212/436 [00:04<00:04, 45.10it/s] 50%|     | 217/436 [00:04<00:04, 44.78it/s] 51%|     | 222/436 [00:04<00:04, 44.57it/s] 52%|    | 227/436 [00:05<00:04, 44.55it/s] 53%|    | 232/436 [00:05<00:04, 44.56it/s] 54%|    | 237/436 [00:05<00:04, 44.71it/s] 56%|    | 242/436 [00:05<00:04, 44.75it/s] 57%|    | 247/436 [00:05<00:04, 44.94it/s] 58%|    | 252/436 [00:05<00:04, 44.93it/s] 59%|    | 257/436 [00:05<00:03, 44.96it/s] 60%|    | 262/436 [00:05<00:03, 44.83it/s] 61%|    | 267/436 [00:05<00:03, 44.61it/s] 62%|   | 272/436 [00:06<00:03, 41.04it/s] 64%|   | 277/436 [00:06<00:03, 42.21it/s] 65%|   | 282/436 [00:06<00:03, 43.15it/s] 66%|   | 287/436 [00:06<00:03, 43.82it/s] 67%|   | 292/436 [00:06<00:03, 44.21it/s] 68%|   | 297/436 [00:06<00:03, 44.45it/s] 69%|   | 302/436 [00:06<00:03, 39.51it/s] 70%|   | 307/436 [00:06<00:03, 41.03it/s] 72%|  | 312/436 [00:07<00:02, 42.08it/s] 73%|  | 317/436 [00:07<00:02, 43.03it/s] 74%|  | 322/436 [00:07<00:02, 43.64it/s] 75%|  | 327/436 [00:07<00:02, 44.25it/s] 76%|  | 332/436 [00:07<00:02, 44.61it/s] 77%|  | 337/436 [00:07<00:02, 44.71it/s] 78%|  | 342/436 [00:07<00:02, 44.44it/s] 80%|  | 347/436 [00:07<00:02, 44.16it/s] 81%|  | 352/436 [00:07<00:01, 44.28it/s] 82%| | 357/436 [00:08<00:01, 44.37it/s] 83%| | 362/436 [00:08<00:01, 44.64it/s] 84%| | 367/436 [00:08<00:01, 44.78it/s] 85%| | 372/436 [00:08<00:01, 44.93it/s] 86%| | 377/436 [00:08<00:01, 45.03it/s] 88%| | 382/436 [00:08<00:01, 44.89it/s] 89%| | 387/436 [00:08<00:01, 44.75it/s] 90%| | 392/436 [00:08<00:00, 44.47it/s] 91%| | 397/436 [00:08<00:00, 44.34it/s] 92%|| 402/436 [00:09<00:01, 32.07it/s] 93%|| 406/436 [00:09<00:00, 32.57it/s] 94%|| 410/436 [00:09<00:00, 33.32it/s] 95%|| 414/436 [00:09<00:00, 34.57it/s] 96%|| 418/436 [00:09<00:00, 23.18it/s] 97%|| 423/436 [00:09<00:00, 27.83it/s] 98%|| 428/436 [00:10<00:00, 31.77it/s] 99%|| 433/436 [00:10<00:00, 35.01it/s]100%|| 436/436 [00:10<00:00, 42.59it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:28:52,751 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   eval_loss               =     1.1192
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   eval_runtime            = 0:00:10.25
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   eval_samples_per_second =    339.623
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   eval_steps_per_second   =     42.526
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:28:52,751 >>   perplexity              =     3.0625
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:05,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:05,793 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:05,793 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:05,793 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:05,793 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:29:06,675 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:29:06,676 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:29:07,301 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:29:08,427 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:29:08,428 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:11,415 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:11,445 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:11,445 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:11,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:29:11,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:29:12,210 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:29:12,211 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:29:12,827 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:29:13,049 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:29:13,049 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-332
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-830
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-664
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/checkpoint-498
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.70it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.62it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:18,  1.61it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:23,  1.60it/s]Extractor Predicting: 40it [00:24,  1.52it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:25,  1.56it/s]Extractor Predicting: 43it [00:26,  1.57it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:29,  1.58it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:30,  1.56it/s]Extractor Predicting: 51it [00:31,  1.55it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:33,  1.56it/s]Extractor Predicting: 55it [00:34,  1.58it/s]Extractor Predicting: 56it [00:34,  1.53it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:36,  1.57it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.55it/s]Extractor Predicting: 64it [00:39,  1.54it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.63it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:43,  1.61it/s]Extractor Predicting: 71it [00:44,  1.63it/s]Extractor Predicting: 72it [00:44,  1.65it/s]Extractor Predicting: 73it [00:45,  1.71it/s]Extractor Predicting: 74it [00:45,  1.72it/s]Extractor Predicting: 75it [00:46,  1.68it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:47,  1.61it/s]Extractor Predicting: 78it [00:48,  1.62it/s]Extractor Predicting: 79it [00:49,  1.61it/s]Extractor Predicting: 80it [00:49,  1.62it/s]Extractor Predicting: 81it [00:50,  1.56it/s]Extractor Predicting: 82it [00:50,  1.57it/s]Extractor Predicting: 83it [00:51,  1.60it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:52,  1.63it/s]Extractor Predicting: 86it [00:53,  1.55it/s]Extractor Predicting: 87it [00:54,  1.60it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:56,  1.57it/s]Extractor Predicting: 91it [00:56,  1.53it/s]Extractor Predicting: 92it [00:57,  1.55it/s]Extractor Predicting: 93it [00:57,  1.60it/s]Extractor Predicting: 94it [00:58,  1.57it/s]Extractor Predicting: 95it [00:59,  1.62it/s]Extractor Predicting: 96it [00:59,  1.61it/s]Extractor Predicting: 97it [01:00,  1.60it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:01,  1.55it/s]Extractor Predicting: 100it [01:02,  1.54it/s]Extractor Predicting: 101it [01:03,  1.53it/s]Extractor Predicting: 102it [01:03,  1.53it/s]Extractor Predicting: 103it [01:04,  1.54it/s]Extractor Predicting: 104it [01:04,  1.58it/s]Extractor Predicting: 105it [01:05,  1.60it/s]Extractor Predicting: 106it [01:06,  1.54it/s]Extractor Predicting: 107it [01:06,  1.55it/s]Extractor Predicting: 108it [01:07,  1.59it/s]Extractor Predicting: 109it [01:08,  1.64it/s]Extractor Predicting: 110it [01:08,  1.64it/s]Extractor Predicting: 111it [01:09,  1.64it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:10,  1.64it/s]Extractor Predicting: 114it [01:11,  1.56it/s]Extractor Predicting: 115it [01:11,  1.56it/s]Extractor Predicting: 116it [01:12,  1.57it/s]Extractor Predicting: 117it [01:13,  1.58it/s]Extractor Predicting: 118it [01:13,  1.58it/s]Extractor Predicting: 119it [01:14,  1.58it/s]Extractor Predicting: 120it [01:14,  1.62it/s]Extractor Predicting: 121it [01:15,  1.61it/s]Extractor Predicting: 122it [01:16,  1.63it/s]Extractor Predicting: 123it [01:16,  1.60it/s]Extractor Predicting: 124it [01:17,  1.61it/s]Extractor Predicting: 125it [01:18,  1.47it/s]Extractor Predicting: 126it [01:18,  1.52it/s]Extractor Predicting: 127it [01:19,  1.53it/s]Extractor Predicting: 128it [01:20,  1.54it/s]Extractor Predicting: 129it [01:20,  1.54it/s]Extractor Predicting: 130it [01:21,  1.54it/s]Extractor Predicting: 131it [01:22,  1.57it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.56it/s]Extractor Predicting: 134it [01:24,  1.51it/s]Extractor Predicting: 135it [01:24,  1.54it/s]Extractor Predicting: 136it [01:25,  1.56it/s]Extractor Predicting: 137it [01:25,  1.57it/s]Extractor Predicting: 138it [01:26,  1.58it/s]Extractor Predicting: 139it [01:27,  1.54it/s]Extractor Predicting: 140it [01:27,  1.52it/s]Extractor Predicting: 141it [01:28,  1.53it/s]Extractor Predicting: 142it [01:29,  1.57it/s]Extractor Predicting: 143it [01:29,  1.61it/s]Extractor Predicting: 143it [01:29,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:59,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:59,071 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:59,071 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:59,071 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:59,071 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:30:59,720 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:30:59,721 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:31:00,332 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:31:01,353 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:31:01,371 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:04,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:04,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:04,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:04,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:31:04,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:31:04,769 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:31:04,770 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:31:05,042 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:31:05,211 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:31:05,211 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.78it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.70it/s]Extractor Predicting: 7it [00:04,  1.71it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.68it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:06,  1.75it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.73it/s]Extractor Predicting: 16it [00:09,  1.76it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:10,  1.73it/s]Extractor Predicting: 20it [00:11,  1.71it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:12,  1.65it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:13,  1.75it/s]Extractor Predicting: 25it [00:14,  1.71it/s]Extractor Predicting: 26it [00:15,  1.79it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:16,  1.79it/s]Extractor Predicting: 29it [00:16,  1.65it/s]Extractor Predicting: 30it [00:17,  1.63it/s]Extractor Predicting: 31it [00:18,  1.59it/s]Extractor Predicting: 32it [00:18,  1.59it/s]Extractor Predicting: 33it [00:19,  1.60it/s]Extractor Predicting: 34it [00:20,  1.60it/s]Extractor Predicting: 35it [00:20,  1.58it/s]Extractor Predicting: 36it [00:21,  1.57it/s]Extractor Predicting: 37it [00:21,  1.59it/s]Extractor Predicting: 38it [00:22,  1.59it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:23,  1.60it/s]Extractor Predicting: 41it [00:24,  1.61it/s]Extractor Predicting: 42it [00:24,  1.63it/s]Extractor Predicting: 43it [00:25,  1.59it/s]Extractor Predicting: 44it [00:26,  1.61it/s]Extractor Predicting: 45it [00:26,  1.61it/s]Extractor Predicting: 46it [00:27,  1.60it/s]Extractor Predicting: 47it [00:28,  1.61it/s]Extractor Predicting: 48it [00:28,  1.60it/s]Extractor Predicting: 49it [00:29,  1.60it/s]Extractor Predicting: 50it [00:29,  1.61it/s]Extractor Predicting: 51it [00:30,  1.59it/s]Extractor Predicting: 52it [00:31,  1.65it/s]Extractor Predicting: 53it [00:31,  1.61it/s]Extractor Predicting: 54it [00:32,  1.62it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:34,  1.64it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.64it/s]Extractor Predicting: 61it [00:36,  1.60it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:37,  1.68it/s]Extractor Predicting: 64it [00:38,  1.70it/s]Extractor Predicting: 65it [00:39,  1.70it/s]Extractor Predicting: 66it [00:39,  1.63it/s]Extractor Predicting: 67it [00:40,  1.65it/s]Extractor Predicting: 68it [00:40,  1.64it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:42,  1.67it/s]Extractor Predicting: 71it [00:42,  1.66it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:44,  1.58it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:45,  1.66it/s]Extractor Predicting: 76it [00:45,  1.67it/s]Extractor Predicting: 77it [00:46,  1.70it/s]Extractor Predicting: 78it [00:46,  1.75it/s]Extractor Predicting: 79it [00:47,  1.70it/s]Extractor Predicting: 80it [00:48,  1.67it/s]Extractor Predicting: 81it [00:48,  1.66it/s]Extractor Predicting: 82it [00:49,  1.69it/s]Extractor Predicting: 83it [00:49,  1.67it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:51,  1.65it/s]Extractor Predicting: 86it [00:51,  1.66it/s]Extractor Predicting: 87it [00:52,  1.66it/s]Extractor Predicting: 88it [00:52,  1.66it/s]Extractor Predicting: 89it [00:53,  1.72it/s]Extractor Predicting: 90it [00:54,  1.73it/s]Extractor Predicting: 91it [00:54,  1.72it/s]Extractor Predicting: 92it [00:55,  1.67it/s]Extractor Predicting: 93it [00:55,  1.63it/s]Extractor Predicting: 94it [00:56,  1.62it/s]Extractor Predicting: 95it [00:57,  1.62it/s]Extractor Predicting: 96it [00:57,  1.61it/s]Extractor Predicting: 97it [00:58,  1.63it/s]Extractor Predicting: 98it [00:59,  1.58it/s]Extractor Predicting: 99it [00:59,  1.61it/s]Extractor Predicting: 100it [01:00,  1.61it/s]Extractor Predicting: 101it [01:00,  1.61it/s]Extractor Predicting: 102it [01:01,  1.64it/s]Extractor Predicting: 103it [01:02,  1.59it/s]Extractor Predicting: 104it [01:02,  1.61it/s]Extractor Predicting: 105it [01:03,  1.61it/s]Extractor Predicting: 106it [01:04,  1.62it/s]Extractor Predicting: 107it [01:04,  1.62it/s]Extractor Predicting: 108it [01:05,  1.62it/s]Extractor Predicting: 109it [01:05,  1.63it/s]Extractor Predicting: 110it [01:06,  1.62it/s]Extractor Predicting: 111it [01:07,  1.62it/s]Extractor Predicting: 112it [01:07,  1.61it/s]Extractor Predicting: 113it [01:08,  1.60it/s]Extractor Predicting: 114it [01:09,  1.42it/s]Extractor Predicting: 115it [01:09,  1.46it/s]Extractor Predicting: 116it [01:10,  1.56it/s]Extractor Predicting: 117it [01:10,  1.68it/s]Extractor Predicting: 118it [01:11,  1.70it/s]Extractor Predicting: 119it [01:12,  1.72it/s]Extractor Predicting: 120it [01:12,  1.73it/s]Extractor Predicting: 121it [01:13,  1.71it/s]Extractor Predicting: 122it [01:13,  1.70it/s]Extractor Predicting: 123it [01:14,  1.69it/s]Extractor Predicting: 124it [01:14,  1.71it/s]Extractor Predicting: 125it [01:15,  1.79it/s]Extractor Predicting: 126it [01:16,  1.78it/s]Extractor Predicting: 127it [01:16,  1.80it/s]Extractor Predicting: 128it [01:17,  1.83it/s]Extractor Predicting: 129it [01:17,  1.81it/s]Extractor Predicting: 130it [01:18,  1.75it/s]Extractor Predicting: 131it [01:18,  1.78it/s]Extractor Predicting: 132it [01:19,  1.78it/s]Extractor Predicting: 133it [01:19,  1.79it/s]Extractor Predicting: 134it [01:20,  1.80it/s]Extractor Predicting: 135it [01:21,  1.83it/s]Extractor Predicting: 136it [01:21,  1.87it/s]Extractor Predicting: 137it [01:22,  1.88it/s]Extractor Predicting: 138it [01:22,  1.83it/s]Extractor Predicting: 139it [01:23,  1.79it/s]Extractor Predicting: 140it [01:23,  1.81it/s]Extractor Predicting: 141it [01:24,  1.76it/s]Extractor Predicting: 142it [01:24,  1.76it/s]Extractor Predicting: 143it [01:25,  1.78it/s]Extractor Predicting: 144it [01:26,  1.73it/s]Extractor Predicting: 145it [01:26,  1.74it/s]Extractor Predicting: 146it [01:27,  1.76it/s]Extractor Predicting: 147it [01:27,  1.77it/s]Extractor Predicting: 148it [01:28,  1.74it/s]Extractor Predicting: 149it [01:28,  1.80it/s]Extractor Predicting: 150it [01:29,  1.75it/s]Extractor Predicting: 151it [01:30,  1.74it/s]Extractor Predicting: 152it [01:30,  1.77it/s]Extractor Predicting: 153it [01:31,  1.83it/s]Extractor Predicting: 154it [01:31,  1.83it/s]Extractor Predicting: 155it [01:32,  1.84it/s]Extractor Predicting: 156it [01:32,  1.78it/s]Extractor Predicting: 157it [01:33,  1.77it/s]Extractor Predicting: 158it [01:33,  1.79it/s]Extractor Predicting: 159it [01:34,  1.74it/s]Extractor Predicting: 160it [01:35,  1.76it/s]Extractor Predicting: 161it [01:35,  1.76it/s]Extractor Predicting: 162it [01:36,  1.73it/s]Extractor Predicting: 163it [01:36,  1.74it/s]Extractor Predicting: 164it [01:37,  1.75it/s]Extractor Predicting: 165it [01:37,  1.77it/s]Extractor Predicting: 166it [01:38,  1.78it/s]Extractor Predicting: 167it [01:39,  1.78it/s]Extractor Predicting: 168it [01:39,  1.71it/s]Extractor Predicting: 169it [01:40,  1.73it/s]Extractor Predicting: 170it [01:40,  1.70it/s]Extractor Predicting: 171it [01:41,  1.68it/s]Extractor Predicting: 172it [01:42,  1.75it/s]Extractor Predicting: 173it [01:42,  1.69it/s]Extractor Predicting: 174it [01:43,  1.65it/s]Extractor Predicting: 175it [01:43,  1.63it/s]Extractor Predicting: 176it [01:44,  1.65it/s]Extractor Predicting: 177it [01:45,  1.64it/s]Extractor Predicting: 178it [01:45,  1.63it/s]Extractor Predicting: 179it [01:46,  1.60it/s]Extractor Predicting: 180it [01:46,  1.65it/s]Extractor Predicting: 181it [01:47,  1.65it/s]Extractor Predicting: 182it [01:48,  1.64it/s]Extractor Predicting: 183it [01:48,  1.64it/s]Extractor Predicting: 184it [01:49,  1.57it/s]Extractor Predicting: 185it [01:50,  1.58it/s]Extractor Predicting: 186it [01:50,  1.59it/s]Extractor Predicting: 187it [01:51,  1.59it/s]Extractor Predicting: 188it [01:51,  1.64it/s]Extractor Predicting: 189it [01:52,  1.65it/s]Extractor Predicting: 190it [01:53,  1.67it/s]Extractor Predicting: 191it [01:53,  1.59it/s]Extractor Predicting: 192it [01:54,  1.60it/s]Extractor Predicting: 193it [01:55,  1.58it/s]Extractor Predicting: 194it [01:55,  1.59it/s]Extractor Predicting: 195it [01:56,  1.59it/s]Extractor Predicting: 196it [01:56,  1.59it/s]Extractor Predicting: 197it [01:57,  1.59it/s]Extractor Predicting: 198it [01:58,  1.60it/s]Extractor Predicting: 199it [01:58,  1.60it/s]Extractor Predicting: 200it [01:59,  1.59it/s]Extractor Predicting: 201it [02:00,  1.58it/s]Extractor Predicting: 202it [02:00,  1.59it/s]Extractor Predicting: 203it [02:01,  1.62it/s]Extractor Predicting: 204it [02:01,  1.64it/s]Extractor Predicting: 205it [02:02,  1.70it/s]Extractor Predicting: 206it [02:03,  1.69it/s]Extractor Predicting: 207it [02:03,  1.65it/s]Extractor Predicting: 208it [02:04,  1.66it/s]Extractor Predicting: 209it [02:04,  1.65it/s]Extractor Predicting: 210it [02:05,  1.68it/s]Extractor Predicting: 211it [02:06,  1.73it/s]Extractor Predicting: 212it [02:06,  1.74it/s]Extractor Predicting: 213it [02:07,  1.73it/s]Extractor Predicting: 214it [02:07,  1.72it/s]Extractor Predicting: 215it [02:08,  1.68it/s]Extractor Predicting: 216it [02:08,  1.70it/s]Extractor Predicting: 217it [02:09,  1.71it/s]Extractor Predicting: 218it [02:10,  1.75it/s]Extractor Predicting: 219it [02:10,  1.74it/s]Extractor Predicting: 220it [02:11,  1.71it/s]Extractor Predicting: 221it [02:11,  1.75it/s]Extractor Predicting: 222it [02:12,  1.78it/s]Extractor Predicting: 223it [02:12,  1.75it/s]Extractor Predicting: 224it [02:13,  1.76it/s]Extractor Predicting: 225it [02:14,  1.73it/s]Extractor Predicting: 226it [02:14,  1.68it/s]Extractor Predicting: 227it [02:15,  1.67it/s]Extractor Predicting: 228it [02:15,  1.69it/s]Extractor Predicting: 229it [02:16,  1.68it/s]Extractor Predicting: 230it [02:17,  1.65it/s]Extractor Predicting: 231it [02:18,  1.45it/s]Extractor Predicting: 232it [02:18,  1.54it/s]Extractor Predicting: 233it [02:19,  1.65it/s]Extractor Predicting: 234it [02:19,  1.69it/s]Extractor Predicting: 235it [02:20,  1.69it/s]Extractor Predicting: 236it [02:20,  1.77it/s]Extractor Predicting: 237it [02:21,  1.79it/s]Extractor Predicting: 238it [02:21,  1.80it/s]Extractor Predicting: 239it [02:22,  1.80it/s]Extractor Predicting: 240it [02:22,  1.83it/s]Extractor Predicting: 241it [02:23,  1.82it/s]Extractor Predicting: 242it [02:24,  1.79it/s]Extractor Predicting: 243it [02:24,  1.86it/s]Extractor Predicting: 244it [02:25,  1.85it/s]Extractor Predicting: 245it [02:25,  1.92it/s]Extractor Predicting: 246it [02:26,  1.96it/s]Extractor Predicting: 247it [02:26,  1.89it/s]Extractor Predicting: 248it [02:27,  1.80it/s]Extractor Predicting: 249it [02:27,  1.83it/s]Extractor Predicting: 250it [02:28,  1.83it/s]Extractor Predicting: 251it [02:28,  1.86it/s]Extractor Predicting: 252it [02:29,  1.88it/s]Extractor Predicting: 253it [02:29,  1.87it/s]Extractor Predicting: 254it [02:30,  1.82it/s]Extractor Predicting: 255it [02:30,  1.86it/s]Extractor Predicting: 256it [02:31,  1.86it/s]Extractor Predicting: 257it [02:32,  1.89it/s]Extractor Predicting: 258it [02:32,  1.89it/s]Extractor Predicting: 259it [02:33,  1.92it/s]Extractor Predicting: 260it [02:33,  1.85it/s]Extractor Predicting: 261it [02:34,  1.76it/s]Extractor Predicting: 262it [02:34,  1.78it/s]Extractor Predicting: 263it [02:35,  1.73it/s]Extractor Predicting: 264it [02:36,  1.67it/s]Extractor Predicting: 265it [02:36,  1.65it/s]Extractor Predicting: 266it [02:37,  1.66it/s]Extractor Predicting: 267it [02:37,  1.70it/s]Extractor Predicting: 268it [02:38,  1.70it/s]Extractor Predicting: 269it [02:39,  1.66it/s]Extractor Predicting: 270it [02:39,  1.60it/s]Extractor Predicting: 271it [02:40,  1.59it/s]Extractor Predicting: 272it [02:40,  1.63it/s]Extractor Predicting: 273it [02:41,  1.63it/s]Extractor Predicting: 274it [02:42,  1.62it/s]Extractor Predicting: 275it [02:42,  1.59it/s]Extractor Predicting: 276it [02:43,  1.61it/s]Extractor Predicting: 277it [02:44,  1.59it/s]Extractor Predicting: 278it [02:44,  1.59it/s]Extractor Predicting: 279it [02:45,  1.63it/s]Extractor Predicting: 280it [02:46,  1.58it/s]Extractor Predicting: 281it [02:46,  1.59it/s]Extractor Predicting: 282it [02:47,  1.59it/s]Extractor Predicting: 283it [02:47,  1.60it/s]Extractor Predicting: 284it [02:48,  1.60it/s]Extractor Predicting: 285it [02:49,  1.62it/s]Extractor Predicting: 286it [02:49,  1.58it/s]Extractor Predicting: 287it [02:50,  1.57it/s]Extractor Predicting: 288it [02:50,  1.64it/s]Extractor Predicting: 289it [02:51,  1.65it/s]Extractor Predicting: 290it [02:52,  1.68it/s]Extractor Predicting: 291it [02:52,  1.69it/s]Extractor Predicting: 292it [02:53,  1.70it/s]Extractor Predicting: 293it [02:53,  1.70it/s]Extractor Predicting: 294it [02:54,  1.73it/s]Extractor Predicting: 295it [02:55,  1.70it/s]Extractor Predicting: 296it [02:55,  1.68it/s]Extractor Predicting: 297it [02:56,  1.69it/s]Extractor Predicting: 298it [02:56,  1.75it/s]Extractor Predicting: 299it [02:57,  1.71it/s]Extractor Predicting: 300it [02:57,  1.72it/s]Extractor Predicting: 301it [02:58,  1.71it/s]Extractor Predicting: 302it [02:59,  1.69it/s]Extractor Predicting: 303it [02:59,  1.69it/s]Extractor Predicting: 304it [03:00,  1.66it/s]Extractor Predicting: 305it [03:00,  1.66it/s]Extractor Predicting: 306it [03:01,  1.69it/s]Extractor Predicting: 307it [03:02,  1.69it/s]Extractor Predicting: 308it [03:02,  1.69it/s]Extractor Predicting: 309it [03:03,  1.65it/s]Extractor Predicting: 310it [03:03,  1.67it/s]Extractor Predicting: 311it [03:04,  1.68it/s]Extractor Predicting: 312it [03:05,  1.66it/s]Extractor Predicting: 313it [03:05,  1.67it/s]Extractor Predicting: 314it [03:06,  1.68it/s]Extractor Predicting: 315it [03:06,  1.70it/s]Extractor Predicting: 316it [03:07,  1.69it/s]Extractor Predicting: 317it [03:08,  1.72it/s]Extractor Predicting: 318it [03:08,  1.76it/s]Extractor Predicting: 319it [03:09,  1.75it/s]Extractor Predicting: 320it [03:09,  1.73it/s]Extractor Predicting: 321it [03:10,  1.70it/s]Extractor Predicting: 322it [03:10,  1.70it/s]Extractor Predicting: 323it [03:11,  1.73it/s]Extractor Predicting: 324it [03:12,  1.74it/s]Extractor Predicting: 325it [03:12,  1.72it/s]Extractor Predicting: 326it [03:13,  1.74it/s]Extractor Predicting: 327it [03:13,  1.73it/s]Extractor Predicting: 328it [03:14,  1.71it/s]Extractor Predicting: 329it [03:15,  1.71it/s]Extractor Predicting: 330it [03:15,  1.68it/s]Extractor Predicting: 331it [03:16,  1.68it/s]Extractor Predicting: 332it [03:16,  1.72it/s]Extractor Predicting: 333it [03:17,  1.46it/s]Extractor Predicting: 334it [03:18,  1.55it/s]Extractor Predicting: 335it [03:18,  1.58it/s]Extractor Predicting: 336it [03:19,  1.63it/s]Extractor Predicting: 337it [03:20,  1.64it/s]Extractor Predicting: 338it [03:20,  1.64it/s]Extractor Predicting: 339it [03:21,  1.66it/s]Extractor Predicting: 340it [03:21,  1.65it/s]Extractor Predicting: 341it [03:22,  1.68it/s]Extractor Predicting: 342it [03:22,  1.71it/s]Extractor Predicting: 343it [03:23,  1.70it/s]Extractor Predicting: 344it [03:24,  1.74it/s]Extractor Predicting: 345it [03:24,  1.70it/s]Extractor Predicting: 346it [03:25,  1.70it/s]Extractor Predicting: 347it [03:25,  1.69it/s]Extractor Predicting: 348it [03:26,  1.66it/s]Extractor Predicting: 349it [03:27,  1.67it/s]Extractor Predicting: 350it [03:27,  1.66it/s]Extractor Predicting: 351it [03:28,  1.68it/s]Extractor Predicting: 352it [03:28,  1.66it/s]Extractor Predicting: 353it [03:29,  1.66it/s]Extractor Predicting: 354it [03:30,  1.67it/s]Extractor Predicting: 355it [03:30,  1.64it/s]Extractor Predicting: 356it [03:31,  1.66it/s]Extractor Predicting: 357it [03:32,  1.63it/s]Extractor Predicting: 358it [03:32,  1.66it/s]Extractor Predicting: 359it [03:33,  1.67it/s]Extractor Predicting: 360it [03:33,  1.68it/s]Extractor Predicting: 361it [03:34,  1.67it/s]Extractor Predicting: 362it [03:34,  1.71it/s]Extractor Predicting: 363it [03:35,  1.70it/s]Extractor Predicting: 364it [03:36,  1.71it/s]Extractor Predicting: 365it [03:36,  1.69it/s]Extractor Predicting: 366it [03:37,  1.71it/s]Extractor Predicting: 367it [03:37,  1.70it/s]Extractor Predicting: 368it [03:38,  1.71it/s]Extractor Predicting: 369it [03:39,  1.66it/s]Extractor Predicting: 370it [03:39,  1.67it/s]Extractor Predicting: 371it [03:40,  1.67it/s]Extractor Predicting: 372it [03:40,  1.67it/s]Extractor Predicting: 373it [03:41,  1.67it/s]Extractor Predicting: 374it [03:42,  1.71it/s]Extractor Predicting: 375it [03:42,  1.70it/s]Extractor Predicting: 376it [03:43,  1.67it/s]Extractor Predicting: 377it [03:43,  1.72it/s]Extractor Predicting: 378it [03:44,  1.73it/s]Extractor Predicting: 379it [03:44,  1.76it/s]Extractor Predicting: 380it [03:45,  1.72it/s]Extractor Predicting: 381it [03:46,  1.67it/s]Extractor Predicting: 382it [03:46,  1.67it/s]Extractor Predicting: 383it [03:47,  1.67it/s]Extractor Predicting: 384it [03:47,  1.66it/s]Extractor Predicting: 385it [03:48,  1.68it/s]Extractor Predicting: 386it [03:49,  1.67it/s]Extractor Predicting: 387it [03:49,  1.67it/s]Extractor Predicting: 388it [03:50,  1.61it/s]Extractor Predicting: 389it [03:51,  1.63it/s]Extractor Predicting: 390it [03:51,  1.60it/s]Extractor Predicting: 391it [03:52,  1.64it/s]Extractor Predicting: 392it [03:52,  1.67it/s]Extractor Predicting: 393it [03:53,  1.66it/s]Extractor Predicting: 394it [03:53,  1.68it/s]Extractor Predicting: 395it [03:54,  1.69it/s]Extractor Predicting: 396it [03:55,  1.67it/s]Extractor Predicting: 397it [03:55,  1.68it/s]Extractor Predicting: 398it [03:56,  1.63it/s]Extractor Predicting: 399it [03:56,  1.69it/s]Extractor Predicting: 400it [03:57,  1.65it/s]Extractor Predicting: 401it [03:58,  1.66it/s]Extractor Predicting: 402it [03:58,  1.66it/s]Extractor Predicting: 403it [03:59,  1.62it/s]Extractor Predicting: 404it [04:00,  1.66it/s]Extractor Predicting: 405it [04:00,  1.68it/s]Extractor Predicting: 406it [04:01,  1.71it/s]Extractor Predicting: 407it [04:01,  1.71it/s]Extractor Predicting: 408it [04:02,  1.69it/s]Extractor Predicting: 409it [04:02,  1.68it/s]Extractor Predicting: 410it [04:03,  1.70it/s]Extractor Predicting: 411it [04:04,  1.70it/s]Extractor Predicting: 412it [04:04,  1.73it/s]Extractor Predicting: 413it [04:05,  1.73it/s]Extractor Predicting: 414it [04:05,  1.72it/s]Extractor Predicting: 415it [04:06,  1.66it/s]Extractor Predicting: 416it [04:07,  1.71it/s]Extractor Predicting: 417it [04:07,  1.71it/s]Extractor Predicting: 418it [04:08,  1.75it/s]Extractor Predicting: 419it [04:08,  1.75it/s]Extractor Predicting: 420it [04:09,  1.77it/s]Extractor Predicting: 421it [04:09,  1.71it/s]Extractor Predicting: 422it [04:10,  1.72it/s]Extractor Predicting: 423it [04:11,  1.73it/s]Extractor Predicting: 424it [04:11,  1.73it/s]Extractor Predicting: 425it [04:12,  1.72it/s]Extractor Predicting: 426it [04:12,  1.76it/s]Extractor Predicting: 427it [04:13,  1.71it/s]Extractor Predicting: 428it [04:14,  1.67it/s]Extractor Predicting: 429it [04:14,  1.93it/s]Extractor Predicting: 429it [04:14,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:33,501 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:33,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:33,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:33,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:33,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:35:34,257 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:35:34,258 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:35:34,855 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:35:35,971 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:35:35,971 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:39,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:35:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:35:39,852 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:35:39,853 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:35:40,461 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:35:40,688 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:35:40,688 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:35:45,006 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:35:45,008 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:35:45,050 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:35:45,052 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:35:45,089 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:35:54,733 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:35:54,753 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:35:54,868 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:35:54,869 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:35:54,947 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:35:54,998 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:35:54,998 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:35:54,998 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:35:54,998 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:35:54,998 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:35:54,998 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:35:55,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:56,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:56,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:57,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:57,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:58,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:59,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:35:59,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:00,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:01,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:01,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:02,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:02,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:03,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:04,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:04,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:05,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:06,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:06,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:07,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:08,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:08,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:09,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:10,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:10,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:11,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:17, 16.73s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:12,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:12,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:13,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:13,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:14,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:15,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:15,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:16,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:16,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:17,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:18,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:18,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:19,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:20,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:20,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:21,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:21,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:22,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:23,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:23,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:24,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:25,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:25,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:33, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:26,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:26,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:28,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:28,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:29,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:30,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:31,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:31,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:33,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:33,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:34,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:35,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:36,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:36,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:37,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:38,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:39,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:40,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:40,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:41,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:42,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:42,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:43,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:44,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:44,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:45,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:50<04:56, 17.41s/it][WARNING|generation_utils.py:914] 2023-08-28 10:36:46,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:46,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:47,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:48,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:48,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:49,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:50,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:51,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:51,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:52,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:52,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:53,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:54,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:54,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:55,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:56,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:57,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:57,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:58,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:58,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:36:59,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:00,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:00,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:06<04:24, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:01,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:02,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:02,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:03,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:04,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:05,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:05,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:06,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:07,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:07,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:08,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:09,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:10,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:10,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:11,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:12,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:13,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:13,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:14,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:15,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:15,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:16,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:17,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:17,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:23<04:11, 16.74s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:18,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:19,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:20,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:21,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:21,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:22,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:23,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:23,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:24,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:25,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:25,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:26,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:27,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:27,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:28,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:29,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:30,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:30,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:31,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:32,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:32,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:33,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:34,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:39<03:52, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:34,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:35,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:36,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:36,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:37,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:38,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:38,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:39,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:40,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:40,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:41,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:42,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:42,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:43,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:43,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:44,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:44,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:45,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:46,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:47,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:47,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:48,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:49,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:50,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:50,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:51,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:52,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:57<03:41, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:52,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:53,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:54,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:54,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:55,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:56,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:56,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:57,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:57,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:58,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:59,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:59,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:00,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:01,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:01,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:02,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:03,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:03,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:04,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:05,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:05,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:06,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:07,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:07,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:08,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:08,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:09,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:14<03:25, 17.16s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:10,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:10,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:11,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:12,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:12,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:13,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:14,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:15,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:15,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:16,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:17,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:17,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:18,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:19,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:20,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:20,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:21,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:22,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:22,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:23,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:24,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:24,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:25,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:26,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:26,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:27,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:33<03:12, 17.48s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:28,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:29,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:29,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:30,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:31,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:31,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:32,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:33,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:34,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:35,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:35,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:36,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:37,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:37,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:38,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:39,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:39,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:40,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:41,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:41,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:42,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:43,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:43,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:44,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:49<02:52, 17.26s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:45,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:45,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:46,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:47,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:47,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:48,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:49,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:49,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:50,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:51,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:51,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:52,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:53,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:53,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:54,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:55,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:55,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:56,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:56,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:57,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:58,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:59,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:59,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:00,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:00,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:06<02:33, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:01,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:02,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:03,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:03,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:04,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:05,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:05,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:06,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:07,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:07,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:08,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:09,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:10,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:11,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:11,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:12,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:13,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:13,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:14,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:15,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:15,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:16,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:17,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:18,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:23<02:16, 17.06s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:18,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:19,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:20,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:20,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:21,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:22,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:23,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:23,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:24,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:25,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:25,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:26,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:27,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:27,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:28,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:29,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:30,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:30,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:31,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:32,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:33,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:33,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:34,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:39<01:57, 16.85s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:35,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:35,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:36,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:37,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:38,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:38,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:39,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:40,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:40,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:41,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:42,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:42,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:43,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:44,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:44,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:45,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:46,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:46,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:47,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:48,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:49,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:50,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:50,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:51,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:56<01:41, 16.91s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:52,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:52,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:53,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:54,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:55,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:55,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:56,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:56,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:57,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:57,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:58,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:59,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:00,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:00,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:01,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:01,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:02,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:03,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:04,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:05,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:05,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:06,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:06,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:07,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:08,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:13<01:24, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:08,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:09,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:10,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:10,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:11,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:12,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:12,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:13,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:14,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:14,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:15,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:16,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:16,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:17,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:18,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:19,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:20,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:20,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:21,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:22,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:23,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:23,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:24,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:25,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:25,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:26,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:31<01:09, 17.31s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:27,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:27,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:28,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:29,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:30,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:30,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:31,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:32,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:32,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:33,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:34,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:34,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:35,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:36,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:36,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:38,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:38,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:39,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:40,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:41,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:41,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:42,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:47<00:50, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:43,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:43,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:44,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:45,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:45,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:46,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:46,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:47,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:48,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:49,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:49,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:50,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:51,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:51,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:52,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:52,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:53,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:54,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:55,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:56,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:56,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:57,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:58,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:03<00:33, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:59,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:59,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:00,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:01,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:01,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:02,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:02,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:03,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:04,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:04,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:05,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:06,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:06,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:07,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:08,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:08,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:09,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:10,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:10,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:11,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:12,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:12,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:13,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:14,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:14,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:15,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:15,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:21<00:16, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 10:41:16,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:17,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:17,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:18,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:19,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:20,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:20,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:21,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:22,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:22,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:23,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:24,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:24,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:25,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:26,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:26,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:27,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:28,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:28,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:29,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:30,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:31,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:31,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:37<00:00, 16.57s/it]Generating: 100%|| 20/20 [05:37<00:00, 16.85s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:40,222 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:40,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:40,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:40,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:40,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:41:40,773 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:41:40,774 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:41:41,080 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:41:42,238 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:41:42,238 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:43,694 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:43,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:43,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:43,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:41:43,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:41:44,193 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:41:44,194 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:41:44,506 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:41:44,718 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:41:44,718 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of Lpez 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of Lpez 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Dmontagne " , including Boulogne \'s " Montessemble des deux de Chteau des Gains " . Head Entity : Montessemble des deux de Chteau des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of Franois Renoul .\')', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.54it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:09,  1.56it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:10,  1.63it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:12,  1.64it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.57it/s]Extractor Estimating: 22it [00:14,  1.59it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.57it/s]Extractor Estimating: 26it [00:16,  1.61it/s]Extractor Estimating: 27it [00:17,  1.62it/s]Extractor Estimating: 28it [00:17,  1.65it/s]Extractor Estimating: 29it [00:18,  1.60it/s]Extractor Estimating: 30it [00:19,  1.59it/s]Extractor Estimating: 31it [00:19,  1.56it/s]Extractor Estimating: 32it [00:20,  1.57it/s]Extractor Estimating: 33it [00:20,  1.53it/s]Extractor Estimating: 34it [00:21,  1.57it/s]Extractor Estimating: 35it [00:22,  1.54it/s]Extractor Estimating: 36it [00:23,  1.45it/s]Extractor Estimating: 37it [00:23,  1.44it/s]Extractor Estimating: 38it [00:24,  1.54it/s]Extractor Estimating: 39it [00:25,  1.50it/s]Extractor Estimating: 40it [00:25,  1.47it/s]Extractor Estimating: 41it [00:26,  1.49it/s]Extractor Estimating: 42it [00:27,  1.49it/s]Extractor Estimating: 43it [00:27,  1.51it/s]Extractor Estimating: 44it [00:28,  1.54it/s]Extractor Estimating: 45it [00:28,  1.54it/s]Extractor Estimating: 46it [00:29,  1.50it/s]Extractor Estimating: 47it [00:30,  1.56it/s]Extractor Estimating: 48it [00:30,  1.52it/s]Extractor Estimating: 49it [00:31,  1.57it/s]Extractor Estimating: 50it [00:32,  1.50it/s]Extractor Estimating: 51it [00:32,  1.50it/s]Extractor Estimating: 52it [00:33,  1.45it/s]Extractor Estimating: 53it [00:34,  1.42it/s]Extractor Estimating: 54it [00:35,  1.48it/s]Extractor Estimating: 55it [00:35,  1.53it/s]Extractor Estimating: 56it [00:36,  1.54it/s]Extractor Estimating: 57it [00:37,  1.47it/s]Extractor Estimating: 58it [00:37,  1.52it/s]Extractor Estimating: 59it [00:38,  1.53it/s]Extractor Estimating: 60it [00:38,  1.51it/s]Extractor Estimating: 61it [00:39,  1.51it/s]Extractor Estimating: 62it [00:40,  1.46it/s]Extractor Estimating: 63it [00:40,  1.52it/s]Extractor Estimating: 64it [00:41,  1.51it/s]Extractor Estimating: 65it [00:42,  1.48it/s]Extractor Estimating: 66it [00:43,  1.45it/s]Extractor Estimating: 67it [00:43,  1.44it/s]Extractor Estimating: 68it [00:44,  1.47it/s]Extractor Estimating: 69it [00:45,  1.45it/s]Extractor Estimating: 70it [00:45,  1.47it/s]Extractor Estimating: 71it [00:46,  1.51it/s]Extractor Estimating: 72it [00:47,  1.51it/s]Extractor Estimating: 73it [00:47,  1.50it/s]Extractor Estimating: 74it [00:48,  1.51it/s]Extractor Estimating: 75it [00:49,  1.51it/s]Extractor Estimating: 76it [00:49,  1.52it/s]Extractor Estimating: 77it [00:50,  1.38it/s]Extractor Estimating: 78it [00:51,  1.40it/s]Extractor Estimating: 79it [00:51,  1.46it/s]Extractor Estimating: 80it [00:52,  1.46it/s]Extractor Estimating: 81it [00:53,  1.46it/s]Extractor Estimating: 82it [00:53,  1.45it/s]Extractor Estimating: 83it [00:54,  1.50it/s]Extractor Estimating: 84it [00:55,  1.45it/s]Extractor Estimating: 85it [00:55,  1.46it/s]Extractor Estimating: 86it [00:56,  1.52it/s]Extractor Estimating: 87it [00:57,  1.51it/s]Extractor Estimating: 88it [00:58,  1.44it/s]Extractor Estimating: 89it [00:58,  1.47it/s]Extractor Estimating: 90it [00:59,  1.50it/s]Extractor Estimating: 91it [00:59,  1.59it/s]Extractor Estimating: 92it [01:00,  1.53it/s]Extractor Estimating: 93it [01:01,  1.48it/s]Extractor Estimating: 94it [01:01,  1.51it/s]Extractor Estimating: 95it [01:02,  1.53it/s]Extractor Estimating: 96it [01:03,  1.54it/s]Extractor Estimating: 97it [01:03,  1.46it/s]Extractor Estimating: 98it [01:04,  1.47it/s]Extractor Estimating: 99it [01:05,  1.49it/s]Extractor Estimating: 100it [01:05,  1.48it/s]Extractor Estimating: 101it [01:06,  1.49it/s]Extractor Estimating: 102it [01:07,  1.52it/s]Extractor Estimating: 103it [01:07,  1.58it/s]Extractor Estimating: 104it [01:08,  1.56it/s]Extractor Estimating: 105it [01:09,  1.59it/s]Extractor Estimating: 106it [01:09,  1.60it/s]Extractor Estimating: 107it [01:10,  1.61it/s]Extractor Estimating: 108it [01:10,  1.60it/s]Extractor Estimating: 109it [01:11,  1.63it/s]Extractor Estimating: 110it [01:12,  1.70it/s]Extractor Estimating: 111it [01:12,  1.58it/s]Extractor Estimating: 112it [01:13,  1.56it/s]Extractor Estimating: 113it [01:14,  1.59it/s]Extractor Estimating: 114it [01:14,  1.62it/s]Extractor Estimating: 115it [01:15,  1.55it/s]Extractor Estimating: 116it [01:16,  1.54it/s]Extractor Estimating: 117it [01:16,  1.51it/s]Extractor Estimating: 118it [01:17,  1.53it/s]Extractor Estimating: 119it [01:17,  1.55it/s]Extractor Estimating: 120it [01:18,  1.55it/s]Extractor Estimating: 121it [01:19,  1.61it/s]Extractor Estimating: 122it [01:19,  1.64it/s]Extractor Estimating: 123it [01:20,  1.59it/s]Extractor Estimating: 124it [01:20,  1.64it/s]Extractor Estimating: 125it [01:21,  1.62it/s]Extractor Estimating: 126it [01:22,  1.66it/s]Extractor Estimating: 127it [01:22,  1.54it/s]Extractor Estimating: 128it [01:23,  1.53it/s]Extractor Estimating: 129it [01:24,  1.52it/s]Extractor Estimating: 130it [01:24,  1.50it/s]Extractor Estimating: 131it [01:25,  1.53it/s]Extractor Estimating: 132it [01:26,  1.48it/s]Extractor Estimating: 133it [01:26,  1.48it/s]Extractor Estimating: 134it [01:27,  1.49it/s]Extractor Estimating: 135it [01:28,  1.48it/s]Extractor Estimating: 136it [01:29,  1.39it/s]Extractor Estimating: 137it [01:29,  1.44it/s]Extractor Estimating: 138it [01:30,  1.46it/s]Extractor Estimating: 139it [01:31,  1.52it/s]Extractor Estimating: 140it [01:31,  1.55it/s]Extractor Estimating: 141it [01:32,  1.50it/s]Extractor Estimating: 142it [01:33,  1.48it/s]Extractor Estimating: 143it [01:33,  1.48it/s]Extractor Estimating: 144it [01:34,  1.44it/s]Extractor Estimating: 145it [01:35,  1.44it/s]Extractor Estimating: 146it [01:35,  1.42it/s]Extractor Estimating: 147it [01:36,  1.45it/s]Extractor Estimating: 148it [01:37,  1.43it/s]Extractor Estimating: 149it [01:37,  1.47it/s]Extractor Estimating: 150it [01:38,  1.46it/s]Extractor Estimating: 151it [01:39,  1.49it/s]Extractor Estimating: 152it [01:39,  1.56it/s]Extractor Estimating: 153it [01:40,  1.60it/s]Extractor Estimating: 154it [01:40,  1.69it/s]Extractor Estimating: 155it [01:41,  1.67it/s]Extractor Estimating: 156it [01:42,  1.63it/s]Extractor Estimating: 157it [01:42,  1.65it/s]Extractor Estimating: 158it [01:43,  1.63it/s]Extractor Estimating: 159it [01:44,  1.61it/s]Extractor Estimating: 160it [01:44,  1.64it/s]Extractor Estimating: 161it [01:45,  1.67it/s]Extractor Estimating: 162it [01:45,  1.65it/s]Extractor Estimating: 163it [01:46,  1.69it/s]Extractor Estimating: 164it [01:46,  1.71it/s]Extractor Estimating: 165it [01:47,  1.68it/s]Extractor Estimating: 166it [01:48,  1.66it/s]Extractor Estimating: 167it [01:48,  1.63it/s]Extractor Estimating: 168it [01:49,  1.69it/s]Extractor Estimating: 169it [01:49,  1.73it/s]Extractor Estimating: 170it [01:50,  1.69it/s]Extractor Estimating: 171it [01:51,  1.46it/s]Extractor Estimating: 172it [01:52,  1.48it/s]Extractor Estimating: 173it [01:52,  1.53it/s]Extractor Estimating: 174it [01:53,  1.58it/s]Extractor Estimating: 175it [01:53,  1.61it/s]Extractor Estimating: 176it [01:54,  1.60it/s]Extractor Estimating: 177it [01:55,  1.61it/s]Extractor Estimating: 178it [01:55,  1.66it/s]Extractor Estimating: 179it [01:56,  1.62it/s]Extractor Estimating: 180it [01:56,  1.61it/s]Extractor Estimating: 181it [01:57,  1.62it/s]Extractor Estimating: 182it [01:58,  1.65it/s]Extractor Estimating: 183it [01:58,  1.63it/s]Extractor Estimating: 184it [01:59,  1.54it/s]Extractor Estimating: 185it [02:00,  1.61it/s]Extractor Estimating: 186it [02:00,  1.59it/s]Extractor Estimating: 187it [02:01,  1.54it/s]Extractor Estimating: 188it [02:02,  1.58it/s]Extractor Estimating: 189it [02:02,  1.56it/s]Extractor Estimating: 190it [02:03,  1.59it/s]Extractor Estimating: 191it [02:03,  1.61it/s]Extractor Estimating: 192it [02:04,  1.65it/s]Extractor Estimating: 193it [02:05,  1.67it/s]Extractor Estimating: 194it [02:05,  1.63it/s]Extractor Estimating: 195it [02:06,  1.60it/s]Extractor Estimating: 196it [02:06,  1.62it/s]Extractor Estimating: 197it [02:07,  1.67it/s]Extractor Estimating: 198it [02:08,  1.65it/s]Extractor Estimating: 199it [02:08,  1.63it/s]Extractor Estimating: 200it [02:09,  1.61it/s]Extractor Estimating: 201it [02:10,  1.59it/s]Extractor Estimating: 202it [02:10,  1.47it/s]Extractor Estimating: 203it [02:11,  1.52it/s]Extractor Estimating: 204it [02:12,  1.50it/s]Extractor Estimating: 205it [02:12,  1.51it/s]Extractor Estimating: 206it [02:13,  1.52it/s]Extractor Estimating: 207it [02:14,  1.28it/s]Extractor Estimating: 208it [02:15,  1.32it/s]Extractor Estimating: 209it [02:15,  1.38it/s]Extractor Estimating: 210it [02:16,  1.42it/s]Extractor Estimating: 211it [02:17,  1.42it/s]Extractor Estimating: 212it [02:18,  1.37it/s]Extractor Estimating: 213it [02:18,  1.41it/s]Extractor Estimating: 214it [02:19,  1.45it/s]Extractor Estimating: 215it [02:20,  1.43it/s]Extractor Estimating: 216it [02:20,  1.40it/s]Extractor Estimating: 217it [02:21,  1.41it/s]Extractor Estimating: 218it [02:22,  1.39it/s]Extractor Estimating: 219it [02:22,  1.39it/s]Extractor Estimating: 220it [02:23,  1.44it/s]Extractor Estimating: 221it [02:24,  1.49it/s]Extractor Estimating: 222it [02:24,  1.42it/s]Extractor Estimating: 223it [02:25,  1.47it/s]Extractor Estimating: 224it [02:26,  1.47it/s]Extractor Estimating: 225it [02:27,  1.45it/s]Extractor Estimating: 226it [02:27,  1.45it/s]Extractor Estimating: 227it [02:28,  1.45it/s]Extractor Estimating: 228it [02:29,  1.45it/s]Extractor Estimating: 229it [02:29,  1.50it/s]Extractor Estimating: 230it [02:30,  1.48it/s]Extractor Estimating: 231it [02:31,  1.49it/s]Extractor Estimating: 232it [02:31,  1.48it/s]Extractor Estimating: 233it [02:32,  1.46it/s]Extractor Estimating: 234it [02:33,  1.53it/s]Extractor Estimating: 235it [02:33,  1.54it/s]Extractor Estimating: 236it [02:34,  1.54it/s]Extractor Estimating: 237it [02:34,  1.54it/s]Extractor Estimating: 238it [02:35,  1.53it/s]Extractor Estimating: 239it [02:36,  1.55it/s]Extractor Estimating: 240it [02:36,  1.55it/s]Extractor Estimating: 241it [02:37,  1.52it/s]Extractor Estimating: 242it [02:38,  1.55it/s]Extractor Estimating: 243it [02:38,  1.58it/s]Extractor Estimating: 244it [02:39,  1.53it/s]Extractor Estimating: 245it [02:40,  1.51it/s]Extractor Estimating: 246it [02:40,  1.50it/s]Extractor Estimating: 247it [02:41,  1.56it/s]Extractor Estimating: 248it [02:42,  1.49it/s]Extractor Estimating: 249it [02:42,  1.48it/s]Extractor Estimating: 250it [02:43,  1.51it/s]Extractor Estimating: 251it [02:44,  1.52it/s]Extractor Estimating: 252it [02:44,  1.50it/s]Extractor Estimating: 253it [02:45,  1.51it/s]Extractor Estimating: 254it [02:46,  1.56it/s]Extractor Estimating: 255it [02:46,  1.52it/s]Extractor Estimating: 256it [02:47,  1.50it/s]Extractor Estimating: 257it [02:48,  1.53it/s]Extractor Estimating: 258it [02:48,  1.56it/s]Extractor Estimating: 259it [02:49,  1.57it/s]Extractor Estimating: 260it [02:50,  1.51it/s]Extractor Estimating: 261it [02:50,  1.49it/s]Extractor Estimating: 262it [02:51,  1.50it/s]Extractor Estimating: 263it [02:52,  1.52it/s]Extractor Estimating: 264it [02:52,  1.54it/s]Extractor Estimating: 265it [02:53,  1.50it/s]Extractor Estimating: 266it [02:54,  1.49it/s]Extractor Estimating: 267it [02:54,  1.54it/s]Extractor Estimating: 268it [02:55,  1.59it/s]Extractor Estimating: 269it [02:55,  1.67it/s]Extractor Estimating: 270it [02:56,  1.57it/s]Extractor Estimating: 271it [02:57,  1.58it/s]Extractor Estimating: 272it [02:57,  1.56it/s]Extractor Estimating: 273it [02:58,  1.60it/s]Extractor Estimating: 274it [02:59,  1.57it/s]Extractor Estimating: 275it [02:59,  1.52it/s]Extractor Estimating: 276it [03:00,  1.49it/s]Extractor Estimating: 277it [03:01,  1.53it/s]Extractor Estimating: 278it [03:01,  1.57it/s]Extractor Estimating: 279it [03:02,  1.58it/s]Extractor Estimating: 280it [03:03,  1.38it/s]Extractor Estimating: 281it [03:03,  1.45it/s]Extractor Estimating: 282it [03:04,  1.46it/s]Extractor Estimating: 283it [03:05,  1.51it/s]Extractor Estimating: 284it [03:05,  1.50it/s]Extractor Estimating: 285it [03:06,  1.49it/s]Extractor Estimating: 286it [03:07,  1.49it/s]Extractor Estimating: 287it [03:07,  1.48it/s]Extractor Estimating: 288it [03:08,  1.47it/s]Extractor Estimating: 289it [03:09,  1.48it/s]Extractor Estimating: 290it [03:09,  1.47it/s]Extractor Estimating: 291it [03:10,  1.45it/s]Extractor Estimating: 292it [03:11,  1.43it/s]Extractor Estimating: 293it [03:11,  1.46it/s]Extractor Estimating: 294it [03:12,  1.47it/s]Extractor Estimating: 295it [03:13,  1.49it/s]Extractor Estimating: 296it [03:14,  1.41it/s]Extractor Estimating: 297it [03:14,  1.37it/s]Extractor Estimating: 298it [03:15,  1.38it/s]Extractor Estimating: 299it [03:16,  1.39it/s]Extractor Estimating: 300it [03:16,  1.46it/s]Extractor Estimating: 301it [03:17,  1.51it/s]Extractor Estimating: 302it [03:18,  1.56it/s]Extractor Estimating: 303it [03:18,  1.57it/s]Extractor Estimating: 304it [03:19,  1.60it/s]Extractor Estimating: 305it [03:19,  1.58it/s]Extractor Estimating: 306it [03:20,  1.57it/s]Extractor Estimating: 307it [03:21,  1.57it/s]Extractor Estimating: 308it [03:21,  1.58it/s]Extractor Estimating: 309it [03:22,  1.64it/s]Extractor Estimating: 310it [03:23,  1.65it/s]Extractor Estimating: 311it [03:23,  1.60it/s]Extractor Estimating: 312it [03:24,  1.59it/s]Extractor Estimating: 313it [03:24,  1.60it/s]Extractor Estimating: 314it [03:25,  1.58it/s]Extractor Estimating: 315it [03:26,  1.62it/s]Extractor Estimating: 316it [03:26,  1.61it/s]Extractor Estimating: 317it [03:27,  1.63it/s]Extractor Estimating: 318it [03:28,  1.62it/s]Extractor Estimating: 319it [03:28,  1.60it/s]Extractor Estimating: 320it [03:29,  1.59it/s]Extractor Estimating: 321it [03:29,  1.54it/s]Extractor Estimating: 322it [03:30,  1.58it/s]Extractor Estimating: 323it [03:31,  1.63it/s]Extractor Estimating: 324it [03:31,  1.64it/s]Extractor Estimating: 325it [03:32,  1.63it/s]Extractor Estimating: 326it [03:33,  1.59it/s]Extractor Estimating: 327it [03:33,  1.57it/s]Extractor Estimating: 328it [03:34,  1.52it/s]Extractor Estimating: 329it [03:35,  1.49it/s]Extractor Estimating: 330it [03:35,  1.53it/s]Extractor Estimating: 331it [03:36,  1.61it/s]Extractor Estimating: 332it [03:36,  1.68it/s]Extractor Estimating: 333it [03:37,  1.63it/s]Extractor Estimating: 334it [03:38,  1.57it/s]Extractor Estimating: 335it [03:38,  1.57it/s]Extractor Estimating: 336it [03:39,  1.58it/s]Extractor Estimating: 337it [03:40,  1.56it/s]Extractor Estimating: 338it [03:40,  1.53it/s]Extractor Estimating: 339it [03:41,  1.55it/s]Extractor Estimating: 340it [03:42,  1.55it/s]Extractor Estimating: 341it [03:42,  1.56it/s]Extractor Estimating: 342it [03:43,  1.63it/s]Extractor Estimating: 343it [03:43,  1.63it/s]Extractor Estimating: 344it [03:44,  1.61it/s]Extractor Estimating: 345it [03:45,  1.59it/s]Extractor Estimating: 346it [03:45,  1.60it/s]Extractor Estimating: 347it [03:46,  1.67it/s]Extractor Estimating: 348it [03:46,  1.65it/s]Extractor Estimating: 349it [03:47,  1.57it/s]Extractor Estimating: 350it [03:48,  1.61it/s]Extractor Estimating: 351it [03:48,  1.61it/s]Extractor Estimating: 352it [03:49,  1.57it/s]Extractor Estimating: 353it [03:50,  1.56it/s]Extractor Estimating: 354it [03:50,  1.57it/s]Extractor Estimating: 355it [03:51,  1.61it/s]Extractor Estimating: 356it [03:51,  1.62it/s]Extractor Estimating: 357it [03:52,  1.62it/s]Extractor Estimating: 358it [03:53,  1.62it/s]Extractor Estimating: 359it [03:53,  1.60it/s]Extractor Estimating: 360it [03:54,  1.56it/s]Extractor Estimating: 361it [03:55,  1.53it/s]Extractor Estimating: 362it [03:55,  1.56it/s]Extractor Estimating: 363it [03:56,  1.59it/s]Extractor Estimating: 364it [03:57,  1.57it/s]Extractor Estimating: 365it [03:57,  1.56it/s]Extractor Estimating: 366it [03:58,  1.58it/s]Extractor Estimating: 367it [03:59,  1.52it/s]Extractor Estimating: 368it [03:59,  1.54it/s]Extractor Estimating: 369it [04:00,  1.37it/s]Extractor Estimating: 370it [04:01,  1.43it/s]Extractor Estimating: 371it [04:01,  1.46it/s]Extractor Estimating: 372it [04:02,  1.37it/s]Extractor Estimating: 373it [04:03,  1.41it/s]Extractor Estimating: 374it [04:03,  1.49it/s]Extractor Estimating: 375it [04:04,  1.51it/s]Extractor Estimating: 376it [04:05,  1.52it/s]Extractor Estimating: 377it [04:05,  1.51it/s]Extractor Estimating: 378it [04:06,  1.52it/s]Extractor Estimating: 379it [04:07,  1.56it/s]Extractor Estimating: 380it [04:07,  1.58it/s]Extractor Estimating: 381it [04:08,  1.54it/s]Extractor Estimating: 382it [04:09,  1.55it/s]Extractor Estimating: 383it [04:09,  1.56it/s]Extractor Estimating: 384it [04:10,  1.51it/s]Extractor Estimating: 385it [04:11,  1.49it/s]Extractor Estimating: 386it [04:11,  1.50it/s]Extractor Estimating: 387it [04:12,  1.48it/s]Extractor Estimating: 388it [04:13,  1.51it/s]Extractor Estimating: 389it [04:13,  1.47it/s]Extractor Estimating: 390it [04:14,  1.47it/s]Extractor Estimating: 391it [04:15,  1.48it/s]Extractor Estimating: 392it [04:15,  1.44it/s]Extractor Estimating: 393it [04:16,  1.42it/s]Extractor Estimating: 394it [04:17,  1.48it/s]Extractor Estimating: 395it [04:17,  1.50it/s]Extractor Estimating: 396it [04:18,  1.43it/s]Extractor Estimating: 397it [04:19,  1.43it/s]Extractor Estimating: 398it [04:20,  1.43it/s]Extractor Estimating: 399it [04:20,  1.45it/s]Extractor Estimating: 400it [04:21,  1.49it/s]Extractor Estimating: 401it [04:21,  1.51it/s]Extractor Estimating: 402it [04:22,  1.46it/s]Extractor Estimating: 403it [04:23,  1.53it/s]Extractor Estimating: 404it [04:23,  1.52it/s]Extractor Estimating: 405it [04:24,  1.57it/s]Extractor Estimating: 406it [04:25,  1.59it/s]Extractor Estimating: 407it [04:25,  1.54it/s]Extractor Estimating: 408it [04:26,  1.58it/s]Extractor Estimating: 409it [04:27,  1.58it/s]Extractor Estimating: 410it [04:27,  1.59it/s]Extractor Estimating: 411it [04:28,  1.58it/s]Extractor Estimating: 412it [04:29,  1.57it/s]Extractor Estimating: 413it [04:29,  1.57it/s]Extractor Estimating: 414it [04:30,  1.55it/s]Extractor Estimating: 415it [04:30,  1.55it/s]Extractor Estimating: 416it [04:31,  1.52it/s]Extractor Estimating: 417it [04:32,  1.55it/s]Extractor Estimating: 418it [04:32,  1.52it/s]Extractor Estimating: 419it [04:33,  1.56it/s]Extractor Estimating: 420it [04:34,  1.56it/s]Extractor Estimating: 421it [04:34,  1.54it/s]Extractor Estimating: 422it [04:35,  1.47it/s]Extractor Estimating: 423it [04:36,  1.46it/s]Extractor Estimating: 424it [04:36,  1.52it/s]Extractor Estimating: 425it [04:37,  1.52it/s]Extractor Estimating: 426it [04:38,  1.54it/s]Extractor Estimating: 427it [04:38,  1.57it/s]Extractor Estimating: 428it [04:39,  1.55it/s]Extractor Estimating: 429it [04:40,  1.60it/s]Extractor Estimating: 430it [04:40,  1.55it/s]Extractor Estimating: 431it [04:41,  1.53it/s]Extractor Estimating: 432it [04:42,  1.55it/s]Extractor Estimating: 433it [04:42,  1.47it/s]Extractor Estimating: 434it [04:43,  1.45it/s]Extractor Estimating: 435it [04:44,  1.46it/s]Extractor Estimating: 436it [04:44,  1.51it/s]Extractor Estimating: 437it [04:45,  1.53it/s]Extractor Estimating: 438it [04:46,  1.50it/s]Extractor Estimating: 439it [04:46,  1.48it/s]Extractor Estimating: 440it [04:47,  1.48it/s]Extractor Estimating: 441it [04:48,  1.47it/s]Extractor Estimating: 442it [04:48,  1.49it/s]Extractor Estimating: 443it [04:49,  1.48it/s]Extractor Estimating: 444it [04:50,  1.48it/s]Extractor Estimating: 445it [04:50,  1.45it/s]Extractor Estimating: 446it [04:51,  1.45it/s]Extractor Estimating: 447it [04:52,  1.45it/s]Extractor Estimating: 448it [04:53,  1.41it/s]Extractor Estimating: 449it [04:53,  1.40it/s]Extractor Estimating: 450it [04:54,  1.47it/s]Extractor Estimating: 451it [04:54,  1.56it/s]Extractor Estimating: 452it [04:55,  1.56it/s]Extractor Estimating: 453it [04:56,  1.50it/s]Extractor Estimating: 454it [04:56,  1.54it/s]Extractor Estimating: 455it [04:57,  1.58it/s]Extractor Estimating: 456it [04:58,  1.44it/s]Extractor Estimating: 457it [04:58,  1.47it/s]Extractor Estimating: 458it [04:59,  1.45it/s]Extractor Estimating: 459it [05:00,  1.51it/s]Extractor Estimating: 460it [05:00,  1.57it/s]Extractor Estimating: 461it [05:01,  1.56it/s]Extractor Estimating: 462it [05:02,  1.55it/s]Extractor Estimating: 463it [05:02,  1.54it/s]Extractor Estimating: 464it [05:03,  1.55it/s]Extractor Estimating: 465it [05:04,  1.51it/s]Extractor Estimating: 466it [05:04,  1.59it/s]Extractor Estimating: 467it [05:05,  1.57it/s]Extractor Estimating: 468it [05:06,  1.56it/s]Extractor Estimating: 469it [05:06,  1.58it/s]Extractor Estimating: 470it [05:07,  1.59it/s]Extractor Estimating: 471it [05:07,  1.60it/s]Extractor Estimating: 472it [05:08,  1.51it/s]Extractor Estimating: 473it [05:09,  1.52it/s]Extractor Estimating: 474it [05:09,  1.55it/s]Extractor Estimating: 475it [05:10,  1.57it/s]Extractor Estimating: 476it [05:11,  1.53it/s]Extractor Estimating: 477it [05:11,  1.52it/s]Extractor Estimating: 478it [05:12,  1.55it/s]Extractor Estimating: 479it [05:13,  1.53it/s]Extractor Estimating: 480it [05:13,  1.50it/s]Extractor Estimating: 481it [05:14,  1.49it/s]Extractor Estimating: 482it [05:15,  1.51it/s]Extractor Estimating: 483it [05:15,  1.53it/s]Extractor Estimating: 484it [05:16,  1.57it/s]Extractor Estimating: 485it [05:17,  1.51it/s]Extractor Estimating: 486it [05:17,  1.54it/s]Extractor Estimating: 487it [05:18,  1.51it/s]Extractor Estimating: 488it [05:19,  1.51it/s]Extractor Estimating: 489it [05:19,  1.52it/s]Extractor Estimating: 490it [05:20,  1.54it/s]Extractor Estimating: 491it [05:21,  1.55it/s]Extractor Estimating: 492it [05:21,  1.54it/s]Extractor Estimating: 493it [05:22,  1.55it/s]Extractor Estimating: 494it [05:22,  1.55it/s]Extractor Estimating: 495it [05:23,  1.53it/s]Extractor Estimating: 496it [05:24,  1.53it/s]Extractor Estimating: 497it [05:25,  1.43it/s]Extractor Estimating: 498it [05:25,  1.48it/s]Extractor Estimating: 499it [05:26,  1.49it/s]Extractor Estimating: 500it [05:27,  1.53it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:30,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:30,275 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:30,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:30,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:30,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:47:31,061 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:47:31,062 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:47:31,668 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:47:32,743 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:47:32,743 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:35,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:35,719 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:35,719 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:35,719 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:35,719 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:47:36,382 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:47:36,384 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:47:37,037 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:47:37,210 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:47:37,210 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:58:06,579 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:58:06,626 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 10465 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 25156
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25256, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25256, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.120, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.084, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.093, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 63, avg_time 1.114, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 163, avg_time 2.205, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 263, avg_time 1.074, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 363, avg_time 1.108, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 26, avg_time 1.099, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 126, avg_time 1.095, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 226, avg_time 2.189, loss:nan
g_step 1200, step 326, avg_time 1.100, loss:nan
g_step 1300, step 426, avg_time 1.112, loss:nan
g_step 1400, step 89, avg_time 1.120, loss:nan
g_step 1500, step 189, avg_time 1.065, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 289, avg_time 2.181, loss:nan
g_step 1700, step 389, avg_time 1.103, loss:nan
g_step 1800, step 52, avg_time 1.096, loss:nan
g_step 1900, step 152, avg_time 1.072, loss:nan
g_step 2000, step 252, avg_time 1.086, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 352, avg_time 2.217, loss:nan
g_step 2200, step 15, avg_time 1.109, loss:nan
g_step 2300, step 115, avg_time 1.087, loss:nan
g_step 2400, step 215, avg_time 1.115, loss:nan
g_step 2500, step 315, avg_time 1.106, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 415, avg_time 2.186, loss:nan
g_step 2700, step 78, avg_time 1.087, loss:nan
g_step 2800, step 178, avg_time 1.097, loss:nan
g_step 2900, step 278, avg_time 1.103, loss:nan
g_step 3000, step 378, avg_time 1.083, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 41, avg_time 2.194, loss:nan
g_step 3200, step 141, avg_time 1.112, loss:nan
g_step 3300, step 241, avg_time 1.092, loss:nan
g_step 3400, step 341, avg_time 1.094, loss:nan
g_step 3500, step 4, avg_time 1.068, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 104, avg_time 2.152, loss:nan
g_step 3700, step 204, avg_time 1.112, loss:nan
g_step 3800, step 304, avg_time 1.093, loss:nan
g_step 3900, step 404, avg_time 1.105, loss:nan
g_step 4000, step 67, avg_time 1.091, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 167, avg_time 2.202, loss:nan
g_step 4200, step 267, avg_time 1.077, loss:nan
g_step 4300, step 367, avg_time 1.094, loss:nan
g_step 4400, step 30, avg_time 1.079, loss:nan
g_step 4500, step 130, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 230, avg_time 2.190, loss:nan
g_step 4700, step 330, avg_time 1.095, loss:nan
g_step 4800, step 430, avg_time 1.091, loss:nan
g_step 4900, step 93, avg_time 1.068, loss:nan
g_step 5000, step 193, avg_time 1.087, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 293, avg_time 2.162, loss:nan
g_step 5200, step 393, avg_time 1.119, loss:nan
g_step 5300, step 56, avg_time 1.089, loss:nan
g_step 5400, step 156, avg_time 1.103, loss:nan
g_step 5500, step 256, avg_time 1.100, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 356, avg_time 2.186, loss:nan
g_step 5700, step 19, avg_time 1.074, loss:nan
g_step 5800, step 119, avg_time 1.057, loss:nan
g_step 5900, step 219, avg_time 1.098, loss:nan
g_step 6000, step 319, avg_time 1.090, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 419, avg_time 2.199, loss:nan
g_step 6200, step 82, avg_time 1.102, loss:nan
g_step 6300, step 182, avg_time 1.093, loss:nan
g_step 6400, step 282, avg_time 1.075, loss:nan
g_step 6500, step 382, avg_time 1.122, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 45, avg_time 2.199, loss:nan
g_step 6700, step 145, avg_time 1.094, loss:nan
g_step 6800, step 245, avg_time 1.087, loss:nan
g_step 6900, step 345, avg_time 1.095, loss:nan
g_step 7000, step 8, avg_time 1.093, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 108, avg_time 2.188, loss:nan
g_step 7200, step 208, avg_time 1.112, loss:nan
g_step 7300, step 308, avg_time 1.108, loss:nan
g_step 7400, step 408, avg_time 1.082, loss:nan
g_step 7500, step 71, avg_time 1.090, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 171, avg_time 2.191, loss:nan
g_step 7700, step 271, avg_time 1.102, loss:nan
g_step 7800, step 371, avg_time 1.087, loss:nan
g_step 7900, step 34, avg_time 1.085, loss:nan
g_step 8000, step 134, avg_time 1.089, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 234, avg_time 2.191, loss:nan
g_step 8200, step 334, avg_time 1.094, loss:nan
g_step 8300, step 434, avg_time 1.093, loss:nan
g_step 8400, step 97, avg_time 1.097, loss:nan
g_step 8500, step 197, avg_time 1.117, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 297, avg_time 2.194, loss:nan
g_step 8700, step 397, avg_time 1.069, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:58:06 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:58:06 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-58-06_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:58:07 - WARNING - datasets.builder -   Using custom data configuration default-64528337d57c4846
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-64528337d57c4846/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:58:10,335 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:58:10,373 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:58:10,374 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:58:10,375 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:58:10,477 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:58:10,525 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:58:10,525 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:58:10,525 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:58:10,525 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:58:10,525 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:58:10,526 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:58:10,953 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:58:14,071 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:58:14,108 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-64528337d57c4846/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.89ba/s] 18%|        | 2/11 [00:00<00:02,  3.75ba/s] 27%|       | 3/11 [00:00<00:01,  4.13ba/s] 36%|      | 4/11 [00:00<00:01,  4.32ba/s] 45%|     | 5/11 [00:01<00:01,  4.41ba/s] 55%|    | 6/11 [00:01<00:01,  4.46ba/s] 64%|   | 7/11 [00:01<00:00,  4.51ba/s] 73%|  | 8/11 [00:02<00:00,  3.73ba/s] 82%| | 9/11 [00:02<00:00,  3.95ba/s] 91%| | 10/11 [00:02<00:00,  4.11ba/s]100%|| 11/11 [00:02<00:00,  4.97ba/s]100%|| 11/11 [00:02<00:00,  4.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.24ba/s] 50%|     | 2/4 [00:00<00:00,  3.84ba/s] 75%|  | 3/4 [00:00<00:00,  4.09ba/s]100%|| 4/4 [00:00<00:00,  5.20ba/s]100%|| 4/4 [00:00<00:00,  4.58ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  4.81ba/s] 27%|       | 3/11 [00:00<00:01,  7.99ba/s] 45%|     | 5/11 [00:00<00:00,  9.02ba/s] 64%|   | 7/11 [00:00<00:00,  9.60ba/s] 73%|  | 8/11 [00:00<00:00,  8.95ba/s] 91%| | 10/11 [00:01<00:00,  9.47ba/s]100%|| 11/11 [00:01<00:00,  9.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.42ba/s] 75%|  | 3/4 [00:00<00:00,  8.45ba/s]100%|| 4/4 [00:00<00:00,  9.47ba/s]
[INFO|trainer.py:414] 2023-08-28 13:58:20,447 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:58:20,495 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:58:20,495 >>   Num examples = 10479
[INFO|trainer.py:1149] 2023-08-28 13:58:20,495 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:58:20,495 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:58:20,495 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:58:20,495 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:58:20,495 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:52,  3.52it/s]  0%|          | 2/820 [00:00<03:48,  3.58it/s]  0%|          | 3/820 [00:00<03:46,  3.61it/s]  0%|          | 4/820 [00:01<03:45,  3.61it/s]  1%|          | 5/820 [00:01<03:45,  3.62it/s]  1%|          | 6/820 [00:01<03:44,  3.62it/s]  1%|          | 7/820 [00:01<03:44,  3.62it/s]  1%|          | 8/820 [00:02<03:43,  3.63it/s]  1%|          | 9/820 [00:02<03:43,  3.62it/s]  1%|          | 10/820 [00:02<03:43,  3.62it/s]  1%|         | 11/820 [00:03<03:43,  3.62it/s]  1%|         | 12/820 [00:03<03:42,  3.62it/s]  2%|         | 13/820 [00:03<04:08,  3.25it/s]  2%|         | 14/820 [00:03<04:00,  3.36it/s]  2%|         | 15/820 [00:04<03:54,  3.43it/s]  2%|         | 16/820 [00:04<03:50,  3.49it/s]  2%|         | 17/820 [00:04<03:47,  3.53it/s]  2%|         | 18/820 [00:05<03:45,  3.56it/s]  2%|         | 19/820 [00:05<03:43,  3.58it/s]  2%|         | 20/820 [00:05<03:42,  3.59it/s]  3%|         | 21/820 [00:05<03:41,  3.60it/s]  3%|         | 22/820 [00:06<03:41,  3.61it/s]  3%|         | 23/820 [00:06<03:40,  3.61it/s]  3%|         | 24/820 [00:06<03:49,  3.47it/s]  3%|         | 25/820 [00:07<03:46,  3.51it/s]  3%|         | 26/820 [00:07<03:43,  3.55it/s]  3%|         | 27/820 [00:07<03:42,  3.57it/s]  3%|         | 28/820 [00:07<03:41,  3.58it/s]  4%|         | 29/820 [00:08<03:40,  3.59it/s]  4%|         | 30/820 [00:08<03:39,  3.60it/s]  4%|         | 31/820 [00:08<03:38,  3.61it/s]  4%|         | 32/820 [00:08<03:38,  3.61it/s]  4%|         | 33/820 [00:09<03:37,  3.62it/s]  4%|         | 34/820 [00:09<03:36,  3.62it/s]  4%|         | 35/820 [00:09<03:43,  3.52it/s]  4%|         | 36/820 [00:10<03:40,  3.55it/s]  5%|         | 37/820 [00:10<03:39,  3.57it/s]  5%|         | 38/820 [00:10<03:38,  3.59it/s]  5%|         | 39/820 [00:10<03:37,  3.59it/s]  5%|         | 40/820 [00:11<03:36,  3.60it/s]  5%|         | 41/820 [00:11<03:35,  3.61it/s]  5%|         | 42/820 [00:11<03:35,  3.61it/s]  5%|         | 43/820 [00:12<03:34,  3.61it/s]  5%|         | 44/820 [00:12<03:34,  3.61it/s]  5%|         | 45/820 [00:12<03:34,  3.61it/s]  6%|         | 46/820 [00:12<03:39,  3.52it/s]  6%|         | 47/820 [00:13<03:37,  3.55it/s]  6%|         | 48/820 [00:13<03:36,  3.57it/s]  6%|         | 49/820 [00:13<03:34,  3.59it/s]  6%|         | 50/820 [00:14<03:34,  3.59it/s]  6%|         | 51/820 [00:14<03:33,  3.60it/s]  6%|         | 52/820 [00:14<03:33,  3.60it/s]  6%|         | 53/820 [00:14<03:32,  3.61it/s]  7%|         | 54/820 [00:15<03:31,  3.61it/s]  7%|         | 55/820 [00:15<03:31,  3.62it/s]  7%|         | 56/820 [00:15<03:31,  3.61it/s]  7%|         | 57/820 [00:15<03:38,  3.50it/s]  7%|         | 58/820 [00:16<03:35,  3.53it/s]  7%|         | 59/820 [00:16<03:33,  3.56it/s]  7%|         | 60/820 [00:16<03:32,  3.57it/s]  7%|         | 61/820 [00:17<03:31,  3.59it/s]  8%|         | 62/820 [00:17<03:30,  3.60it/s]  8%|         | 63/820 [00:17<03:30,  3.60it/s]  8%|         | 64/820 [00:17<03:29,  3.61it/s]  8%|         | 65/820 [00:18<03:29,  3.61it/s]  8%|         | 66/820 [00:18<03:28,  3.61it/s]  8%|         | 67/820 [00:18<03:28,  3.61it/s]  8%|         | 68/820 [00:19<03:31,  3.56it/s]  8%|         | 69/820 [00:19<03:29,  3.58it/s]  9%|         | 70/820 [00:19<03:29,  3.59it/s]  9%|         | 71/820 [00:19<03:28,  3.59it/s]  9%|         | 72/820 [00:20<03:27,  3.60it/s]  9%|         | 73/820 [00:20<03:26,  3.61it/s]  9%|         | 74/820 [00:20<03:26,  3.61it/s]  9%|         | 75/820 [00:20<03:26,  3.60it/s]  9%|         | 76/820 [00:21<03:27,  3.59it/s]  9%|         | 77/820 [00:21<03:27,  3.58it/s] 10%|         | 78/820 [00:21<03:27,  3.58it/s] 10%|         | 79/820 [00:22<03:43,  3.32it/s] 10%|         | 80/820 [00:22<03:38,  3.38it/s] 10%|         | 81/820 [00:22<03:35,  3.44it/s] 10%|         | 82/820 [00:23<03:32,  3.47it/s] 10%|         | 83/820 [00:23<03:30,  3.50it/s] 10%|         | 84/820 [00:23<03:29,  3.52it/s] 10%|         | 85/820 [00:23<03:28,  3.53it/s] 10%|         | 86/820 [00:24<03:27,  3.54it/s] 11%|         | 87/820 [00:24<03:26,  3.55it/s] 11%|         | 88/820 [00:24<03:25,  3.56it/s] 11%|         | 89/820 [00:24<03:25,  3.56it/s] 11%|         | 90/820 [00:25<03:38,  3.35it/s] 11%|         | 91/820 [00:25<03:33,  3.41it/s] 11%|         | 92/820 [00:25<03:30,  3.46it/s] 11%|        | 93/820 [00:26<03:28,  3.49it/s] 11%|        | 94/820 [00:26<03:26,  3.51it/s] 12%|        | 95/820 [00:26<03:25,  3.53it/s] 12%|        | 96/820 [00:26<03:24,  3.54it/s] 12%|        | 97/820 [00:27<03:23,  3.55it/s] 12%|        | 98/820 [00:27<03:22,  3.56it/s] 12%|        | 99/820 [00:27<03:22,  3.56it/s] 12%|        | 100/820 [00:28<03:21,  3.57it/s] 12%|        | 101/820 [00:28<03:29,  3.43it/s] 12%|        | 102/820 [00:28<03:27,  3.47it/s] 13%|        | 103/820 [00:28<03:24,  3.50it/s] 13%|        | 104/820 [00:29<03:23,  3.52it/s] 13%|        | 105/820 [00:29<03:22,  3.53it/s] 13%|        | 106/820 [00:29<03:21,  3.54it/s] 13%|        | 107/820 [00:30<03:20,  3.56it/s] 13%|        | 108/820 [00:30<03:20,  3.56it/s] 13%|        | 109/820 [00:30<03:19,  3.56it/s] 13%|        | 110/820 [00:30<03:19,  3.57it/s] 14%|        | 111/820 [00:31<03:18,  3.57it/s] 14%|        | 112/820 [00:31<03:18,  3.57it/s] 14%|        | 113/820 [00:31<03:18,  3.57it/s] 14%|        | 114/820 [00:32<03:17,  3.57it/s] 14%|        | 115/820 [00:32<03:17,  3.57it/s] 14%|        | 116/820 [00:32<03:17,  3.57it/s] 14%|        | 117/820 [00:32<03:16,  3.57it/s] 14%|        | 118/820 [00:33<03:16,  3.57it/s] 15%|        | 119/820 [00:33<03:16,  3.57it/s] 15%|        | 120/820 [00:33<03:16,  3.57it/s] 15%|        | 121/820 [00:34<03:15,  3.57it/s] 15%|        | 122/820 [00:34<03:21,  3.47it/s] 15%|        | 123/820 [00:34<03:19,  3.49it/s] 15%|        | 124/820 [00:34<03:18,  3.51it/s] 15%|        | 125/820 [00:35<03:17,  3.53it/s] 15%|        | 126/820 [00:35<03:16,  3.54it/s] 15%|        | 127/820 [00:35<03:15,  3.54it/s] 16%|        | 128/820 [00:36<03:15,  3.55it/s] 16%|        | 129/820 [00:36<03:14,  3.55it/s] 16%|        | 130/820 [00:36<03:14,  3.56it/s] 16%|        | 131/820 [00:36<03:13,  3.56it/s] 16%|        | 132/820 [00:37<03:13,  3.56it/s] 16%|        | 133/820 [00:37<03:19,  3.44it/s] 16%|        | 134/820 [00:37<03:17,  3.48it/s] 16%|        | 135/820 [00:38<03:15,  3.50it/s] 17%|        | 136/820 [00:38<03:14,  3.52it/s] 17%|        | 137/820 [00:38<03:13,  3.53it/s] 17%|        | 138/820 [00:38<03:12,  3.54it/s] 17%|        | 139/820 [00:39<03:11,  3.55it/s] 17%|        | 140/820 [00:39<03:11,  3.55it/s] 17%|        | 141/820 [00:39<03:10,  3.56it/s] 17%|        | 142/820 [00:39<03:10,  3.56it/s] 17%|        | 143/820 [00:40<03:10,  3.56it/s] 18%|        | 144/820 [00:40<03:16,  3.44it/s] 18%|        | 145/820 [00:40<03:14,  3.48it/s] 18%|        | 146/820 [00:41<03:12,  3.50it/s] 18%|        | 147/820 [00:41<03:11,  3.52it/s] 18%|        | 148/820 [00:41<03:10,  3.53it/s] 18%|        | 149/820 [00:41<03:09,  3.54it/s] 18%|        | 150/820 [00:42<03:08,  3.55it/s] 18%|        | 151/820 [00:42<03:08,  3.55it/s] 19%|        | 152/820 [00:42<03:07,  3.56it/s] 19%|        | 153/820 [00:43<03:10,  3.50it/s] 19%|        | 154/820 [00:43<03:09,  3.51it/s] 19%|        | 155/820 [00:43<03:14,  3.42it/s] 19%|        | 156/820 [00:43<03:11,  3.46it/s] 19%|        | 157/820 [00:44<03:10,  3.49it/s] 19%|        | 158/820 [00:44<03:08,  3.51it/s] 19%|        | 159/820 [00:44<03:07,  3.52it/s] 20%|        | 160/820 [00:45<03:06,  3.53it/s] 20%|        | 161/820 [00:45<03:51,  2.85it/s] 20%|        | 162/820 [00:45<03:42,  2.96it/s] 20%|        | 163/820 [00:46<03:31,  3.11it/s] 20%|        | 164/820 [00:46<03:09,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 13:59:06,959 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:59:06,959 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 13:59:06,959 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.97it/s][A
  3%|         | 12/436 [00:00<00:08, 49.33it/s][A
  4%|         | 17/436 [00:00<00:08, 47.62it/s][A
  5%|         | 22/436 [00:00<00:08, 46.90it/s][A
  6%|         | 27/436 [00:00<00:08, 46.40it/s][A
  7%|         | 32/436 [00:00<00:08, 45.67it/s][A
  8%|         | 37/436 [00:00<00:08, 45.25it/s][A
 10%|         | 42/436 [00:00<00:08, 44.70it/s][A
 11%|         | 47/436 [00:01<00:08, 44.53it/s][A
 12%|        | 52/436 [00:01<00:08, 43.63it/s][A
 13%|        | 57/436 [00:01<00:08, 44.19it/s][A
 14%|        | 62/436 [00:01<00:08, 44.57it/s][A
 15%|        | 67/436 [00:01<00:08, 44.84it/s][A
 17%|        | 72/436 [00:01<00:08, 44.97it/s][A
 18%|        | 77/436 [00:01<00:07, 44.98it/s][A
 19%|        | 82/436 [00:01<00:07, 44.79it/s][A
 20%|        | 87/436 [00:01<00:07, 44.47it/s][A
 21%|        | 92/436 [00:02<00:07, 44.37it/s][A
 22%|       | 97/436 [00:02<00:07, 44.52it/s][A
 23%|       | 102/436 [00:02<00:07, 44.60it/s][A
 25%|       | 107/436 [00:02<00:07, 44.79it/s][A
 26%|       | 112/436 [00:02<00:07, 44.97it/s][A
 27%|       | 117/436 [00:02<00:07, 45.18it/s][A
 28%|       | 122/436 [00:02<00:06, 45.09it/s][A
 29%|       | 127/436 [00:02<00:06, 44.91it/s][A
 30%|       | 132/436 [00:02<00:06, 44.67it/s][A
 31%|      | 137/436 [00:03<00:06, 44.49it/s][A
 33%|      | 142/436 [00:03<00:06, 43.99it/s][A
 34%|      | 147/436 [00:03<00:06, 44.35it/s][A
 35%|      | 152/436 [00:03<00:06, 44.61it/s][A
 36%|      | 157/436 [00:03<00:06, 44.82it/s][A
 37%|      | 162/436 [00:03<00:06, 45.03it/s][A
 38%|      | 167/436 [00:03<00:05, 44.98it/s][A
 39%|      | 172/436 [00:03<00:05, 44.82it/s][A
 41%|      | 177/436 [00:03<00:05, 44.28it/s][A
 42%|     | 182/436 [00:04<00:05, 44.26it/s][A
 43%|     | 187/436 [00:04<00:05, 44.35it/s][A
 44%|     | 192/436 [00:04<00:05, 44.50it/s][A
 45%|     | 197/436 [00:04<00:05, 44.72it/s][A
 46%|     | 202/436 [00:04<00:05, 44.92it/s][A
 47%|     | 207/436 [00:04<00:05, 45.10it/s][A
 49%|     | 212/436 [00:04<00:04, 45.06it/s][A
 50%|     | 217/436 [00:04<00:04, 44.81it/s][A
 51%|     | 222/436 [00:04<00:04, 44.62it/s][A
 52%|    | 227/436 [00:05<00:04, 44.59it/s][A
 53%|    | 232/436 [00:05<00:04, 44.62it/s][A
 54%|    | 237/436 [00:05<00:04, 44.67it/s][A
 56%|    | 242/436 [00:05<00:04, 44.72it/s][A
 57%|    | 247/436 [00:05<00:04, 44.88it/s][A
 58%|    | 252/436 [00:05<00:04, 45.07it/s][A
 59%|    | 257/436 [00:05<00:03, 45.04it/s][A
 60%|    | 262/436 [00:05<00:03, 44.95it/s][A
 61%|    | 267/436 [00:05<00:03, 44.79it/s][A
 62%|   | 272/436 [00:06<00:03, 44.70it/s][A
 64%|   | 277/436 [00:06<00:03, 42.04it/s][A
 65%|   | 282/436 [00:06<00:03, 42.88it/s][A
 66%|   | 287/436 [00:06<00:03, 43.58it/s][A
 67%|   | 292/436 [00:06<00:03, 44.03it/s][A
 68%|   | 297/436 [00:06<00:03, 44.39it/s][A
 69%|   | 302/436 [00:06<00:03, 43.34it/s][A
 70%|   | 307/436 [00:06<00:02, 43.85it/s][A
 72%|  | 312/436 [00:06<00:02, 44.06it/s][A
 73%|  | 317/436 [00:07<00:02, 44.06it/s][A
 74%|  | 322/436 [00:07<00:02, 44.29it/s][A
 75%|  | 327/436 [00:07<00:02, 44.44it/s][A
 76%|  | 332/436 [00:07<00:02, 44.72it/s][A
 77%|  | 337/436 [00:07<00:02, 44.88it/s][A
 78%|  | 342/436 [00:07<00:02, 44.80it/s][A
 80%|  | 347/436 [00:07<00:01, 44.70it/s][A
 81%|  | 352/436 [00:07<00:01, 44.79it/s][A
 82%| | 357/436 [00:07<00:01, 44.73it/s][A
 83%| | 362/436 [00:08<00:01, 44.49it/s][A
 84%| | 367/436 [00:08<00:01, 44.56it/s][A
 85%| | 372/436 [00:08<00:01, 44.70it/s][A
 86%| | 377/436 [00:08<00:01, 44.88it/s][A
 88%| | 382/436 [00:08<00:01, 45.00it/s][A
 89%| | 387/436 [00:08<00:01, 44.94it/s][A
 90%| | 392/436 [00:08<00:00, 44.91it/s][A
 91%| | 397/436 [00:08<00:00, 44.84it/s][A
 92%|| 402/436 [00:08<00:00, 44.81it/s][A
 93%|| 407/436 [00:09<00:00, 44.64it/s][A
 94%|| 412/436 [00:09<00:00, 41.69it/s][A
 96%|| 417/436 [00:09<00:00, 42.71it/s][A
 97%|| 422/436 [00:09<00:00, 43.49it/s][A
 98%|| 427/436 [00:09<00:00, 44.08it/s][A
 99%|| 432/436 [00:09<00:00, 44.37it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.37it/s][A 20%|        | 164/820 [00:56<03:09,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:59:16,949 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 13:59:17,166 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:59:20,866 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:59:21,047 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:59:21,109 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164/special_tokens_map.json
 20%|        | 165/820 [01:01<52:02,  4.77s/it] 20%|        | 166/820 [01:01<37:17,  3.42s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/820 [01:02<26:59,  2.48s/it] 20%|        | 168/820 [01:02<19:46,  1.82s/it] 21%|        | 169/820 [01:02<14:44,  1.36s/it] 21%|        | 170/820 [01:03<11:12,  1.03s/it] 21%|        | 171/820 [01:03<08:44,  1.24it/s] 21%|        | 172/820 [01:03<07:01,  1.54it/s] 21%|        | 173/820 [01:03<05:48,  1.85it/s] 21%|        | 174/820 [01:04<04:58,  2.17it/s] 21%|       | 175/820 [01:04<04:22,  2.46it/s] 21%|       | 176/820 [01:04<03:57,  2.71it/s] 22%|       | 177/820 [01:05<03:40,  2.92it/s] 22%|       | 178/820 [01:05<03:36,  2.97it/s] 22%|       | 179/820 [01:05<03:25,  3.12it/s] 22%|       | 180/820 [01:05<03:17,  3.24it/s] 22%|       | 181/820 [01:06<03:11,  3.34it/s] 22%|       | 182/820 [01:06<03:07,  3.40it/s] 22%|       | 183/820 [01:06<03:04,  3.45it/s] 22%|       | 184/820 [01:07<03:02,  3.48it/s] 23%|       | 185/820 [01:07<03:01,  3.50it/s] 23%|       | 186/820 [01:07<03:00,  3.52it/s] 23%|       | 187/820 [01:07<02:59,  3.53it/s] 23%|       | 188/820 [01:08<02:58,  3.54it/s] 23%|       | 189/820 [01:08<03:03,  3.44it/s] 23%|       | 190/820 [01:08<03:01,  3.47it/s] 23%|       | 191/820 [01:09<02:59,  3.50it/s] 23%|       | 192/820 [01:09<02:58,  3.52it/s] 24%|       | 193/820 [01:09<02:57,  3.53it/s] 24%|       | 194/820 [01:09<02:56,  3.54it/s] 24%|       | 195/820 [01:10<02:56,  3.55it/s] 24%|       | 196/820 [01:10<02:55,  3.55it/s] 24%|       | 197/820 [01:10<02:55,  3.55it/s] 24%|       | 198/820 [01:10<02:55,  3.55it/s] 24%|       | 199/820 [01:11<02:54,  3.55it/s] 24%|       | 200/820 [01:11<03:00,  3.44it/s] 25%|       | 201/820 [01:11<02:58,  3.48it/s] 25%|       | 202/820 [01:12<02:56,  3.50it/s] 25%|       | 203/820 [01:12<02:55,  3.52it/s] 25%|       | 204/820 [01:12<02:54,  3.53it/s] 25%|       | 205/820 [01:12<02:53,  3.54it/s] 25%|       | 206/820 [01:13<02:53,  3.54it/s] 25%|       | 207/820 [01:13<02:52,  3.55it/s] 25%|       | 208/820 [01:13<02:52,  3.55it/s] 25%|       | 209/820 [01:14<02:51,  3.55it/s] 26%|       | 210/820 [01:14<02:51,  3.56it/s] 26%|       | 211/820 [01:14<02:57,  3.43it/s] 26%|       | 212/820 [01:14<02:55,  3.47it/s] 26%|       | 213/820 [01:15<02:53,  3.50it/s] 26%|       | 214/820 [01:15<02:52,  3.51it/s] 26%|       | 215/820 [01:15<02:51,  3.53it/s] 26%|       | 216/820 [01:16<02:50,  3.54it/s] 26%|       | 217/820 [01:16<02:49,  3.55it/s] 27%|       | 218/820 [01:16<02:49,  3.55it/s] 27%|       | 219/820 [01:16<02:49,  3.56it/s] 27%|       | 220/820 [01:17<02:48,  3.56it/s] 27%|       | 221/820 [01:17<02:48,  3.56it/s] 27%|       | 222/820 [01:17<02:55,  3.41it/s] 27%|       | 223/820 [01:18<02:52,  3.46it/s] 27%|       | 224/820 [01:18<02:50,  3.49it/s] 27%|       | 225/820 [01:18<02:49,  3.51it/s] 28%|       | 226/820 [01:18<02:48,  3.53it/s] 28%|       | 227/820 [01:19<02:47,  3.54it/s] 28%|       | 228/820 [01:19<02:46,  3.57it/s] 28%|       | 229/820 [01:19<02:45,  3.58it/s] 28%|       | 230/820 [01:20<02:44,  3.59it/s] 28%|       | 231/820 [01:20<02:43,  3.60it/s] 28%|       | 232/820 [01:20<02:43,  3.60it/s] 28%|       | 233/820 [01:20<02:49,  3.46it/s] 29%|       | 234/820 [01:21<02:46,  3.51it/s] 29%|       | 235/820 [01:21<02:45,  3.54it/s] 29%|       | 236/820 [01:21<02:43,  3.56it/s] 29%|       | 237/820 [01:22<02:42,  3.58it/s] 29%|       | 238/820 [01:22<02:42,  3.59it/s] 29%|       | 239/820 [01:22<02:41,  3.60it/s] 29%|       | 240/820 [01:22<02:40,  3.61it/s] 29%|       | 241/820 [01:23<02:40,  3.60it/s] 30%|       | 242/820 [01:23<02:40,  3.61it/s] 30%|       | 243/820 [01:23<02:39,  3.61it/s] 30%|       | 244/820 [01:24<02:53,  3.33it/s] 30%|       | 245/820 [01:24<02:48,  3.41it/s] 30%|       | 246/820 [01:24<02:45,  3.47it/s] 30%|       | 247/820 [01:24<02:43,  3.51it/s] 30%|       | 248/820 [01:25<02:41,  3.54it/s] 30%|       | 249/820 [01:25<02:40,  3.56it/s] 30%|       | 250/820 [01:25<02:39,  3.58it/s] 31%|       | 251/820 [01:25<02:38,  3.59it/s] 31%|       | 252/820 [01:26<02:37,  3.60it/s] 31%|       | 253/820 [01:26<02:37,  3.60it/s] 31%|       | 254/820 [01:26<02:36,  3.61it/s] 31%|       | 255/820 [01:27<02:43,  3.46it/s] 31%|       | 256/820 [01:27<02:41,  3.50it/s] 31%|      | 257/820 [01:27<02:39,  3.54it/s] 31%|      | 258/820 [01:27<02:37,  3.56it/s] 32%|      | 259/820 [01:28<02:36,  3.58it/s] 32%|      | 260/820 [01:28<02:36,  3.59it/s] 32%|      | 261/820 [01:28<02:35,  3.59it/s] 32%|      | 262/820 [01:29<02:34,  3.60it/s] 32%|      | 263/820 [01:29<02:34,  3.61it/s] 32%|      | 264/820 [01:29<02:34,  3.61it/s] 32%|      | 265/820 [01:29<02:33,  3.61it/s] 32%|      | 266/820 [01:30<02:41,  3.42it/s] 33%|      | 267/820 [01:30<02:38,  3.48it/s] 33%|      | 268/820 [01:30<02:36,  3.52it/s] 33%|      | 269/820 [01:31<02:35,  3.55it/s] 33%|      | 270/820 [01:31<02:34,  3.57it/s] 33%|      | 271/820 [01:31<02:33,  3.58it/s] 33%|      | 272/820 [01:31<02:32,  3.59it/s] 33%|      | 273/820 [01:32<02:31,  3.60it/s] 33%|      | 274/820 [01:32<02:31,  3.61it/s] 34%|      | 275/820 [01:32<02:30,  3.61it/s] 34%|      | 276/820 [01:32<02:30,  3.61it/s] 34%|      | 277/820 [01:33<02:30,  3.62it/s] 34%|      | 278/820 [01:33<02:29,  3.62it/s] 34%|      | 279/820 [01:33<02:29,  3.62it/s] 34%|      | 280/820 [01:34<02:29,  3.62it/s] 34%|      | 281/820 [01:34<02:29,  3.61it/s] 34%|      | 282/820 [01:34<02:28,  3.61it/s] 35%|      | 283/820 [01:34<02:28,  3.61it/s] 35%|      | 284/820 [01:35<02:28,  3.62it/s] 35%|      | 285/820 [01:35<02:27,  3.62it/s] 35%|      | 286/820 [01:35<02:31,  3.53it/s] 35%|      | 287/820 [01:36<02:29,  3.55it/s] 35%|      | 288/820 [01:36<02:29,  3.57it/s] 35%|      | 289/820 [01:36<02:28,  3.58it/s] 35%|      | 290/820 [01:36<02:27,  3.59it/s] 35%|      | 291/820 [01:37<02:26,  3.60it/s] 36%|      | 292/820 [01:37<02:26,  3.60it/s] 36%|      | 293/820 [01:37<02:26,  3.61it/s] 36%|      | 294/820 [01:37<02:25,  3.61it/s] 36%|      | 295/820 [01:38<02:25,  3.61it/s] 36%|      | 296/820 [01:38<02:25,  3.61it/s] 36%|      | 297/820 [01:38<02:31,  3.46it/s] 36%|      | 298/820 [01:39<02:28,  3.51it/s] 36%|      | 299/820 [01:39<02:27,  3.54it/s] 37%|      | 300/820 [01:39<02:26,  3.56it/s] 37%|      | 301/820 [01:39<02:25,  3.58it/s] 37%|      | 302/820 [01:40<02:24,  3.59it/s] 37%|      | 303/820 [01:40<02:23,  3.60it/s] 37%|      | 304/820 [01:40<02:23,  3.60it/s] 37%|      | 305/820 [01:41<02:22,  3.61it/s] 37%|      | 306/820 [01:41<02:22,  3.61it/s] 37%|      | 307/820 [01:41<02:22,  3.61it/s] 38%|      | 308/820 [01:41<02:30,  3.39it/s] 38%|      | 309/820 [01:42<02:27,  3.46it/s] 38%|      | 310/820 [01:42<02:25,  3.50it/s] 38%|      | 311/820 [01:42<02:23,  3.54it/s] 38%|      | 312/820 [01:43<02:22,  3.56it/s] 38%|      | 313/820 [01:43<02:26,  3.47it/s] 38%|      | 314/820 [01:43<02:24,  3.50it/s] 38%|      | 315/820 [01:43<02:22,  3.53it/s] 39%|      | 316/820 [01:44<02:21,  3.56it/s] 39%|      | 317/820 [01:44<02:20,  3.57it/s] 39%|      | 318/820 [01:44<02:20,  3.58it/s] 39%|      | 319/820 [01:45<02:23,  3.49it/s] 39%|      | 320/820 [01:45<02:21,  3.52it/s] 39%|      | 321/820 [01:45<02:36,  3.18it/s] 39%|      | 322/820 [01:46<02:35,  3.19it/s] 39%|      | 323/820 [01:46<02:30,  3.30it/s] 40%|      | 324/820 [01:46<02:26,  3.39it/s] 40%|      | 325/820 [01:46<02:23,  3.46it/s] 40%|      | 326/820 [01:47<02:21,  3.50it/s] 40%|      | 327/820 [01:47<02:19,  3.54it/s] 40%|      | 328/820 [01:47<02:08,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 14:00:08,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:00:08,140 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:00:08,140 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7868, 'eval_samples_per_second': 355.784, 'eval_steps_per_second': 44.55, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.79it/s][A
  3%|         | 12/436 [00:00<00:08, 49.37it/s][A
  4%|         | 17/436 [00:00<00:09, 44.27it/s][A
  5%|         | 22/436 [00:00<00:09, 44.26it/s][A
  6%|         | 27/436 [00:00<00:09, 44.26it/s][A
  7%|         | 32/436 [00:00<00:09, 44.34it/s][A
  8%|         | 37/436 [00:00<00:08, 44.36it/s][A
 10%|         | 42/436 [00:00<00:08, 44.35it/s][A
 11%|         | 47/436 [00:01<00:08, 44.49it/s][A
 12%|        | 52/436 [00:01<00:08, 44.53it/s][A
 13%|        | 57/436 [00:01<00:08, 44.77it/s][A
 14%|        | 62/436 [00:01<00:08, 44.96it/s][A
 15%|        | 67/436 [00:01<00:08, 44.99it/s][A
 17%|        | 72/436 [00:01<00:08, 44.85it/s][A
 18%|        | 77/436 [00:01<00:08, 44.81it/s][A
 19%|        | 82/436 [00:01<00:07, 44.56it/s][A
 20%|        | 87/436 [00:01<00:07, 44.55it/s][A
 21%|        | 92/436 [00:02<00:07, 44.62it/s][A
 22%|       | 97/436 [00:02<00:07, 44.74it/s][A
 23%|       | 102/436 [00:02<00:07, 44.87it/s][A
 25%|       | 107/436 [00:02<00:07, 44.98it/s][A
 26%|       | 112/436 [00:02<00:07, 44.96it/s][A
 27%|       | 117/436 [00:02<00:07, 44.81it/s][A
 28%|       | 122/436 [00:02<00:07, 44.76it/s][A
 29%|       | 127/436 [00:02<00:06, 44.66it/s][A
 30%|       | 132/436 [00:02<00:06, 44.60it/s][A
 31%|      | 137/436 [00:03<00:06, 44.65it/s][A
 33%|      | 142/436 [00:03<00:06, 44.73it/s][A
 34%|      | 147/436 [00:03<00:06, 44.88it/s][A
 35%|      | 152/436 [00:03<00:06, 43.43it/s][A
 36%|      | 157/436 [00:03<00:06, 43.92it/s][A
 37%|      | 162/436 [00:03<00:06, 44.21it/s][A
 38%|      | 167/436 [00:03<00:06, 44.33it/s][A
 39%|      | 172/436 [00:03<00:05, 44.29it/s][A
 41%|      | 177/436 [00:03<00:05, 44.38it/s][A
 42%|     | 182/436 [00:04<00:05, 44.51it/s][A
 43%|     | 187/436 [00:04<00:05, 44.72it/s][A
 44%|     | 192/436 [00:04<00:05, 44.76it/s][A
 45%|     | 197/436 [00:04<00:05, 44.67it/s][A
 46%|     | 202/436 [00:04<00:05, 44.88it/s][A
 47%|     | 207/436 [00:04<00:05, 44.88it/s][A
 49%|     | 212/436 [00:04<00:05, 44.73it/s][A
 50%|     | 217/436 [00:04<00:04, 44.59it/s][A
 51%|     | 222/436 [00:04<00:04, 44.61it/s][A
 52%|    | 227/436 [00:05<00:04, 44.76it/s][A
 53%|    | 232/436 [00:05<00:04, 44.80it/s][A
 54%|    | 237/436 [00:05<00:04, 44.81it/s][A
 56%|    | 242/436 [00:05<00:04, 41.67it/s][A
 57%|    | 247/436 [00:05<00:04, 42.73it/s][A
 58%|    | 252/436 [00:05<00:04, 43.44it/s][A
 59%|    | 257/436 [00:05<00:04, 43.80it/s][A
 60%|    | 262/436 [00:05<00:03, 43.83it/s][A
 61%|    | 267/436 [00:05<00:03, 43.94it/s][A
 62%|   | 272/436 [00:06<00:03, 44.29it/s][A
 64%|   | 277/436 [00:06<00:03, 44.42it/s][A
 65%|   | 282/436 [00:06<00:03, 44.28it/s][A
 66%|   | 287/436 [00:06<00:03, 44.43it/s][A
 67%|   | 292/436 [00:06<00:03, 44.59it/s][A
 68%|   | 297/436 [00:06<00:03, 44.78it/s][A
 69%|   | 302/436 [00:06<00:02, 44.74it/s][A
 70%|   | 307/436 [00:06<00:02, 44.81it/s][A
 72%|  | 312/436 [00:07<00:02, 44.75it/s][A
 73%|  | 317/436 [00:07<00:02, 44.74it/s][A
 74%|  | 322/436 [00:07<00:02, 44.63it/s][A
 75%|  | 327/436 [00:07<00:02, 44.58it/s][A
 76%|  | 332/436 [00:07<00:02, 44.64it/s][A
 77%|  | 337/436 [00:07<00:02, 44.74it/s][A
 78%|  | 342/436 [00:07<00:02, 44.79it/s][A
 80%|  | 347/436 [00:07<00:01, 44.85it/s][A
 81%|  | 352/436 [00:07<00:01, 44.87it/s][A
 82%| | 357/436 [00:08<00:01, 44.88it/s][A
 83%| | 362/436 [00:08<00:01, 44.82it/s][A
 84%| | 367/436 [00:08<00:01, 44.66it/s][A
 85%| | 372/436 [00:08<00:01, 44.61it/s][A
 86%| | 377/436 [00:08<00:01, 41.83it/s][A
 88%| | 382/436 [00:08<00:01, 42.80it/s][A
 89%| | 387/436 [00:08<00:01, 43.56it/s][A
 90%| | 392/436 [00:08<00:00, 44.00it/s][A
 91%| | 397/436 [00:08<00:00, 44.31it/s][A
 92%|| 402/436 [00:09<00:00, 44.52it/s][A
 93%|| 407/436 [00:09<00:00, 44.53it/s][A
 94%|| 412/436 [00:09<00:00, 44.54it/s][A
 96%|| 417/436 [00:09<00:00, 44.30it/s][A
 97%|| 422/436 [00:09<00:00, 44.36it/s][A
 98%|| 427/436 [00:09<00:00, 44.47it/s][A
 99%|| 432/436 [00:09<00:00, 44.74it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.74it/s][A 40%|      | 328/820 [01:57<02:08,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:00:18,100 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 14:00:18,285 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:00:20,951 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:00:21,104 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:00:21,159 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-328/special_tokens_map.json
 40%|      | 329/820 [02:01<36:43,  4.49s/it] 40%|      | 330/820 [02:02<26:25,  3.24s/it] 40%|      | 331/820 [02:02<19:09,  2.35s/it] 40%|      | 332/820 [02:02<14:04,  1.73s/it] 41%|      | 333/820 [02:03<10:30,  1.29s/it] 41%|      | 334/820 [02:03<08:01,  1.01it/s] 41%|      | 335/820 [02:03<06:17,  1.29it/s] 41%|      | 336/820 [02:03<05:04,  1.59it/s] 41%|      | 337/820 [02:04<04:13,  1.91it/s] 41%|      | 338/820 [02:04<03:37,  2.21it/s] 41%|     | 339/820 [02:04<03:12,  2.50it/s] 41%|     | 340/820 [02:05<02:54,  2.74it/s] 42%|     | 341/820 [02:05<02:45,  2.89it/s] 42%|     | 342/820 [02:05<02:36,  3.06it/s] 42%|     | 343/820 [02:05<02:29,  3.19it/s] 42%|     | 344/820 [02:06<02:24,  3.29it/s] 42%|     | 345/820 [02:06<02:21,  3.37it/s] 42%|     | 346/820 [02:06<02:18,  3.42it/s] 42%|     | 347/820 [02:07<02:16,  3.46it/s] 42%|     | 348/820 [02:07<02:15,  3.49it/s] 43%|     | 349/820 [02:07<02:14,  3.51it/s] 43%|     | 350/820 [02:07<02:13,  3.52it/s] 43%|     | 351/820 [02:08<02:12,  3.54it/s] 43%|     | 352/820 [02:08<02:13,  3.50it/s] 43%|     | 353/820 [02:08<02:12,  3.52it/s] 43%|     | 354/820 [02:09<02:12,  3.53it/s] 43%|     | 355/820 [02:09<02:11,  3.53it/s] 43%|     | 356/820 [02:09<02:11,  3.54it/s] 44%|     | 357/820 [02:09<02:10,  3.55it/s] 44%|     | 358/820 [02:10<02:10,  3.55it/s] 44%|     | 359/820 [02:10<02:09,  3.55it/s] 44%|     | 360/820 [02:10<02:09,  3.56it/s] 44%|     | 361/820 [02:11<02:09,  3.56it/s] 44%|     | 362/820 [02:11<02:08,  3.56it/s] 44%|     | 363/820 [02:11<02:13,  3.43it/s] 44%|     | 364/820 [02:11<02:11,  3.47it/s] 45%|     | 365/820 [02:12<02:10,  3.50it/s] 45%|     | 366/820 [02:12<02:09,  3.52it/s] 45%|     | 367/820 [02:12<02:08,  3.53it/s] 45%|     | 368/820 [02:13<02:07,  3.54it/s] 45%|     | 369/820 [02:13<02:07,  3.55it/s] 45%|     | 370/820 [02:13<02:06,  3.55it/s] 45%|     | 371/820 [02:13<02:06,  3.56it/s] 45%|     | 372/820 [02:14<02:05,  3.56it/s] 45%|     | 373/820 [02:14<02:05,  3.56it/s] 46%|     | 374/820 [02:14<02:12,  3.37it/s] 46%|     | 375/820 [02:15<02:09,  3.42it/s] 46%|     | 376/820 [02:15<02:08,  3.46it/s] 46%|     | 377/820 [02:15<02:06,  3.49it/s] 46%|     | 378/820 [02:15<02:05,  3.52it/s] 46%|     | 379/820 [02:16<02:04,  3.53it/s] 46%|     | 380/820 [02:16<02:04,  3.54it/s] 46%|     | 381/820 [02:16<02:03,  3.54it/s] 47%|     | 382/820 [02:17<02:03,  3.55it/s] 47%|     | 383/820 [02:17<02:03,  3.55it/s] 47%|     | 384/820 [02:17<02:02,  3.55it/s] 47%|     | 385/820 [02:17<02:08,  3.39it/s] 47%|     | 386/820 [02:18<02:06,  3.44it/s] 47%|     | 387/820 [02:18<02:04,  3.48it/s] 47%|     | 388/820 [02:18<02:03,  3.50it/s] 47%|     | 389/820 [02:19<02:02,  3.52it/s] 48%|     | 390/820 [02:19<02:01,  3.53it/s] 48%|     | 391/820 [02:19<02:01,  3.54it/s] 48%|     | 392/820 [02:19<02:00,  3.55it/s] 48%|     | 393/820 [02:20<02:00,  3.55it/s] 48%|     | 394/820 [02:20<01:59,  3.55it/s] 48%|     | 395/820 [02:20<01:59,  3.55it/s] 48%|     | 396/820 [02:21<02:02,  3.46it/s] 48%|     | 397/820 [02:21<02:01,  3.49it/s] 49%|     | 398/820 [02:21<02:00,  3.51it/s] 49%|     | 399/820 [02:21<01:59,  3.52it/s] 49%|     | 400/820 [02:22<01:58,  3.53it/s] 49%|     | 401/820 [02:22<01:58,  3.54it/s] 49%|     | 402/820 [02:22<01:57,  3.55it/s] 49%|     | 403/820 [02:23<01:57,  3.55it/s] 49%|     | 404/820 [02:23<01:56,  3.56it/s] 49%|     | 405/820 [02:23<01:56,  3.55it/s] 50%|     | 406/820 [02:23<01:56,  3.55it/s] 50%|     | 407/820 [02:24<01:59,  3.45it/s] 50%|     | 408/820 [02:24<01:58,  3.48it/s] 50%|     | 409/820 [02:24<01:57,  3.51it/s] 50%|     | 410/820 [02:25<01:56,  3.52it/s] 50%|     | 411/820 [02:25<01:55,  3.54it/s] 50%|     | 412/820 [02:25<01:55,  3.54it/s] 50%|     | 413/820 [02:25<01:54,  3.55it/s] 50%|     | 414/820 [02:26<01:54,  3.55it/s] 51%|     | 415/820 [02:26<01:53,  3.55it/s] 51%|     | 416/820 [02:26<01:53,  3.55it/s] 51%|     | 417/820 [02:26<01:53,  3.55it/s] 51%|     | 418/820 [02:27<01:58,  3.38it/s] 51%|     | 419/820 [02:27<01:56,  3.43it/s] 51%|     | 420/820 [02:27<01:55,  3.47it/s] 51%|    | 421/820 [02:28<01:54,  3.49it/s] 51%|    | 422/820 [02:28<01:53,  3.51it/s] 52%|    | 423/820 [02:28<01:52,  3.53it/s] 52%|    | 424/820 [02:28<01:51,  3.54it/s] 52%|    | 425/820 [02:29<01:51,  3.54it/s] 52%|    | 426/820 [02:29<01:51,  3.54it/s] 52%|    | 427/820 [02:29<01:50,  3.55it/s] 52%|    | 428/820 [02:30<01:50,  3.55it/s] 52%|    | 429/820 [02:30<01:50,  3.55it/s] 52%|    | 430/820 [02:30<01:49,  3.55it/s] 53%|    | 431/820 [02:30<01:49,  3.55it/s] 53%|    | 432/820 [02:31<01:49,  3.55it/s] 53%|    | 433/820 [02:31<01:49,  3.55it/s] 53%|    | 434/820 [02:31<01:48,  3.55it/s] 53%|    | 435/820 [02:32<01:55,  3.32it/s] 53%|    | 436/820 [02:32<01:53,  3.39it/s] 53%|    | 437/820 [02:32<01:51,  3.44it/s] 53%|    | 438/820 [02:32<01:50,  3.47it/s] 54%|    | 439/820 [02:33<01:49,  3.49it/s] 54%|    | 440/820 [02:33<01:48,  3.51it/s] 54%|    | 441/820 [02:33<01:47,  3.52it/s] 54%|    | 442/820 [02:34<01:46,  3.55it/s] 54%|    | 443/820 [02:34<01:45,  3.57it/s] 54%|    | 444/820 [02:34<01:45,  3.58it/s] 54%|    | 445/820 [02:34<01:44,  3.59it/s] 54%|    | 446/820 [02:35<01:44,  3.60it/s] 55%|    | 447/820 [02:35<01:43,  3.60it/s] 55%|    | 448/820 [02:35<01:43,  3.60it/s] 55%|    | 449/820 [02:36<01:42,  3.61it/s] 55%|    | 450/820 [02:36<01:42,  3.61it/s] 55%|    | 451/820 [02:36<01:42,  3.61it/s] 55%|    | 452/820 [02:36<01:41,  3.61it/s] 55%|    | 453/820 [02:37<01:41,  3.61it/s] 55%|    | 454/820 [02:37<01:45,  3.46it/s] 55%|    | 455/820 [02:37<01:44,  3.51it/s] 56%|    | 456/820 [02:38<01:42,  3.53it/s] 56%|    | 457/820 [02:38<01:42,  3.56it/s] 56%|    | 458/820 [02:38<01:41,  3.57it/s] 56%|    | 459/820 [02:38<01:40,  3.58it/s] 56%|    | 460/820 [02:39<01:40,  3.59it/s] 56%|    | 461/820 [02:39<01:39,  3.60it/s] 56%|    | 462/820 [02:39<01:39,  3.60it/s] 56%|    | 463/820 [02:39<01:39,  3.60it/s] 57%|    | 464/820 [02:40<01:38,  3.60it/s] 57%|    | 465/820 [02:40<01:44,  3.41it/s] 57%|    | 466/820 [02:40<01:42,  3.47it/s] 57%|    | 467/820 [02:41<01:40,  3.51it/s] 57%|    | 468/820 [02:41<01:39,  3.54it/s] 57%|    | 469/820 [02:41<01:38,  3.56it/s] 57%|    | 470/820 [02:41<01:38,  3.57it/s] 57%|    | 471/820 [02:42<01:37,  3.58it/s] 58%|    | 472/820 [02:42<01:36,  3.59it/s] 58%|    | 473/820 [02:42<01:36,  3.60it/s] 58%|    | 474/820 [02:43<01:36,  3.60it/s] 58%|    | 475/820 [02:43<01:39,  3.47it/s] 58%|    | 476/820 [02:43<01:40,  3.41it/s] 58%|    | 477/820 [02:43<01:38,  3.47it/s] 58%|    | 478/820 [02:44<01:37,  3.50it/s] 58%|    | 479/820 [02:44<01:36,  3.53it/s] 59%|    | 480/820 [02:44<01:35,  3.56it/s] 59%|    | 481/820 [02:45<01:34,  3.57it/s] 59%|    | 482/820 [02:45<01:34,  3.58it/s] 59%|    | 483/820 [02:45<01:58,  2.84it/s] 59%|    | 484/820 [02:46<01:54,  2.95it/s] 59%|    | 485/820 [02:46<01:47,  3.11it/s] 59%|    | 486/820 [02:46<01:42,  3.25it/s] 59%|    | 487/820 [02:47<01:39,  3.35it/s] 60%|    | 488/820 [02:47<01:36,  3.43it/s] 60%|    | 489/820 [02:47<01:38,  3.37it/s] 60%|    | 490/820 [02:47<01:35,  3.44it/s] 60%|    | 491/820 [02:48<01:34,  3.49it/s] 60%|    | 492/820 [02:48<01:26,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 14:01:08,873 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:01:08,873 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:01:08,873 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8089, 'eval_samples_per_second': 354.984, 'eval_steps_per_second': 44.449, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.42it/s][A
  3%|         | 12/436 [00:00<00:08, 48.96it/s][A
  4%|         | 17/436 [00:00<00:08, 47.04it/s][A
  5%|         | 22/436 [00:00<00:09, 45.89it/s][A
  6%|         | 27/436 [00:00<00:09, 45.30it/s][A
  7%|         | 32/436 [00:00<00:08, 44.94it/s][A
  8%|         | 37/436 [00:00<00:08, 44.81it/s][A
 10%|         | 42/436 [00:00<00:08, 44.81it/s][A
 11%|         | 47/436 [00:01<00:09, 39.16it/s][A
 12%|        | 52/436 [00:01<00:09, 40.83it/s][A
 13%|        | 57/436 [00:01<00:09, 42.05it/s][A
 14%|        | 62/436 [00:01<00:08, 43.00it/s][A
 15%|        | 67/436 [00:01<00:08, 43.72it/s][A
 17%|        | 72/436 [00:01<00:08, 44.20it/s][A
 18%|        | 77/436 [00:01<00:08, 44.40it/s][A
 19%|        | 82/436 [00:01<00:08, 44.24it/s][A
 20%|        | 87/436 [00:01<00:07, 44.05it/s][A
 21%|        | 92/436 [00:02<00:07, 43.95it/s][A
 22%|       | 97/436 [00:02<00:07, 44.13it/s][A
 23%|       | 102/436 [00:02<00:07, 44.38it/s][A
 25%|       | 107/436 [00:02<00:07, 44.66it/s][A
 26%|       | 112/436 [00:02<00:07, 44.88it/s][A
 27%|       | 117/436 [00:02<00:07, 45.06it/s][A
 28%|       | 122/436 [00:02<00:06, 45.17it/s][A
 29%|       | 127/436 [00:02<00:06, 44.95it/s][A
 30%|       | 132/436 [00:02<00:06, 44.59it/s][A
 31%|      | 137/436 [00:03<00:06, 44.34it/s][A
 33%|      | 142/436 [00:03<00:06, 44.41it/s][A
 34%|      | 147/436 [00:03<00:06, 44.59it/s][A
 35%|      | 152/436 [00:03<00:06, 44.82it/s][A
 36%|      | 157/436 [00:03<00:06, 44.96it/s][A
 37%|      | 162/436 [00:03<00:06, 45.05it/s][A
 38%|      | 167/436 [00:03<00:05, 45.01it/s][A
 39%|      | 172/436 [00:03<00:05, 44.86it/s][A
 41%|      | 177/436 [00:04<00:05, 44.60it/s][A
 42%|     | 182/436 [00:04<00:06, 41.80it/s][A
 43%|     | 187/436 [00:04<00:05, 42.67it/s][A
 44%|     | 192/436 [00:04<00:05, 43.47it/s][A
 45%|     | 197/436 [00:04<00:05, 43.98it/s][A
 46%|     | 202/436 [00:04<00:05, 44.38it/s][A
 47%|     | 207/436 [00:04<00:05, 44.63it/s][A
 49%|     | 212/436 [00:04<00:05, 44.58it/s][A
 50%|     | 217/436 [00:04<00:04, 44.51it/s][A
 51%|     | 222/436 [00:05<00:04, 44.31it/s][A
 52%|    | 227/436 [00:05<00:04, 44.25it/s][A
 53%|    | 232/436 [00:05<00:04, 44.46it/s][A
 54%|    | 237/436 [00:05<00:04, 44.70it/s][A
 56%|    | 242/436 [00:05<00:04, 44.86it/s][A
 57%|    | 247/436 [00:05<00:04, 45.05it/s][A
 58%|    | 252/436 [00:05<00:04, 45.12it/s][A
 59%|    | 257/436 [00:05<00:03, 44.92it/s][A
 60%|    | 262/436 [00:05<00:03, 44.73it/s][A
 61%|    | 267/436 [00:06<00:03, 44.44it/s][A
 62%|   | 272/436 [00:06<00:04, 39.77it/s][A
 64%|   | 277/436 [00:06<00:03, 41.30it/s][A
 65%|   | 282/436 [00:06<00:03, 42.46it/s][A
 66%|   | 287/436 [00:06<00:03, 43.24it/s][A
 67%|   | 292/436 [00:06<00:03, 43.86it/s][A
 68%|   | 297/436 [00:06<00:03, 44.28it/s][A
 69%|   | 302/436 [00:06<00:03, 44.61it/s][A
 70%|   | 307/436 [00:06<00:02, 44.71it/s][A
 72%|  | 312/436 [00:07<00:02, 44.41it/s][A
 73%|  | 317/436 [00:07<00:02, 44.11it/s][A
 74%|  | 322/436 [00:07<00:02, 44.19it/s][A
 75%|  | 327/436 [00:07<00:02, 44.47it/s][A
 76%|  | 332/436 [00:07<00:02, 44.67it/s][A
 77%|  | 337/436 [00:07<00:02, 44.88it/s][A
 78%|  | 342/436 [00:07<00:02, 44.89it/s][A
 80%|  | 347/436 [00:07<00:01, 45.03it/s][A
 81%|  | 352/436 [00:07<00:01, 44.97it/s][A
 82%| | 357/436 [00:08<00:01, 44.65it/s][A
 83%| | 362/436 [00:08<00:01, 44.51it/s][A
 84%| | 367/436 [00:08<00:01, 44.44it/s][A
 85%| | 372/436 [00:08<00:01, 44.47it/s][A
 86%| | 377/436 [00:08<00:01, 44.73it/s][A
 88%| | 382/436 [00:08<00:01, 44.88it/s][A
 89%| | 387/436 [00:08<00:01, 45.05it/s][A
 90%| | 392/436 [00:08<00:00, 45.07it/s][A
 91%| | 397/436 [00:08<00:00, 44.94it/s][A
 92%|| 402/436 [00:09<00:00, 44.79it/s][A
 93%|| 407/436 [00:09<00:00, 42.70it/s][A
 94%|| 412/436 [00:09<00:00, 43.35it/s][A
 96%|| 417/436 [00:09<00:00, 43.59it/s][A
 97%|| 422/436 [00:09<00:00, 44.02it/s][A
 98%|| 427/436 [00:09<00:00, 44.35it/s][A
 99%|| 432/436 [00:09<00:00, 44.56it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.56it/s][A 60%|    | 492/820 [02:58<01:26,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:01:18,916 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 14:01:19,132 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:01:22,646 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:01:22,897 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:01:22,989 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-492/special_tokens_map.json
 60%|    | 493/820 [03:04<26:35,  4.88s/it] 60%|    | 494/820 [03:04<19:00,  3.50s/it] 60%|    | 495/820 [03:04<13:43,  2.53s/it] 60%|    | 496/820 [03:04<10:02,  1.86s/it] 61%|    | 497/820 [03:05<07:27,  1.39s/it] 61%|    | 498/820 [03:05<05:39,  1.05s/it] 61%|    | 499/820 [03:05<04:25,  1.21it/s] 61%|    | 500/820 [03:06<03:32,  1.51it/s]                                                  61%|    | 500/820 [03:06<03:32,  1.51it/s] 61%|    | 501/820 [03:06<02:55,  1.82it/s] 61%|    | 502/820 [03:06<02:29,  2.13it/s] 61%|   | 503/820 [03:06<02:10,  2.43it/s] 61%|   | 504/820 [03:07<01:57,  2.68it/s] 62%|   | 505/820 [03:07<01:48,  2.90it/s] 62%|   | 506/820 [03:07<01:42,  3.07it/s] 62%|   | 507/820 [03:07<01:37,  3.20it/s] 62%|   | 508/820 [03:08<01:34,  3.30it/s] 62%|   | 509/820 [03:08<01:31,  3.38it/s] 62%|   | 510/820 [03:08<01:35,  3.25it/s] 62%|   | 511/820 [03:09<01:32,  3.35it/s] 62%|   | 512/820 [03:09<01:29,  3.43it/s] 63%|   | 513/820 [03:09<01:28,  3.48it/s] 63%|   | 514/820 [03:09<01:26,  3.52it/s] 63%|   | 515/820 [03:10<01:25,  3.55it/s] 63%|   | 516/820 [03:10<01:25,  3.57it/s] 63%|   | 517/820 [03:10<01:24,  3.59it/s] 63%|   | 518/820 [03:11<01:24,  3.60it/s] 63%|   | 519/820 [03:11<01:23,  3.60it/s] 63%|   | 520/820 [03:11<01:23,  3.61it/s] 64%|   | 521/820 [03:11<01:25,  3.50it/s] 64%|   | 522/820 [03:12<01:24,  3.54it/s] 64%|   | 523/820 [03:12<01:23,  3.56it/s] 64%|   | 524/820 [03:12<01:22,  3.58it/s] 64%|   | 525/820 [03:13<01:22,  3.59it/s] 64%|   | 526/820 [03:13<01:21,  3.60it/s] 64%|   | 527/820 [03:13<01:21,  3.60it/s] 64%|   | 528/820 [03:13<01:20,  3.61it/s] 65%|   | 529/820 [03:14<01:20,  3.61it/s] 65%|   | 530/820 [03:14<01:20,  3.61it/s] 65%|   | 531/820 [03:14<01:20,  3.61it/s] 65%|   | 532/820 [03:15<01:22,  3.48it/s] 65%|   | 533/820 [03:15<01:21,  3.52it/s] 65%|   | 534/820 [03:15<01:20,  3.55it/s] 65%|   | 535/820 [03:15<01:19,  3.57it/s] 65%|   | 536/820 [03:16<01:19,  3.58it/s] 65%|   | 537/820 [03:16<01:18,  3.59it/s] 66%|   | 538/820 [03:16<01:18,  3.60it/s] 66%|   | 539/820 [03:16<01:17,  3.61it/s] 66%|   | 540/820 [03:17<01:17,  3.61it/s] 66%|   | 541/820 [03:17<01:17,  3.61it/s] 66%|   | 542/820 [03:17<01:16,  3.61it/s] 66%|   | 543/820 [03:18<01:19,  3.50it/s] 66%|   | 544/820 [03:18<01:18,  3.53it/s] 66%|   | 545/820 [03:18<01:17,  3.56it/s] 67%|   | 546/820 [03:18<01:16,  3.57it/s] 67%|   | 547/820 [03:19<01:16,  3.59it/s] 67%|   | 548/820 [03:19<01:15,  3.60it/s] 67%|   | 549/820 [03:19<01:15,  3.60it/s] 67%|   | 550/820 [03:20<01:14,  3.60it/s] 67%|   | 551/820 [03:20<01:14,  3.60it/s] 67%|   | 552/820 [03:20<01:14,  3.61it/s] 67%|   | 553/820 [03:20<01:13,  3.61it/s] 68%|   | 554/820 [03:21<01:15,  3.54it/s] 68%|   | 555/820 [03:21<01:14,  3.56it/s] 68%|   | 556/820 [03:21<01:13,  3.57it/s] 68%|   | 557/820 [03:21<01:13,  3.58it/s] 68%|   | 558/820 [03:22<01:12,  3.59it/s] 68%|   | 559/820 [03:22<01:12,  3.60it/s] 68%|   | 560/820 [03:22<01:12,  3.61it/s] 68%|   | 561/820 [03:23<01:11,  3.61it/s] 69%|   | 562/820 [03:23<01:11,  3.61it/s] 69%|   | 563/820 [03:23<01:11,  3.61it/s] 69%|   | 564/820 [03:23<01:10,  3.61it/s] 69%|   | 565/820 [03:24<01:12,  3.51it/s] 69%|   | 566/820 [03:24<01:11,  3.54it/s] 69%|   | 567/820 [03:24<01:11,  3.56it/s] 69%|   | 568/820 [03:25<01:10,  3.58it/s] 69%|   | 569/820 [03:25<01:09,  3.59it/s] 70%|   | 570/820 [03:25<01:09,  3.60it/s] 70%|   | 571/820 [03:25<01:09,  3.60it/s] 70%|   | 572/820 [03:26<01:08,  3.61it/s] 70%|   | 573/820 [03:26<01:08,  3.61it/s] 70%|   | 574/820 [03:26<01:08,  3.62it/s] 70%|   | 575/820 [03:26<01:07,  3.62it/s] 70%|   | 576/820 [03:27<01:07,  3.62it/s] 70%|   | 577/820 [03:27<01:07,  3.62it/s] 70%|   | 578/820 [03:27<01:06,  3.62it/s] 71%|   | 579/820 [03:28<01:06,  3.62it/s] 71%|   | 580/820 [03:28<01:06,  3.61it/s] 71%|   | 581/820 [03:28<01:08,  3.47it/s] 71%|   | 582/820 [03:28<01:07,  3.51it/s] 71%|   | 583/820 [03:29<01:06,  3.54it/s] 71%|   | 584/820 [03:29<01:06,  3.56it/s] 71%|  | 585/820 [03:29<01:05,  3.58it/s] 71%|  | 586/820 [03:30<01:05,  3.59it/s] 72%|  | 587/820 [03:30<01:04,  3.60it/s] 72%|  | 588/820 [03:30<01:04,  3.60it/s] 72%|  | 589/820 [03:30<01:04,  3.60it/s] 72%|  | 590/820 [03:31<01:03,  3.60it/s] 72%|  | 591/820 [03:31<01:03,  3.61it/s] 72%|  | 592/820 [03:31<01:06,  3.44it/s] 72%|  | 593/820 [03:32<01:05,  3.49it/s] 72%|  | 594/820 [03:32<01:04,  3.53it/s] 73%|  | 595/820 [03:32<01:03,  3.56it/s] 73%|  | 596/820 [03:32<01:02,  3.57it/s] 73%|  | 597/820 [03:33<01:02,  3.58it/s] 73%|  | 598/820 [03:33<01:01,  3.59it/s] 73%|  | 599/820 [03:33<01:01,  3.60it/s] 73%|  | 600/820 [03:33<01:00,  3.61it/s] 73%|  | 601/820 [03:34<01:00,  3.61it/s] 73%|  | 602/820 [03:34<01:00,  3.61it/s] 74%|  | 603/820 [03:34<01:03,  3.40it/s] 74%|  | 604/820 [03:35<01:02,  3.47it/s] 74%|  | 605/820 [03:35<01:01,  3.51it/s] 74%|  | 606/820 [03:35<01:00,  3.54it/s] 74%|  | 607/820 [03:35<00:59,  3.56it/s] 74%|  | 608/820 [03:36<00:59,  3.58it/s] 74%|  | 609/820 [03:36<00:58,  3.59it/s] 74%|  | 610/820 [03:36<00:58,  3.60it/s] 75%|  | 611/820 [03:37<00:57,  3.61it/s] 75%|  | 612/820 [03:37<00:57,  3.61it/s] 75%|  | 613/820 [03:37<00:57,  3.61it/s] 75%|  | 614/820 [03:37<00:59,  3.43it/s] 75%|  | 615/820 [03:38<00:58,  3.49it/s] 75%|  | 616/820 [03:38<00:57,  3.52it/s] 75%|  | 617/820 [03:38<00:57,  3.55it/s] 75%|  | 618/820 [03:39<00:56,  3.57it/s] 75%|  | 619/820 [03:39<00:56,  3.59it/s] 76%|  | 620/820 [03:39<00:55,  3.59it/s] 76%|  | 621/820 [03:39<00:55,  3.60it/s] 76%|  | 622/820 [03:40<00:54,  3.60it/s] 76%|  | 623/820 [03:40<00:54,  3.60it/s] 76%|  | 624/820 [03:40<00:54,  3.60it/s] 76%|  | 625/820 [03:41<00:54,  3.61it/s] 76%|  | 626/820 [03:41<00:53,  3.61it/s] 76%|  | 627/820 [03:41<00:53,  3.61it/s] 77%|  | 628/820 [03:41<00:53,  3.61it/s] 77%|  | 629/820 [03:42<00:52,  3.61it/s] 77%|  | 630/820 [03:42<00:52,  3.61it/s] 77%|  | 631/820 [03:42<00:52,  3.61it/s] 77%|  | 632/820 [03:42<00:51,  3.62it/s] 77%|  | 633/820 [03:43<00:51,  3.61it/s] 77%|  | 634/820 [03:43<00:51,  3.61it/s] 77%|  | 635/820 [03:43<00:52,  3.54it/s] 78%|  | 636/820 [03:44<00:51,  3.56it/s] 78%|  | 637/820 [03:44<00:51,  3.58it/s] 78%|  | 638/820 [03:44<00:50,  3.59it/s] 78%|  | 639/820 [03:44<00:50,  3.60it/s] 78%|  | 640/820 [03:45<00:50,  3.60it/s] 78%|  | 641/820 [03:45<00:49,  3.60it/s] 78%|  | 642/820 [03:46<01:04,  2.75it/s] 78%|  | 643/820 [03:46<00:59,  2.96it/s] 79%|  | 644/820 [03:46<00:56,  3.13it/s] 79%|  | 645/820 [03:46<00:53,  3.26it/s] 79%|  | 646/820 [03:47<00:51,  3.37it/s] 79%|  | 647/820 [03:47<00:50,  3.44it/s] 79%|  | 648/820 [03:47<00:49,  3.49it/s] 79%|  | 649/820 [03:47<00:48,  3.52it/s] 79%|  | 650/820 [03:48<00:47,  3.55it/s] 79%|  | 651/820 [03:48<00:47,  3.57it/s] 80%|  | 652/820 [03:48<00:46,  3.59it/s] 80%|  | 653/820 [03:49<00:49,  3.41it/s] 80%|  | 654/820 [03:49<00:47,  3.47it/s] 80%|  | 655/820 [03:49<00:46,  3.51it/s] 80%|  | 656/820 [03:49<00:43,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 14:02:10,372 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:02:10,372 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:02:10,372 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.8609, 'eval_samples_per_second': 353.112, 'eval_steps_per_second': 44.215, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.07it/s][A
  3%|         | 12/436 [00:00<00:08, 49.12it/s][A
  4%|         | 17/436 [00:00<00:08, 47.28it/s][A
  5%|         | 22/436 [00:00<00:08, 46.16it/s][A
  6%|         | 27/436 [00:00<00:08, 45.56it/s][A
  7%|         | 32/436 [00:00<00:08, 45.05it/s][A
  8%|         | 37/436 [00:00<00:08, 44.91it/s][A
 10%|         | 42/436 [00:00<00:08, 44.79it/s][A
 11%|         | 47/436 [00:01<00:08, 44.71it/s][A
 12%|        | 52/436 [00:01<00:08, 44.95it/s][A
 13%|        | 57/436 [00:01<00:08, 45.03it/s][A
 14%|        | 62/436 [00:01<00:08, 45.12it/s][A
 15%|        | 67/436 [00:01<00:08, 44.96it/s][A
 17%|        | 72/436 [00:01<00:08, 44.83it/s][A
 18%|        | 77/436 [00:01<00:08, 44.71it/s][A
 19%|        | 82/436 [00:01<00:07, 44.62it/s][A
 20%|        | 87/436 [00:01<00:07, 44.52it/s][A
 21%|        | 92/436 [00:02<00:07, 43.02it/s][A
 22%|       | 97/436 [00:02<00:07, 43.73it/s][A
 23%|       | 102/436 [00:02<00:07, 44.29it/s][A
 25%|       | 107/436 [00:02<00:07, 44.46it/s][A
 26%|       | 112/436 [00:02<00:07, 44.63it/s][A
 27%|       | 117/436 [00:02<00:07, 44.61it/s][A
 28%|       | 122/436 [00:02<00:07, 44.53it/s][A
 29%|       | 127/436 [00:02<00:06, 44.57it/s][A
 30%|       | 132/436 [00:02<00:06, 44.30it/s][A
 31%|      | 137/436 [00:03<00:06, 44.38it/s][A
 33%|      | 142/436 [00:03<00:06, 44.73it/s][A
 34%|      | 147/436 [00:03<00:06, 44.85it/s][A
 35%|      | 152/436 [00:03<00:06, 45.03it/s][A
 36%|      | 157/436 [00:03<00:06, 44.83it/s][A
 37%|      | 162/436 [00:03<00:06, 44.74it/s][A
 38%|      | 167/436 [00:03<00:06, 44.69it/s][A
 39%|      | 172/436 [00:03<00:05, 44.39it/s][A
 41%|      | 177/436 [00:03<00:05, 44.37it/s][A
 42%|     | 182/436 [00:04<00:05, 44.43it/s][A
 43%|     | 187/436 [00:04<00:05, 44.69it/s][A
 44%|     | 192/436 [00:04<00:05, 44.94it/s][A
 45%|     | 197/436 [00:04<00:05, 45.07it/s][A
 46%|     | 202/436 [00:04<00:05, 44.98it/s][A
 47%|     | 207/436 [00:04<00:05, 44.86it/s][A
 49%|     | 212/436 [00:04<00:05, 44.66it/s][A
 50%|     | 217/436 [00:04<00:04, 44.44it/s][A
 51%|     | 222/436 [00:04<00:04, 44.38it/s][A
 52%|    | 227/436 [00:05<00:04, 42.87it/s][A
 53%|    | 232/436 [00:05<00:04, 43.63it/s][A
 54%|    | 237/436 [00:05<00:04, 44.07it/s][A
 56%|    | 242/436 [00:05<00:04, 44.53it/s][A
 57%|    | 247/436 [00:05<00:04, 44.80it/s][A
 58%|    | 252/436 [00:05<00:04, 44.77it/s][A
 59%|    | 257/436 [00:05<00:04, 44.59it/s][A
 60%|    | 262/436 [00:05<00:03, 44.45it/s][A
 61%|    | 267/436 [00:05<00:03, 44.20it/s][A
 62%|   | 272/436 [00:06<00:03, 44.44it/s][A
 64%|   | 277/436 [00:06<00:03, 44.60it/s][A
 65%|   | 282/436 [00:06<00:03, 44.80it/s][A
 66%|   | 287/436 [00:06<00:03, 44.87it/s][A
 67%|   | 292/436 [00:06<00:03, 45.00it/s][A
 68%|   | 297/436 [00:06<00:03, 45.07it/s][A
 69%|   | 302/436 [00:06<00:02, 44.78it/s][A
 70%|   | 307/436 [00:06<00:02, 44.61it/s][A
 72%|  | 312/436 [00:06<00:02, 44.40it/s][A
 73%|  | 317/436 [00:07<00:02, 44.48it/s][A
 74%|  | 322/436 [00:07<00:02, 44.74it/s][A
 75%|  | 327/436 [00:07<00:02, 44.84it/s][A
 76%|  | 332/436 [00:07<00:02, 44.99it/s][A
 77%|  | 337/436 [00:07<00:02, 44.98it/s][A
 78%|  | 342/436 [00:07<00:02, 44.96it/s][A
 80%|  | 347/436 [00:07<00:01, 44.77it/s][A
 81%|  | 352/436 [00:07<00:01, 44.54it/s][A
 82%| | 357/436 [00:07<00:01, 44.50it/s][A
 83%| | 362/436 [00:08<00:01, 44.07it/s][A
 84%| | 367/436 [00:08<00:01, 44.37it/s][A
 85%| | 372/436 [00:08<00:01, 44.60it/s][A
 86%| | 377/436 [00:08<00:01, 44.80it/s][A
 88%| | 382/436 [00:08<00:01, 44.89it/s][A
 89%| | 387/436 [00:08<00:01, 44.81it/s][A
 90%| | 392/436 [00:08<00:00, 44.81it/s][A
 91%| | 397/436 [00:08<00:00, 44.63it/s][A
 92%|| 402/436 [00:08<00:00, 44.46it/s][A
 93%|| 407/436 [00:09<00:00, 44.58it/s][A
 94%|| 412/436 [00:09<00:00, 44.74it/s][A
 96%|| 417/436 [00:09<00:00, 44.90it/s][A
 97%|| 422/436 [00:09<00:00, 44.92it/s][A
 98%|| 427/436 [00:09<00:00, 44.94it/s][A
 99%|| 432/436 [00:09<00:00, 44.79it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.79it/s][A 80%|  | 656/820 [03:59<00:43,  3.81it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:02:20,242 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 14:02:20,372 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:02:23,406 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:02:23,572 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:02:23,621 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-656/special_tokens_map.json
 80%|  | 657/820 [04:04<12:17,  4.53s/it] 80%|  | 658/820 [04:04<08:46,  3.25s/it] 80%|  | 659/820 [04:04<06:20,  2.36s/it] 80%|  | 660/820 [04:05<04:38,  1.74s/it] 81%|  | 661/820 [04:05<03:26,  1.30s/it] 81%|  | 662/820 [04:05<02:37,  1.01it/s] 81%|  | 663/820 [04:06<02:02,  1.28it/s] 81%|  | 664/820 [04:06<01:39,  1.56it/s] 81%|  | 665/820 [04:06<01:22,  1.88it/s] 81%|  | 666/820 [04:06<01:10,  2.19it/s] 81%| | 667/820 [04:07<01:01,  2.47it/s] 81%| | 668/820 [04:07<00:55,  2.72it/s] 82%| | 669/820 [04:07<00:51,  2.93it/s] 82%| | 670/820 [04:08<00:48,  3.09it/s] 82%| | 671/820 [04:08<00:46,  3.22it/s] 82%| | 672/820 [04:08<00:44,  3.32it/s] 82%| | 673/820 [04:08<00:43,  3.39it/s] 82%| | 674/820 [04:09<00:42,  3.44it/s] 82%| | 675/820 [04:09<00:43,  3.31it/s] 82%| | 676/820 [04:09<00:42,  3.38it/s] 83%| | 677/820 [04:10<00:41,  3.43it/s] 83%| | 678/820 [04:10<00:41,  3.46it/s] 83%| | 679/820 [04:10<00:40,  3.49it/s] 83%| | 680/820 [04:10<00:39,  3.51it/s] 83%| | 681/820 [04:11<00:39,  3.53it/s] 83%| | 682/820 [04:11<00:39,  3.54it/s] 83%| | 683/820 [04:11<00:38,  3.55it/s] 83%| | 684/820 [04:12<00:38,  3.55it/s] 84%| | 685/820 [04:12<00:37,  3.56it/s] 84%| | 686/820 [04:12<00:39,  3.37it/s] 84%| | 687/820 [04:12<00:38,  3.43it/s] 84%| | 688/820 [04:13<00:38,  3.46it/s] 84%| | 689/820 [04:13<00:37,  3.49it/s] 84%| | 690/820 [04:13<00:37,  3.51it/s] 84%| | 691/820 [04:14<00:36,  3.53it/s] 84%| | 692/820 [04:14<00:36,  3.54it/s] 85%| | 693/820 [04:14<00:35,  3.55it/s] 85%| | 694/820 [04:14<00:35,  3.55it/s] 85%| | 695/820 [04:15<00:35,  3.56it/s] 85%| | 696/820 [04:15<00:34,  3.56it/s] 85%| | 697/820 [04:15<00:35,  3.44it/s] 85%| | 698/820 [04:16<00:35,  3.48it/s] 85%| | 699/820 [04:16<00:34,  3.50it/s] 85%| | 700/820 [04:16<00:35,  3.36it/s] 85%| | 701/820 [04:16<00:34,  3.42it/s] 86%| | 702/820 [04:17<00:34,  3.46it/s] 86%| | 703/820 [04:17<00:33,  3.49it/s] 86%| | 704/820 [04:17<00:33,  3.51it/s] 86%| | 705/820 [04:18<00:32,  3.52it/s] 86%| | 706/820 [04:18<00:32,  3.53it/s] 86%| | 707/820 [04:18<00:31,  3.54it/s] 86%| | 708/820 [04:18<00:31,  3.54it/s] 86%| | 709/820 [04:19<00:31,  3.54it/s] 87%| | 710/820 [04:19<00:31,  3.55it/s] 87%| | 711/820 [04:19<00:32,  3.38it/s] 87%| | 712/820 [04:20<00:31,  3.43it/s] 87%| | 713/820 [04:20<00:30,  3.47it/s] 87%| | 714/820 [04:20<00:30,  3.49it/s] 87%| | 715/820 [04:20<00:29,  3.51it/s] 87%| | 716/820 [04:21<00:29,  3.52it/s] 87%| | 717/820 [04:21<00:29,  3.53it/s] 88%| | 718/820 [04:21<00:28,  3.54it/s] 88%| | 719/820 [04:22<00:28,  3.55it/s] 88%| | 720/820 [04:22<00:28,  3.55it/s] 88%| | 721/820 [04:22<00:27,  3.55it/s] 88%| | 722/820 [04:22<00:28,  3.41it/s] 88%| | 723/820 [04:23<00:28,  3.46it/s] 88%| | 724/820 [04:23<00:27,  3.49it/s] 88%| | 725/820 [04:23<00:27,  3.51it/s] 89%| | 726/820 [04:24<00:26,  3.52it/s] 89%| | 727/820 [04:24<00:26,  3.53it/s] 89%| | 728/820 [04:24<00:25,  3.54it/s] 89%| | 729/820 [04:24<00:25,  3.55it/s] 89%| | 730/820 [04:25<00:25,  3.55it/s] 89%| | 731/820 [04:25<00:25,  3.55it/s] 89%| | 732/820 [04:25<00:24,  3.55it/s] 89%| | 733/820 [04:26<00:25,  3.42it/s] 90%| | 734/820 [04:26<00:24,  3.46it/s] 90%| | 735/820 [04:26<00:24,  3.49it/s] 90%| | 736/820 [04:26<00:23,  3.50it/s] 90%| | 737/820 [04:27<00:23,  3.52it/s] 90%| | 738/820 [04:27<00:23,  3.53it/s] 90%| | 739/820 [04:27<00:22,  3.54it/s] 90%| | 740/820 [04:28<00:22,  3.55it/s] 90%| | 741/820 [04:28<00:22,  3.55it/s] 90%| | 742/820 [04:28<00:21,  3.55it/s] 91%| | 743/820 [04:28<00:21,  3.56it/s] 91%| | 744/820 [04:29<00:22,  3.35it/s] 91%| | 745/820 [04:29<00:21,  3.41it/s] 91%| | 746/820 [04:29<00:21,  3.45it/s] 91%| | 747/820 [04:30<00:20,  3.49it/s] 91%| | 748/820 [04:30<00:20,  3.51it/s] 91%|| 749/820 [04:30<00:20,  3.52it/s] 91%|| 750/820 [04:30<00:19,  3.54it/s] 92%|| 751/820 [04:31<00:19,  3.54it/s] 92%|| 752/820 [04:31<00:19,  3.55it/s] 92%|| 753/820 [04:31<00:18,  3.55it/s] 92%|| 754/820 [04:32<00:18,  3.55it/s] 92%|| 755/820 [04:32<00:19,  3.38it/s] 92%|| 756/820 [04:32<00:18,  3.45it/s] 92%|| 757/820 [04:32<00:18,  3.49it/s] 92%|| 758/820 [04:33<00:17,  3.53it/s] 93%|| 759/820 [04:33<00:17,  3.55it/s] 93%|| 760/820 [04:33<00:16,  3.57it/s] 93%|| 761/820 [04:33<00:16,  3.58it/s] 93%|| 762/820 [04:34<00:16,  3.59it/s] 93%|| 763/820 [04:34<00:15,  3.60it/s] 93%|| 764/820 [04:34<00:15,  3.60it/s] 93%|| 765/820 [04:35<00:15,  3.60it/s] 93%|| 766/820 [04:35<00:16,  3.37it/s] 94%|| 767/820 [04:35<00:15,  3.44it/s] 94%|| 768/820 [04:35<00:14,  3.49it/s] 94%|| 769/820 [04:36<00:14,  3.52it/s] 94%|| 770/820 [04:36<00:14,  3.55it/s] 94%|| 771/820 [04:36<00:13,  3.57it/s] 94%|| 772/820 [04:37<00:13,  3.58it/s] 94%|| 773/820 [04:37<00:13,  3.59it/s] 94%|| 774/820 [04:37<00:12,  3.60it/s] 95%|| 775/820 [04:37<00:12,  3.60it/s] 95%|| 776/820 [04:38<00:12,  3.61it/s] 95%|| 777/820 [04:38<00:12,  3.50it/s] 95%|| 778/820 [04:38<00:11,  3.53it/s] 95%|| 779/820 [04:39<00:11,  3.56it/s] 95%|| 780/820 [04:39<00:11,  3.57it/s] 95%|| 781/820 [04:39<00:10,  3.59it/s] 95%|| 782/820 [04:39<00:10,  3.59it/s] 95%|| 783/820 [04:40<00:10,  3.60it/s] 96%|| 784/820 [04:40<00:10,  3.60it/s] 96%|| 785/820 [04:40<00:09,  3.60it/s] 96%|| 786/820 [04:41<00:09,  3.61it/s] 96%|| 787/820 [04:41<00:09,  3.61it/s] 96%|| 788/820 [04:41<00:09,  3.41it/s] 96%|| 789/820 [04:41<00:08,  3.47it/s] 96%|| 790/820 [04:42<00:08,  3.51it/s] 96%|| 791/820 [04:42<00:08,  3.54it/s] 97%|| 792/820 [04:42<00:07,  3.56it/s] 97%|| 793/820 [04:42<00:07,  3.57it/s] 97%|| 794/820 [04:43<00:07,  3.58it/s] 97%|| 795/820 [04:43<00:06,  3.59it/s] 97%|| 796/820 [04:43<00:06,  3.60it/s] 97%|| 797/820 [04:44<00:06,  3.60it/s] 97%|| 798/820 [04:44<00:06,  3.60it/s] 97%|| 799/820 [04:44<00:06,  3.36it/s] 98%|| 800/820 [04:45<00:05,  3.43it/s] 98%|| 801/820 [04:45<00:05,  3.48it/s] 98%|| 802/820 [04:45<00:05,  3.52it/s] 98%|| 803/820 [04:46<00:06,  2.65it/s] 98%|| 804/820 [04:46<00:05,  2.87it/s] 98%|| 805/820 [04:46<00:04,  3.06it/s] 98%|| 806/820 [04:46<00:04,  3.20it/s] 98%|| 807/820 [04:47<00:03,  3.32it/s] 99%|| 808/820 [04:47<00:03,  3.40it/s] 99%|| 809/820 [04:47<00:03,  3.46it/s] 99%|| 810/820 [04:48<00:02,  3.51it/s] 99%|| 811/820 [04:48<00:02,  3.54it/s] 99%|| 812/820 [04:48<00:02,  3.56it/s] 99%|| 813/820 [04:48<00:01,  3.58it/s] 99%|| 814/820 [04:49<00:01,  3.59it/s] 99%|| 815/820 [04:49<00:01,  3.59it/s]100%|| 816/820 [04:49<00:01,  3.60it/s]100%|| 817/820 [04:50<00:00,  3.60it/s]100%|| 818/820 [04:50<00:00,  3.33it/s]100%|| 819/820 [04:50<00:00,  3.41it/s]100%|| 820/820 [04:50<00:00,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 14:03:11,375 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:03:11,375 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:03:11,375 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7606, 'eval_samples_per_second': 356.742, 'eval_steps_per_second': 44.67, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.15it/s][A
  3%|         | 12/436 [00:00<00:08, 49.22it/s][A
  4%|         | 17/436 [00:00<00:08, 47.18it/s][A
  5%|         | 22/436 [00:00<00:08, 46.11it/s][A
  6%|         | 27/436 [00:00<00:08, 45.47it/s][A
  7%|         | 32/436 [00:00<00:08, 45.03it/s][A
  8%|         | 37/436 [00:00<00:08, 44.96it/s][A
 10%|         | 42/436 [00:00<00:08, 44.80it/s][A
 11%|         | 47/436 [00:01<00:08, 44.81it/s][A
 12%|        | 52/436 [00:01<00:08, 44.99it/s][A
 13%|        | 57/436 [00:01<00:08, 45.02it/s][A
 14%|        | 62/436 [00:01<00:08, 45.12it/s][A
 15%|        | 67/436 [00:01<00:08, 44.96it/s][A
 17%|        | 72/436 [00:01<00:08, 44.75it/s][A
 18%|        | 77/436 [00:01<00:08, 44.70it/s][A
 19%|        | 82/436 [00:01<00:07, 44.54it/s][A
 20%|        | 87/436 [00:01<00:07, 44.59it/s][A
 21%|        | 92/436 [00:02<00:07, 44.75it/s][A
 22%|       | 97/436 [00:02<00:07, 44.93it/s][A
 23%|       | 102/436 [00:02<00:07, 44.93it/s][A
 25%|       | 107/436 [00:02<00:07, 41.93it/s][A
 26%|       | 112/436 [00:02<00:07, 43.03it/s][A
 27%|       | 117/436 [00:02<00:07, 43.61it/s][A
 28%|       | 122/436 [00:02<00:07, 43.97it/s][A
 29%|       | 127/436 [00:02<00:07, 44.14it/s][A
 30%|       | 132/436 [00:02<00:06, 44.29it/s][A
 31%|      | 137/436 [00:03<00:06, 44.59it/s][A
 33%|      | 142/436 [00:03<00:06, 44.61it/s][A
 34%|      | 147/436 [00:03<00:06, 44.53it/s][A
 35%|      | 152/436 [00:03<00:06, 44.59it/s][A
 36%|      | 157/436 [00:03<00:06, 44.80it/s][A
 37%|      | 162/436 [00:03<00:06, 44.80it/s][A
 38%|      | 167/436 [00:03<00:06, 44.77it/s][A
 39%|      | 172/436 [00:03<00:05, 44.78it/s][A
 41%|      | 177/436 [00:03<00:05, 44.87it/s][A
 42%|     | 182/436 [00:04<00:05, 44.86it/s][A
 43%|     | 187/436 [00:04<00:05, 44.81it/s][A
 44%|     | 192/436 [00:04<00:05, 44.71it/s][A
 45%|     | 197/436 [00:04<00:05, 44.73it/s][A
 46%|     | 202/436 [00:04<00:05, 44.83it/s][A
 47%|     | 207/436 [00:04<00:05, 44.83it/s][A
 49%|     | 212/436 [00:04<00:05, 44.78it/s][A
 50%|     | 217/436 [00:04<00:04, 44.77it/s][A
 51%|     | 222/436 [00:04<00:04, 44.85it/s][A
 52%|    | 227/436 [00:05<00:04, 44.79it/s][A
 53%|    | 232/436 [00:05<00:04, 44.83it/s][A
 54%|    | 237/436 [00:05<00:04, 44.71it/s][A
 56%|    | 242/436 [00:05<00:04, 43.63it/s][A
 57%|    | 247/436 [00:05<00:04, 44.13it/s][A
 58%|    | 252/436 [00:05<00:04, 44.44it/s][A
 59%|    | 257/436 [00:05<00:04, 44.49it/s][A
 60%|    | 262/436 [00:05<00:03, 44.59it/s][A
 61%|    | 267/436 [00:05<00:03, 44.66it/s][A
 62%|   | 272/436 [00:06<00:03, 44.75it/s][A
 64%|   | 277/436 [00:06<00:03, 44.70it/s][A
 65%|   | 282/436 [00:06<00:03, 44.55it/s][A
 66%|   | 287/436 [00:06<00:03, 44.62it/s][A
 67%|   | 292/436 [00:06<00:03, 44.74it/s][A
 68%|   | 297/436 [00:06<00:03, 44.86it/s][A
 69%|   | 302/436 [00:06<00:02, 44.83it/s][A
 70%|   | 307/436 [00:06<00:02, 44.91it/s][A
 72%|  | 312/436 [00:06<00:02, 44.72it/s][A
 73%|  | 317/436 [00:07<00:02, 44.83it/s][A
 74%|  | 322/436 [00:07<00:02, 44.77it/s][A
 75%|  | 327/436 [00:07<00:02, 44.62it/s][A
 76%|  | 332/436 [00:07<00:02, 44.67it/s][A
 77%|  | 337/436 [00:07<00:02, 44.65it/s][A
 78%|  | 342/436 [00:07<00:02, 44.84it/s][A
 80%|  | 347/436 [00:07<00:01, 44.85it/s][A
 81%|  | 352/436 [00:07<00:01, 44.85it/s][A
 82%| | 357/436 [00:07<00:01, 44.89it/s][A
 83%| | 362/436 [00:08<00:01, 44.78it/s][A
 84%| | 367/436 [00:08<00:01, 44.84it/s][A
 85%| | 372/436 [00:08<00:01, 44.69it/s][A
 86%| | 377/436 [00:08<00:01, 39.68it/s][A
 88%| | 382/436 [00:08<00:01, 41.22it/s][A
 89%| | 387/436 [00:08<00:01, 42.31it/s][A
 90%| | 392/436 [00:08<00:01, 43.25it/s][A
 91%| | 397/436 [00:08<00:00, 43.78it/s][A
 92%|| 402/436 [00:09<00:00, 44.35it/s][A
 93%|| 407/436 [00:09<00:00, 44.64it/s][A
 94%|| 412/436 [00:09<00:00, 44.50it/s][A
 96%|| 417/436 [00:09<00:00, 44.26it/s][A
 97%|| 422/436 [00:09<00:00, 44.13it/s][A
 98%|| 427/436 [00:09<00:00, 44.27it/s][A
 99%|| 432/436 [00:09<00:00, 44.59it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.59it/s][A100%|| 820/820 [05:00<00:00,  3.73it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:03:21,560 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 14:03:21,932 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:03:26,350 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:03:26,651 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:03:26,757 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:03:28,005 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:03:28,005 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164 (score: 1.1192312240600586).
                                                 100%|| 820/820 [05:18<00:00,  3.73it/s]100%|| 820/820 [05:18<00:00,  2.57it/s]
[INFO|trainer.py:1894] 2023-08-28 14:03:39,147 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 14:03:39,280 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:03:43,767 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:03:43,920 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:03:44,005 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:03:44,564 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:44,564 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:44,564 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:44,565 >>   train_runtime            = 0:05:18.61
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:44,565 >>   train_samples            =      10479
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:44,565 >>   train_samples_per_second =    164.445
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:44,565 >>   train_steps_per_second   =      2.574
{'eval_loss': 1.1192312240600586, 'eval_runtime': 9.7991, 'eval_samples_per_second': 355.34, 'eval_steps_per_second': 44.494, 'epoch': 5.0}
{'train_runtime': 318.6167, 'train_samples_per_second': 164.445, 'train_steps_per_second': 2.574, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 14:03:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:03:44,914 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:03:44,914 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:03:44,914 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.86it/s]  3%|         | 12/436 [00:00<00:08, 49.39it/s]  4%|         | 17/436 [00:00<00:08, 47.69it/s]  5%|         | 22/436 [00:00<00:08, 46.79it/s]  6%|         | 27/436 [00:00<00:08, 46.30it/s]  7%|         | 32/436 [00:00<00:08, 46.09it/s]  8%|         | 37/436 [00:00<00:08, 45.96it/s] 10%|         | 42/436 [00:00<00:08, 45.58it/s] 11%|         | 47/436 [00:01<00:08, 44.95it/s] 12%|        | 52/436 [00:01<00:08, 44.71it/s] 13%|        | 57/436 [00:01<00:08, 44.88it/s] 14%|        | 62/436 [00:01<00:08, 45.04it/s] 15%|        | 67/436 [00:01<00:08, 45.05it/s] 17%|        | 72/436 [00:01<00:08, 45.11it/s] 18%|        | 77/436 [00:01<00:07, 45.20it/s] 19%|        | 82/436 [00:01<00:07, 45.31it/s] 20%|        | 87/436 [00:01<00:07, 45.15it/s] 21%|        | 92/436 [00:02<00:07, 43.18it/s] 22%|       | 97/436 [00:02<00:07, 43.66it/s] 23%|       | 102/436 [00:02<00:07, 43.99it/s] 25%|       | 107/436 [00:02<00:07, 44.40it/s] 26%|       | 112/436 [00:02<00:07, 44.70it/s] 27%|       | 117/436 [00:02<00:07, 44.86it/s] 28%|       | 122/436 [00:02<00:06, 45.03it/s] 29%|       | 127/436 [00:02<00:06, 45.01it/s] 30%|       | 132/436 [00:02<00:06, 44.75it/s] 31%|      | 137/436 [00:03<00:06, 44.63it/s] 33%|      | 142/436 [00:03<00:06, 44.71it/s] 34%|      | 147/436 [00:03<00:06, 44.80it/s] 35%|      | 152/436 [00:03<00:06, 44.83it/s] 36%|      | 157/436 [00:03<00:06, 45.00it/s] 37%|      | 162/436 [00:03<00:06, 45.18it/s] 38%|      | 167/436 [00:03<00:05, 45.25it/s] 39%|      | 172/436 [00:03<00:05, 45.06it/s] 41%|      | 177/436 [00:03<00:05, 44.83it/s] 42%|     | 182/436 [00:04<00:05, 44.79it/s] 43%|     | 187/436 [00:04<00:05, 44.79it/s] 44%|     | 192/436 [00:04<00:05, 44.72it/s] 45%|     | 197/436 [00:04<00:05, 44.83it/s] 46%|     | 202/436 [00:04<00:05, 44.99it/s] 47%|     | 207/436 [00:04<00:05, 45.03it/s] 49%|     | 212/436 [00:04<00:04, 45.13it/s] 50%|     | 217/436 [00:04<00:04, 45.00it/s] 51%|     | 222/436 [00:04<00:04, 44.87it/s] 52%|    | 227/436 [00:05<00:05, 41.52it/s] 53%|    | 232/436 [00:05<00:04, 42.55it/s] 54%|    | 237/436 [00:05<00:04, 43.39it/s] 56%|    | 242/436 [00:05<00:04, 43.89it/s] 57%|    | 247/436 [00:05<00:04, 44.30it/s] 58%|    | 252/436 [00:05<00:04, 44.59it/s] 59%|    | 257/436 [00:05<00:04, 44.64it/s] 60%|    | 262/436 [00:05<00:03, 44.67it/s] 61%|    | 267/436 [00:05<00:03, 44.42it/s] 62%|   | 272/436 [00:06<00:03, 44.50it/s] 64%|   | 277/436 [00:06<00:03, 44.66it/s] 65%|   | 282/436 [00:06<00:03, 44.80it/s] 66%|   | 287/436 [00:06<00:03, 44.97it/s] 67%|   | 292/436 [00:06<00:03, 45.04it/s] 68%|   | 297/436 [00:06<00:03, 45.08it/s] 69%|   | 302/436 [00:06<00:02, 44.82it/s] 70%|   | 307/436 [00:06<00:02, 44.66it/s] 72%|  | 312/436 [00:06<00:02, 44.55it/s] 73%|  | 317/436 [00:07<00:02, 44.60it/s] 74%|  | 322/436 [00:07<00:02, 44.65it/s] 75%|  | 327/436 [00:07<00:02, 44.84it/s] 76%|  | 332/436 [00:07<00:02, 44.98it/s] 77%|  | 337/436 [00:07<00:02, 45.09it/s] 78%|  | 342/436 [00:07<00:02, 45.02it/s] 80%|  | 347/436 [00:07<00:01, 44.97it/s] 81%|  | 352/436 [00:07<00:01, 44.84it/s] 82%| | 357/436 [00:07<00:01, 44.72it/s] 83%| | 362/436 [00:08<00:01, 37.43it/s] 84%| | 367/436 [00:08<00:01, 39.49it/s] 85%| | 372/436 [00:08<00:01, 41.17it/s] 86%| | 377/436 [00:08<00:01, 42.40it/s] 88%| | 382/436 [00:08<00:01, 43.27it/s] 89%| | 387/436 [00:08<00:01, 43.88it/s] 90%| | 392/436 [00:08<00:00, 44.26it/s] 91%| | 397/436 [00:08<00:00, 44.45it/s] 92%|| 402/436 [00:09<00:00, 44.29it/s] 93%|| 407/436 [00:09<00:00, 44.05it/s] 94%|| 412/436 [00:09<00:00, 43.95it/s] 96%|| 417/436 [00:09<00:00, 44.29it/s] 97%|| 422/436 [00:09<00:00, 44.53it/s] 98%|| 427/436 [00:09<00:00, 44.78it/s] 99%|| 432/436 [00:09<00:00, 44.97it/s]100%|| 436/436 [00:09<00:00, 44.54it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:03:54,721 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,721 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,721 >>   eval_loss               =     1.1192
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,721 >>   eval_runtime            = 0:00:09.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,721 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,721 >>   eval_samples_per_second =    355.059
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,721 >>   eval_steps_per_second   =     44.459
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:03:54,722 >>   perplexity              =     3.0625
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:09,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:09,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:09,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:09,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:09,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:04:10,846 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:04:10,847 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:04:11,488 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:04:12,656 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:04:12,717 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:15,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:15,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:15,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:15,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:04:15,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:04:16,860 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:04:16,861 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:04:17,507 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:04:17,826 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:04:17,826 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-820
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-656
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:05,  1.66it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.53it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.63it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.70it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:14,  1.66it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:17,  1.58it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:18,  1.61it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:20,  1.56it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:24,  1.58it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.59it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.57it/s]Extractor Predicting: 50it [00:30,  1.52it/s]Extractor Predicting: 51it [00:31,  1.55it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:33,  1.56it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:34,  1.54it/s]Extractor Predicting: 57it [00:35,  1.59it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:36,  1.58it/s]Extractor Predicting: 60it [00:37,  1.53it/s]Extractor Predicting: 61it [00:37,  1.57it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:39,  1.56it/s]Extractor Predicting: 64it [00:39,  1.55it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.63it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:43,  1.65it/s]Extractor Predicting: 71it [00:44,  1.67it/s]Extractor Predicting: 72it [00:44,  1.69it/s]Extractor Predicting: 73it [00:45,  1.75it/s]Extractor Predicting: 74it [00:45,  1.75it/s]Extractor Predicting: 75it [00:46,  1.71it/s]Extractor Predicting: 76it [00:47,  1.62it/s]Extractor Predicting: 77it [00:47,  1.64it/s]Extractor Predicting: 78it [00:48,  1.65it/s]Extractor Predicting: 79it [00:48,  1.60it/s]Extractor Predicting: 80it [00:49,  1.62it/s]Extractor Predicting: 81it [00:50,  1.62it/s]Extractor Predicting: 82it [00:50,  1.63it/s]Extractor Predicting: 83it [00:51,  1.66it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:52,  1.65it/s]Extractor Predicting: 86it [00:53,  1.61it/s]Extractor Predicting: 87it [00:53,  1.66it/s]Extractor Predicting: 88it [00:54,  1.63it/s]Extractor Predicting: 89it [00:55,  1.56it/s]Extractor Predicting: 90it [00:55,  1.56it/s]Extractor Predicting: 91it [00:56,  1.55it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:57,  1.62it/s]Extractor Predicting: 94it [00:58,  1.57it/s]Extractor Predicting: 95it [00:58,  1.63it/s]Extractor Predicting: 96it [00:59,  1.64it/s]Extractor Predicting: 97it [01:00,  1.63it/s]Extractor Predicting: 98it [01:00,  1.62it/s]Extractor Predicting: 99it [01:01,  1.55it/s]Extractor Predicting: 100it [01:02,  1.54it/s]Extractor Predicting: 101it [01:02,  1.58it/s]Extractor Predicting: 102it [01:03,  1.57it/s]Extractor Predicting: 103it [01:04,  1.42it/s]Extractor Predicting: 104it [01:04,  1.47it/s]Extractor Predicting: 105it [01:05,  1.51it/s]Extractor Predicting: 106it [01:06,  1.50it/s]Extractor Predicting: 107it [01:06,  1.53it/s]Extractor Predicting: 108it [01:07,  1.57it/s]Extractor Predicting: 109it [01:07,  1.59it/s]Extractor Predicting: 110it [01:08,  1.61it/s]Extractor Predicting: 111it [01:09,  1.67it/s]Extractor Predicting: 112it [01:09,  1.65it/s]Extractor Predicting: 113it [01:10,  1.64it/s]Extractor Predicting: 114it [01:10,  1.58it/s]Extractor Predicting: 115it [01:11,  1.57it/s]Extractor Predicting: 116it [01:12,  1.58it/s]Extractor Predicting: 117it [01:12,  1.57it/s]Extractor Predicting: 118it [01:13,  1.57it/s]Extractor Predicting: 119it [01:14,  1.57it/s]Extractor Predicting: 120it [01:14,  1.62it/s]Extractor Predicting: 121it [01:15,  1.60it/s]Extractor Predicting: 122it [01:15,  1.62it/s]Extractor Predicting: 123it [01:16,  1.59it/s]Extractor Predicting: 124it [01:17,  1.57it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.60it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:19,  1.58it/s]Extractor Predicting: 129it [01:20,  1.58it/s]Extractor Predicting: 130it [01:21,  1.56it/s]Extractor Predicting: 131it [01:21,  1.59it/s]Extractor Predicting: 132it [01:22,  1.61it/s]Extractor Predicting: 133it [01:23,  1.54it/s]Extractor Predicting: 134it [01:23,  1.51it/s]Extractor Predicting: 135it [01:24,  1.53it/s]Extractor Predicting: 136it [01:24,  1.56it/s]Extractor Predicting: 137it [01:25,  1.57it/s]Extractor Predicting: 138it [01:26,  1.57it/s]Extractor Predicting: 139it [01:26,  1.55it/s]Extractor Predicting: 140it [01:27,  1.54it/s]Extractor Predicting: 141it [01:28,  1.54it/s]Extractor Predicting: 142it [01:28,  1.58it/s]Extractor Predicting: 143it [01:29,  1.60it/s]Extractor Predicting: 143it [01:29,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:05,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:05,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:05,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:05,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:05,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:06:06,291 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:06:06,293 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:06:07,104 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:06:08,252 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:06:08,252 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:11,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:11,407 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:11,407 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:11,407 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:11,407 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:06:12,216 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:06:12,217 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:06:12,898 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:06:13,162 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:06:13,162 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.74it/s]Extractor Predicting: 16it [00:09,  1.76it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:11,  1.74it/s]Extractor Predicting: 20it [00:11,  1.71it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:12,  1.65it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.74it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:15,  1.81it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:16,  1.81it/s]Extractor Predicting: 29it [00:16,  1.77it/s]Extractor Predicting: 30it [00:17,  1.71it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:18,  1.64it/s]Extractor Predicting: 33it [00:19,  1.64it/s]Extractor Predicting: 34it [00:19,  1.59it/s]Extractor Predicting: 35it [00:20,  1.59it/s]Extractor Predicting: 36it [00:21,  1.59it/s]Extractor Predicting: 37it [00:21,  1.61it/s]Extractor Predicting: 38it [00:22,  1.60it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:23,  1.60it/s]Extractor Predicting: 41it [00:24,  1.63it/s]Extractor Predicting: 42it [00:24,  1.65it/s]Extractor Predicting: 43it [00:25,  1.60it/s]Extractor Predicting: 44it [00:26,  1.60it/s]Extractor Predicting: 45it [00:26,  1.60it/s]Extractor Predicting: 46it [00:27,  1.61it/s]Extractor Predicting: 47it [00:28,  1.62it/s]Extractor Predicting: 48it [00:28,  1.62it/s]Extractor Predicting: 49it [00:29,  1.58it/s]Extractor Predicting: 50it [00:29,  1.61it/s]Extractor Predicting: 51it [00:30,  1.62it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:31,  1.63it/s]Extractor Predicting: 54it [00:32,  1.61it/s]Extractor Predicting: 55it [00:32,  1.60it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:36,  1.61it/s]Extractor Predicting: 61it [00:36,  1.60it/s]Extractor Predicting: 62it [00:37,  1.62it/s]Extractor Predicting: 63it [00:37,  1.68it/s]Extractor Predicting: 64it [00:38,  1.63it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:39,  1.60it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:40,  1.64it/s]Extractor Predicting: 69it [00:41,  1.60it/s]Extractor Predicting: 70it [00:42,  1.63it/s]Extractor Predicting: 71it [00:42,  1.64it/s]Extractor Predicting: 72it [00:43,  1.63it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:44,  1.61it/s]Extractor Predicting: 75it [00:45,  1.65it/s]Extractor Predicting: 76it [00:45,  1.66it/s]Extractor Predicting: 77it [00:46,  1.70it/s]Extractor Predicting: 78it [00:46,  1.75it/s]Extractor Predicting: 79it [00:47,  1.74it/s]Extractor Predicting: 80it [00:48,  1.65it/s]Extractor Predicting: 81it [00:48,  1.65it/s]Extractor Predicting: 82it [00:49,  1.68it/s]Extractor Predicting: 83it [00:49,  1.67it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:51,  1.60it/s]Extractor Predicting: 86it [00:51,  1.63it/s]Extractor Predicting: 87it [00:52,  1.67it/s]Extractor Predicting: 88it [00:52,  1.67it/s]Extractor Predicting: 89it [00:53,  1.73it/s]Extractor Predicting: 90it [00:54,  1.73it/s]Extractor Predicting: 91it [00:54,  1.73it/s]Extractor Predicting: 92it [00:55,  1.68it/s]Extractor Predicting: 93it [00:55,  1.68it/s]Extractor Predicting: 94it [00:56,  1.63it/s]Extractor Predicting: 95it [00:57,  1.63it/s]Extractor Predicting: 96it [00:57,  1.62it/s]Extractor Predicting: 97it [00:58,  1.64it/s]Extractor Predicting: 98it [00:59,  1.63it/s]Extractor Predicting: 99it [00:59,  1.63it/s]Extractor Predicting: 100it [01:00,  1.62it/s]Extractor Predicting: 101it [01:00,  1.62it/s]Extractor Predicting: 102it [01:01,  1.65it/s]Extractor Predicting: 103it [01:02,  1.66it/s]Extractor Predicting: 104it [01:02,  1.63it/s]Extractor Predicting: 105it [01:03,  1.64it/s]Extractor Predicting: 106it [01:03,  1.64it/s]Extractor Predicting: 107it [01:04,  1.63it/s]Extractor Predicting: 108it [01:05,  1.65it/s]Extractor Predicting: 109it [01:05,  1.62it/s]Extractor Predicting: 110it [01:06,  1.62it/s]Extractor Predicting: 111it [01:06,  1.62it/s]Extractor Predicting: 112it [01:07,  1.61it/s]Extractor Predicting: 113it [01:08,  1.64it/s]Extractor Predicting: 114it [01:08,  1.61it/s]Extractor Predicting: 115it [01:09,  1.61it/s]Extractor Predicting: 116it [01:09,  1.69it/s]Extractor Predicting: 117it [01:10,  1.79it/s]Extractor Predicting: 118it [01:10,  1.83it/s]Extractor Predicting: 119it [01:11,  1.82it/s]Extractor Predicting: 120it [01:12,  1.77it/s]Extractor Predicting: 121it [01:12,  1.76it/s]Extractor Predicting: 122it [01:13,  1.74it/s]Extractor Predicting: 123it [01:13,  1.73it/s]Extractor Predicting: 124it [01:14,  1.78it/s]Extractor Predicting: 125it [01:14,  1.85it/s]Extractor Predicting: 126it [01:15,  1.79it/s]Extractor Predicting: 127it [01:16,  1.80it/s]Extractor Predicting: 128it [01:16,  1.84it/s]Extractor Predicting: 129it [01:17,  1.82it/s]Extractor Predicting: 130it [01:17,  1.81it/s]Extractor Predicting: 131it [01:18,  1.58it/s]Extractor Predicting: 132it [01:19,  1.62it/s]Extractor Predicting: 133it [01:19,  1.67it/s]Extractor Predicting: 134it [01:20,  1.72it/s]Extractor Predicting: 135it [01:20,  1.77it/s]Extractor Predicting: 136it [01:21,  1.83it/s]Extractor Predicting: 137it [01:21,  1.85it/s]Extractor Predicting: 138it [01:22,  1.79it/s]Extractor Predicting: 139it [01:22,  1.76it/s]Extractor Predicting: 140it [01:23,  1.79it/s]Extractor Predicting: 141it [01:24,  1.75it/s]Extractor Predicting: 142it [01:24,  1.75it/s]Extractor Predicting: 143it [01:25,  1.78it/s]Extractor Predicting: 144it [01:25,  1.75it/s]Extractor Predicting: 145it [01:26,  1.75it/s]Extractor Predicting: 146it [01:26,  1.77it/s]Extractor Predicting: 147it [01:27,  1.73it/s]Extractor Predicting: 148it [01:28,  1.71it/s]Extractor Predicting: 149it [01:28,  1.78it/s]Extractor Predicting: 150it [01:29,  1.77it/s]Extractor Predicting: 151it [01:29,  1.74it/s]Extractor Predicting: 152it [01:30,  1.78it/s]Extractor Predicting: 153it [01:30,  1.78it/s]Extractor Predicting: 154it [01:31,  1.79it/s]Extractor Predicting: 155it [01:31,  1.81it/s]Extractor Predicting: 156it [01:32,  1.79it/s]Extractor Predicting: 157it [01:33,  1.78it/s]Extractor Predicting: 158it [01:33,  1.79it/s]Extractor Predicting: 159it [01:34,  1.71it/s]Extractor Predicting: 160it [01:34,  1.73it/s]Extractor Predicting: 161it [01:35,  1.74it/s]Extractor Predicting: 162it [01:36,  1.75it/s]Extractor Predicting: 163it [01:36,  1.75it/s]Extractor Predicting: 164it [01:37,  1.76it/s]Extractor Predicting: 165it [01:37,  1.71it/s]Extractor Predicting: 166it [01:38,  1.74it/s]Extractor Predicting: 167it [01:38,  1.76it/s]Extractor Predicting: 168it [01:39,  1.72it/s]Extractor Predicting: 169it [01:40,  1.74it/s]Extractor Predicting: 170it [01:40,  1.71it/s]Extractor Predicting: 171it [01:41,  1.65it/s]Extractor Predicting: 172it [01:41,  1.73it/s]Extractor Predicting: 173it [01:42,  1.68it/s]Extractor Predicting: 174it [01:43,  1.67it/s]Extractor Predicting: 175it [01:43,  1.65it/s]Extractor Predicting: 176it [01:44,  1.64it/s]Extractor Predicting: 177it [01:44,  1.63it/s]Extractor Predicting: 178it [01:45,  1.62it/s]Extractor Predicting: 179it [01:46,  1.63it/s]Extractor Predicting: 180it [01:46,  1.67it/s]Extractor Predicting: 181it [01:47,  1.63it/s]Extractor Predicting: 182it [01:47,  1.62it/s]Extractor Predicting: 183it [01:48,  1.62it/s]Extractor Predicting: 184it [01:49,  1.61it/s]Extractor Predicting: 185it [01:49,  1.60it/s]Extractor Predicting: 186it [01:50,  1.58it/s]Extractor Predicting: 187it [01:51,  1.58it/s]Extractor Predicting: 188it [01:51,  1.63it/s]Extractor Predicting: 189it [01:52,  1.64it/s]Extractor Predicting: 190it [01:52,  1.66it/s]Extractor Predicting: 191it [01:53,  1.58it/s]Extractor Predicting: 192it [01:54,  1.60it/s]Extractor Predicting: 193it [01:54,  1.58it/s]Extractor Predicting: 194it [01:55,  1.59it/s]Extractor Predicting: 195it [01:56,  1.60it/s]Extractor Predicting: 196it [01:56,  1.62it/s]Extractor Predicting: 197it [01:57,  1.61it/s]Extractor Predicting: 198it [01:57,  1.58it/s]Extractor Predicting: 199it [01:58,  1.59it/s]Extractor Predicting: 200it [01:59,  1.58it/s]Extractor Predicting: 201it [01:59,  1.61it/s]Extractor Predicting: 202it [02:00,  1.61it/s]Extractor Predicting: 203it [02:01,  1.60it/s]Extractor Predicting: 204it [02:01,  1.63it/s]Extractor Predicting: 205it [02:02,  1.70it/s]Extractor Predicting: 206it [02:02,  1.70it/s]Extractor Predicting: 207it [02:03,  1.68it/s]Extractor Predicting: 208it [02:04,  1.68it/s]Extractor Predicting: 209it [02:04,  1.64it/s]Extractor Predicting: 210it [02:05,  1.67it/s]Extractor Predicting: 211it [02:05,  1.72it/s]Extractor Predicting: 212it [02:06,  1.73it/s]Extractor Predicting: 213it [02:06,  1.76it/s]Extractor Predicting: 214it [02:07,  1.74it/s]Extractor Predicting: 215it [02:08,  1.66it/s]Extractor Predicting: 216it [02:08,  1.69it/s]Extractor Predicting: 217it [02:09,  1.70it/s]Extractor Predicting: 218it [02:09,  1.74it/s]Extractor Predicting: 219it [02:10,  1.77it/s]Extractor Predicting: 220it [02:10,  1.74it/s]Extractor Predicting: 221it [02:11,  1.74it/s]Extractor Predicting: 222it [02:12,  1.77it/s]Extractor Predicting: 223it [02:12,  1.74it/s]Extractor Predicting: 224it [02:13,  1.75it/s]Extractor Predicting: 225it [02:13,  1.75it/s]Extractor Predicting: 226it [02:14,  1.70it/s]Extractor Predicting: 227it [02:15,  1.66it/s]Extractor Predicting: 228it [02:15,  1.68it/s]Extractor Predicting: 229it [02:16,  1.68it/s]Extractor Predicting: 230it [02:16,  1.67it/s]Extractor Predicting: 231it [02:17,  1.68it/s]Extractor Predicting: 232it [02:18,  1.69it/s]Extractor Predicting: 233it [02:18,  1.77it/s]Extractor Predicting: 234it [02:19,  1.79it/s]Extractor Predicting: 235it [02:19,  1.80it/s]Extractor Predicting: 236it [02:20,  1.85it/s]Extractor Predicting: 237it [02:20,  1.86it/s]Extractor Predicting: 238it [02:21,  1.83it/s]Extractor Predicting: 239it [02:21,  1.83it/s]Extractor Predicting: 240it [02:22,  1.87it/s]Extractor Predicting: 241it [02:22,  1.86it/s]Extractor Predicting: 242it [02:23,  1.86it/s]Extractor Predicting: 243it [02:23,  1.92it/s]Extractor Predicting: 244it [02:24,  1.86it/s]Extractor Predicting: 245it [02:24,  1.94it/s]Extractor Predicting: 246it [02:25,  1.98it/s]Extractor Predicting: 247it [02:25,  1.92it/s]Extractor Predicting: 248it [02:26,  1.61it/s]Extractor Predicting: 249it [02:27,  1.69it/s]Extractor Predicting: 250it [02:27,  1.74it/s]Extractor Predicting: 251it [02:28,  1.80it/s]Extractor Predicting: 252it [02:28,  1.78it/s]Extractor Predicting: 253it [02:29,  1.81it/s]Extractor Predicting: 254it [02:30,  1.80it/s]Extractor Predicting: 255it [02:30,  1.86it/s]Extractor Predicting: 256it [02:31,  1.86it/s]Extractor Predicting: 257it [02:31,  1.90it/s]Extractor Predicting: 258it [02:32,  1.84it/s]Extractor Predicting: 259it [02:32,  1.88it/s]Extractor Predicting: 260it [02:33,  1.86it/s]Extractor Predicting: 261it [02:33,  1.76it/s]Extractor Predicting: 262it [02:34,  1.78it/s]Extractor Predicting: 263it [02:35,  1.73it/s]Extractor Predicting: 264it [02:35,  1.64it/s]Extractor Predicting: 265it [02:36,  1.64it/s]Extractor Predicting: 266it [02:36,  1.65it/s]Extractor Predicting: 267it [02:37,  1.69it/s]Extractor Predicting: 268it [02:38,  1.69it/s]Extractor Predicting: 269it [02:38,  1.58it/s]Extractor Predicting: 270it [02:39,  1.57it/s]Extractor Predicting: 271it [02:40,  1.58it/s]Extractor Predicting: 272it [02:40,  1.62it/s]Extractor Predicting: 273it [02:41,  1.62it/s]Extractor Predicting: 274it [02:41,  1.56it/s]Extractor Predicting: 275it [02:42,  1.58it/s]Extractor Predicting: 276it [02:43,  1.60it/s]Extractor Predicting: 277it [02:43,  1.59it/s]Extractor Predicting: 278it [02:44,  1.60it/s]Extractor Predicting: 279it [02:45,  1.60it/s]Extractor Predicting: 280it [02:45,  1.59it/s]Extractor Predicting: 281it [02:46,  1.59it/s]Extractor Predicting: 282it [02:46,  1.60it/s]Extractor Predicting: 283it [02:47,  1.61it/s]Extractor Predicting: 284it [02:48,  1.55it/s]Extractor Predicting: 285it [02:48,  1.58it/s]Extractor Predicting: 286it [02:49,  1.58it/s]Extractor Predicting: 287it [02:50,  1.57it/s]Extractor Predicting: 288it [02:50,  1.64it/s]Extractor Predicting: 289it [02:51,  1.59it/s]Extractor Predicting: 290it [02:51,  1.63it/s]Extractor Predicting: 291it [02:52,  1.66it/s]Extractor Predicting: 292it [02:53,  1.67it/s]Extractor Predicting: 293it [02:53,  1.68it/s]Extractor Predicting: 294it [02:54,  1.67it/s]Extractor Predicting: 295it [02:54,  1.68it/s]Extractor Predicting: 296it [02:55,  1.68it/s]Extractor Predicting: 297it [02:56,  1.70it/s]Extractor Predicting: 298it [02:56,  1.76it/s]Extractor Predicting: 299it [02:57,  1.71it/s]Extractor Predicting: 300it [02:57,  1.72it/s]Extractor Predicting: 301it [02:58,  1.71it/s]Extractor Predicting: 302it [02:58,  1.70it/s]Extractor Predicting: 303it [02:59,  1.66it/s]Extractor Predicting: 304it [03:00,  1.66it/s]Extractor Predicting: 305it [03:00,  1.67it/s]Extractor Predicting: 306it [03:01,  1.69it/s]Extractor Predicting: 307it [03:01,  1.70it/s]Extractor Predicting: 308it [03:02,  1.69it/s]Extractor Predicting: 309it [03:03,  1.63it/s]Extractor Predicting: 310it [03:03,  1.65it/s]Extractor Predicting: 311it [03:04,  1.66it/s]Extractor Predicting: 312it [03:04,  1.67it/s]Extractor Predicting: 313it [03:05,  1.68it/s]Extractor Predicting: 314it [03:06,  1.63it/s]Extractor Predicting: 315it [03:06,  1.67it/s]Extractor Predicting: 316it [03:07,  1.67it/s]Extractor Predicting: 317it [03:07,  1.71it/s]Extractor Predicting: 318it [03:08,  1.75it/s]Extractor Predicting: 319it [03:09,  1.74it/s]Extractor Predicting: 320it [03:09,  1.69it/s]Extractor Predicting: 321it [03:10,  1.69it/s]Extractor Predicting: 322it [03:10,  1.69it/s]Extractor Predicting: 323it [03:11,  1.73it/s]Extractor Predicting: 324it [03:11,  1.74it/s]Extractor Predicting: 325it [03:12,  1.72it/s]Extractor Predicting: 326it [03:13,  1.71it/s]Extractor Predicting: 327it [03:13,  1.72it/s]Extractor Predicting: 328it [03:14,  1.70it/s]Extractor Predicting: 329it [03:14,  1.71it/s]Extractor Predicting: 330it [03:15,  1.70it/s]Extractor Predicting: 331it [03:16,  1.69it/s]Extractor Predicting: 332it [03:16,  1.69it/s]Extractor Predicting: 333it [03:17,  1.67it/s]Extractor Predicting: 334it [03:17,  1.71it/s]Extractor Predicting: 335it [03:18,  1.71it/s]Extractor Predicting: 336it [03:19,  1.72it/s]Extractor Predicting: 337it [03:19,  1.71it/s]Extractor Predicting: 338it [03:20,  1.69it/s]Extractor Predicting: 339it [03:20,  1.71it/s]Extractor Predicting: 340it [03:21,  1.69it/s]Extractor Predicting: 341it [03:21,  1.72it/s]Extractor Predicting: 342it [03:22,  1.74it/s]Extractor Predicting: 343it [03:23,  1.74it/s]Extractor Predicting: 344it [03:23,  1.74it/s]Extractor Predicting: 345it [03:24,  1.72it/s]Extractor Predicting: 346it [03:24,  1.72it/s]Extractor Predicting: 347it [03:25,  1.73it/s]Extractor Predicting: 348it [03:26,  1.69it/s]Extractor Predicting: 349it [03:26,  1.71it/s]Extractor Predicting: 350it [03:27,  1.69it/s]Extractor Predicting: 351it [03:27,  1.71it/s]Extractor Predicting: 352it [03:28,  1.69it/s]Extractor Predicting: 353it [03:28,  1.70it/s]Extractor Predicting: 354it [03:29,  1.71it/s]Extractor Predicting: 355it [03:30,  1.64it/s]Extractor Predicting: 356it [03:30,  1.67it/s]Extractor Predicting: 357it [03:31,  1.65it/s]Extractor Predicting: 358it [03:31,  1.68it/s]Extractor Predicting: 359it [03:32,  1.69it/s]Extractor Predicting: 360it [03:33,  1.66it/s]Extractor Predicting: 361it [03:33,  1.67it/s]Extractor Predicting: 362it [03:34,  1.49it/s]Extractor Predicting: 363it [03:35,  1.55it/s]Extractor Predicting: 364it [03:35,  1.62it/s]Extractor Predicting: 365it [03:36,  1.60it/s]Extractor Predicting: 366it [03:36,  1.64it/s]Extractor Predicting: 367it [03:37,  1.66it/s]Extractor Predicting: 368it [03:38,  1.67it/s]Extractor Predicting: 369it [03:38,  1.64it/s]Extractor Predicting: 370it [03:39,  1.60it/s]Extractor Predicting: 371it [03:40,  1.62it/s]Extractor Predicting: 372it [03:40,  1.64it/s]Extractor Predicting: 373it [03:41,  1.65it/s]Extractor Predicting: 374it [03:41,  1.68it/s]Extractor Predicting: 375it [03:42,  1.62it/s]Extractor Predicting: 376it [03:43,  1.61it/s]Extractor Predicting: 377it [03:43,  1.68it/s]Extractor Predicting: 378it [03:44,  1.69it/s]Extractor Predicting: 379it [03:44,  1.73it/s]Extractor Predicting: 380it [03:45,  1.69it/s]Extractor Predicting: 381it [03:46,  1.65it/s]Extractor Predicting: 382it [03:46,  1.66it/s]Extractor Predicting: 383it [03:47,  1.64it/s]Extractor Predicting: 384it [03:47,  1.63it/s]Extractor Predicting: 385it [03:48,  1.65it/s]Extractor Predicting: 386it [03:49,  1.61it/s]Extractor Predicting: 387it [03:49,  1.61it/s]Extractor Predicting: 388it [03:50,  1.57it/s]Extractor Predicting: 389it [03:51,  1.59it/s]Extractor Predicting: 390it [03:51,  1.61it/s]Extractor Predicting: 391it [03:52,  1.60it/s]Extractor Predicting: 392it [03:52,  1.63it/s]Extractor Predicting: 393it [03:53,  1.63it/s]Extractor Predicting: 394it [03:54,  1.65it/s]Extractor Predicting: 395it [03:54,  1.67it/s]Extractor Predicting: 396it [03:55,  1.60it/s]Extractor Predicting: 397it [03:55,  1.63it/s]Extractor Predicting: 398it [03:56,  1.62it/s]Extractor Predicting: 399it [03:57,  1.67it/s]Extractor Predicting: 400it [03:57,  1.64it/s]Extractor Predicting: 401it [03:58,  1.66it/s]Extractor Predicting: 402it [03:58,  1.66it/s]Extractor Predicting: 403it [03:59,  1.64it/s]Extractor Predicting: 404it [04:00,  1.67it/s]Extractor Predicting: 405it [04:00,  1.68it/s]Extractor Predicting: 406it [04:01,  1.67it/s]Extractor Predicting: 407it [04:01,  1.67it/s]Extractor Predicting: 408it [04:02,  1.65it/s]Extractor Predicting: 409it [04:03,  1.68it/s]Extractor Predicting: 410it [04:03,  1.69it/s]Extractor Predicting: 411it [04:04,  1.68it/s]Extractor Predicting: 412it [04:04,  1.68it/s]Extractor Predicting: 413it [04:05,  1.68it/s]Extractor Predicting: 414it [04:06,  1.68it/s]Extractor Predicting: 415it [04:06,  1.65it/s]Extractor Predicting: 416it [04:07,  1.70it/s]Extractor Predicting: 417it [04:07,  1.70it/s]Extractor Predicting: 418it [04:08,  1.70it/s]Extractor Predicting: 419it [04:08,  1.72it/s]Extractor Predicting: 420it [04:09,  1.74it/s]Extractor Predicting: 421it [04:10,  1.71it/s]Extractor Predicting: 422it [04:10,  1.72it/s]Extractor Predicting: 423it [04:11,  1.73it/s]Extractor Predicting: 424it [04:11,  1.70it/s]Extractor Predicting: 425it [04:12,  1.69it/s]Extractor Predicting: 426it [04:13,  1.73it/s]Extractor Predicting: 427it [04:13,  1.72it/s]Extractor Predicting: 428it [04:14,  1.68it/s]Extractor Predicting: 429it [04:14,  1.93it/s]Extractor Predicting: 429it [04:14,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:43,097 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:43,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:43,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:43,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:43,129 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:10:43,597 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:10:43,598 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:10:43,894 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:10:45,038 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:10:45,038 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:48,180 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:48,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:48,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:48,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:10:48,213 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:10:48,886 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:10:48,887 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:10:49,481 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:10:49,647 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:10:49,648 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:10:54,205 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:10:54,206 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:10:54,246 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:10:54,247 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:10:54,257 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:11:11,971 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:11:12,003 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:11:12,154 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:11:12,155 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:11:12,251 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:12,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:12,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:12,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:12,321 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:12,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:11:12,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:11:12,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:13,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:14,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:14,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:15,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:15,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:16,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:17,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:17,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:18,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:19,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:19,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:20,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:20,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:21,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:22,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:22,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:23,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:23,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:24,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:25,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:25,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:26,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:27,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:28,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:28,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:13, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 14:11:29,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:29,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:30,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:30,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:31,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:32,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:32,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:33,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:34,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:34,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:35,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:35,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:36,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:37,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:37,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:38,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:39,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:39,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:40,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:40,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:41,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:42,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:42,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:31, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-28 14:11:43,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:44,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:45,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:45,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:46,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:47,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:48,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:48,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:50,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:51,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:52,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:53,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:53,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:54,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:55,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:56,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:57,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:57,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:58,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:59,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:11:59,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:00,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:01,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:01,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:02,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:50<04:53, 17.25s/it][WARNING|generation_utils.py:914] 2023-08-28 14:12:03,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:03,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:04,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:05,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:05,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:06,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:07,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:07,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:08,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:09,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:09,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:10,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:11,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:11,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:12,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:13,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:13,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:14,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:15,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:15,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:16,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:17,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:17,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:05<04:22, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 14:12:18,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:18,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:19,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:20,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:21,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:21,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:22,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:23,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:23,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:24,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:25,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:26,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:26,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:27,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:28,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:28,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:29,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:30,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:31,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:31,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:32,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:32,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:33,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:34,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:22<04:08, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-28 14:12:35,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:35,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:36,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:37,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:38,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:38,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:39,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:40,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:40,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:41,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:42,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:42,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:43,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:44,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:44,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:45,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:46,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:47,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:47,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:48,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:49,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:49,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:50,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:38<03:49, 16.41s/it][WARNING|generation_utils.py:914] 2023-08-28 14:12:51,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:51,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:52,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:53,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:54,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:54,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:55,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:56,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:56,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:57,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:57,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:58,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:59,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:12:59,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:00,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:00,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:01,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:02,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:03,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:03,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:04,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:05,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:06,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:06,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:07,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:08,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:08,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:56<03:41, 17.05s/it][WARNING|generation_utils.py:914] 2023-08-28 14:13:09,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:10,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:10,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:11,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:12,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:12,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:13,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:14,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:14,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:15,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:15,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:16,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:17,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:18,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:18,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:19,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:20,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:20,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:21,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:21,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:22,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:23,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:23,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:24,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:25,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:25,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:26,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:14<03:25, 17.16s/it][WARNING|generation_utils.py:914] 2023-08-28 14:13:26,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:27,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:28,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:29,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:29,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:30,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:31,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:31,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:32,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:33,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:33,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:34,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:35,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:36,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:36,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:37,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:38,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:38,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:39,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:40,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:40,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:41,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:42,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:42,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:43,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:44,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:32<03:12, 17.46s/it][WARNING|generation_utils.py:914] 2023-08-28 14:13:45,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:45,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:46,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:47,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:47,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:48,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:49,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:50,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:50,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:51,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:52,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:53,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:53,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:54,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:55,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:55,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:56,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:57,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:57,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:58,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:59,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:59,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:00,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:01,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:49<02:52, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:01,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:02,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:03,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:03,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:04,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:05,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:05,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:06,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:07,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:07,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:08,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:09,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:09,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:10,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:11,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:11,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:12,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:12,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:13,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:14,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:14,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:15,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:16,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:16,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:17,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:05<02:33, 17.02s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:18,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:19,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:19,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:20,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:20,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:21,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:22,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:23,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:23,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:24,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:25,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:26,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:27,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:27,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:28,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:29,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:29,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:30,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:31,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:31,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:32,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:33,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:34,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:34,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:22<02:16, 17.03s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:35,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:36,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:36,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:37,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:38,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:38,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:39,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:40,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:41,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:41,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:42,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:43,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:43,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:44,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:45,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:46,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:46,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:47,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:48,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:48,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:49,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:50,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:50,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:39<01:57, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:51,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:52,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:53,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:53,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:54,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:55,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:55,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:56,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:57,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:57,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:58,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:59,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:00,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:00,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:01,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:02,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:02,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:03,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:04,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:05,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:05,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:06,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:07,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:07,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:55<01:41, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:08,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:09,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:10,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:10,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:11,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:12,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:12,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:13,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:13,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:14,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:15,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:15,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:16,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:17,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:17,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:18,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:19,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:19,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:20,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:21,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:22,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:22,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:23,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:24,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:24,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:12<01:23, 16.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:25,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:26,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:26,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:27,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:28,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:28,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:29,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:30,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:30,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:31,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:32,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:32,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:33,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:34,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:34,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:35,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:36,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:37,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:37,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:38,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:39,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:40,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:40,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:41,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:42,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:43,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:30<01:08, 17.25s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:43,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:44,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:45,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:45,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:46,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:47,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:47,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:48,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:49,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:49,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:50,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:51,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:51,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:52,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:53,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:53,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:54,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:55,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:55,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:56,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:57,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:58,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:58,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:46<00:50, 16.83s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:59,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:00,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:00,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:01,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:01,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:02,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:03,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:03,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:04,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:05,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:06,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:06,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:07,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:07,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:08,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:09,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:09,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:10,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:11,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:11,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:12,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:13,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:14,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:14,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:02<00:33, 16.54s/it][WARNING|generation_utils.py:914] 2023-08-28 14:16:15,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:16,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:18,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:18,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:19,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:20,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:20,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:21,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:22,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:22,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:23,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:24,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:24,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:25,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:25,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:26,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:27,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:27,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:28,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:29,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:29,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:30,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:31,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:31,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:32,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:33,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:33,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:21<00:17, 17.22s/it][WARNING|generation_utils.py:914] 2023-08-28 14:16:34,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:34,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:35,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:36,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:36,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:37,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:38,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:39,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:39,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:40,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:41,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:41,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:42,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:43,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:43,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:44,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:44,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:45,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:46,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:46,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:48,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:48,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:49,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:37<00:00, 16.77s/it]Generating: 100%|| 20/20 [05:37<00:00, 16.86s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:16:59,158 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:16:59,189 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:16:59,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:16:59,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:16:59,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:16:59,968 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:16:59,969 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:17:00,575 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:17:01,720 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:17:01,720 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:04,634 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:04,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:04,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:04,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:17:04,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:17:05,455 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:17:05,456 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:17:06,086 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:17:06,314 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:17:06,314 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of Lpez 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of Lpez 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Dmontagne " , including Boulogne \'s " Montessemble des deux de Chteau des Gains " . Head Entity : Montessemble des deux de Chteau des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of Franois Renoul .\')', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.38it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.51it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.52it/s]Extractor Estimating: 9it [00:05,  1.53it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.56it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:10,  1.63it/s]Extractor Estimating: 17it [00:10,  1.66it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:12,  1.64it/s]Extractor Estimating: 20it [00:12,  1.57it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.58it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:16,  1.55it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:17,  1.64it/s]Extractor Estimating: 28it [00:17,  1.66it/s]Extractor Estimating: 29it [00:18,  1.61it/s]Extractor Estimating: 30it [00:19,  1.57it/s]Extractor Estimating: 31it [00:19,  1.60it/s]Extractor Estimating: 32it [00:20,  1.59it/s]Extractor Estimating: 33it [00:21,  1.55it/s]Extractor Estimating: 34it [00:21,  1.58it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.49it/s]Extractor Estimating: 37it [00:23,  1.47it/s]Extractor Estimating: 38it [00:24,  1.56it/s]Extractor Estimating: 39it [00:25,  1.52it/s]Extractor Estimating: 40it [00:25,  1.45it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:27,  1.51it/s]Extractor Estimating: 44it [00:28,  1.55it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:29,  1.51it/s]Extractor Estimating: 47it [00:30,  1.57it/s]Extractor Estimating: 48it [00:30,  1.53it/s]Extractor Estimating: 49it [00:31,  1.57it/s]Extractor Estimating: 50it [00:32,  1.62it/s]Extractor Estimating: 51it [00:32,  1.59it/s]Extractor Estimating: 52it [00:33,  1.55it/s]Extractor Estimating: 53it [00:34,  1.49it/s]Extractor Estimating: 54it [00:34,  1.54it/s]Extractor Estimating: 55it [00:35,  1.55it/s]Extractor Estimating: 56it [00:36,  1.57it/s]Extractor Estimating: 57it [00:36,  1.51it/s]Extractor Estimating: 58it [00:37,  1.57it/s]Extractor Estimating: 59it [00:37,  1.57it/s]Extractor Estimating: 60it [00:38,  1.54it/s]Extractor Estimating: 61it [00:39,  1.53it/s]Extractor Estimating: 62it [00:40,  1.48it/s]Extractor Estimating: 63it [00:40,  1.54it/s]Extractor Estimating: 64it [00:41,  1.53it/s]Extractor Estimating: 65it [00:42,  1.49it/s]Extractor Estimating: 66it [00:42,  1.46it/s]Extractor Estimating: 67it [00:43,  1.45it/s]Extractor Estimating: 68it [00:44,  1.48it/s]Extractor Estimating: 69it [00:44,  1.47it/s]Extractor Estimating: 70it [00:45,  1.48it/s]Extractor Estimating: 71it [00:46,  1.53it/s]Extractor Estimating: 72it [00:46,  1.50it/s]Extractor Estimating: 73it [00:47,  1.51it/s]Extractor Estimating: 74it [00:48,  1.51it/s]Extractor Estimating: 75it [00:48,  1.52it/s]Extractor Estimating: 76it [00:49,  1.53it/s]Extractor Estimating: 77it [00:50,  1.38it/s]Extractor Estimating: 78it [00:50,  1.41it/s]Extractor Estimating: 79it [00:51,  1.47it/s]Extractor Estimating: 80it [00:52,  1.48it/s]Extractor Estimating: 81it [00:52,  1.49it/s]Extractor Estimating: 82it [00:53,  1.45it/s]Extractor Estimating: 83it [00:54,  1.50it/s]Extractor Estimating: 84it [00:54,  1.47it/s]Extractor Estimating: 85it [00:55,  1.48it/s]Extractor Estimating: 86it [00:56,  1.54it/s]Extractor Estimating: 87it [00:56,  1.50it/s]Extractor Estimating: 88it [00:57,  1.31it/s]Extractor Estimating: 89it [00:58,  1.37it/s]Extractor Estimating: 90it [00:59,  1.43it/s]Extractor Estimating: 91it [00:59,  1.52it/s]Extractor Estimating: 92it [01:00,  1.32it/s]Extractor Estimating: 93it [01:01,  1.32it/s]Extractor Estimating: 94it [01:02,  1.39it/s]Extractor Estimating: 95it [01:02,  1.43it/s]Extractor Estimating: 96it [01:03,  1.40it/s]Extractor Estimating: 97it [01:04,  1.37it/s]Extractor Estimating: 98it [01:04,  1.42it/s]Extractor Estimating: 99it [01:05,  1.46it/s]Extractor Estimating: 100it [01:06,  1.45it/s]Extractor Estimating: 101it [01:07,  1.39it/s]Extractor Estimating: 102it [01:07,  1.44it/s]Extractor Estimating: 103it [01:08,  1.53it/s]Extractor Estimating: 104it [01:08,  1.52it/s]Extractor Estimating: 105it [01:09,  1.56it/s]Extractor Estimating: 106it [01:10,  1.57it/s]Extractor Estimating: 107it [01:10,  1.52it/s]Extractor Estimating: 108it [01:11,  1.55it/s]Extractor Estimating: 109it [01:12,  1.58it/s]Extractor Estimating: 110it [01:12,  1.66it/s]Extractor Estimating: 111it [01:13,  1.56it/s]Extractor Estimating: 112it [01:14,  1.47it/s]Extractor Estimating: 113it [01:14,  1.54it/s]Extractor Estimating: 114it [01:15,  1.57it/s]Extractor Estimating: 115it [01:15,  1.51it/s]Extractor Estimating: 116it [01:16,  1.50it/s]Extractor Estimating: 117it [01:17,  1.42it/s]Extractor Estimating: 118it [01:18,  1.47it/s]Extractor Estimating: 119it [01:18,  1.50it/s]Extractor Estimating: 120it [01:19,  1.52it/s]Extractor Estimating: 121it [01:19,  1.58it/s]Extractor Estimating: 122it [01:20,  1.53it/s]Extractor Estimating: 123it [01:21,  1.55it/s]Extractor Estimating: 124it [01:21,  1.60it/s]Extractor Estimating: 125it [01:22,  1.59it/s]Extractor Estimating: 126it [01:23,  1.63it/s]Extractor Estimating: 127it [01:23,  1.48it/s]Extractor Estimating: 128it [01:24,  1.50it/s]Extractor Estimating: 129it [01:25,  1.49it/s]Extractor Estimating: 130it [01:25,  1.48it/s]Extractor Estimating: 131it [01:26,  1.51it/s]Extractor Estimating: 132it [01:27,  1.43it/s]Extractor Estimating: 133it [01:27,  1.46it/s]Extractor Estimating: 134it [01:28,  1.47it/s]Extractor Estimating: 135it [01:29,  1.46it/s]Extractor Estimating: 136it [01:29,  1.52it/s]Extractor Estimating: 137it [01:30,  1.51it/s]Extractor Estimating: 138it [01:31,  1.53it/s]Extractor Estimating: 139it [01:31,  1.58it/s]Extractor Estimating: 140it [01:32,  1.60it/s]Extractor Estimating: 141it [01:33,  1.53it/s]Extractor Estimating: 142it [01:33,  1.50it/s]Extractor Estimating: 143it [01:34,  1.50it/s]Extractor Estimating: 144it [01:35,  1.46it/s]Extractor Estimating: 145it [01:35,  1.49it/s]Extractor Estimating: 146it [01:36,  1.47it/s]Extractor Estimating: 147it [01:37,  1.47it/s]Extractor Estimating: 148it [01:37,  1.44it/s]Extractor Estimating: 149it [01:38,  1.49it/s]Extractor Estimating: 150it [01:39,  1.50it/s]Extractor Estimating: 151it [01:39,  1.53it/s]Extractor Estimating: 152it [01:40,  1.59it/s]Extractor Estimating: 153it [01:41,  1.63it/s]Extractor Estimating: 154it [01:41,  1.71it/s]Extractor Estimating: 155it [01:42,  1.72it/s]Extractor Estimating: 156it [01:42,  1.67it/s]Extractor Estimating: 157it [01:43,  1.69it/s]Extractor Estimating: 158it [01:43,  1.67it/s]Extractor Estimating: 159it [01:44,  1.65it/s]Extractor Estimating: 160it [01:45,  1.68it/s]Extractor Estimating: 161it [01:45,  1.72it/s]Extractor Estimating: 162it [01:46,  1.69it/s]Extractor Estimating: 163it [01:46,  1.72it/s]Extractor Estimating: 164it [01:47,  1.74it/s]Extractor Estimating: 165it [01:48,  1.70it/s]Extractor Estimating: 166it [01:48,  1.66it/s]Extractor Estimating: 167it [01:49,  1.64it/s]Extractor Estimating: 168it [01:49,  1.70it/s]Extractor Estimating: 169it [01:50,  1.74it/s]Extractor Estimating: 170it [01:50,  1.70it/s]Extractor Estimating: 171it [01:51,  1.46it/s]Extractor Estimating: 172it [01:52,  1.48it/s]Extractor Estimating: 173it [01:53,  1.55it/s]Extractor Estimating: 174it [01:53,  1.60it/s]Extractor Estimating: 175it [01:54,  1.63it/s]Extractor Estimating: 176it [01:55,  1.46it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:56,  1.58it/s]Extractor Estimating: 179it [01:56,  1.57it/s]Extractor Estimating: 180it [01:57,  1.56it/s]Extractor Estimating: 181it [01:58,  1.59it/s]Extractor Estimating: 182it [01:58,  1.63it/s]Extractor Estimating: 183it [01:59,  1.61it/s]Extractor Estimating: 184it [02:00,  1.53it/s]Extractor Estimating: 185it [02:00,  1.60it/s]Extractor Estimating: 186it [02:01,  1.57it/s]Extractor Estimating: 187it [02:02,  1.48it/s]Extractor Estimating: 188it [02:02,  1.53it/s]Extractor Estimating: 189it [02:03,  1.52it/s]Extractor Estimating: 190it [02:04,  1.55it/s]Extractor Estimating: 191it [02:04,  1.58it/s]Extractor Estimating: 192it [02:05,  1.57it/s]Extractor Estimating: 193it [02:05,  1.61it/s]Extractor Estimating: 194it [02:06,  1.63it/s]Extractor Estimating: 195it [02:07,  1.60it/s]Extractor Estimating: 196it [02:07,  1.61it/s]Extractor Estimating: 197it [02:08,  1.61it/s]Extractor Estimating: 198it [02:08,  1.59it/s]Extractor Estimating: 199it [02:09,  1.63it/s]Extractor Estimating: 200it [02:10,  1.60it/s]Extractor Estimating: 201it [02:10,  1.58it/s]Extractor Estimating: 202it [02:11,  1.44it/s]Extractor Estimating: 203it [02:12,  1.48it/s]Extractor Estimating: 204it [02:13,  1.48it/s]Extractor Estimating: 205it [02:13,  1.49it/s]Extractor Estimating: 206it [02:14,  1.50it/s]Extractor Estimating: 207it [02:15,  1.38it/s]Extractor Estimating: 208it [02:15,  1.37it/s]Extractor Estimating: 209it [02:16,  1.42it/s]Extractor Estimating: 210it [02:17,  1.45it/s]Extractor Estimating: 211it [02:17,  1.44it/s]Extractor Estimating: 212it [02:18,  1.38it/s]Extractor Estimating: 213it [02:19,  1.42it/s]Extractor Estimating: 214it [02:20,  1.45it/s]Extractor Estimating: 215it [02:20,  1.43it/s]Extractor Estimating: 216it [02:21,  1.41it/s]Extractor Estimating: 217it [02:22,  1.42it/s]Extractor Estimating: 218it [02:22,  1.42it/s]Extractor Estimating: 219it [02:23,  1.40it/s]Extractor Estimating: 220it [02:24,  1.46it/s]Extractor Estimating: 221it [02:24,  1.51it/s]Extractor Estimating: 222it [02:25,  1.44it/s]Extractor Estimating: 223it [02:26,  1.48it/s]Extractor Estimating: 224it [02:26,  1.48it/s]Extractor Estimating: 225it [02:27,  1.45it/s]Extractor Estimating: 226it [02:28,  1.46it/s]Extractor Estimating: 227it [02:29,  1.46it/s]Extractor Estimating: 228it [02:29,  1.48it/s]Extractor Estimating: 229it [02:30,  1.53it/s]Extractor Estimating: 230it [02:30,  1.50it/s]Extractor Estimating: 231it [02:31,  1.51it/s]Extractor Estimating: 232it [02:32,  1.51it/s]Extractor Estimating: 233it [02:33,  1.47it/s]Extractor Estimating: 234it [02:33,  1.54it/s]Extractor Estimating: 235it [02:34,  1.55it/s]Extractor Estimating: 236it [02:34,  1.56it/s]Extractor Estimating: 237it [02:35,  1.55it/s]Extractor Estimating: 238it [02:36,  1.52it/s]Extractor Estimating: 239it [02:36,  1.54it/s]Extractor Estimating: 240it [02:37,  1.56it/s]Extractor Estimating: 241it [02:38,  1.53it/s]Extractor Estimating: 242it [02:38,  1.56it/s]Extractor Estimating: 243it [02:39,  1.55it/s]Extractor Estimating: 244it [02:40,  1.52it/s]Extractor Estimating: 245it [02:40,  1.53it/s]Extractor Estimating: 246it [02:41,  1.51it/s]Extractor Estimating: 247it [02:41,  1.57it/s]Extractor Estimating: 248it [02:42,  1.49it/s]Extractor Estimating: 249it [02:43,  1.46it/s]Extractor Estimating: 250it [02:44,  1.51it/s]Extractor Estimating: 251it [02:44,  1.52it/s]Extractor Estimating: 252it [02:45,  1.50it/s]Extractor Estimating: 253it [02:46,  1.51it/s]Extractor Estimating: 254it [02:46,  1.56it/s]Extractor Estimating: 255it [02:47,  1.53it/s]Extractor Estimating: 256it [02:48,  1.48it/s]Extractor Estimating: 257it [02:48,  1.51it/s]Extractor Estimating: 258it [02:49,  1.55it/s]Extractor Estimating: 259it [02:49,  1.57it/s]Extractor Estimating: 260it [02:50,  1.55it/s]Extractor Estimating: 261it [02:51,  1.48it/s]Extractor Estimating: 262it [02:51,  1.50it/s]Extractor Estimating: 263it [02:52,  1.37it/s]Extractor Estimating: 264it [02:53,  1.43it/s]Extractor Estimating: 265it [02:54,  1.45it/s]Extractor Estimating: 266it [02:54,  1.42it/s]Extractor Estimating: 267it [02:55,  1.49it/s]Extractor Estimating: 268it [02:56,  1.55it/s]Extractor Estimating: 269it [02:56,  1.63it/s]Extractor Estimating: 270it [02:57,  1.58it/s]Extractor Estimating: 271it [02:57,  1.55it/s]Extractor Estimating: 272it [02:58,  1.53it/s]Extractor Estimating: 273it [02:59,  1.57it/s]Extractor Estimating: 274it [02:59,  1.54it/s]Extractor Estimating: 275it [03:00,  1.52it/s]Extractor Estimating: 276it [03:01,  1.44it/s]Extractor Estimating: 277it [03:01,  1.49it/s]Extractor Estimating: 278it [03:02,  1.54it/s]Extractor Estimating: 279it [03:03,  1.56it/s]Extractor Estimating: 280it [03:03,  1.52it/s]Extractor Estimating: 281it [03:04,  1.52it/s]Extractor Estimating: 282it [03:05,  1.52it/s]Extractor Estimating: 283it [03:05,  1.55it/s]Extractor Estimating: 284it [03:06,  1.54it/s]Extractor Estimating: 285it [03:07,  1.52it/s]Extractor Estimating: 286it [03:07,  1.47it/s]Extractor Estimating: 287it [03:08,  1.46it/s]Extractor Estimating: 288it [03:09,  1.46it/s]Extractor Estimating: 289it [03:09,  1.47it/s]Extractor Estimating: 290it [03:10,  1.39it/s]Extractor Estimating: 291it [03:11,  1.41it/s]Extractor Estimating: 292it [03:12,  1.41it/s]Extractor Estimating: 293it [03:12,  1.44it/s]Extractor Estimating: 294it [03:13,  1.46it/s]Extractor Estimating: 295it [03:14,  1.46it/s]Extractor Estimating: 296it [03:14,  1.42it/s]Extractor Estimating: 297it [03:15,  1.38it/s]Extractor Estimating: 298it [03:16,  1.39it/s]Extractor Estimating: 299it [03:17,  1.40it/s]Extractor Estimating: 300it [03:17,  1.48it/s]Extractor Estimating: 301it [03:18,  1.54it/s]Extractor Estimating: 302it [03:18,  1.56it/s]Extractor Estimating: 303it [03:19,  1.58it/s]Extractor Estimating: 304it [03:20,  1.62it/s]Extractor Estimating: 305it [03:20,  1.62it/s]Extractor Estimating: 306it [03:21,  1.63it/s]Extractor Estimating: 307it [03:21,  1.62it/s]Extractor Estimating: 308it [03:22,  1.62it/s]Extractor Estimating: 309it [03:23,  1.68it/s]Extractor Estimating: 310it [03:23,  1.69it/s]Extractor Estimating: 311it [03:24,  1.64it/s]Extractor Estimating: 312it [03:24,  1.63it/s]Extractor Estimating: 313it [03:25,  1.65it/s]Extractor Estimating: 314it [03:26,  1.62it/s]Extractor Estimating: 315it [03:26,  1.65it/s]Extractor Estimating: 316it [03:27,  1.70it/s]Extractor Estimating: 317it [03:27,  1.70it/s]Extractor Estimating: 318it [03:28,  1.67it/s]Extractor Estimating: 319it [03:29,  1.61it/s]Extractor Estimating: 320it [03:29,  1.60it/s]Extractor Estimating: 321it [03:30,  1.62it/s]Extractor Estimating: 322it [03:31,  1.61it/s]Extractor Estimating: 323it [03:31,  1.67it/s]Extractor Estimating: 324it [03:32,  1.67it/s]Extractor Estimating: 325it [03:32,  1.67it/s]Extractor Estimating: 326it [03:33,  1.66it/s]Extractor Estimating: 327it [03:34,  1.59it/s]Extractor Estimating: 328it [03:34,  1.54it/s]Extractor Estimating: 329it [03:35,  1.51it/s]Extractor Estimating: 330it [03:36,  1.55it/s]Extractor Estimating: 331it [03:36,  1.63it/s]Extractor Estimating: 332it [03:37,  1.70it/s]Extractor Estimating: 333it [03:37,  1.65it/s]Extractor Estimating: 334it [03:38,  1.61it/s]Extractor Estimating: 335it [03:39,  1.59it/s]Extractor Estimating: 336it [03:39,  1.58it/s]Extractor Estimating: 337it [03:40,  1.57it/s]Extractor Estimating: 338it [03:41,  1.54it/s]Extractor Estimating: 339it [03:41,  1.62it/s]Extractor Estimating: 340it [03:42,  1.60it/s]Extractor Estimating: 341it [03:42,  1.60it/s]Extractor Estimating: 342it [03:43,  1.66it/s]Extractor Estimating: 343it [03:44,  1.66it/s]Extractor Estimating: 344it [03:44,  1.61it/s]Extractor Estimating: 345it [03:45,  1.60it/s]Extractor Estimating: 346it [03:45,  1.61it/s]Extractor Estimating: 347it [03:46,  1.67it/s]Extractor Estimating: 348it [03:47,  1.49it/s]Extractor Estimating: 349it [03:48,  1.48it/s]Extractor Estimating: 350it [03:48,  1.54it/s]Extractor Estimating: 351it [03:49,  1.56it/s]Extractor Estimating: 352it [03:49,  1.51it/s]Extractor Estimating: 353it [03:50,  1.51it/s]Extractor Estimating: 354it [03:51,  1.56it/s]Extractor Estimating: 355it [03:51,  1.60it/s]Extractor Estimating: 356it [03:52,  1.61it/s]Extractor Estimating: 357it [03:53,  1.61it/s]Extractor Estimating: 358it [03:53,  1.60it/s]Extractor Estimating: 359it [03:54,  1.62it/s]Extractor Estimating: 360it [03:54,  1.55it/s]Extractor Estimating: 361it [03:55,  1.52it/s]Extractor Estimating: 362it [03:56,  1.55it/s]Extractor Estimating: 363it [03:56,  1.57it/s]Extractor Estimating: 364it [03:57,  1.56it/s]Extractor Estimating: 365it [03:58,  1.54it/s]Extractor Estimating: 366it [03:58,  1.56it/s]Extractor Estimating: 367it [03:59,  1.50it/s]Extractor Estimating: 368it [04:00,  1.49it/s]Extractor Estimating: 369it [04:01,  1.35it/s]Extractor Estimating: 370it [04:01,  1.38it/s]Extractor Estimating: 371it [04:02,  1.42it/s]Extractor Estimating: 372it [04:03,  1.48it/s]Extractor Estimating: 373it [04:03,  1.52it/s]Extractor Estimating: 374it [04:04,  1.57it/s]Extractor Estimating: 375it [04:05,  1.49it/s]Extractor Estimating: 376it [04:05,  1.51it/s]Extractor Estimating: 377it [04:06,  1.51it/s]Extractor Estimating: 378it [04:06,  1.52it/s]Extractor Estimating: 379it [04:07,  1.56it/s]Extractor Estimating: 380it [04:08,  1.58it/s]Extractor Estimating: 381it [04:08,  1.53it/s]Extractor Estimating: 382it [04:09,  1.56it/s]Extractor Estimating: 383it [04:10,  1.53it/s]Extractor Estimating: 384it [04:10,  1.50it/s]Extractor Estimating: 385it [04:11,  1.48it/s]Extractor Estimating: 386it [04:12,  1.50it/s]Extractor Estimating: 387it [04:12,  1.49it/s]Extractor Estimating: 388it [04:13,  1.52it/s]Extractor Estimating: 389it [04:14,  1.48it/s]Extractor Estimating: 390it [04:14,  1.48it/s]Extractor Estimating: 391it [04:15,  1.44it/s]Extractor Estimating: 392it [04:16,  1.46it/s]Extractor Estimating: 393it [04:17,  1.44it/s]Extractor Estimating: 394it [04:17,  1.49it/s]Extractor Estimating: 395it [04:18,  1.52it/s]Extractor Estimating: 396it [04:19,  1.45it/s]Extractor Estimating: 397it [04:19,  1.47it/s]Extractor Estimating: 398it [04:20,  1.43it/s]Extractor Estimating: 399it [04:21,  1.46it/s]Extractor Estimating: 400it [04:21,  1.50it/s]Extractor Estimating: 401it [04:22,  1.53it/s]Extractor Estimating: 402it [04:23,  1.51it/s]Extractor Estimating: 403it [04:23,  1.58it/s]Extractor Estimating: 404it [04:24,  1.55it/s]Extractor Estimating: 405it [04:24,  1.60it/s]Extractor Estimating: 406it [04:25,  1.58it/s]Extractor Estimating: 407it [04:26,  1.57it/s]Extractor Estimating: 408it [04:26,  1.61it/s]Extractor Estimating: 409it [04:27,  1.60it/s]Extractor Estimating: 410it [04:28,  1.60it/s]Extractor Estimating: 411it [04:28,  1.57it/s]Extractor Estimating: 412it [04:29,  1.61it/s]Extractor Estimating: 413it [04:29,  1.60it/s]Extractor Estimating: 414it [04:30,  1.57it/s]Extractor Estimating: 415it [04:31,  1.57it/s]Extractor Estimating: 416it [04:31,  1.49it/s]Extractor Estimating: 417it [04:32,  1.56it/s]Extractor Estimating: 418it [04:33,  1.53it/s]Extractor Estimating: 419it [04:33,  1.57it/s]Extractor Estimating: 420it [04:34,  1.55it/s]Extractor Estimating: 421it [04:35,  1.52it/s]Extractor Estimating: 422it [04:35,  1.49it/s]Extractor Estimating: 423it [04:36,  1.48it/s]Extractor Estimating: 424it [04:37,  1.54it/s]Extractor Estimating: 425it [04:37,  1.53it/s]Extractor Estimating: 426it [04:38,  1.53it/s]Extractor Estimating: 427it [04:39,  1.56it/s]Extractor Estimating: 428it [04:39,  1.56it/s]Extractor Estimating: 429it [04:40,  1.61it/s]Extractor Estimating: 430it [04:40,  1.56it/s]Extractor Estimating: 431it [04:41,  1.50it/s]Extractor Estimating: 432it [04:42,  1.52it/s]Extractor Estimating: 433it [04:43,  1.48it/s]Extractor Estimating: 434it [04:43,  1.46it/s]Extractor Estimating: 435it [04:44,  1.46it/s]Extractor Estimating: 436it [04:45,  1.49it/s]Extractor Estimating: 437it [04:45,  1.53it/s]Extractor Estimating: 438it [04:46,  1.52it/s]Extractor Estimating: 439it [04:47,  1.49it/s]Extractor Estimating: 440it [04:47,  1.49it/s]Extractor Estimating: 441it [04:48,  1.31it/s]Extractor Estimating: 442it [04:49,  1.37it/s]Extractor Estimating: 443it [04:50,  1.41it/s]Extractor Estimating: 444it [04:50,  1.42it/s]Extractor Estimating: 445it [04:51,  1.41it/s]Extractor Estimating: 446it [04:52,  1.39it/s]Extractor Estimating: 447it [04:52,  1.40it/s]Extractor Estimating: 448it [04:53,  1.39it/s]Extractor Estimating: 449it [04:54,  1.38it/s]Extractor Estimating: 450it [04:54,  1.45it/s]Extractor Estimating: 451it [04:55,  1.54it/s]Extractor Estimating: 452it [04:56,  1.51it/s]Extractor Estimating: 453it [04:56,  1.49it/s]Extractor Estimating: 454it [04:57,  1.52it/s]Extractor Estimating: 455it [04:58,  1.55it/s]Extractor Estimating: 456it [04:58,  1.58it/s]Extractor Estimating: 457it [04:59,  1.56it/s]Extractor Estimating: 458it [05:00,  1.53it/s]Extractor Estimating: 459it [05:00,  1.58it/s]Extractor Estimating: 460it [05:01,  1.61it/s]Extractor Estimating: 461it [05:01,  1.59it/s]Extractor Estimating: 462it [05:02,  1.55it/s]Extractor Estimating: 463it [05:03,  1.57it/s]Extractor Estimating: 464it [05:03,  1.57it/s]Extractor Estimating: 465it [05:04,  1.53it/s]Extractor Estimating: 466it [05:05,  1.60it/s]Extractor Estimating: 467it [05:05,  1.54it/s]Extractor Estimating: 468it [05:06,  1.57it/s]Extractor Estimating: 469it [05:06,  1.60it/s]Extractor Estimating: 470it [05:07,  1.60it/s]Extractor Estimating: 471it [05:08,  1.61it/s]Extractor Estimating: 472it [05:08,  1.52it/s]Extractor Estimating: 473it [05:09,  1.53it/s]Extractor Estimating: 474it [05:10,  1.56it/s]Extractor Estimating: 475it [05:10,  1.59it/s]Extractor Estimating: 476it [05:11,  1.49it/s]Extractor Estimating: 477it [05:12,  1.48it/s]Extractor Estimating: 478it [05:12,  1.52it/s]Extractor Estimating: 479it [05:13,  1.51it/s]Extractor Estimating: 480it [05:14,  1.50it/s]Extractor Estimating: 481it [05:14,  1.49it/s]Extractor Estimating: 482it [05:15,  1.52it/s]Extractor Estimating: 483it [05:16,  1.53it/s]Extractor Estimating: 484it [05:16,  1.54it/s]Extractor Estimating: 485it [05:17,  1.52it/s]Extractor Estimating: 486it [05:18,  1.55it/s]Extractor Estimating: 487it [05:18,  1.51it/s]Extractor Estimating: 488it [05:19,  1.51it/s]Extractor Estimating: 489it [05:20,  1.53it/s]Extractor Estimating: 490it [05:20,  1.57it/s]Extractor Estimating: 491it [05:21,  1.57it/s]Extractor Estimating: 492it [05:22,  1.50it/s]Extractor Estimating: 493it [05:22,  1.52it/s]Extractor Estimating: 494it [05:23,  1.53it/s]Extractor Estimating: 495it [05:24,  1.54it/s]Extractor Estimating: 496it [05:24,  1.54it/s]Extractor Estimating: 497it [05:25,  1.44it/s]Extractor Estimating: 498it [05:26,  1.48it/s]Extractor Estimating: 499it [05:26,  1.50it/s]Extractor Estimating: 500it [05:27,  1.51it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,732 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:22:55,376 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:22:55,377 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:22:55,973 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:22:57,066 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:22:57,066 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:00,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:00,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:00,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:00,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:00,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:23:00,919 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:23:00,920 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:23:01,530 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:23:01,699 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:23:01,699 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:33:25,743 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:33:25,865 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 10290 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 22688
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22788, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22788, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.131, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.109, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.084, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.130, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 71, avg_time 1.116, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 171, avg_time 2.206, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 271, avg_time 1.101, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 371, avg_time 1.124, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 42, avg_time 1.133, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 142, avg_time 1.099, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 242, avg_time 2.204, loss:nan
g_step 1200, step 342, avg_time 1.110, loss:nan
g_step 1300, step 13, avg_time 1.114, loss:nan
g_step 1400, step 113, avg_time 1.094, loss:nan
g_step 1500, step 213, avg_time 1.130, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 313, avg_time 2.191, loss:nan
g_step 1700, step 413, avg_time 1.121, loss:nan
g_step 1800, step 84, avg_time 1.093, loss:nan
g_step 1900, step 184, avg_time 1.122, loss:nan
g_step 2000, step 284, avg_time 1.126, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 384, avg_time 2.209, loss:nan
g_step 2200, step 55, avg_time 1.093, loss:nan
g_step 2300, step 155, avg_time 1.075, loss:nan
g_step 2400, step 255, avg_time 1.145, loss:nan
g_step 2500, step 355, avg_time 1.119, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 26, avg_time 2.196, loss:nan
g_step 2700, step 126, avg_time 1.101, loss:nan
g_step 2800, step 226, avg_time 1.151, loss:nan
g_step 2900, step 326, avg_time 1.104, loss:nan
g_step 3000, step 426, avg_time 1.111, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 97, avg_time 2.199, loss:nan
g_step 3200, step 197, avg_time 1.108, loss:nan
g_step 3300, step 297, avg_time 1.097, loss:nan
g_step 3400, step 397, avg_time 1.120, loss:nan
g_step 3500, step 68, avg_time 1.134, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 168, avg_time 2.185, loss:nan
g_step 3700, step 268, avg_time 1.119, loss:nan
g_step 3800, step 368, avg_time 1.119, loss:nan
g_step 3900, step 39, avg_time 1.123, loss:nan
g_step 4000, step 139, avg_time 1.120, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 239, avg_time 2.193, loss:nan
g_step 4200, step 339, avg_time 1.107, loss:nan
g_step 4300, step 10, avg_time 1.129, loss:nan
g_step 4400, step 110, avg_time 1.098, loss:nan
g_step 4500, step 210, avg_time 1.088, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.215, loss:nan
g_step 4700, step 410, avg_time 1.112, loss:nan
g_step 4800, step 81, avg_time 1.125, loss:nan
g_step 4900, step 181, avg_time 1.138, loss:nan
g_step 5000, step 281, avg_time 1.121, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 381, avg_time 2.187, loss:nan
g_step 5200, step 52, avg_time 1.131, loss:nan
g_step 5300, step 152, avg_time 1.106, loss:nan
g_step 5400, step 252, avg_time 1.100, loss:nan
g_step 5500, step 352, avg_time 1.089, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 23, avg_time 2.229, loss:nan
g_step 5700, step 123, avg_time 1.119, loss:nan
g_step 5800, step 223, avg_time 1.125, loss:nan
g_step 5900, step 323, avg_time 1.101, loss:nan
g_step 6000, step 423, avg_time 1.118, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 94, avg_time 2.207, loss:nan
g_step 6200, step 194, avg_time 1.105, loss:nan
g_step 6300, step 294, avg_time 1.087, loss:nan
g_step 6400, step 394, avg_time 1.146, loss:nan
g_step 6500, step 65, avg_time 1.122, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 165, avg_time 2.192, loss:nan
g_step 6700, step 265, avg_time 1.099, loss:nan
g_step 6800, step 365, avg_time 1.142, loss:nan
g_step 6900, step 36, avg_time 1.130, loss:nan
g_step 7000, step 136, avg_time 1.122, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 236, avg_time 2.199, loss:nan
g_step 7200, step 336, avg_time 1.107, loss:nan
g_step 7300, step 7, avg_time 1.133, loss:nan
g_step 7400, step 107, avg_time 1.116, loss:nan
g_step 7500, step 207, avg_time 1.121, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 307, avg_time 2.204, loss:nan
g_step 7700, step 407, avg_time 1.118, loss:nan
g_step 7800, step 78, avg_time 1.139, loss:nan
g_step 7900, step 178, avg_time 1.119, loss:nan
g_step 8000, step 278, avg_time 1.091, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 378, avg_time 2.222, loss:nan
g_step 8200, step 49, avg_time 1.100, loss:nan
g_step 8300, step 149, avg_time 1.105, loss:nan
g_step 8400, step 249, avg_time 1.126, loss:nan
g_step 8500, step 349, avg_time 1.098, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:33:25 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:33:25 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-33-25_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:33:27 - WARNING - datasets.builder -   Using custom data configuration default-6087b5dc80b218a3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6087b5dc80b218a3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:33:32,453 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:33:32,489 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:33:32,490 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:33:32,491 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:33:32,591 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:32,641 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:32,641 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:32,641 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:32,641 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:32,641 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:32,642 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:33:33,173 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:33:36,431 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:33:36,431 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6087b5dc80b218a3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:06,  1.48ba/s] 18%|        | 2/11 [00:00<00:03,  2.49ba/s] 27%|       | 3/11 [00:01<00:02,  3.20ba/s] 36%|      | 4/11 [00:01<00:01,  3.65ba/s] 45%|     | 5/11 [00:01<00:01,  3.93ba/s] 55%|    | 6/11 [00:02<00:01,  2.73ba/s] 64%|   | 7/11 [00:02<00:01,  2.75ba/s] 73%|  | 8/11 [00:02<00:00,  3.15ba/s] 82%| | 9/11 [00:02<00:00,  3.50ba/s] 91%| | 10/11 [00:03<00:00,  3.78ba/s]100%|| 11/11 [00:03<00:00,  3.45ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:01<00:03,  1.24s/ba] 50%|     | 2/4 [00:01<00:01,  1.55ba/s] 75%|  | 3/4 [00:01<00:00,  2.20ba/s]100%|| 4/4 [00:01<00:00,  3.13ba/s]100%|| 4/4 [00:01<00:00,  2.21ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:09,  1.07ba/s] 18%|        | 2/11 [00:01<00:04,  2.03ba/s] 27%|       | 3/11 [00:01<00:02,  3.16ba/s] 45%|     | 5/11 [00:01<00:01,  5.23ba/s] 64%|   | 7/11 [00:01<00:00,  6.76ba/s] 82%| | 9/11 [00:01<00:00,  7.85ba/s]100%|| 11/11 [00:01<00:00,  9.61ba/s]100%|| 11/11 [00:01<00:00,  5.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.71ba/s] 50%|     | 2/4 [00:00<00:00,  4.53ba/s]100%|| 4/4 [00:00<00:00,  7.84ba/s]100%|| 4/4 [00:00<00:00,  6.35ba/s]
[INFO|trainer.py:414] 2023-08-28 17:33:56,450 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:33:58,186 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:33:58,186 >>   Num examples = 10300
[INFO|trainer.py:1149] 2023-08-28 17:33:58,186 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:33:58,186 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:33:58,186 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:33:58,186 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:33:58,186 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:03<52:10,  3.89s/it]  0%|          | 2/805 [00:04<28:38,  2.14s/it]  0%|          | 3/805 [00:05<21:34,  1.61s/it]  0%|          | 4/805 [00:06<18:35,  1.39s/it]  1%|          | 5/805 [00:07<13:59,  1.05s/it]  1%|          | 6/805 [00:07<10:49,  1.23it/s]  1%|          | 7/805 [00:08<09:34,  1.39it/s]  1%|          | 8/805 [00:08<09:10,  1.45it/s]  1%|          | 9/805 [00:09<07:28,  1.78it/s]  1%|          | 10/805 [00:09<06:43,  1.97it/s]  1%|         | 11/805 [00:09<05:47,  2.28it/s]  1%|         | 12/805 [00:10<05:40,  2.33it/s]  2%|         | 13/805 [00:10<05:04,  2.60it/s]  2%|         | 14/805 [00:11<05:58,  2.20it/s]  2%|         | 15/805 [00:11<05:17,  2.49it/s]  2%|         | 16/805 [00:11<05:39,  2.32it/s]  2%|         | 17/805 [00:12<05:03,  2.60it/s]  2%|         | 18/805 [00:12<04:38,  2.83it/s]  2%|         | 19/805 [00:12<04:20,  3.02it/s]  2%|         | 20/805 [00:12<04:08,  3.16it/s]  3%|         | 21/805 [00:13<03:59,  3.27it/s]  3%|         | 22/805 [00:13<04:20,  3.00it/s]  3%|         | 23/805 [00:14<05:00,  2.60it/s]  3%|         | 24/805 [00:14<04:35,  2.83it/s]  3%|         | 25/805 [00:14<04:18,  3.01it/s]  3%|         | 26/805 [00:15<05:11,  2.50it/s]  3%|         | 27/805 [00:15<04:43,  2.74it/s]  3%|         | 28/805 [00:15<04:23,  2.94it/s]  4%|         | 29/805 [00:16<04:09,  3.11it/s]  4%|         | 30/805 [00:16<04:00,  3.23it/s]  4%|         | 31/805 [00:17<05:58,  2.16it/s]  4%|         | 32/805 [00:17<05:47,  2.23it/s]  4%|         | 33/805 [00:17<05:07,  2.51it/s]  4%|         | 34/805 [00:18<04:40,  2.75it/s]  4%|         | 35/805 [00:18<04:20,  2.95it/s]  4%|         | 36/805 [00:18<04:07,  3.11it/s]  5%|         | 37/805 [00:19<03:57,  3.23it/s]  5%|         | 38/805 [00:19<03:50,  3.32it/s]  5%|         | 39/805 [00:19<03:46,  3.39it/s]  5%|         | 40/805 [00:19<03:42,  3.44it/s]  5%|         | 41/805 [00:20<03:39,  3.47it/s]  5%|         | 42/805 [00:20<03:37,  3.50it/s]  5%|         | 43/805 [00:21<06:01,  2.11it/s]  5%|         | 44/805 [00:21<05:16,  2.40it/s]  6%|         | 45/805 [00:21<04:45,  2.66it/s]  6%|         | 46/805 [00:22<04:23,  2.88it/s]  6%|         | 47/805 [00:22<04:08,  3.05it/s]  6%|         | 48/805 [00:22<03:57,  3.19it/s]  6%|         | 49/805 [00:23<03:49,  3.29it/s]  6%|         | 50/805 [00:23<03:44,  3.36it/s]  6%|         | 51/805 [00:23<03:40,  3.41it/s]  6%|         | 52/805 [00:24<04:16,  2.93it/s]  7%|         | 53/805 [00:24<04:02,  3.10it/s]  7%|         | 54/805 [00:24<03:52,  3.22it/s]  7%|         | 55/805 [00:24<03:46,  3.32it/s]  7%|         | 56/805 [00:25<03:41,  3.39it/s]  7%|         | 57/805 [00:25<03:37,  3.44it/s]  7%|         | 58/805 [00:25<03:35,  3.47it/s]  7%|         | 59/805 [00:26<03:33,  3.49it/s]  7%|         | 60/805 [00:26<03:32,  3.51it/s]  8%|         | 61/805 [00:26<03:31,  3.52it/s]  8%|         | 62/805 [00:26<03:30,  3.53it/s]  8%|         | 63/805 [00:27<06:25,  1.93it/s]  8%|         | 64/805 [00:28<05:32,  2.23it/s]  8%|         | 65/805 [00:28<04:55,  2.51it/s]  8%|         | 66/805 [00:29<06:09,  2.00it/s]  8%|         | 67/805 [00:29<06:06,  2.02it/s]  8%|         | 68/805 [00:30<05:18,  2.32it/s]  9%|         | 69/805 [00:30<04:44,  2.59it/s]  9%|         | 70/805 [00:30<04:20,  2.82it/s]  9%|         | 71/805 [00:30<04:04,  3.00it/s]  9%|         | 72/805 [00:31<03:52,  3.15it/s]  9%|         | 73/805 [00:31<03:44,  3.26it/s]  9%|         | 74/805 [00:31<03:38,  3.34it/s]  9%|         | 75/805 [00:31<03:34,  3.40it/s]  9%|         | 76/805 [00:32<03:31,  3.44it/s] 10%|         | 77/805 [00:32<04:42,  2.58it/s] 10%|         | 78/805 [00:33<04:19,  2.80it/s] 10%|         | 79/805 [00:33<04:02,  2.99it/s] 10%|         | 80/805 [00:34<05:34,  2.17it/s] 10%|         | 81/805 [00:34<04:55,  2.45it/s] 10%|         | 82/805 [00:34<04:27,  2.70it/s] 10%|         | 83/805 [00:35<04:07,  2.91it/s] 10%|         | 84/805 [00:35<03:54,  3.08it/s] 11%|         | 85/805 [00:35<03:59,  3.01it/s] 11%|         | 86/805 [00:35<03:48,  3.15it/s] 11%|         | 87/805 [00:36<03:40,  3.26it/s] 11%|         | 88/805 [00:36<03:34,  3.34it/s] 11%|         | 89/805 [00:37<04:25,  2.69it/s] 11%|         | 90/805 [00:37<04:07,  2.89it/s] 11%|        | 91/805 [00:37<03:53,  3.06it/s] 11%|        | 92/805 [00:37<03:43,  3.19it/s] 12%|        | 93/805 [00:38<03:36,  3.28it/s] 12%|        | 94/805 [00:38<03:31,  3.36it/s] 12%|        | 95/805 [00:38<03:37,  3.27it/s] 12%|        | 96/805 [00:39<03:31,  3.34it/s] 12%|        | 97/805 [00:39<03:28,  3.40it/s] 12%|        | 98/805 [00:39<03:25,  3.44it/s] 12%|        | 99/805 [00:39<03:23,  3.47it/s] 12%|        | 100/805 [00:40<03:22,  3.49it/s] 13%|        | 101/805 [00:40<03:20,  3.51it/s] 13%|        | 102/805 [00:40<03:19,  3.52it/s] 13%|        | 103/805 [00:41<03:18,  3.53it/s] 13%|        | 104/805 [00:41<03:18,  3.54it/s] 13%|        | 105/805 [00:41<03:17,  3.54it/s] 13%|        | 106/805 [00:42<03:48,  3.06it/s] 13%|        | 107/805 [00:42<03:39,  3.19it/s] 13%|        | 108/805 [00:42<03:32,  3.29it/s] 14%|        | 109/805 [00:42<03:26,  3.36it/s] 14%|        | 110/805 [00:43<03:23,  3.42it/s] 14%|        | 111/805 [00:43<03:20,  3.46it/s] 14%|        | 112/805 [00:43<03:19,  3.48it/s] 14%|        | 113/805 [00:44<03:17,  3.50it/s] 14%|        | 114/805 [00:44<03:16,  3.51it/s] 14%|        | 115/805 [00:44<03:16,  3.52it/s] 14%|        | 116/805 [00:44<03:15,  3.53it/s] 15%|        | 117/805 [00:45<03:38,  3.14it/s] 15%|        | 118/805 [00:45<03:31,  3.25it/s] 15%|        | 119/805 [00:45<03:25,  3.34it/s] 15%|        | 120/805 [00:46<03:21,  3.40it/s] 15%|        | 121/805 [00:46<03:18,  3.44it/s] 15%|        | 122/805 [00:46<03:16,  3.47it/s] 15%|        | 123/805 [00:46<03:15,  3.49it/s] 15%|        | 124/805 [00:47<03:14,  3.51it/s] 16%|        | 125/805 [00:47<03:13,  3.51it/s] 16%|        | 126/805 [00:47<03:12,  3.52it/s] 16%|        | 127/805 [00:48<03:12,  3.53it/s] 16%|        | 128/805 [00:48<04:33,  2.47it/s] 16%|        | 129/805 [00:49<04:08,  2.72it/s] 16%|        | 130/805 [00:49<03:50,  2.92it/s] 16%|        | 131/805 [00:49<03:38,  3.08it/s] 16%|        | 132/805 [00:49<03:29,  3.21it/s] 17%|        | 133/805 [00:50<03:23,  3.30it/s] 17%|        | 134/805 [00:50<03:18,  3.37it/s] 17%|        | 135/805 [00:50<03:15,  3.42it/s] 17%|        | 136/805 [00:51<03:13,  3.46it/s] 17%|        | 137/805 [00:51<03:11,  3.48it/s] 17%|        | 138/805 [00:51<03:49,  2.91it/s] 17%|        | 139/805 [00:52<03:36,  3.07it/s] 17%|        | 140/805 [00:52<03:27,  3.20it/s] 18%|        | 141/805 [00:52<03:21,  3.29it/s] 18%|        | 142/805 [00:52<03:17,  3.36it/s] 18%|        | 143/805 [00:53<03:13,  3.41it/s] 18%|        | 144/805 [00:53<03:11,  3.45it/s] 18%|        | 145/805 [00:53<03:09,  3.48it/s] 18%|        | 146/805 [00:54<03:08,  3.50it/s] 18%|        | 147/805 [00:54<03:07,  3.51it/s] 18%|        | 148/805 [00:54<03:20,  3.27it/s] 19%|        | 149/805 [00:55<04:12,  2.60it/s] 19%|        | 150/805 [00:55<04:07,  2.65it/s] 19%|        | 151/805 [00:55<03:48,  2.87it/s] 19%|        | 152/805 [00:56<03:34,  3.05it/s] 19%|        | 153/805 [00:56<03:24,  3.19it/s] 19%|        | 154/805 [00:56<03:17,  3.30it/s] 19%|        | 155/805 [00:57<03:12,  3.38it/s] 19%|        | 156/805 [00:57<03:34,  3.02it/s] 20%|        | 157/805 [00:57<03:40,  2.94it/s] 20%|        | 158/805 [00:58<03:27,  3.11it/s] 20%|        | 159/805 [00:58<03:19,  3.25it/s] 20%|        | 160/805 [00:58<03:13,  3.34it/s] 20%|        | 161/805 [00:58<03:06,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 17:34:57,086 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:34:57,086 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:34:57,086 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.45it/s][A
  3%|         | 12/436 [00:00<00:08, 48.94it/s][A
  4%|         | 17/436 [00:00<00:08, 47.35it/s][A
  5%|         | 22/436 [00:00<00:08, 46.61it/s][A
  6%|         | 27/436 [00:00<00:08, 46.06it/s][A
  7%|         | 32/436 [00:00<00:08, 45.60it/s][A
  8%|         | 37/436 [00:00<00:08, 45.06it/s][A
 10%|         | 42/436 [00:01<00:08, 44.68it/s][A
 11%|         | 47/436 [00:01<00:19, 19.58it/s][A
 12%|        | 51/436 [00:01<00:19, 20.25it/s][A
 13%|        | 56/436 [00:01<00:15, 24.57it/s][A
 14%|        | 60/436 [00:02<00:18, 20.67it/s][A
 15%|        | 65/436 [00:02<00:14, 25.05it/s][A
 16%|        | 70/436 [00:02<00:12, 29.10it/s][A
 17%|        | 75/436 [00:02<00:11, 32.73it/s][A
 18%|        | 80/436 [00:02<00:09, 35.77it/s][A
 19%|        | 85/436 [00:02<00:09, 38.19it/s][A
 21%|        | 90/436 [00:02<00:08, 40.09it/s][A
 22%|       | 95/436 [00:02<00:08, 41.52it/s][A
 23%|       | 100/436 [00:02<00:07, 42.09it/s][A
 24%|       | 105/436 [00:03<00:15, 21.78it/s][A
 25%|       | 110/436 [00:03<00:12, 26.02it/s][A
 26%|       | 115/436 [00:03<00:10, 29.76it/s][A
 28%|       | 120/436 [00:03<00:09, 33.19it/s][A
 29%|       | 125/436 [00:04<00:08, 36.11it/s][A
 30%|       | 130/436 [00:04<00:12, 25.10it/s][A
 31%|       | 135/436 [00:04<00:10, 28.89it/s][A
 32%|      | 140/436 [00:04<00:09, 32.42it/s][A
 33%|      | 145/436 [00:04<00:08, 35.44it/s][A
 34%|      | 150/436 [00:04<00:07, 37.89it/s][A
 36%|      | 155/436 [00:04<00:07, 39.85it/s][A
 37%|      | 160/436 [00:04<00:06, 41.28it/s][A
 38%|      | 165/436 [00:04<00:06, 42.32it/s][A
 39%|      | 170/436 [00:05<00:06, 42.66it/s][A
 40%|      | 175/436 [00:05<00:09, 28.94it/s][A
 41%|     | 180/436 [00:05<00:07, 32.53it/s][A
 42%|     | 184/436 [00:05<00:09, 27.03it/s][A
 43%|     | 189/436 [00:05<00:07, 30.97it/s][A
 44%|     | 194/436 [00:05<00:07, 34.25it/s][A
 46%|     | 199/436 [00:06<00:06, 37.00it/s][A
 47%|     | 204/436 [00:06<00:05, 39.12it/s][A
 48%|     | 209/436 [00:06<00:05, 40.82it/s][A
 49%|     | 214/436 [00:06<00:05, 42.11it/s][A
 50%|     | 219/436 [00:06<00:05, 43.04it/s][A
 51%|    | 224/436 [00:06<00:04, 43.20it/s][A
 53%|    | 229/436 [00:06<00:04, 43.30it/s][A
 54%|    | 234/436 [00:07<00:06, 30.29it/s][A
 55%|    | 238/436 [00:07<00:07, 26.65it/s][A
 56%|    | 243/436 [00:07<00:06, 30.63it/s][A
 57%|    | 248/436 [00:07<00:05, 34.04it/s][A
 58%|    | 253/436 [00:07<00:04, 36.84it/s][A
 59%|    | 258/436 [00:07<00:04, 39.02it/s][A
 60%|    | 263/436 [00:07<00:04, 40.73it/s][A
 61%|   | 268/436 [00:07<00:03, 42.02it/s][A
 63%|   | 273/436 [00:07<00:03, 42.82it/s][A
 64%|   | 278/436 [00:08<00:03, 43.04it/s][A
 65%|   | 283/436 [00:08<00:03, 43.20it/s][A
 66%|   | 288/436 [00:08<00:03, 43.50it/s][A
 67%|   | 293/436 [00:08<00:03, 43.85it/s][A
 68%|   | 298/436 [00:09<00:07, 19.68it/s][A
 69%|   | 303/436 [00:09<00:05, 23.89it/s][A
 70%|   | 307/436 [00:09<00:06, 20.81it/s][A
 72%|  | 312/436 [00:09<00:04, 25.13it/s][A
 73%|  | 317/436 [00:09<00:04, 29.11it/s][A
 74%|  | 322/436 [00:09<00:03, 32.71it/s][A
 75%|  | 327/436 [00:09<00:03, 35.69it/s][A
 76%|  | 332/436 [00:09<00:02, 38.17it/s][A
 77%|  | 337/436 [00:10<00:02, 40.10it/s][A
 78%|  | 342/436 [00:10<00:04, 19.46it/s][A
 80%|  | 347/436 [00:10<00:03, 23.43it/s][A
 81%|  | 352/436 [00:10<00:03, 27.39it/s][A
 82%| | 357/436 [00:10<00:02, 31.11it/s][A
 83%| | 362/436 [00:11<00:02, 34.36it/s][A
 84%| | 367/436 [00:11<00:01, 37.01it/s][A
 85%| | 372/436 [00:11<00:01, 39.19it/s][A
 86%| | 377/436 [00:11<00:01, 40.64it/s][A
 88%| | 382/436 [00:11<00:01, 41.45it/s][A
 89%| | 387/436 [00:11<00:01, 42.11it/s][A
 90%| | 392/436 [00:11<00:01, 42.75it/s][A
 91%| | 397/436 [00:11<00:00, 43.28it/s][A
 92%|| 402/436 [00:11<00:00, 43.86it/s][A
 93%|| 407/436 [00:12<00:00, 44.19it/s][A
 94%|| 412/436 [00:12<00:00, 44.59it/s][A
 96%|| 417/436 [00:12<00:00, 44.82it/s][A
 97%|| 422/436 [00:12<00:00, 44.63it/s][A
 98%|| 427/436 [00:12<00:00, 44.41it/s][A
 99%|| 432/436 [00:12<00:00, 44.22it/s][A
                                                 [A                                                 
100%|| 436/436 [00:12<00:00, 44.22it/s][A 20%|        | 161/805 [01:11<03:06,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:35:10,601 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 17:35:10,929 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:35:25,271 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:35:25,441 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:35:25,529 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161/special_tokens_map.json
 20%|        | 162/805 [01:30<1:45:19,  9.83s/it] 20%|        | 163/805 [01:31<1:14:30,  6.96s/it] 20%|        | 164/805 [01:31<52:58,  4.96s/it]   20%|        | 165/805 [01:31<37:54,  3.55s/it] 21%|        | 166/805 [01:32<27:22,  2.57s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|        | 167/805 [01:32<20:01,  1.88s/it] 21%|        | 168/805 [01:32<14:52,  1.40s/it] 21%|        | 169/805 [01:33<12:12,  1.15s/it] 21%|        | 170/805 [01:33<09:25,  1.12it/s] 21%|        | 171/805 [01:34<09:46,  1.08it/s] 21%|       | 172/805 [01:34<07:43,  1.37it/s] 21%|       | 173/805 [01:35<06:16,  1.68it/s] 22%|       | 174/805 [01:35<05:16,  2.00it/s] 22%|       | 175/805 [01:35<04:33,  2.30it/s] 22%|       | 176/805 [01:35<04:03,  2.58it/s] 22%|       | 177/805 [01:36<03:42,  2.82it/s] 22%|       | 178/805 [01:36<03:27,  3.02it/s] 22%|       | 179/805 [01:36<03:17,  3.17it/s] 22%|       | 180/805 [01:37<04:39,  2.24it/s] 22%|       | 181/805 [01:37<04:07,  2.52it/s] 23%|       | 182/805 [01:38<03:44,  2.77it/s] 23%|       | 183/805 [01:38<03:28,  2.98it/s] 23%|       | 184/805 [01:38<03:18,  3.14it/s] 23%|       | 185/805 [01:38<03:09,  3.26it/s] 23%|       | 186/805 [01:39<03:04,  3.36it/s] 23%|       | 187/805 [01:39<03:00,  3.43it/s] 23%|       | 188/805 [01:39<02:57,  3.47it/s] 23%|       | 189/805 [01:39<02:55,  3.51it/s] 24%|       | 190/805 [01:41<05:19,  1.93it/s] 24%|       | 191/805 [01:41<04:34,  2.24it/s] 24%|       | 192/805 [01:41<04:03,  2.52it/s] 24%|       | 193/805 [01:41<03:41,  2.77it/s] 24%|       | 194/805 [01:42<03:25,  2.97it/s] 24%|       | 195/805 [01:42<03:14,  3.13it/s] 24%|       | 196/805 [01:42<03:07,  3.26it/s] 24%|       | 197/805 [01:43<03:01,  3.35it/s] 25%|       | 198/805 [01:43<03:06,  3.26it/s] 25%|       | 199/805 [01:43<03:00,  3.35it/s] 25%|       | 200/805 [01:43<02:56,  3.42it/s] 25%|       | 201/805 [01:44<02:54,  3.47it/s] 25%|       | 202/805 [01:44<02:52,  3.50it/s] 25%|       | 203/805 [01:44<02:50,  3.52it/s] 25%|       | 204/805 [01:45<02:49,  3.54it/s] 25%|       | 205/805 [01:45<02:48,  3.56it/s] 26%|       | 206/805 [01:45<02:47,  3.57it/s] 26%|       | 207/805 [01:45<02:47,  3.58it/s] 26%|       | 208/805 [01:46<02:46,  3.58it/s] 26%|       | 209/805 [01:46<03:46,  2.63it/s] 26%|       | 210/805 [01:47<03:27,  2.86it/s] 26%|       | 211/805 [01:47<03:14,  3.05it/s] 26%|       | 212/805 [01:47<03:05,  3.19it/s] 26%|       | 213/805 [01:47<02:59,  3.30it/s] 27%|       | 214/805 [01:48<02:54,  3.38it/s] 27%|       | 215/805 [01:48<02:51,  3.43it/s] 27%|       | 216/805 [01:48<02:49,  3.48it/s] 27%|       | 217/805 [01:48<02:47,  3.50it/s] 27%|       | 218/805 [01:49<02:46,  3.53it/s] 27%|       | 219/805 [01:49<04:03,  2.41it/s] 27%|       | 220/805 [01:50<03:38,  2.67it/s] 27%|       | 221/805 [01:50<03:21,  2.89it/s] 28%|       | 222/805 [01:50<03:09,  3.07it/s] 28%|       | 223/805 [01:51<03:01,  3.21it/s] 28%|       | 224/805 [01:51<02:55,  3.31it/s] 28%|       | 225/805 [01:51<02:51,  3.39it/s] 28%|       | 226/805 [01:51<02:48,  3.44it/s] 28%|       | 227/805 [01:52<02:45,  3.48it/s] 28%|       | 228/805 [01:52<02:44,  3.51it/s] 28%|       | 229/805 [01:53<03:40,  2.62it/s] 29%|       | 230/805 [01:53<03:21,  2.85it/s] 29%|       | 231/805 [01:53<03:09,  3.03it/s] 29%|       | 232/805 [01:53<03:00,  3.18it/s] 29%|       | 233/805 [01:54<02:53,  3.29it/s] 29%|       | 234/805 [01:54<02:49,  3.37it/s] 29%|       | 235/805 [01:54<02:46,  3.43it/s] 29%|       | 236/805 [01:55<02:43,  3.48it/s] 29%|       | 237/805 [01:55<03:07,  3.03it/s] 30%|       | 238/805 [01:55<02:58,  3.17it/s] 30%|       | 239/805 [01:56<03:00,  3.13it/s] 30%|       | 240/805 [01:56<03:29,  2.70it/s] 30%|       | 241/805 [01:56<03:13,  2.92it/s] 30%|       | 242/805 [01:57<03:02,  3.09it/s] 30%|       | 243/805 [01:57<02:54,  3.23it/s] 30%|       | 244/805 [01:57<02:48,  3.33it/s] 30%|       | 245/805 [01:57<02:44,  3.40it/s] 31%|       | 246/805 [01:58<02:41,  3.46it/s] 31%|       | 247/805 [01:58<02:39,  3.50it/s] 31%|       | 248/805 [01:58<02:38,  3.53it/s] 31%|       | 249/805 [01:59<02:36,  3.54it/s] 31%|       | 250/805 [01:59<03:17,  2.81it/s] 31%|       | 251/805 [01:59<03:04,  3.01it/s] 31%|      | 252/805 [02:00<02:54,  3.16it/s] 31%|      | 253/805 [02:00<02:48,  3.28it/s] 32%|      | 254/805 [02:00<02:43,  3.37it/s] 32%|      | 255/805 [02:00<02:40,  3.43it/s] 32%|      | 256/805 [02:01<02:37,  3.48it/s] 32%|      | 257/805 [02:01<02:42,  3.37it/s] 32%|      | 258/805 [02:01<02:39,  3.43it/s] 32%|      | 259/805 [02:02<02:37,  3.48it/s] 32%|      | 260/805 [02:02<02:35,  3.51it/s] 32%|      | 261/805 [02:02<02:33,  3.54it/s] 33%|      | 262/805 [02:02<02:32,  3.55it/s] 33%|      | 263/805 [02:03<02:32,  3.56it/s] 33%|      | 264/805 [02:03<02:31,  3.57it/s] 33%|      | 265/805 [02:03<02:31,  3.58it/s] 33%|      | 266/805 [02:04<02:30,  3.58it/s] 33%|      | 267/805 [02:04<02:30,  3.58it/s] 33%|      | 268/805 [02:04<02:51,  3.12it/s] 33%|      | 269/805 [02:05<02:44,  3.25it/s] 34%|      | 270/805 [02:05<02:39,  3.35it/s] 34%|      | 271/805 [02:05<02:36,  3.42it/s] 34%|      | 272/805 [02:05<02:33,  3.47it/s] 34%|      | 273/805 [02:06<02:31,  3.51it/s] 34%|      | 274/805 [02:06<02:30,  3.53it/s] 34%|      | 275/805 [02:06<03:08,  2.81it/s] 34%|      | 276/805 [02:07<02:55,  3.01it/s] 34%|      | 277/805 [02:07<02:46,  3.16it/s] 35%|      | 278/805 [02:07<02:40,  3.28it/s] 35%|      | 279/805 [02:08<02:36,  3.37it/s] 35%|      | 280/805 [02:08<02:33,  3.43it/s] 35%|      | 281/805 [02:08<02:30,  3.47it/s] 35%|      | 282/805 [02:08<02:29,  3.50it/s] 35%|      | 283/805 [02:09<02:27,  3.53it/s] 35%|      | 284/805 [02:09<02:27,  3.54it/s] 35%|      | 285/805 [02:10<03:22,  2.57it/s] 36%|      | 286/805 [02:10<03:04,  2.81it/s] 36%|      | 287/805 [02:10<02:52,  3.00it/s] 36%|      | 288/805 [02:10<02:43,  3.16it/s] 36%|      | 289/805 [02:11<02:37,  3.27it/s] 36%|      | 290/805 [02:11<02:33,  3.36it/s] 36%|      | 291/805 [02:11<02:30,  3.42it/s] 36%|      | 292/805 [02:12<02:27,  3.47it/s] 36%|      | 293/805 [02:12<02:25,  3.51it/s] 37%|      | 294/805 [02:12<02:24,  3.53it/s] 37%|      | 295/805 [02:13<02:47,  3.05it/s] 37%|      | 296/805 [02:13<02:39,  3.19it/s] 37%|      | 297/805 [02:13<03:16,  2.58it/s] 37%|      | 298/805 [02:14<03:00,  2.81it/s] 37%|      | 299/805 [02:14<04:04,  2.07it/s] 37%|      | 300/805 [02:15<03:33,  2.36it/s] 37%|      | 301/805 [02:15<03:11,  2.64it/s] 38%|      | 302/805 [02:15<02:55,  2.86it/s] 38%|      | 303/805 [02:16<04:17,  1.95it/s] 38%|      | 304/805 [02:17<03:42,  2.26it/s] 38%|      | 305/805 [02:17<03:16,  2.54it/s] 38%|      | 306/805 [02:17<02:59,  2.78it/s] 38%|      | 307/805 [02:17<02:46,  2.98it/s] 38%|      | 308/805 [02:18<02:56,  2.81it/s] 38%|      | 309/805 [02:18<02:45,  2.99it/s] 39%|      | 310/805 [02:18<02:37,  3.15it/s] 39%|      | 311/805 [02:19<02:31,  3.27it/s] 39%|      | 312/805 [02:19<02:50,  2.89it/s] 39%|      | 313/805 [02:19<02:40,  3.07it/s] 39%|      | 314/805 [02:20<02:33,  3.20it/s] 39%|      | 315/805 [02:20<02:28,  3.30it/s] 39%|      | 316/805 [02:20<02:24,  3.38it/s] 39%|      | 317/805 [02:20<02:21,  3.44it/s] 40%|      | 318/805 [02:21<02:20,  3.48it/s] 40%|      | 319/805 [02:21<02:18,  3.51it/s] 40%|      | 320/805 [02:21<02:17,  3.53it/s] 40%|      | 321/805 [02:22<02:16,  3.54it/s] 40%|      | 322/805 [02:22<02:14,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 17:36:21,085 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:36:21,085 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:36:21,085 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 12.7395, 'eval_samples_per_second': 273.324, 'eval_steps_per_second': 34.224, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.42it/s][A
  3%|         | 12/436 [00:00<00:08, 49.09it/s][A
  4%|         | 17/436 [00:00<00:08, 47.33it/s][A
  5%|         | 22/436 [00:00<00:08, 46.59it/s][A
  6%|         | 27/436 [00:00<00:08, 46.12it/s][A
  7%|         | 32/436 [00:00<00:08, 45.82it/s][A
  8%|         | 37/436 [00:00<00:08, 45.59it/s][A
 10%|         | 42/436 [00:00<00:08, 45.09it/s][A
 11%|         | 47/436 [00:01<00:08, 44.65it/s][A
 12%|        | 52/436 [00:01<00:08, 44.39it/s][A
 13%|        | 57/436 [00:01<00:08, 44.41it/s][A
 14%|        | 62/436 [00:01<00:08, 44.56it/s][A
 15%|        | 67/436 [00:01<00:08, 44.72it/s][A
 17%|        | 72/436 [00:01<00:08, 44.92it/s][A
 18%|        | 77/436 [00:01<00:07, 45.01it/s][A
 19%|        | 82/436 [00:01<00:07, 45.04it/s][A
 20%|        | 87/436 [00:01<00:07, 44.67it/s][A
 21%|        | 92/436 [00:02<00:07, 44.48it/s][A
 22%|       | 97/436 [00:02<00:07, 44.36it/s][A
 23%|       | 102/436 [00:02<00:07, 44.41it/s][A
 25%|       | 107/436 [00:02<00:07, 44.44it/s][A
 26%|       | 112/436 [00:03<00:07, 44.56it/s][A
 27%|       | 117/436 [00:03<00:18, 17.55it/s][A
 28%|       | 122/436 [00:03<00:14, 21.47it/s][A
 29%|       | 127/436 [00:03<00:12, 25.46it/s][A
 30%|       | 132/436 [00:03<00:10, 29.33it/s][A
 31%|      | 137/436 [00:03<00:09, 32.77it/s][A
 33%|      | 142/436 [00:03<00:08, 35.76it/s][A
 34%|      | 147/436 [00:03<00:07, 38.13it/s][A
 35%|      | 152/436 [00:03<00:07, 39.89it/s][A
 36%|      | 157/436 [00:04<00:06, 40.86it/s][A
 37%|      | 162/436 [00:04<00:06, 41.72it/s][A
 38%|      | 167/436 [00:04<00:06, 42.36it/s][A
 39%|      | 172/436 [00:04<00:06, 43.11it/s][A
 41%|      | 177/436 [00:04<00:05, 43.65it/s][A
 42%|     | 182/436 [00:04<00:05, 44.16it/s][A
 43%|     | 187/436 [00:04<00:05, 44.48it/s][A
 44%|     | 192/436 [00:04<00:05, 44.67it/s][A
 45%|     | 197/436 [00:04<00:05, 44.64it/s][A
 46%|     | 202/436 [00:05<00:05, 44.38it/s][A
 47%|     | 207/436 [00:05<00:05, 44.20it/s][A
 49%|     | 212/436 [00:05<00:05, 44.19it/s][A
 50%|     | 217/436 [00:05<00:04, 44.38it/s][A
 51%|     | 222/436 [00:05<00:04, 44.59it/s][A
 52%|    | 227/436 [00:05<00:07, 28.71it/s][A
 53%|    | 232/436 [00:05<00:06, 32.19it/s][A
 54%|    | 237/436 [00:06<00:05, 35.19it/s][A
 56%|    | 242/436 [00:06<00:05, 37.53it/s][A
 57%|    | 247/436 [00:06<00:04, 39.55it/s][A
 58%|    | 252/436 [00:06<00:05, 34.62it/s][A
 59%|    | 258/436 [00:06<00:04, 38.65it/s][A
 60%|    | 263/436 [00:06<00:04, 40.32it/s][A
 61%|   | 268/436 [00:06<00:04, 41.60it/s][A
 63%|   | 273/436 [00:06<00:03, 42.62it/s][A
 64%|   | 278/436 [00:07<00:03, 43.27it/s][A
 65%|   | 283/436 [00:07<00:03, 43.85it/s][A
 66%|   | 288/436 [00:07<00:03, 44.22it/s][A
 67%|   | 293/436 [00:07<00:03, 44.25it/s][A
 68%|   | 298/436 [00:07<00:03, 44.00it/s][A
 69%|   | 303/436 [00:07<00:03, 43.97it/s][A
 71%|   | 308/436 [00:07<00:02, 44.10it/s][A
 72%|  | 313/436 [00:07<00:02, 44.41it/s][A
 73%|  | 318/436 [00:07<00:02, 44.61it/s][A
 74%|  | 323/436 [00:08<00:02, 44.77it/s][A
 75%|  | 328/436 [00:08<00:02, 44.83it/s][A
 76%|  | 333/436 [00:08<00:02, 44.81it/s][A
 78%|  | 338/436 [00:08<00:02, 44.61it/s][A
 79%|  | 343/436 [00:08<00:02, 44.33it/s][A
 80%|  | 348/436 [00:08<00:01, 44.12it/s][A
 81%|  | 353/436 [00:08<00:02, 39.40it/s][A
 82%| | 358/436 [00:08<00:01, 41.04it/s][A
 83%| | 363/436 [00:08<00:01, 42.19it/s][A
 84%| | 368/436 [00:09<00:01, 43.09it/s][A
 86%| | 373/436 [00:09<00:01, 43.57it/s][A
 87%| | 378/436 [00:09<00:01, 44.12it/s][A
 88%| | 383/436 [00:09<00:01, 44.42it/s][A
 89%| | 388/436 [00:09<00:01, 44.40it/s][A
 90%| | 393/436 [00:09<00:00, 44.06it/s][A
 91%|| 398/436 [00:09<00:00, 44.00it/s][A
 92%|| 403/436 [00:09<00:00, 44.15it/s][A
 94%|| 408/436 [00:09<00:00, 44.37it/s][A
 95%|| 413/436 [00:10<00:00, 36.00it/s][A
 96%|| 417/436 [00:10<00:00, 31.66it/s][A
 97%|| 423/436 [00:10<00:00, 36.46it/s][A
 98%|| 428/436 [00:10<00:00, 38.70it/s][A
 99%|| 433/436 [00:10<00:00, 40.40it/s][A
                                                 [A                                                 
100%|| 436/436 [00:10<00:00, 40.40it/s][A 40%|      | 322/805 [02:33<02:14,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:36:32,193 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 17:36:32,807 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:36:46,556 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:36:48,161 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:36:48,422 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-322/special_tokens_map.json
 40%|      | 323/805 [02:57<1:25:40, 10.66s/it] 40%|      | 324/805 [02:57<1:00:36,  7.56s/it] 40%|      | 325/805 [02:57<43:01,  5.38s/it]   40%|      | 326/805 [02:58<30:43,  3.85s/it] 41%|      | 327/805 [02:58<22:08,  2.78s/it] 41%|      | 328/805 [02:58<16:07,  2.03s/it] 41%|      | 329/805 [02:58<11:56,  1.50s/it] 41%|      | 330/805 [02:59<09:00,  1.14s/it] 41%|      | 331/805 [02:59<06:57,  1.14it/s] 41%|      | 332/805 [02:59<05:31,  1.43it/s] 41%|     | 333/805 [03:00<04:31,  1.74it/s] 41%|     | 334/805 [03:00<03:48,  2.06it/s] 42%|     | 335/805 [03:00<03:34,  2.19it/s] 42%|     | 336/805 [03:01<03:09,  2.47it/s] 42%|     | 337/805 [03:01<02:52,  2.72it/s] 42%|     | 338/805 [03:01<02:39,  2.92it/s] 42%|     | 339/805 [03:01<02:30,  3.09it/s] 42%|     | 340/805 [03:02<02:24,  3.21it/s] 42%|     | 341/805 [03:02<02:20,  3.31it/s] 42%|     | 342/805 [03:02<02:16,  3.38it/s] 43%|     | 343/805 [03:02<02:14,  3.43it/s] 43%|     | 344/805 [03:03<02:12,  3.47it/s] 43%|     | 345/805 [03:03<02:11,  3.49it/s] 43%|     | 346/805 [03:03<02:34,  2.98it/s] 43%|     | 347/805 [03:04<02:26,  3.13it/s] 43%|     | 348/805 [03:04<02:20,  3.24it/s] 43%|     | 349/805 [03:04<02:16,  3.33it/s] 43%|     | 350/805 [03:05<02:13,  3.40it/s] 44%|     | 351/805 [03:05<02:11,  3.44it/s] 44%|     | 352/805 [03:05<02:10,  3.47it/s] 44%|     | 353/805 [03:05<02:09,  3.49it/s] 44%|     | 354/805 [03:06<02:08,  3.51it/s] 44%|     | 355/805 [03:06<02:07,  3.52it/s] 44%|     | 356/805 [03:06<02:07,  3.53it/s] 44%|     | 357/805 [03:07<02:18,  3.25it/s] 44%|     | 358/805 [03:07<02:14,  3.33it/s] 45%|     | 359/805 [03:07<02:11,  3.39it/s] 45%|     | 360/805 [03:08<02:09,  3.44it/s] 45%|     | 361/805 [03:08<02:22,  3.13it/s] 45%|     | 362/805 [03:08<02:16,  3.24it/s] 45%|     | 363/805 [03:08<02:12,  3.33it/s] 45%|     | 364/805 [03:09<02:09,  3.39it/s] 45%|     | 365/805 [03:09<02:08,  3.44it/s] 45%|     | 366/805 [03:09<02:06,  3.47it/s] 46%|     | 367/805 [03:10<02:05,  3.49it/s] 46%|     | 368/805 [03:10<02:04,  3.51it/s] 46%|     | 369/805 [03:10<02:04,  3.52it/s] 46%|     | 370/805 [03:10<02:03,  3.53it/s] 46%|     | 371/805 [03:11<02:02,  3.53it/s] 46%|     | 372/805 [03:11<02:20,  3.09it/s] 46%|     | 373/805 [03:11<02:14,  3.22it/s] 46%|     | 374/805 [03:12<02:10,  3.31it/s] 47%|     | 375/805 [03:12<02:07,  3.38it/s] 47%|     | 376/805 [03:12<02:05,  3.43it/s] 47%|     | 377/805 [03:13<02:03,  3.47it/s] 47%|     | 378/805 [03:13<02:02,  3.49it/s] 47%|     | 379/805 [03:13<02:01,  3.51it/s] 47%|     | 380/805 [03:13<02:00,  3.52it/s] 47%|     | 381/805 [03:14<02:00,  3.52it/s] 47%|     | 382/805 [03:14<01:59,  3.53it/s] 48%|     | 383/805 [03:15<03:22,  2.08it/s] 48%|     | 384/805 [03:15<02:57,  2.37it/s] 48%|     | 385/805 [03:15<02:39,  2.64it/s] 48%|     | 386/805 [03:16<02:26,  2.86it/s] 48%|     | 387/805 [03:16<02:17,  3.04it/s] 48%|     | 388/805 [03:16<02:11,  3.17it/s] 48%|     | 389/805 [03:17<02:07,  3.27it/s] 48%|     | 390/805 [03:17<02:03,  3.35it/s] 49%|     | 391/805 [03:17<02:01,  3.40it/s] 49%|     | 392/805 [03:18<02:14,  3.06it/s] 49%|     | 393/805 [03:18<02:08,  3.19it/s] 49%|     | 394/805 [03:18<02:04,  3.29it/s] 49%|     | 395/805 [03:18<02:01,  3.37it/s] 49%|     | 396/805 [03:19<01:59,  3.42it/s] 49%|     | 397/805 [03:19<01:58,  3.45it/s] 49%|     | 398/805 [03:19<01:56,  3.48it/s] 50%|     | 399/805 [03:20<01:56,  3.50it/s] 50%|     | 400/805 [03:20<01:55,  3.51it/s] 50%|     | 401/805 [03:20<01:54,  3.52it/s] 50%|     | 402/805 [03:20<01:54,  3.53it/s] 50%|     | 403/805 [03:21<01:59,  3.35it/s] 50%|     | 404/805 [03:21<01:57,  3.40it/s] 50%|     | 405/805 [03:21<01:56,  3.44it/s] 50%|     | 406/805 [03:22<01:54,  3.47it/s] 51%|     | 407/805 [03:22<01:53,  3.49it/s] 51%|     | 408/805 [03:22<01:53,  3.51it/s] 51%|     | 409/805 [03:22<01:52,  3.52it/s] 51%|     | 410/805 [03:23<01:52,  3.53it/s] 51%|     | 411/805 [03:23<01:51,  3.53it/s] 51%|     | 412/805 [03:23<01:51,  3.53it/s] 51%|    | 413/805 [03:24<01:50,  3.54it/s] 51%|    | 414/805 [03:24<01:57,  3.33it/s] 52%|    | 415/805 [03:24<01:54,  3.39it/s] 52%|    | 416/805 [03:24<01:53,  3.44it/s] 52%|    | 417/805 [03:25<01:51,  3.47it/s] 52%|    | 418/805 [03:25<01:50,  3.49it/s] 52%|    | 419/805 [03:25<01:50,  3.51it/s] 52%|    | 420/805 [03:26<01:49,  3.51it/s] 52%|    | 421/805 [03:26<01:49,  3.52it/s] 52%|    | 422/805 [03:26<01:48,  3.53it/s] 53%|    | 423/805 [03:26<01:48,  3.53it/s] 53%|    | 424/805 [03:27<01:47,  3.54it/s] 53%|    | 425/805 [03:27<02:44,  2.31it/s] 53%|    | 426/805 [03:28<02:26,  2.58it/s] 53%|    | 427/805 [03:28<02:14,  2.81it/s] 53%|    | 428/805 [03:28<02:05,  2.99it/s] 53%|    | 429/805 [03:29<01:59,  3.14it/s] 53%|    | 430/805 [03:29<02:33,  2.45it/s] 54%|    | 431/805 [03:30<02:18,  2.70it/s] 54%|    | 432/805 [03:30<02:08,  2.90it/s] 54%|    | 433/805 [03:30<02:46,  2.24it/s] 54%|    | 434/805 [03:31<02:27,  2.51it/s] 54%|    | 435/805 [03:31<02:14,  2.75it/s] 54%|    | 436/805 [03:31<02:05,  2.95it/s] 54%|    | 437/805 [03:32<01:58,  3.10it/s] 54%|    | 438/805 [03:32<01:53,  3.23it/s] 55%|    | 439/805 [03:32<01:50,  3.31it/s] 55%|    | 440/805 [03:32<01:47,  3.38it/s] 55%|    | 441/805 [03:33<01:46,  3.43it/s] 55%|    | 442/805 [03:33<01:44,  3.46it/s] 55%|    | 443/805 [03:33<01:55,  3.14it/s] 55%|    | 444/805 [03:34<01:51,  3.25it/s] 55%|    | 445/805 [03:34<01:47,  3.33it/s] 55%|    | 446/805 [03:34<01:45,  3.39it/s] 56%|    | 447/805 [03:35<01:44,  3.44it/s] 56%|    | 448/805 [03:35<01:42,  3.47it/s] 56%|    | 449/805 [03:35<01:42,  3.49it/s] 56%|    | 450/805 [03:35<01:41,  3.51it/s] 56%|    | 451/805 [03:36<01:40,  3.51it/s] 56%|    | 452/805 [03:36<01:40,  3.52it/s] 56%|    | 453/805 [03:36<01:39,  3.53it/s] 56%|    | 454/805 [03:37<02:15,  2.58it/s] 57%|    | 455/805 [03:37<02:04,  2.81it/s] 57%|    | 456/805 [03:37<01:56,  3.00it/s] 57%|    | 457/805 [03:38<01:50,  3.14it/s] 57%|    | 458/805 [03:38<01:46,  3.25it/s] 57%|    | 459/805 [03:38<01:43,  3.34it/s] 57%|    | 460/805 [03:39<01:55,  3.00it/s] 57%|    | 461/805 [03:39<01:49,  3.14it/s] 57%|    | 462/805 [03:39<01:45,  3.25it/s] 58%|    | 463/805 [03:40<01:42,  3.33it/s] 58%|    | 464/805 [03:40<01:40,  3.39it/s] 58%|    | 465/805 [03:40<01:38,  3.43it/s] 58%|    | 466/805 [03:40<01:37,  3.46it/s] 58%|    | 467/805 [03:41<01:37,  3.48it/s] 58%|    | 468/805 [03:41<01:36,  3.50it/s] 58%|    | 469/805 [03:41<01:35,  3.52it/s] 58%|    | 470/805 [03:42<01:56,  2.88it/s] 59%|    | 471/805 [03:42<01:49,  3.05it/s] 59%|    | 472/805 [03:42<01:44,  3.18it/s] 59%|    | 473/805 [03:43<01:41,  3.28it/s] 59%|    | 474/805 [03:43<01:38,  3.36it/s] 59%|    | 475/805 [03:43<01:36,  3.41it/s] 59%|    | 476/805 [03:43<01:35,  3.45it/s] 59%|    | 477/805 [03:44<01:34,  3.47it/s] 59%|    | 478/805 [03:44<01:33,  3.49it/s] 60%|    | 479/805 [03:44<01:32,  3.51it/s] 60%|    | 480/805 [03:45<01:51,  2.90it/s] 60%|    | 481/805 [03:45<01:45,  3.07it/s] 60%|    | 482/805 [03:45<01:41,  3.20it/s] 60%|    | 483/805 [03:46<01:36,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 17:37:44,285 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:37:44,285 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:37:44,285 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 10.7865, 'eval_samples_per_second': 322.811, 'eval_steps_per_second': 40.421, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.95it/s][A
  3%|         | 12/436 [00:00<00:08, 48.74it/s][A
  4%|         | 17/436 [00:00<00:08, 47.35it/s][A
  5%|         | 22/436 [00:00<00:08, 46.51it/s][A
  6%|         | 27/436 [00:00<00:08, 46.09it/s][A
  7%|         | 32/436 [00:00<00:08, 45.50it/s][A
  8%|         | 37/436 [00:00<00:08, 44.91it/s][A
 10%|         | 42/436 [00:00<00:08, 44.61it/s][A
 11%|         | 47/436 [00:01<00:08, 44.53it/s][A
 12%|        | 52/436 [00:01<00:08, 44.49it/s][A
 13%|        | 57/436 [00:01<00:08, 44.74it/s][A
 14%|        | 62/436 [00:01<00:08, 44.85it/s][A
 15%|        | 67/436 [00:01<00:08, 44.90it/s][A
 17%|        | 72/436 [00:01<00:08, 45.00it/s][A
 18%|        | 77/436 [00:01<00:07, 44.94it/s][A
 19%|        | 82/436 [00:01<00:08, 39.72it/s][A
 20%|        | 87/436 [00:01<00:08, 41.25it/s][A
 21%|        | 92/436 [00:02<00:08, 42.35it/s][A
 22%|       | 97/436 [00:02<00:07, 43.12it/s][A
 23%|       | 102/436 [00:02<00:07, 43.70it/s][A
 25%|       | 107/436 [00:02<00:07, 44.08it/s][A
 26%|       | 112/436 [00:02<00:07, 44.36it/s][A
 27%|       | 117/436 [00:02<00:07, 44.48it/s][A
 28%|       | 122/436 [00:02<00:07, 44.18it/s][A
 29%|       | 127/436 [00:02<00:07, 43.97it/s][A
 30%|       | 132/436 [00:02<00:06, 44.16it/s][A
 31%|      | 137/436 [00:03<00:06, 44.36it/s][A
 33%|      | 142/436 [00:03<00:06, 44.58it/s][A
 34%|      | 147/436 [00:03<00:06, 44.67it/s][A
 35%|      | 152/436 [00:03<00:06, 44.86it/s][A
 36%|      | 157/436 [00:03<00:06, 44.98it/s][A
 37%|      | 162/436 [00:03<00:06, 44.84it/s][A
 38%|      | 167/436 [00:03<00:06, 44.54it/s][A
 39%|      | 172/436 [00:03<00:05, 44.39it/s][A
 41%|      | 177/436 [00:03<00:05, 44.43it/s][A
 42%|     | 182/436 [00:04<00:05, 44.61it/s][A
 43%|     | 187/436 [00:04<00:05, 44.71it/s][A
 44%|     | 192/436 [00:04<00:05, 44.78it/s][A
 45%|     | 197/436 [00:04<00:05, 44.78it/s][A
 46%|     | 202/436 [00:04<00:05, 44.91it/s][A
 47%|     | 207/436 [00:04<00:05, 44.79it/s][A
 49%|     | 212/436 [00:04<00:05, 44.55it/s][A
 50%|     | 217/436 [00:04<00:05, 39.45it/s][A
 51%|     | 222/436 [00:05<00:05, 41.07it/s][A
 52%|    | 227/436 [00:05<00:04, 42.19it/s][A
 53%|    | 232/436 [00:05<00:04, 43.04it/s][A
 54%|    | 237/436 [00:05<00:04, 43.70it/s][A
 56%|    | 242/436 [00:05<00:04, 44.13it/s][A
 57%|    | 247/436 [00:05<00:04, 44.48it/s][A
 58%|    | 252/436 [00:05<00:04, 44.49it/s][A
 59%|    | 257/436 [00:05<00:04, 44.23it/s][A
 60%|    | 262/436 [00:05<00:03, 44.11it/s][A
 61%|    | 267/436 [00:06<00:03, 44.17it/s][A
 62%|   | 272/436 [00:06<00:03, 44.42it/s][A
 64%|   | 277/436 [00:06<00:03, 44.62it/s][A
 65%|   | 282/436 [00:06<00:03, 44.82it/s][A
 66%|   | 287/436 [00:06<00:03, 44.86it/s][A
 67%|   | 292/436 [00:06<00:03, 44.78it/s][A
 68%|   | 297/436 [00:06<00:03, 44.69it/s][A
 69%|   | 302/436 [00:06<00:03, 44.37it/s][A
 70%|   | 307/436 [00:06<00:02, 44.25it/s][A
 72%|  | 312/436 [00:07<00:02, 44.29it/s][A
 73%|  | 317/436 [00:07<00:02, 44.39it/s][A
 74%|  | 322/436 [00:07<00:02, 44.68it/s][A
 75%|  | 327/436 [00:07<00:02, 44.85it/s][A
 76%|  | 332/436 [00:07<00:02, 44.94it/s][A
 77%|  | 337/436 [00:07<00:02, 44.91it/s][A
 78%|  | 342/436 [00:07<00:02, 44.77it/s][A
 80%|  | 347/436 [00:07<00:01, 44.57it/s][A
 81%|  | 352/436 [00:08<00:02, 36.53it/s][A
 82%| | 357/436 [00:08<00:02, 38.76it/s][A
 83%| | 362/436 [00:08<00:01, 40.50it/s][A
 84%| | 367/436 [00:08<00:01, 41.71it/s][A
 85%| | 372/436 [00:08<00:01, 42.76it/s][A
 86%| | 377/436 [00:08<00:01, 43.48it/s][A
 88%| | 382/436 [00:08<00:01, 43.97it/s][A
 89%| | 387/436 [00:08<00:01, 44.20it/s][A
 90%| | 392/436 [00:08<00:00, 44.03it/s][A
 91%| | 397/436 [00:09<00:00, 43.89it/s][A
 92%|| 402/436 [00:09<00:00, 43.99it/s][A
 93%|| 407/436 [00:09<00:00, 44.22it/s][A
 94%|| 412/436 [00:09<00:00, 44.43it/s][A
 96%|| 417/436 [00:09<00:00, 44.68it/s][A
 97%|| 422/436 [00:10<00:00, 15.38it/s][A
 98%|| 427/436 [00:10<00:00, 19.18it/s][A
 99%|| 432/436 [00:10<00:00, 23.18it/s][A
                                                 [A                                                 
100%|| 436/436 [00:10<00:00, 23.18it/s][A 60%|    | 483/805 [03:56<01:36,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:37:55,304 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 17:37:58,085 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:38:10,711 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:38:11,286 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:38:11,443 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-483/special_tokens_map.json
 60%|    | 484/805 [04:17<52:16,  9.77s/it] 60%|    | 485/805 [04:18<37:07,  6.96s/it] 60%|    | 486/805 [04:18<26:21,  4.96s/it] 60%|    | 487/805 [04:18<18:50,  3.55s/it] 61%|    | 488/805 [04:19<13:35,  2.57s/it] 61%|    | 489/805 [04:19<09:55,  1.88s/it] 61%|    | 490/805 [04:19<07:22,  1.40s/it] 61%|    | 491/805 [04:20<05:34,  1.07s/it] 61%|    | 492/805 [04:20<04:20,  1.20it/s] 61%|    | 493/805 [04:20<03:27,  1.50it/s] 61%|   | 494/805 [04:20<02:51,  1.81it/s] 61%|   | 495/805 [04:21<02:25,  2.12it/s] 62%|   | 496/805 [04:22<02:59,  1.72it/s] 62%|   | 497/805 [04:22<02:31,  2.04it/s] 62%|   | 498/805 [04:22<02:11,  2.34it/s] 62%|   | 499/805 [04:22<01:57,  2.60it/s] 62%|   | 500/805 [04:23<01:47,  2.83it/s]                                                  62%|   | 500/805 [04:23<01:47,  2.83it/s] 62%|   | 501/805 [04:23<01:40,  3.01it/s] 62%|   | 502/805 [04:23<01:35,  3.16it/s] 62%|   | 503/805 [04:23<01:32,  3.27it/s] 63%|   | 504/805 [04:24<01:29,  3.35it/s] 63%|   | 505/805 [04:24<01:34,  3.16it/s] 63%|   | 506/805 [04:24<01:31,  3.26it/s] 63%|   | 507/805 [04:25<01:29,  3.34it/s] 63%|   | 508/805 [04:25<01:27,  3.39it/s] 63%|   | 509/805 [04:25<01:26,  3.44it/s] 63%|   | 510/805 [04:26<01:25,  3.46it/s] 63%|   | 511/805 [04:26<01:24,  3.49it/s] 64%|   | 512/805 [04:26<01:23,  3.51it/s] 64%|   | 513/805 [04:26<01:22,  3.54it/s] 64%|   | 514/805 [04:27<01:21,  3.56it/s] 64%|   | 515/805 [04:27<01:21,  3.57it/s] 64%|   | 516/805 [04:27<01:26,  3.36it/s] 64%|   | 517/805 [04:28<01:24,  3.41it/s] 64%|   | 518/805 [04:28<01:23,  3.45it/s] 64%|   | 519/805 [04:28<01:22,  3.48it/s] 65%|   | 520/805 [04:28<01:21,  3.50it/s] 65%|   | 521/805 [04:29<01:20,  3.51it/s] 65%|   | 522/805 [04:29<01:20,  3.52it/s] 65%|   | 523/805 [04:29<01:20,  3.52it/s] 65%|   | 524/805 [04:30<01:19,  3.53it/s] 65%|   | 525/805 [04:30<01:19,  3.53it/s] 65%|   | 526/805 [04:30<01:18,  3.54it/s] 65%|   | 527/805 [04:30<01:22,  3.37it/s] 66%|   | 528/805 [04:31<01:21,  3.42it/s] 66%|   | 529/805 [04:31<01:19,  3.45it/s] 66%|   | 530/805 [04:31<01:19,  3.47it/s] 66%|   | 531/805 [04:32<01:18,  3.50it/s] 66%|   | 532/805 [04:32<01:17,  3.52it/s] 66%|   | 533/805 [04:32<01:17,  3.53it/s] 66%|   | 534/805 [04:32<01:16,  3.54it/s] 66%|   | 535/805 [04:33<01:16,  3.54it/s] 67%|   | 536/805 [04:33<01:15,  3.54it/s] 67%|   | 537/805 [04:33<01:15,  3.54it/s] 67%|   | 538/805 [04:34<01:18,  3.42it/s] 67%|   | 539/805 [04:34<01:17,  3.45it/s] 67%|   | 540/805 [04:34<01:16,  3.48it/s] 67%|   | 541/805 [04:34<01:15,  3.50it/s] 67%|   | 542/805 [04:35<01:14,  3.51it/s] 67%|   | 543/805 [04:35<01:14,  3.52it/s] 68%|   | 544/805 [04:35<01:13,  3.53it/s] 68%|   | 545/805 [04:36<01:13,  3.54it/s] 68%|   | 546/805 [04:36<01:13,  3.54it/s] 68%|   | 547/805 [04:36<01:12,  3.54it/s] 68%|   | 548/805 [04:36<01:12,  3.54it/s] 68%|   | 549/805 [04:37<01:26,  2.95it/s] 68%|   | 550/805 [04:37<01:22,  3.09it/s] 68%|   | 551/805 [04:37<01:19,  3.21it/s] 69%|   | 552/805 [04:38<01:16,  3.31it/s] 69%|   | 553/805 [04:38<01:14,  3.37it/s] 69%|   | 554/805 [04:38<01:13,  3.42it/s] 69%|   | 555/805 [04:39<01:12,  3.46it/s] 69%|   | 556/805 [04:39<01:11,  3.49it/s] 69%|   | 557/805 [04:39<01:10,  3.51it/s] 69%|   | 558/805 [04:39<01:10,  3.52it/s] 69%|   | 559/805 [04:40<01:20,  3.06it/s] 70%|   | 560/805 [04:40<01:20,  3.06it/s] 70%|   | 561/805 [04:40<01:16,  3.19it/s] 70%|   | 562/805 [04:41<01:13,  3.29it/s] 70%|   | 563/805 [04:41<01:11,  3.36it/s] 70%|   | 564/805 [04:41<01:10,  3.43it/s] 70%|   | 565/805 [04:42<01:08,  3.48it/s] 70%|   | 566/805 [04:42<01:08,  3.51it/s] 70%|   | 567/805 [04:42<01:07,  3.54it/s] 71%|   | 568/805 [04:42<01:06,  3.56it/s] 71%|   | 569/805 [04:43<01:06,  3.57it/s] 71%|   | 570/805 [04:43<01:15,  3.12it/s] 71%|   | 571/805 [04:43<01:12,  3.25it/s] 71%|   | 572/805 [04:44<01:09,  3.35it/s] 71%|   | 573/805 [04:44<01:07,  3.42it/s] 71%|  | 574/805 [04:44<01:06,  3.47it/s] 71%|  | 575/805 [04:44<01:05,  3.51it/s] 72%|  | 576/805 [04:45<01:04,  3.54it/s] 72%|  | 577/805 [04:45<01:04,  3.55it/s] 72%|  | 578/805 [04:45<01:03,  3.57it/s] 72%|  | 579/805 [04:46<01:03,  3.57it/s] 72%|  | 580/805 [04:46<01:02,  3.58it/s] 72%|  | 581/805 [04:46<01:15,  2.95it/s] 72%|  | 582/805 [04:47<01:11,  3.12it/s] 72%|  | 583/805 [04:47<01:08,  3.25it/s] 73%|  | 584/805 [04:47<01:06,  3.34it/s] 73%|  | 585/805 [04:47<01:04,  3.42it/s] 73%|  | 586/805 [04:48<01:03,  3.47it/s] 73%|  | 587/805 [04:48<01:02,  3.51it/s] 73%|  | 588/805 [04:48<01:01,  3.53it/s] 73%|  | 589/805 [04:49<01:00,  3.55it/s] 73%|  | 590/805 [04:49<01:00,  3.56it/s] 73%|  | 591/805 [04:49<00:59,  3.57it/s] 74%|  | 592/805 [04:50<01:14,  2.85it/s] 74%|  | 593/805 [04:50<01:09,  3.04it/s] 74%|  | 594/805 [04:50<01:06,  3.19it/s] 74%|  | 595/805 [04:50<01:03,  3.29it/s] 74%|  | 596/805 [04:51<01:01,  3.38it/s] 74%|  | 597/805 [04:51<01:00,  3.44it/s] 74%|  | 598/805 [04:51<00:59,  3.49it/s] 74%|  | 599/805 [04:52<00:58,  3.52it/s] 75%|  | 600/805 [04:52<00:57,  3.54it/s] 75%|  | 601/805 [04:52<00:57,  3.55it/s] 75%|  | 602/805 [04:53<01:11,  2.82it/s] 75%|  | 603/805 [04:53<01:06,  3.02it/s] 75%|  | 604/805 [04:53<01:03,  3.17it/s] 75%|  | 605/805 [04:54<01:00,  3.29it/s] 75%|  | 606/805 [04:54<00:58,  3.37it/s] 75%|  | 607/805 [04:54<00:57,  3.43it/s] 76%|  | 608/805 [04:54<00:56,  3.48it/s] 76%|  | 609/805 [04:55<00:55,  3.51it/s] 76%|  | 610/805 [04:55<00:55,  3.53it/s] 76%|  | 611/805 [04:55<00:54,  3.55it/s] 76%|  | 612/805 [04:56<01:45,  1.83it/s] 76%|  | 613/805 [04:57<01:29,  2.14it/s] 76%|  | 614/805 [04:57<01:18,  2.43it/s] 76%|  | 615/805 [04:57<01:10,  2.69it/s] 77%|  | 616/805 [04:57<01:04,  2.91it/s] 77%|  | 617/805 [04:58<01:00,  3.09it/s] 77%|  | 618/805 [04:58<01:05,  2.85it/s] 77%|  | 619/805 [04:58<01:01,  3.04it/s] 77%|  | 620/805 [04:59<01:03,  2.91it/s] 77%|  | 621/805 [04:59<00:59,  3.09it/s] 77%|  | 622/805 [04:59<00:56,  3.23it/s] 77%|  | 623/805 [05:00<00:54,  3.33it/s] 78%|  | 624/805 [05:00<00:53,  3.40it/s] 78%|  | 625/805 [05:00<00:52,  3.45it/s] 78%|  | 626/805 [05:00<00:51,  3.49it/s] 78%|  | 627/805 [05:01<00:50,  3.52it/s] 78%|  | 628/805 [05:01<00:49,  3.54it/s] 78%|  | 629/805 [05:01<00:49,  3.56it/s] 78%|  | 630/805 [05:02<00:49,  3.57it/s] 78%|  | 631/805 [05:02<00:54,  3.21it/s] 79%|  | 632/805 [05:02<00:52,  3.32it/s] 79%|  | 633/805 [05:03<00:50,  3.39it/s] 79%|  | 634/805 [05:03<00:49,  3.45it/s] 79%|  | 635/805 [05:03<00:48,  3.50it/s] 79%|  | 636/805 [05:03<00:47,  3.53it/s] 79%|  | 637/805 [05:04<00:47,  3.55it/s] 79%|  | 638/805 [05:04<00:46,  3.56it/s] 79%|  | 639/805 [05:04<00:46,  3.57it/s] 80%|  | 640/805 [05:04<00:46,  3.57it/s] 80%|  | 641/805 [05:05<00:45,  3.58it/s] 80%|  | 642/805 [05:05<00:51,  3.16it/s] 80%|  | 643/805 [05:05<00:49,  3.28it/s] 80%|  | 644/805 [05:06<00:47,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 17:39:04,398 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:39:04,398 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:39:04,398 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 10.621, 'eval_samples_per_second': 327.84, 'eval_steps_per_second': 41.051, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.24it/s][A
  3%|         | 12/436 [00:00<00:08, 48.72it/s][A
  4%|         | 17/436 [00:00<00:08, 47.19it/s][A
  5%|         | 22/436 [00:00<00:08, 46.44it/s][A
  6%|         | 27/436 [00:00<00:08, 46.01it/s][A
  7%|         | 32/436 [00:00<00:08, 45.47it/s][A
  8%|         | 37/436 [00:00<00:08, 44.98it/s][A
 10%|         | 42/436 [00:00<00:08, 44.70it/s][A
 11%|         | 47/436 [00:01<00:08, 44.68it/s][A
 12%|        | 52/436 [00:01<00:08, 44.74it/s][A
 13%|        | 57/436 [00:01<00:08, 44.60it/s][A
 14%|        | 62/436 [00:01<00:08, 44.39it/s][A
 15%|        | 67/436 [00:01<00:08, 44.63it/s][A
 17%|        | 72/436 [00:01<00:08, 44.74it/s][A
 18%|        | 77/436 [00:01<00:08, 44.62it/s][A
 19%|        | 82/436 [00:01<00:07, 44.45it/s][A
 20%|        | 87/436 [00:01<00:07, 44.32it/s][A
 21%|        | 92/436 [00:02<00:07, 44.45it/s][A
 22%|       | 97/436 [00:02<00:07, 44.47it/s][A
 23%|       | 102/436 [00:02<00:14, 23.55it/s][A
 25%|       | 107/436 [00:02<00:11, 27.45it/s][A
 26%|       | 112/436 [00:02<00:10, 31.12it/s][A
 27%|       | 117/436 [00:02<00:09, 34.33it/s][A
 28%|       | 122/436 [00:03<00:08, 36.99it/s][A
 29%|       | 127/436 [00:03<00:07, 39.07it/s][A
 30%|       | 132/436 [00:03<00:07, 40.77it/s][A
 31%|      | 137/436 [00:03<00:07, 41.88it/s][A
 33%|      | 142/436 [00:03<00:06, 42.32it/s][A
 34%|      | 147/436 [00:03<00:06, 42.76it/s][A
 35%|      | 152/436 [00:03<00:06, 43.27it/s][A
 36%|      | 157/436 [00:03<00:06, 43.67it/s][A
 37%|      | 162/436 [00:03<00:06, 44.14it/s][A
 38%|      | 167/436 [00:04<00:06, 44.48it/s][A
 39%|      | 172/436 [00:04<00:05, 44.70it/s][A
 41%|      | 177/436 [00:04<00:05, 44.82it/s][A
 42%|     | 182/436 [00:04<00:05, 44.67it/s][A
 43%|     | 187/436 [00:04<00:05, 44.35it/s][A
 44%|     | 192/436 [00:04<00:05, 44.33it/s][A
 45%|     | 197/436 [00:05<00:05, 44.26it/s][A
 46%|     | 202/436 [00:05<00:15, 14.84it/s][A
 47%|     | 207/436 [00:05<00:12, 18.57it/s][A
 49%|     | 212/436 [00:05<00:09, 22.55it/s][A
 50%|     | 217/436 [00:05<00:08, 26.52it/s][A
 51%|     | 222/436 [00:06<00:07, 30.24it/s][A
 52%|    | 227/436 [00:06<00:06, 33.54it/s][A
 53%|    | 232/436 [00:06<00:05, 36.31it/s][A
 54%|    | 237/436 [00:06<00:05, 38.42it/s][A
 56%|    | 242/436 [00:06<00:04, 39.71it/s][A
 57%|    | 247/436 [00:06<00:04, 40.88it/s][A
 58%|    | 252/436 [00:06<00:04, 41.82it/s][A
 59%|    | 257/436 [00:06<00:04, 42.63it/s][A
 60%|    | 262/436 [00:06<00:04, 43.38it/s][A
 61%|    | 267/436 [00:07<00:03, 43.82it/s][A
 62%|   | 272/436 [00:07<00:03, 44.22it/s][A
 64%|   | 277/436 [00:07<00:03, 44.45it/s][A
 65%|   | 282/436 [00:07<00:03, 44.37it/s][A
 66%|   | 287/436 [00:07<00:03, 44.16it/s][A
 67%|   | 292/436 [00:07<00:04, 30.98it/s][A
 68%|   | 296/436 [00:07<00:04, 29.44it/s][A
 69%|   | 301/436 [00:08<00:04, 33.02it/s][A
 70%|   | 306/436 [00:08<00:03, 36.00it/s][A
 71%|  | 311/436 [00:08<00:03, 38.37it/s][A
 72%|  | 316/436 [00:08<00:02, 40.22it/s][A
 74%|  | 321/436 [00:08<00:02, 41.67it/s][A
 75%|  | 326/436 [00:08<00:02, 42.64it/s][A
 76%|  | 331/436 [00:08<00:02, 43.40it/s][A
 77%|  | 336/436 [00:08<00:02, 43.37it/s][A
 78%|  | 341/436 [00:09<00:04, 20.78it/s][A
 79%|  | 346/436 [00:09<00:03, 25.03it/s][A
 81%|  | 351/436 [00:09<00:02, 28.92it/s][A
 82%| | 356/436 [00:09<00:02, 32.45it/s][A
 83%| | 361/436 [00:09<00:02, 35.46it/s][A
 84%| | 366/436 [00:09<00:01, 37.92it/s][A
 85%| | 371/436 [00:09<00:01, 39.79it/s][A
 86%| | 376/436 [00:10<00:01, 41.31it/s][A
 87%| | 381/436 [00:10<00:01, 42.21it/s][A
 89%| | 386/436 [00:10<00:01, 42.58it/s][A
 90%| | 391/436 [00:10<00:01, 42.97it/s][A
 91%| | 396/436 [00:10<00:00, 43.35it/s][A
 92%|| 401/436 [00:10<00:00, 43.94it/s][A
 93%|| 406/436 [00:11<00:00, 44.35it/s][A
 94%|| 411/436 [00:11<00:01, 22.12it/s][A
 95%|| 416/436 [00:11<00:00, 26.10it/s][A
 97%|| 421/436 [00:11<00:00, 29.90it/s][A
 98%|| 426/436 [00:11<00:00, 33.31it/s][A
 99%|| 431/436 [00:11<00:00, 36.16it/s][A
100%|| 436/436 [00:11<00:00, 38.43it/s][A
                                                 [A                                                 
100%|| 436/436 [00:11<00:00, 38.43it/s][A 80%|  | 644/805 [05:18<00:47,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:39:17,578 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 17:39:18,980 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:39:39,823 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:39:40,965 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:39:41,234 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-644/special_tokens_map.json
 80%|  | 645/805 [05:48<34:10, 12.81s/it] 80%|  | 646/805 [05:49<24:48,  9.36s/it] 80%|  | 647/805 [05:49<17:28,  6.64s/it] 80%|  | 648/805 [05:50<12:22,  4.73s/it] 81%|  | 649/805 [05:50<08:49,  3.39s/it] 81%|  | 650/805 [05:50<06:21,  2.46s/it] 81%|  | 651/805 [05:50<04:38,  1.81s/it] 81%|  | 652/805 [05:51<03:26,  1.35s/it] 81%|  | 653/805 [05:51<02:35,  1.03s/it] 81%|  | 654/805 [05:51<02:05,  1.20it/s] 81%| | 655/805 [05:52<01:39,  1.51it/s] 81%| | 656/805 [05:52<01:21,  1.83it/s] 82%| | 657/805 [05:52<01:08,  2.15it/s] 82%| | 658/805 [05:52<01:00,  2.44it/s] 82%| | 659/805 [05:53<00:53,  2.71it/s] 82%| | 660/805 [05:53<00:49,  2.93it/s] 82%| | 661/805 [05:53<00:46,  3.10it/s] 82%| | 662/805 [05:54<00:44,  3.24it/s] 82%| | 663/805 [05:54<00:42,  3.35it/s] 82%| | 664/805 [05:54<00:41,  3.43it/s] 83%| | 665/805 [05:54<00:43,  3.24it/s] 83%| | 666/805 [05:55<00:41,  3.35it/s] 83%| | 667/805 [05:55<00:40,  3.43it/s] 83%| | 668/805 [05:55<00:39,  3.48it/s] 83%| | 669/805 [05:56<00:38,  3.52it/s] 83%| | 670/805 [05:56<00:38,  3.55it/s] 83%| | 671/805 [05:56<00:50,  2.66it/s] 83%| | 672/805 [05:57<00:46,  2.88it/s] 84%| | 673/805 [05:57<00:43,  3.07it/s] 84%| | 674/805 [05:57<00:40,  3.21it/s] 84%| | 675/805 [05:58<00:42,  3.06it/s] 84%| | 676/805 [05:58<00:40,  3.21it/s] 84%| | 677/805 [05:59<00:56,  2.28it/s] 84%| | 678/805 [05:59<00:49,  2.55it/s] 84%| | 679/805 [05:59<00:44,  2.80it/s] 84%| | 680/805 [05:59<00:41,  3.01it/s] 85%| | 681/805 [06:00<00:39,  3.16it/s] 85%| | 682/805 [06:00<00:37,  3.29it/s] 85%| | 683/805 [06:00<00:36,  3.38it/s] 85%| | 684/805 [06:01<01:02,  1.94it/s] 85%| | 685/805 [06:02<00:53,  2.25it/s] 85%| | 686/805 [06:02<00:46,  2.54it/s] 85%| | 687/805 [06:02<00:42,  2.79it/s] 85%| | 688/805 [06:02<00:39,  2.99it/s] 86%| | 689/805 [06:03<00:36,  3.16it/s] 86%| | 690/805 [06:03<00:35,  3.28it/s] 86%| | 691/805 [06:03<00:33,  3.38it/s] 86%| | 692/805 [06:04<00:32,  3.45it/s] 86%| | 693/805 [06:04<00:38,  2.94it/s] 86%| | 694/805 [06:04<00:35,  3.12it/s] 86%| | 695/805 [06:05<00:33,  3.25it/s] 86%| | 696/805 [06:05<00:32,  3.36it/s] 87%| | 697/805 [06:05<00:31,  3.43it/s] 87%| | 698/805 [06:05<00:30,  3.49it/s] 87%| | 699/805 [06:06<00:30,  3.53it/s] 87%| | 700/805 [06:06<00:29,  3.55it/s] 87%| | 701/805 [06:06<00:29,  3.57it/s] 87%| | 702/805 [06:07<00:28,  3.59it/s] 87%| | 703/805 [06:07<00:28,  3.60it/s] 87%| | 704/805 [06:07<00:33,  3.01it/s] 88%| | 705/805 [06:08<00:31,  3.16it/s] 88%| | 706/805 [06:08<00:30,  3.28it/s] 88%| | 707/805 [06:08<00:29,  3.37it/s] 88%| | 708/805 [06:08<00:28,  3.44it/s] 88%| | 709/805 [06:09<00:27,  3.48it/s] 88%| | 710/805 [06:09<00:27,  3.52it/s] 88%| | 711/805 [06:09<00:26,  3.54it/s] 88%| | 712/805 [06:09<00:26,  3.56it/s] 89%| | 713/805 [06:10<00:25,  3.57it/s] 89%| | 714/805 [06:10<00:25,  3.57it/s] 89%| | 715/805 [06:11<00:32,  2.80it/s] 89%| | 716/805 [06:11<00:29,  3.00it/s] 89%| | 717/805 [06:11<00:27,  3.15it/s] 89%| | 718/805 [06:11<00:26,  3.28it/s] 89%| | 719/805 [06:12<00:25,  3.36it/s] 89%| | 720/805 [06:12<00:24,  3.43it/s] 90%| | 721/805 [06:12<00:24,  3.48it/s] 90%| | 722/805 [06:13<00:23,  3.52it/s] 90%| | 723/805 [06:13<00:23,  3.54it/s] 90%| | 724/805 [06:13<00:22,  3.56it/s] 90%| | 725/805 [06:14<00:26,  2.98it/s] 90%| | 726/805 [06:14<00:25,  3.14it/s] 90%| | 727/805 [06:14<00:23,  3.27it/s] 90%| | 728/805 [06:14<00:22,  3.36it/s] 91%| | 729/805 [06:15<00:25,  2.97it/s] 91%| | 730/805 [06:15<00:25,  2.90it/s] 91%| | 731/805 [06:15<00:25,  2.92it/s] 91%| | 732/805 [06:16<00:23,  3.10it/s] 91%| | 733/805 [06:16<00:22,  3.23it/s] 91%| | 734/805 [06:16<00:21,  3.33it/s] 91%|| 735/805 [06:17<00:20,  3.41it/s] 91%|| 736/805 [06:17<00:19,  3.46it/s] 92%|| 737/805 [06:17<00:19,  3.51it/s] 92%|| 738/805 [06:17<00:18,  3.54it/s] 92%|| 739/805 [06:18<00:18,  3.56it/s] 92%|| 740/805 [06:18<00:18,  3.57it/s] 92%|| 741/805 [06:18<00:19,  3.29it/s] 92%|| 742/805 [06:19<00:20,  3.11it/s] 92%|| 743/805 [06:19<00:19,  3.24it/s] 92%|| 744/805 [06:19<00:18,  3.34it/s] 93%|| 745/805 [06:20<00:17,  3.41it/s] 93%|| 746/805 [06:20<00:17,  3.47it/s] 93%|| 747/805 [06:20<00:16,  3.50it/s] 93%|| 748/805 [06:20<00:16,  3.53it/s] 93%|| 749/805 [06:21<00:15,  3.55it/s] 93%|| 750/805 [06:21<00:15,  3.56it/s] 93%|| 751/805 [06:21<00:15,  3.57it/s] 93%|| 752/805 [06:21<00:14,  3.58it/s] 94%|| 753/805 [06:22<00:15,  3.29it/s] 94%|| 754/805 [06:22<00:15,  3.37it/s] 94%|| 755/805 [06:22<00:14,  3.44it/s] 94%|| 756/805 [06:23<00:14,  3.48it/s] 94%|| 757/805 [06:23<00:13,  3.52it/s] 94%|| 758/805 [06:23<00:13,  3.54it/s] 94%|| 759/805 [06:24<00:12,  3.56it/s] 94%|| 760/805 [06:24<00:12,  3.57it/s] 95%|| 761/805 [06:24<00:12,  3.57it/s] 95%|| 762/805 [06:24<00:12,  3.58it/s] 95%|| 763/805 [06:25<00:11,  3.58it/s] 95%|| 764/805 [06:25<00:12,  3.25it/s] 95%|| 765/805 [06:25<00:11,  3.34it/s] 95%|| 766/805 [06:26<00:11,  3.42it/s] 95%|| 767/805 [06:26<00:10,  3.47it/s] 95%|| 768/805 [06:26<00:10,  3.50it/s] 96%|| 769/805 [06:26<00:10,  3.53it/s] 96%|| 770/805 [06:27<00:09,  3.55it/s] 96%|| 771/805 [06:27<00:09,  3.56it/s] 96%|| 772/805 [06:27<00:09,  3.57it/s] 96%|| 773/805 [06:28<00:08,  3.57it/s] 96%|| 774/805 [06:28<00:08,  3.58it/s] 96%|| 775/805 [06:28<00:12,  2.47it/s] 96%|| 776/805 [06:29<00:10,  2.72it/s] 97%|| 777/805 [06:29<00:09,  2.94it/s] 97%|| 778/805 [06:29<00:08,  3.11it/s] 97%|| 779/805 [06:30<00:08,  3.23it/s] 97%|| 780/805 [06:30<00:07,  3.32it/s] 97%|| 781/805 [06:30<00:07,  3.40it/s] 97%|| 782/805 [06:30<00:06,  3.45it/s] 97%|| 783/805 [06:31<00:06,  3.49it/s] 97%|| 784/805 [06:31<00:05,  3.53it/s] 98%|| 785/805 [06:31<00:06,  3.20it/s] 98%|| 786/805 [06:32<00:05,  3.31it/s] 98%|| 787/805 [06:32<00:05,  3.40it/s] 98%|| 788/805 [06:32<00:04,  3.45it/s] 98%|| 789/805 [06:32<00:04,  3.49it/s] 98%|| 790/805 [06:33<00:06,  2.41it/s] 98%|| 791/805 [06:33<00:05,  2.67it/s] 98%|| 792/805 [06:34<00:04,  2.73it/s] 99%|| 793/805 [06:34<00:04,  2.93it/s] 99%|| 794/805 [06:35<00:04,  2.67it/s] 99%|| 795/805 [06:35<00:03,  2.89it/s] 99%|| 796/805 [06:35<00:02,  3.07it/s] 99%|| 797/805 [06:35<00:02,  3.21it/s] 99%|| 798/805 [06:36<00:02,  3.32it/s] 99%|| 799/805 [06:36<00:01,  3.40it/s] 99%|| 800/805 [06:36<00:01,  3.46it/s]100%|| 801/805 [06:37<00:01,  3.50it/s]100%|| 802/805 [06:37<00:00,  3.52it/s]100%|| 803/805 [06:37<00:00,  3.47it/s]100%|| 804/805 [06:37<00:00,  3.50it/s]100%|| 805/805 [06:38<00:00,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 17:40:36,357 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:40:36,357 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:40:36,357 >>   Batch size = 8
{'eval_loss': 1.1192312240600586, 'eval_runtime': 11.8383, 'eval_samples_per_second': 294.129, 'eval_steps_per_second': 36.83, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.73it/s][A
  3%|         | 12/436 [00:00<00:08, 48.57it/s][A
  4%|         | 17/436 [00:00<00:08, 47.29it/s][A
  5%|         | 22/436 [00:00<00:08, 46.44it/s][A
  6%|         | 27/436 [00:00<00:08, 46.13it/s][A
  7%|         | 32/436 [00:00<00:08, 45.37it/s][A
  8%|         | 37/436 [00:00<00:08, 44.91it/s][A
 10%|         | 42/436 [00:00<00:08, 44.44it/s][A
 11%|         | 47/436 [00:01<00:08, 44.48it/s][A
 12%|        | 52/436 [00:01<00:08, 44.70it/s][A
 13%|        | 57/436 [00:01<00:08, 44.80it/s][A
 14%|        | 62/436 [00:01<00:08, 44.89it/s][A
 15%|        | 67/436 [00:01<00:08, 44.91it/s][A
 17%|        | 72/436 [00:01<00:08, 44.92it/s][A
 18%|        | 77/436 [00:01<00:08, 44.73it/s][A
 19%|        | 82/436 [00:01<00:07, 44.40it/s][A
 20%|        | 87/436 [00:01<00:07, 44.24it/s][A
 21%|        | 92/436 [00:02<00:07, 44.29it/s][A
 22%|       | 97/436 [00:02<00:07, 44.54it/s][A
 23%|       | 102/436 [00:02<00:07, 44.71it/s][A
 25%|       | 107/436 [00:02<00:07, 44.89it/s][A
 26%|       | 112/436 [00:02<00:07, 44.98it/s][A
 27%|       | 117/436 [00:02<00:07, 44.83it/s][A
 28%|       | 122/436 [00:02<00:07, 44.62it/s][A
 29%|       | 127/436 [00:02<00:07, 40.28it/s][A
 30%|       | 132/436 [00:02<00:07, 41.58it/s][A
 31%|      | 137/436 [00:03<00:07, 42.57it/s][A
 33%|      | 142/436 [00:03<00:06, 43.36it/s][A
 34%|      | 147/436 [00:03<00:06, 43.92it/s][A
 35%|      | 152/436 [00:03<00:06, 44.25it/s][A
 36%|      | 157/436 [00:03<00:06, 44.47it/s][A
 37%|      | 162/436 [00:03<00:06, 44.38it/s][A
 38%|      | 167/436 [00:03<00:06, 44.18it/s][A
 39%|      | 172/436 [00:03<00:05, 44.05it/s][A
 41%|      | 177/436 [00:03<00:05, 44.17it/s][A
 42%|     | 182/436 [00:04<00:05, 44.43it/s][A
 43%|     | 187/436 [00:04<00:05, 44.69it/s][A
 44%|     | 192/436 [00:04<00:05, 44.87it/s][A
 45%|     | 197/436 [00:04<00:05, 45.00it/s][A
 46%|     | 202/436 [00:04<00:05, 44.99it/s][A
 47%|     | 207/436 [00:04<00:05, 44.78it/s][A
 49%|     | 212/436 [00:04<00:05, 44.53it/s][A
 50%|     | 217/436 [00:05<00:04, 44.34it/s][A
 51%|     | 222/436 [00:05<00:13, 15.46it/s][A
 52%|    | 227/436 [00:05<00:10, 19.25it/s][A
 53%|    | 232/436 [00:05<00:08, 23.27it/s][A
 54%|    | 237/436 [00:06<00:07, 27.23it/s][A
 56%|    | 242/436 [00:06<00:06, 30.95it/s][A
 57%|    | 247/436 [00:06<00:05, 34.20it/s][A
 58%|    | 252/436 [00:06<00:04, 36.86it/s][A
 59%|    | 257/436 [00:06<00:04, 38.93it/s][A
 60%|    | 262/436 [00:06<00:04, 40.23it/s][A
 61%|    | 267/436 [00:06<00:04, 41.17it/s][A
 62%|   | 272/436 [00:06<00:03, 42.09it/s][A
 64%|   | 277/436 [00:06<00:03, 42.87it/s][A
 65%|   | 282/436 [00:07<00:03, 43.49it/s][A
 66%|   | 287/436 [00:07<00:03, 43.99it/s][A
 67%|   | 292/436 [00:07<00:03, 44.28it/s][A
 68%|   | 297/436 [00:07<00:03, 44.63it/s][A
 69%|   | 302/436 [00:07<00:03, 44.59it/s][A
 70%|   | 307/436 [00:07<00:02, 44.39it/s][A
 72%|  | 312/436 [00:07<00:02, 44.28it/s][A
 73%|  | 317/436 [00:07<00:02, 44.27it/s][A
 74%|  | 322/436 [00:08<00:02, 44.42it/s][A
 75%|  | 327/436 [00:08<00:03, 33.40it/s][A
 76%|  | 332/436 [00:08<00:02, 36.16it/s][A
 77%|  | 337/436 [00:08<00:02, 38.49it/s][A
 78%|  | 342/436 [00:08<00:02, 40.30it/s][A
 80%|  | 347/436 [00:08<00:02, 41.66it/s][A
 81%|  | 352/436 [00:08<00:01, 42.68it/s][A
 82%| | 357/436 [00:08<00:01, 43.47it/s][A
 83%| | 362/436 [00:08<00:01, 43.83it/s][A
 84%| | 367/436 [00:09<00:01, 43.61it/s][A
 85%| | 372/436 [00:09<00:01, 43.66it/s][A
 86%| | 377/436 [00:09<00:01, 43.86it/s][A
 88%| | 382/436 [00:09<00:01, 44.15it/s][A
 89%| | 387/436 [00:09<00:01, 44.32it/s][A
 90%| | 392/436 [00:09<00:00, 44.66it/s][A
 91%| | 397/436 [00:09<00:00, 44.91it/s][A
 92%|| 402/436 [00:09<00:00, 44.97it/s][A
 93%|| 407/436 [00:09<00:00, 44.77it/s][A
 94%|| 412/436 [00:10<00:00, 44.42it/s][A
 96%|| 417/436 [00:10<00:00, 44.17it/s][A
 97%|| 422/436 [00:10<00:00, 44.28it/s][A
 98%|| 427/436 [00:10<00:00, 44.42it/s][A
 99%|| 432/436 [00:10<00:00, 44.63it/s][A
                                                 [A                                                 
100%|| 436/436 [00:10<00:00, 44.63it/s][A100%|| 805/805 [06:48<00:00,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:40:48,115 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 17:40:48,650 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:41:17,176 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:41:18,427 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:41:19,162 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:41:26,546 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:41:26,754 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161 (score: 1.1192312240600586).
                                                 100%|| 805/805 [08:03<00:00,  3.49it/s]100%|| 805/805 [08:03<00:00,  1.67it/s]
[INFO|trainer.py:1894] 2023-08-28 17:42:01,594 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 17:42:01,864 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:42:13,307 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:42:13,938 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:42:15,351 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:42:19,617 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:19,618 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:19,618 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:19,618 >>   train_runtime            = 0:08:02.96
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:19,618 >>   train_samples            =      10300
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:19,618 >>   train_samples_per_second =    106.632
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:19,618 >>   train_steps_per_second   =      1.667
{'eval_loss': 1.1192312240600586, 'eval_runtime': 10.6104, 'eval_samples_per_second': 328.17, 'eval_steps_per_second': 41.092, 'epoch': 5.0}
{'train_runtime': 482.9687, 'train_samples_per_second': 106.632, 'train_steps_per_second': 1.667, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 17:42:20 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:42:20,855 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:42:20,855 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:42:20,855 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 56.57it/s]  3%|         | 12/436 [00:00<00:08, 49.45it/s]  4%|         | 18/436 [00:00<00:08, 47.43it/s]  5%|         | 23/436 [00:00<00:08, 46.70it/s]  6%|         | 28/436 [00:00<00:08, 46.24it/s]  8%|         | 33/436 [00:00<00:08, 45.95it/s]  9%|         | 38/436 [00:00<00:12, 32.33it/s] 10%|         | 43/436 [00:01<00:11, 35.56it/s] 11%|         | 48/436 [00:01<00:10, 38.09it/s] 12%|        | 53/436 [00:01<00:09, 40.05it/s] 13%|        | 58/436 [00:01<00:09, 41.43it/s] 14%|        | 63/436 [00:01<00:08, 42.54it/s] 16%|        | 68/436 [00:01<00:08, 43.34it/s] 17%|        | 73/436 [00:01<00:08, 43.82it/s] 18%|        | 78/436 [00:01<00:08, 43.81it/s] 19%|        | 83/436 [00:02<00:09, 38.35it/s] 20%|        | 88/436 [00:02<00:08, 40.32it/s] 21%|       | 93/436 [00:02<00:09, 35.88it/s] 22%|       | 98/436 [00:02<00:08, 38.32it/s] 24%|       | 103/436 [00:02<00:08, 40.12it/s] 25%|       | 108/436 [00:02<00:07, 41.61it/s] 26%|       | 113/436 [00:02<00:07, 42.61it/s] 27%|       | 118/436 [00:02<00:07, 43.38it/s] 28%|       | 123/436 [00:02<00:07, 43.95it/s] 29%|       | 128/436 [00:03<00:06, 44.30it/s] 31%|       | 133/436 [00:03<00:06, 44.19it/s] 32%|      | 138/436 [00:03<00:06, 44.14it/s] 33%|      | 143/436 [00:03<00:06, 44.33it/s] 34%|      | 148/436 [00:03<00:06, 44.51it/s] 35%|      | 153/436 [00:03<00:06, 44.77it/s] 36%|      | 158/436 [00:03<00:06, 44.89it/s] 37%|      | 163/436 [00:03<00:06, 40.96it/s] 39%|      | 168/436 [00:03<00:06, 42.23it/s] 40%|      | 173/436 [00:04<00:06, 43.16it/s] 41%|      | 178/436 [00:04<00:05, 43.60it/s] 42%|     | 183/436 [00:04<00:05, 43.92it/s] 43%|     | 188/436 [00:04<00:05, 44.20it/s] 44%|     | 193/436 [00:04<00:05, 44.42it/s] 45%|     | 198/436 [00:04<00:05, 44.57it/s] 47%|     | 203/436 [00:04<00:05, 44.29it/s] 48%|     | 208/436 [00:04<00:05, 44.43it/s] 49%|     | 213/436 [00:05<00:04, 44.62it/s] 50%|     | 218/436 [00:05<00:04, 44.77it/s] 51%|     | 223/436 [00:05<00:04, 44.93it/s] 52%|    | 228/436 [00:05<00:04, 45.07it/s] 53%|    | 233/436 [00:05<00:04, 45.17it/s] 55%|    | 238/436 [00:05<00:04, 45.30it/s] 56%|    | 243/436 [00:05<00:04, 45.27it/s] 57%|    | 248/436 [00:05<00:04, 45.11it/s] 58%|    | 253/436 [00:05<00:04, 45.02it/s] 59%|    | 258/436 [00:05<00:03, 45.07it/s] 60%|    | 263/436 [00:06<00:03, 45.20it/s] 61%|   | 268/436 [00:06<00:03, 45.23it/s] 63%|   | 273/436 [00:06<00:03, 45.29it/s] 64%|   | 278/436 [00:06<00:03, 45.31it/s] 65%|   | 283/436 [00:06<00:03, 45.33it/s] 66%|   | 288/436 [00:06<00:03, 45.16it/s] 67%|   | 293/436 [00:06<00:03, 45.11it/s] 68%|   | 298/436 [00:07<00:04, 29.98it/s] 69%|   | 303/436 [00:07<00:03, 33.38it/s] 71%|   | 308/436 [00:07<00:03, 36.35it/s] 72%|  | 313/436 [00:07<00:03, 36.77it/s] 73%|  | 318/436 [00:07<00:03, 39.21it/s] 74%|  | 323/436 [00:07<00:02, 40.92it/s] 75%|  | 328/436 [00:07<00:02, 42.18it/s] 76%|  | 333/436 [00:07<00:02, 43.05it/s] 78%|  | 338/436 [00:07<00:02, 43.42it/s] 79%|  | 343/436 [00:08<00:02, 44.00it/s] 80%|  | 348/436 [00:08<00:01, 44.48it/s] 81%|  | 353/436 [00:08<00:01, 44.69it/s] 82%| | 358/436 [00:08<00:01, 44.63it/s] 83%| | 363/436 [00:08<00:01, 44.87it/s] 84%| | 368/436 [00:08<00:01, 44.99it/s] 86%| | 373/436 [00:08<00:01, 45.13it/s] 87%| | 378/436 [00:08<00:01, 45.03it/s] 88%| | 383/436 [00:08<00:01, 44.91it/s] 89%| | 388/436 [00:09<00:01, 44.97it/s] 90%| | 393/436 [00:09<00:00, 45.15it/s] 91%|| 398/436 [00:09<00:00, 45.18it/s] 92%|| 403/436 [00:09<00:00, 45.06it/s] 94%|| 408/436 [00:09<00:00, 45.13it/s] 95%|| 413/436 [00:09<00:00, 45.29it/s] 96%|| 418/436 [00:09<00:00, 45.29it/s] 97%|| 423/436 [00:09<00:00, 45.19it/s] 98%|| 428/436 [00:10<00:00, 35.84it/s] 99%|| 433/436 [00:10<00:00, 38.36it/s]100%|| 436/436 [00:10<00:00, 42.61it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:42:31,124 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   eval_loss               =     1.1192
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   eval_runtime            = 0:00:10.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   eval_samples_per_second =    339.072
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   eval_steps_per_second   =     42.457
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:42:31,124 >>   perplexity              =     3.0625
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:52,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:52,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:52,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:52,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:52,573 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:42:54,163 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:42:54,164 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:42:54,719 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:42:55,941 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:42:56,384 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:43:00,403 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:43:00,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:43:00,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:43:00,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:43:00,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:43:01,850 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:43:01,852 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:43:02,474 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:43:02,719 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:43:02,719 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-161
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-483
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-322
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-644
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/generator/iter5/model/checkpoint-805
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:14,  1.70it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.42it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:20,  1.51it/s]Extractor Predicting: 34it [00:21,  1.51it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:26,  1.57it/s]Extractor Predicting: 43it [00:26,  1.57it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:28,  1.56it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.57it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:32,  1.47it/s]Extractor Predicting: 52it [00:32,  1.53it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:34,  1.56it/s]Extractor Predicting: 56it [00:35,  1.47it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:38,  1.59it/s]Extractor Predicting: 62it [00:39,  1.62it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:40,  1.35it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.49it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:43,  1.58it/s]Extractor Predicting: 69it [00:44,  1.43it/s]Extractor Predicting: 70it [00:44,  1.52it/s]Extractor Predicting: 71it [00:45,  1.57it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:46,  1.69it/s]Extractor Predicting: 74it [00:47,  1.56it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:48,  1.57it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:49,  1.62it/s]Extractor Predicting: 79it [00:50,  1.50it/s]Extractor Predicting: 80it [00:50,  1.54it/s]Extractor Predicting: 81it [00:51,  1.57it/s]Extractor Predicting: 82it [00:52,  1.59it/s]Extractor Predicting: 83it [00:52,  1.63it/s]Extractor Predicting: 84it [00:53,  1.44it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:54,  1.53it/s]Extractor Predicting: 87it [00:55,  1.59it/s]Extractor Predicting: 88it [00:56,  1.57it/s]Extractor Predicting: 89it [00:56,  1.50it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:58,  1.44it/s]Extractor Predicting: 92it [00:58,  1.48it/s]Extractor Predicting: 93it [00:59,  1.54it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:00,  1.49it/s]Extractor Predicting: 96it [01:01,  1.53it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:02,  1.55it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.36it/s]Extractor Predicting: 101it [01:04,  1.44it/s]Extractor Predicting: 102it [01:05,  1.46it/s]Extractor Predicting: 103it [01:06,  1.48it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.46it/s]Extractor Predicting: 106it [01:08,  1.46it/s]Extractor Predicting: 107it [01:08,  1.50it/s]Extractor Predicting: 108it [01:09,  1.55it/s]Extractor Predicting: 109it [01:10,  1.61it/s]Extractor Predicting: 110it [01:10,  1.55it/s]Extractor Predicting: 111it [01:11,  1.62it/s]Extractor Predicting: 112it [01:11,  1.62it/s]Extractor Predicting: 113it [01:12,  1.63it/s]Extractor Predicting: 114it [01:13,  1.61it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:14,  1.57it/s]Extractor Predicting: 117it [01:15,  1.58it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:16,  1.59it/s]Extractor Predicting: 120it [01:17,  1.60it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:18,  1.61it/s]Extractor Predicting: 123it [01:18,  1.58it/s]Extractor Predicting: 124it [01:19,  1.60it/s]Extractor Predicting: 125it [01:20,  1.52it/s]Extractor Predicting: 126it [01:20,  1.55it/s]Extractor Predicting: 127it [01:21,  1.57it/s]Extractor Predicting: 128it [01:22,  1.56it/s]Extractor Predicting: 129it [01:22,  1.58it/s]Extractor Predicting: 130it [01:23,  1.31it/s]Extractor Predicting: 131it [01:24,  1.40it/s]Extractor Predicting: 132it [01:25,  1.47it/s]Extractor Predicting: 133it [01:25,  1.48it/s]Extractor Predicting: 134it [01:26,  1.47it/s]Extractor Predicting: 135it [01:27,  1.14it/s]Extractor Predicting: 136it [01:28,  1.26it/s]Extractor Predicting: 137it [01:29,  1.19it/s]Extractor Predicting: 138it [01:29,  1.29it/s]Extractor Predicting: 139it [01:30,  1.36it/s]Extractor Predicting: 140it [01:31,  1.40it/s]Extractor Predicting: 141it [01:31,  1.45it/s]Extractor Predicting: 142it [01:32,  1.42it/s]Extractor Predicting: 143it [01:33,  1.50it/s]Extractor Predicting: 143it [01:33,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:10,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:10,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:10,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:10,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:10,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:45:11,430 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:45:11,431 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:45:11,818 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:45:13,090 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:45:13,090 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:16,115 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:16,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:16,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:16,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:45:16,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:45:17,240 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:45:17,306 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:45:17,653 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:45:17,901 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:45:17,901 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.77it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.79it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.65it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.44it/s]Extractor Predicting: 13it [00:07,  1.53it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.70it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:11,  1.71it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:14,  1.76it/s]Extractor Predicting: 25it [00:14,  1.76it/s]Extractor Predicting: 26it [00:15,  1.83it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.74it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.53it/s]Extractor Predicting: 33it [00:19,  1.56it/s]Extractor Predicting: 34it [00:20,  1.58it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:21,  1.60it/s]Extractor Predicting: 37it [00:22,  1.50it/s]Extractor Predicting: 38it [00:23,  1.52it/s]Extractor Predicting: 39it [00:23,  1.56it/s]Extractor Predicting: 40it [00:24,  1.57it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.55it/s]Extractor Predicting: 43it [00:26,  1.54it/s]Extractor Predicting: 44it [00:26,  1.57it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:28,  1.60it/s]Extractor Predicting: 47it [00:28,  1.53it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:30,  1.57it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:31,  1.57it/s]Extractor Predicting: 53it [00:32,  1.56it/s]Extractor Predicting: 54it [00:33,  1.58it/s]Extractor Predicting: 55it [00:33,  1.58it/s]Extractor Predicting: 56it [00:34,  1.66it/s]Extractor Predicting: 57it [00:35,  1.38it/s]Extractor Predicting: 58it [00:35,  1.46it/s]Extractor Predicting: 59it [00:36,  1.50it/s]Extractor Predicting: 60it [00:37,  1.54it/s]Extractor Predicting: 61it [00:37,  1.54it/s]Extractor Predicting: 62it [00:38,  1.50it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:40,  1.64it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.45it/s]Extractor Predicting: 68it [00:42,  1.50it/s]Extractor Predicting: 69it [00:42,  1.56it/s]Extractor Predicting: 70it [00:43,  1.56it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:44,  1.59it/s]Extractor Predicting: 73it [00:45,  1.59it/s]Extractor Predicting: 74it [00:46,  1.63it/s]Extractor Predicting: 75it [00:46,  1.63it/s]Extractor Predicting: 76it [00:47,  1.64it/s]Extractor Predicting: 77it [00:47,  1.68it/s]Extractor Predicting: 78it [00:48,  1.55it/s]Extractor Predicting: 79it [00:49,  1.60it/s]Extractor Predicting: 80it [00:49,  1.53it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.62it/s]Extractor Predicting: 83it [00:51,  1.62it/s]Extractor Predicting: 84it [00:52,  1.62it/s]Extractor Predicting: 85it [00:52,  1.55it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.64it/s]Extractor Predicting: 88it [00:54,  1.65it/s]Extractor Predicting: 89it [00:55,  1.71it/s]Extractor Predicting: 90it [00:55,  1.71it/s]Extractor Predicting: 91it [00:56,  1.51it/s]Extractor Predicting: 92it [00:57,  1.53it/s]Extractor Predicting: 93it [00:57,  1.56it/s]Extractor Predicting: 94it [00:58,  1.57it/s]Extractor Predicting: 95it [00:59,  1.58it/s]Extractor Predicting: 96it [00:59,  1.59it/s]Extractor Predicting: 97it [01:00,  1.61it/s]Extractor Predicting: 98it [01:01,  1.60it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:02,  1.56it/s]Extractor Predicting: 101it [01:02,  1.57it/s]Extractor Predicting: 102it [01:03,  1.61it/s]Extractor Predicting: 103it [01:04,  1.49it/s]Extractor Predicting: 104it [01:04,  1.53it/s]Extractor Predicting: 105it [01:05,  1.55it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:06,  1.58it/s]Extractor Predicting: 108it [01:07,  1.45it/s]Extractor Predicting: 109it [01:08,  1.49it/s]Extractor Predicting: 110it [01:08,  1.52it/s]Extractor Predicting: 111it [01:09,  1.54it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:10,  1.57it/s]Extractor Predicting: 114it [01:11,  1.59it/s]Extractor Predicting: 115it [01:12,  1.59it/s]Extractor Predicting: 116it [01:12,  1.48it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:13,  1.69it/s]Extractor Predicting: 119it [01:14,  1.70it/s]Extractor Predicting: 120it [01:14,  1.71it/s]Extractor Predicting: 121it [01:15,  1.70it/s]Extractor Predicting: 122it [01:16,  1.69it/s]Extractor Predicting: 123it [01:16,  1.69it/s]Extractor Predicting: 124it [01:17,  1.73it/s]Extractor Predicting: 125it [01:17,  1.75it/s]Extractor Predicting: 126it [01:18,  1.75it/s]Extractor Predicting: 127it [01:18,  1.78it/s]Extractor Predicting: 128it [01:19,  1.82it/s]Extractor Predicting: 129it [01:20,  1.61it/s]Extractor Predicting: 130it [01:20,  1.65it/s]Extractor Predicting: 131it [01:21,  1.70it/s]Extractor Predicting: 132it [01:21,  1.74it/s]Extractor Predicting: 133it [01:22,  1.76it/s]Extractor Predicting: 134it [01:23,  1.74it/s]Extractor Predicting: 135it [01:23,  1.78it/s]Extractor Predicting: 136it [01:24,  1.84it/s]Extractor Predicting: 137it [01:24,  1.86it/s]Extractor Predicting: 138it [01:25,  1.58it/s]Extractor Predicting: 139it [01:26,  1.61it/s]Extractor Predicting: 140it [01:26,  1.68it/s]Extractor Predicting: 141it [01:27,  1.68it/s]Extractor Predicting: 142it [01:27,  1.70it/s]Extractor Predicting: 143it [01:28,  1.68it/s]Extractor Predicting: 144it [01:28,  1.69it/s]Extractor Predicting: 145it [01:29,  1.71it/s]Extractor Predicting: 146it [01:30,  1.75it/s]Extractor Predicting: 147it [01:30,  1.76it/s]Extractor Predicting: 148it [01:31,  1.74it/s]Extractor Predicting: 149it [01:31,  1.81it/s]Extractor Predicting: 150it [01:32,  1.79it/s]Extractor Predicting: 151it [01:32,  1.77it/s]Extractor Predicting: 152it [01:33,  1.80it/s]Extractor Predicting: 153it [01:34,  1.74it/s]Extractor Predicting: 154it [01:34,  1.77it/s]Extractor Predicting: 155it [01:35,  1.79it/s]Extractor Predicting: 156it [01:35,  1.62it/s]Extractor Predicting: 157it [01:36,  1.66it/s]Extractor Predicting: 158it [01:37,  1.72it/s]Extractor Predicting: 159it [01:37,  1.70it/s]Extractor Predicting: 160it [01:38,  1.73it/s]Extractor Predicting: 161it [01:38,  1.64it/s]Extractor Predicting: 162it [01:39,  1.68it/s]Extractor Predicting: 163it [01:39,  1.71it/s]Extractor Predicting: 164it [01:40,  1.73it/s]Extractor Predicting: 165it [01:41,  1.77it/s]Extractor Predicting: 166it [01:41,  1.78it/s]Extractor Predicting: 167it [01:42,  1.79it/s]Extractor Predicting: 168it [01:42,  1.75it/s]Extractor Predicting: 169it [01:43,  1.77it/s]Extractor Predicting: 170it [01:44,  1.62it/s]Extractor Predicting: 171it [01:44,  1.64it/s]Extractor Predicting: 172it [01:45,  1.73it/s]Extractor Predicting: 173it [01:45,  1.69it/s]Extractor Predicting: 174it [01:46,  1.68it/s]Extractor Predicting: 175it [01:46,  1.66it/s]Extractor Predicting: 176it [01:47,  1.68it/s]Extractor Predicting: 177it [01:48,  1.66it/s]Extractor Predicting: 178it [01:48,  1.64it/s]Extractor Predicting: 179it [01:49,  1.44it/s]Extractor Predicting: 180it [01:50,  1.53it/s]Extractor Predicting: 181it [01:51,  1.38it/s]Extractor Predicting: 182it [01:51,  1.44it/s]Extractor Predicting: 183it [01:52,  1.49it/s]Extractor Predicting: 184it [01:53,  1.51it/s]Extractor Predicting: 185it [01:53,  1.54it/s]Extractor Predicting: 186it [01:54,  1.51it/s]Extractor Predicting: 187it [01:54,  1.53it/s]Extractor Predicting: 188it [01:55,  1.59it/s]Extractor Predicting: 189it [01:56,  1.61it/s]Extractor Predicting: 190it [01:56,  1.63it/s]Extractor Predicting: 191it [01:57,  1.59it/s]Extractor Predicting: 192it [01:58,  1.60it/s]Extractor Predicting: 193it [01:58,  1.58it/s]Extractor Predicting: 194it [01:59,  1.53it/s]Extractor Predicting: 195it [02:00,  1.55it/s]Extractor Predicting: 196it [02:00,  1.58it/s]Extractor Predicting: 197it [02:01,  1.58it/s]Extractor Predicting: 198it [02:01,  1.58it/s]Extractor Predicting: 199it [02:02,  1.58it/s]Extractor Predicting: 200it [02:03,  1.57it/s]Extractor Predicting: 201it [02:03,  1.59it/s]Extractor Predicting: 202it [02:04,  1.53it/s]Extractor Predicting: 203it [02:05,  1.56it/s]Extractor Predicting: 204it [02:05,  1.60it/s]Extractor Predicting: 205it [02:06,  1.67it/s]Extractor Predicting: 206it [02:06,  1.67it/s]Extractor Predicting: 207it [02:07,  1.64it/s]Extractor Predicting: 208it [02:08,  1.65it/s]Extractor Predicting: 209it [02:08,  1.64it/s]Extractor Predicting: 210it [02:09,  1.67it/s]Extractor Predicting: 211it [02:09,  1.69it/s]Extractor Predicting: 212it [02:10,  1.70it/s]Extractor Predicting: 213it [02:10,  1.73it/s]Extractor Predicting: 214it [02:11,  1.71it/s]Extractor Predicting: 215it [02:12,  1.67it/s]Extractor Predicting: 216it [02:12,  1.69it/s]Extractor Predicting: 217it [02:13,  1.70it/s]Extractor Predicting: 218it [02:13,  1.74it/s]Extractor Predicting: 219it [02:14,  1.76it/s]Extractor Predicting: 220it [02:15,  1.62it/s]Extractor Predicting: 221it [02:15,  1.68it/s]Extractor Predicting: 222it [02:16,  1.72it/s]Extractor Predicting: 223it [02:17,  1.29it/s]Extractor Predicting: 224it [02:18,  1.39it/s]Extractor Predicting: 225it [02:18,  1.48it/s]Extractor Predicting: 226it [02:19,  1.50it/s]Extractor Predicting: 227it [02:20,  1.45it/s]Extractor Predicting: 228it [02:20,  1.52it/s]Extractor Predicting: 229it [02:21,  1.55it/s]Extractor Predicting: 230it [02:21,  1.57it/s]Extractor Predicting: 231it [02:22,  1.60it/s]Extractor Predicting: 232it [02:23,  1.65it/s]Extractor Predicting: 233it [02:23,  1.73it/s]Extractor Predicting: 234it [02:24,  1.75it/s]Extractor Predicting: 235it [02:24,  1.76it/s]Extractor Predicting: 236it [02:25,  1.75it/s]Extractor Predicting: 237it [02:25,  1.77it/s]Extractor Predicting: 238it [02:26,  1.79it/s]Extractor Predicting: 239it [02:26,  1.80it/s]Extractor Predicting: 240it [02:27,  1.83it/s]Extractor Predicting: 241it [02:27,  1.82it/s]Extractor Predicting: 242it [02:28,  1.83it/s]Extractor Predicting: 243it [02:28,  1.89it/s]Extractor Predicting: 244it [02:29,  1.87it/s]Extractor Predicting: 245it [02:29,  1.94it/s]Extractor Predicting: 246it [02:30,  1.78it/s]Extractor Predicting: 247it [02:31,  1.78it/s]Extractor Predicting: 248it [02:31,  1.76it/s]Extractor Predicting: 249it [02:33,  1.19it/s]Extractor Predicting: 250it [02:33,  1.34it/s]Extractor Predicting: 251it [02:34,  1.48it/s]Extractor Predicting: 252it [02:34,  1.60it/s]Extractor Predicting: 253it [02:35,  1.59it/s]Extractor Predicting: 254it [02:36,  1.65it/s]Extractor Predicting: 255it [02:36,  1.73it/s]Extractor Predicting: 256it [02:37,  1.77it/s]Extractor Predicting: 257it [02:37,  1.82it/s]Extractor Predicting: 258it [02:38,  1.43it/s]Extractor Predicting: 259it [02:39,  1.56it/s]Extractor Predicting: 260it [02:39,  1.64it/s]Extractor Predicting: 261it [02:40,  1.62it/s]Extractor Predicting: 262it [02:40,  1.68it/s]Extractor Predicting: 263it [02:41,  1.51it/s]Extractor Predicting: 264it [02:42,  1.53it/s]Extractor Predicting: 265it [02:42,  1.57it/s]Extractor Predicting: 266it [02:43,  1.61it/s]Extractor Predicting: 267it [02:44,  1.32it/s]Extractor Predicting: 268it [02:45,  1.42it/s]Extractor Predicting: 269it [02:45,  1.47it/s]Extractor Predicting: 270it [02:46,  1.50it/s]Extractor Predicting: 271it [02:47,  1.22it/s]Extractor Predicting: 272it [02:48,  1.34it/s]Extractor Predicting: 273it [02:48,  1.42it/s]Extractor Predicting: 274it [02:49,  1.48it/s]Extractor Predicting: 275it [02:49,  1.53it/s]Extractor Predicting: 276it [02:50,  1.57it/s]Extractor Predicting: 277it [02:51,  1.58it/s]Extractor Predicting: 278it [02:51,  1.60it/s]Extractor Predicting: 279it [02:52,  1.55it/s]Extractor Predicting: 280it [02:53,  1.56it/s]Extractor Predicting: 281it [02:53,  1.57it/s]Extractor Predicting: 282it [02:54,  1.59it/s]Extractor Predicting: 283it [02:54,  1.61it/s]Extractor Predicting: 284it [02:55,  1.46it/s]Extractor Predicting: 285it [02:56,  1.52it/s]Extractor Predicting: 286it [02:56,  1.55it/s]Extractor Predicting: 287it [02:57,  1.56it/s]Extractor Predicting: 288it [02:58,  1.63it/s]Extractor Predicting: 289it [02:58,  1.53it/s]Extractor Predicting: 290it [02:59,  1.59it/s]Extractor Predicting: 291it [03:00,  1.63it/s]Extractor Predicting: 292it [03:00,  1.44it/s]Extractor Predicting: 293it [03:01,  1.51it/s]Extractor Predicting: 294it [03:02,  1.19it/s]Extractor Predicting: 295it [03:03,  1.31it/s]Extractor Predicting: 296it [03:03,  1.40it/s]Extractor Predicting: 297it [03:04,  1.49it/s]Extractor Predicting: 298it [03:05,  1.55it/s]Extractor Predicting: 299it [03:05,  1.57it/s]Extractor Predicting: 300it [03:06,  1.62it/s]Extractor Predicting: 301it [03:06,  1.64it/s]Extractor Predicting: 302it [03:07,  1.64it/s]Extractor Predicting: 303it [03:08,  1.65it/s]Extractor Predicting: 304it [03:08,  1.61it/s]Extractor Predicting: 305it [03:09,  1.62it/s]Extractor Predicting: 306it [03:09,  1.66it/s]Extractor Predicting: 307it [03:10,  1.67it/s]Extractor Predicting: 308it [03:11,  1.66it/s]Extractor Predicting: 309it [03:11,  1.58it/s]Extractor Predicting: 310it [03:12,  1.62it/s]Extractor Predicting: 311it [03:13,  1.64it/s]Extractor Predicting: 312it [03:13,  1.65it/s]Extractor Predicting: 313it [03:14,  1.66it/s]Extractor Predicting: 314it [03:15,  1.34it/s]Extractor Predicting: 315it [03:15,  1.44it/s]Extractor Predicting: 316it [03:16,  1.50it/s]Extractor Predicting: 317it [03:17,  1.57it/s]Extractor Predicting: 318it [03:17,  1.64it/s]Extractor Predicting: 319it [03:18,  1.38it/s]Extractor Predicting: 320it [03:19,  1.45it/s]Extractor Predicting: 321it [03:19,  1.51it/s]Extractor Predicting: 322it [03:20,  1.56it/s]Extractor Predicting: 323it [03:20,  1.61it/s]Extractor Predicting: 324it [03:21,  1.64it/s]Extractor Predicting: 325it [03:22,  1.65it/s]Extractor Predicting: 326it [03:22,  1.68it/s]Extractor Predicting: 327it [03:23,  1.69it/s]Extractor Predicting: 328it [03:23,  1.64it/s]Extractor Predicting: 329it [03:24,  1.66it/s]Extractor Predicting: 330it [03:25,  1.66it/s]Extractor Predicting: 331it [03:26,  1.35it/s]Extractor Predicting: 332it [03:26,  1.46it/s]Extractor Predicting: 333it [03:27,  1.49it/s]Extractor Predicting: 334it [03:27,  1.56it/s]Extractor Predicting: 335it [03:28,  1.50it/s]Extractor Predicting: 336it [03:29,  1.57it/s]Extractor Predicting: 337it [03:29,  1.60it/s]Extractor Predicting: 338it [03:30,  1.63it/s]Extractor Predicting: 339it [03:31,  1.66it/s]Extractor Predicting: 340it [03:31,  1.65it/s]Extractor Predicting: 341it [03:32,  1.69it/s]Extractor Predicting: 342it [03:32,  1.72it/s]Extractor Predicting: 343it [03:33,  1.71it/s]Extractor Predicting: 344it [03:33,  1.76it/s]Extractor Predicting: 345it [03:34,  1.67it/s]Extractor Predicting: 346it [03:35,  1.69it/s]Extractor Predicting: 347it [03:35,  1.71it/s]Extractor Predicting: 348it [03:36,  1.36it/s]Extractor Predicting: 349it [03:37,  1.45it/s]Extractor Predicting: 350it [03:37,  1.51it/s]Extractor Predicting: 351it [03:38,  1.57it/s]Extractor Predicting: 352it [03:39,  1.60it/s]Extractor Predicting: 353it [03:39,  1.45it/s]Extractor Predicting: 354it [03:40,  1.53it/s]Extractor Predicting: 355it [03:41,  1.56it/s]Extractor Predicting: 356it [03:42,  1.41it/s]Extractor Predicting: 357it [03:42,  1.46it/s]Extractor Predicting: 358it [03:43,  1.54it/s]Extractor Predicting: 359it [03:43,  1.59it/s]Extractor Predicting: 360it [03:44,  1.62it/s]Extractor Predicting: 361it [03:45,  1.44it/s]Extractor Predicting: 362it [03:45,  1.54it/s]Extractor Predicting: 363it [03:46,  1.58it/s]Extractor Predicting: 364it [03:47,  1.44it/s]Extractor Predicting: 365it [03:47,  1.50it/s]Extractor Predicting: 366it [03:48,  1.57it/s]Extractor Predicting: 367it [03:48,  1.61it/s]Extractor Predicting: 368it [03:49,  1.64it/s]Extractor Predicting: 369it [03:50,  1.47it/s]Extractor Predicting: 370it [03:50,  1.53it/s]Extractor Predicting: 371it [03:51,  1.58it/s]Extractor Predicting: 372it [03:52,  1.62it/s]Extractor Predicting: 373it [03:52,  1.63it/s]Extractor Predicting: 374it [03:53,  1.46it/s]Extractor Predicting: 375it [03:54,  1.52it/s]Extractor Predicting: 376it [03:54,  1.54it/s]Extractor Predicting: 377it [03:55,  1.64it/s]Extractor Predicting: 378it [03:55,  1.67it/s]Extractor Predicting: 379it [03:56,  1.62it/s]Extractor Predicting: 380it [03:57,  1.62it/s]Extractor Predicting: 381it [03:57,  1.65it/s]Extractor Predicting: 382it [03:58,  1.66it/s]Extractor Predicting: 383it [03:58,  1.66it/s]Extractor Predicting: 384it [04:00,  1.31it/s]Extractor Predicting: 385it [04:00,  1.41it/s]Extractor Predicting: 386it [04:01,  1.47it/s]Extractor Predicting: 387it [04:01,  1.53it/s]Extractor Predicting: 388it [04:02,  1.52it/s]Extractor Predicting: 389it [04:03,  1.56it/s]Extractor Predicting: 390it [04:03,  1.60it/s]Extractor Predicting: 391it [04:04,  1.64it/s]Extractor Predicting: 392it [04:05,  1.46it/s]Extractor Predicting: 393it [04:06,  1.27it/s]Extractor Predicting: 394it [04:06,  1.38it/s]Extractor Predicting: 395it [04:07,  1.46it/s]Extractor Predicting: 396it [04:09,  1.04it/s]Extractor Predicting: 397it [04:09,  1.17it/s]Extractor Predicting: 398it [04:10,  1.27it/s]Extractor Predicting: 399it [04:10,  1.34it/s]Extractor Predicting: 400it [04:11,  1.41it/s]Extractor Predicting: 401it [04:12,  1.49it/s]Extractor Predicting: 402it [04:12,  1.53it/s]Extractor Predicting: 403it [04:13,  1.56it/s]Extractor Predicting: 404it [04:14,  1.28it/s]Extractor Predicting: 405it [04:15,  1.39it/s]Extractor Predicting: 406it [04:15,  1.49it/s]Extractor Predicting: 407it [04:16,  1.55it/s]Extractor Predicting: 408it [04:16,  1.58it/s]Extractor Predicting: 409it [04:17,  1.59it/s]Extractor Predicting: 410it [04:17,  1.63it/s]Extractor Predicting: 411it [04:18,  1.65it/s]Extractor Predicting: 412it [04:19,  1.69it/s]Extractor Predicting: 413it [04:19,  1.63it/s]Extractor Predicting: 414it [04:20,  1.64it/s]Extractor Predicting: 415it [04:20,  1.63it/s]Extractor Predicting: 416it [04:21,  1.68it/s]Extractor Predicting: 417it [04:22,  1.69it/s]Extractor Predicting: 418it [04:22,  1.67it/s]Extractor Predicting: 419it [04:23,  1.70it/s]Extractor Predicting: 420it [04:23,  1.73it/s]Extractor Predicting: 421it [04:24,  1.70it/s]Extractor Predicting: 422it [04:25,  1.71it/s]Extractor Predicting: 423it [04:25,  1.72it/s]Extractor Predicting: 424it [04:26,  1.66it/s]Extractor Predicting: 425it [04:26,  1.66it/s]Extractor Predicting: 426it [04:27,  1.71it/s]Extractor Predicting: 427it [04:28,  1.70it/s]Extractor Predicting: 428it [04:28,  1.66it/s]Extractor Predicting: 429it [04:29,  1.73it/s]Extractor Predicting: 429it [04:29,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:19,436 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:19,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:19,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:19,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:19,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:50:20,152 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:50:20,153 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:50:21,135 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:50:22,160 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:50:22,161 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:25,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:25,976 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:25,976 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:25,976 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:50:25,976 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:50:26,793 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:50:26,794 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:50:27,736 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:50:27,903 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:50:27,903 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 5it [00:03,  1.50it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_2', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:19<06:04, 19.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:34<04:59, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:53<05:06, 18.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:09<04:31, 16.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:25<04:12, 16.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:42<03:54, 16.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:00<03:46, 17.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:18<03:31, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:38<03:18, 18.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:55<02:58, 17.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:12<02:38, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:31<02:23, 17.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:49<02:05, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:07<01:48, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:24<01:28, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:44<01:13, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:00<00:53, 17.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:17<00:34, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:34<00:17, 17.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:50<00:00, 16.93s/it]Generating: 100%|| 20/20 [05:50<00:00, 17.52s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of Lpez 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of Lpez 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Dmontagne " , including Boulogne \'s " Montessemble des deux de Chteau des Gains " . Head Entity : Montessemble des deux de Chteau des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 11411143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')", 'not enough values to unpack (expected 2, got 1)', '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of Franois Renoul .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:20, 20.43s/it]Extractor Estimating: 2it [00:22,  9.58s/it]Extractor Estimating: 3it [00:23,  5.50s/it]Extractor Estimating: 4it [00:23,  3.57s/it]Extractor Estimating: 5it [00:24,  2.53s/it]Extractor Estimating: 6it [00:24,  1.85s/it]Extractor Estimating: 7it [00:27,  1.99s/it]Extractor Estimating: 8it [00:27,  1.57s/it]Extractor Estimating: 9it [00:28,  1.27s/it]Extractor Estimating: 10it [00:29,  1.08s/it]Extractor Estimating: 11it [00:29,  1.05it/s]Extractor Estimating: 12it [00:30,  1.20it/s]Extractor Estimating: 13it [00:31,  1.18it/s]Extractor Estimating: 14it [00:31,  1.27it/s]Extractor Estimating: 15it [00:32,  1.39it/s]Extractor Estimating: 16it [00:32,  1.49it/s]Extractor Estimating: 17it [00:33,  1.57it/s]Extractor Estimating: 18it [00:34,  1.41it/s]Extractor Estimating: 19it [00:34,  1.49it/s]Extractor Estimating: 20it [00:35,  1.52it/s]Extractor Estimating: 21it [00:36,  1.53it/s]Extractor Estimating: 22it [00:36,  1.58it/s]Extractor Estimating: 23it [00:37,  1.56it/s]Extractor Estimating: 24it [00:38,  1.59it/s]Extractor Estimating: 25it [00:38,  1.60it/s]Extractor Estimating: 26it [00:39,  1.68it/s]Extractor Estimating: 27it [00:39,  1.69it/s]Extractor Estimating: 28it [00:40,  1.72it/s]Extractor Estimating: 29it [00:41,  1.52it/s]Extractor Estimating: 30it [00:41,  1.55it/s]Extractor Estimating: 31it [00:42,  1.59it/s]Extractor Estimating: 32it [00:43,  1.59it/s]Extractor Estimating: 33it [00:43,  1.57it/s]Extractor Estimating: 34it [00:44,  1.48it/s]Extractor Estimating: 35it [00:45,  1.50it/s]Extractor Estimating: 36it [00:45,  1.50it/s]Extractor Estimating: 37it [00:46,  1.48it/s]Extractor Estimating: 38it [00:47,  1.57it/s]Extractor Estimating: 39it [00:47,  1.40it/s]Extractor Estimating: 40it [00:48,  1.41it/s]Extractor Estimating: 41it [00:49,  1.50it/s]Extractor Estimating: 42it [00:49,  1.50it/s]Extractor Estimating: 43it [00:50,  1.52it/s]Extractor Estimating: 44it [00:51,  1.48it/s]Extractor Estimating: 45it [00:51,  1.51it/s]Extractor Estimating: 46it [00:52,  1.51it/s]Extractor Estimating: 47it [00:53,  1.58it/s]Extractor Estimating: 48it [00:53,  1.54it/s]Extractor Estimating: 49it [00:54,  1.54it/s]Extractor Estimating: 50it [00:54,  1.61it/s]Extractor Estimating: 51it [00:55,  1.59it/s]Extractor Estimating: 52it [00:56,  1.56it/s]Extractor Estimating: 53it [00:57,  1.50it/s]Extractor Estimating: 54it [00:57,  1.42it/s]Extractor Estimating: 55it [00:58,  1.50it/s]Extractor Estimating: 56it [00:59,  1.53it/s]Extractor Estimating: 57it [00:59,  1.50it/s]Extractor Estimating: 58it [01:00,  1.57it/s]Extractor Estimating: 59it [01:00,  1.53it/s]Extractor Estimating: 60it [01:01,  1.52it/s]Extractor Estimating: 61it [01:02,  1.53it/s]Extractor Estimating: 62it [01:02,  1.52it/s]Extractor Estimating: 63it [01:03,  1.58it/s]Extractor Estimating: 64it [01:04,  1.34it/s]Extractor Estimating: 65it [01:05,  1.37it/s]Extractor Estimating: 66it [01:05,  1.39it/s]Extractor Estimating: 67it [01:06,  1.42it/s]Extractor Estimating: 68it [01:07,  1.32it/s]Extractor Estimating: 69it [01:08,  1.35it/s]Extractor Estimating: 70it [01:08,  1.41it/s]Extractor Estimating: 71it [01:09,  1.48it/s]Extractor Estimating: 72it [01:10,  1.51it/s]Extractor Estimating: 73it [01:10,  1.35it/s]Extractor Estimating: 74it [01:11,  1.40it/s]Extractor Estimating: 75it [01:12,  1.45it/s]Extractor Estimating: 76it [01:12,  1.48it/s]Extractor Estimating: 77it [01:14,  1.14it/s]Extractor Estimating: 78it [01:15,  1.14it/s]Extractor Estimating: 79it [01:15,  1.26it/s]Extractor Estimating: 80it [01:16,  1.32it/s]Extractor Estimating: 81it [01:17,  1.38it/s]Extractor Estimating: 82it [01:17,  1.43it/s]Extractor Estimating: 83it [01:18,  1.39it/s]Extractor Estimating: 84it [01:19,  1.41it/s]Extractor Estimating: 85it [01:19,  1.43it/s]Extractor Estimating: 86it [01:20,  1.51it/s]Extractor Estimating: 87it [01:21,  1.40it/s]Extractor Estimating: 88it [01:22,  1.32it/s]Extractor Estimating: 89it [01:22,  1.38it/s]Extractor Estimating: 90it [01:23,  1.45it/s]Extractor Estimating: 91it [01:23,  1.55it/s]Extractor Estimating: 92it [01:24,  1.53it/s]Extractor Estimating: 93it [01:25,  1.25it/s]Extractor Estimating: 94it [01:26,  1.35it/s]Extractor Estimating: 95it [01:26,  1.41it/s]Extractor Estimating: 96it [01:27,  1.47it/s]Extractor Estimating: 97it [01:28,  1.43it/s]Extractor Estimating: 98it [01:29,  1.42it/s]Extractor Estimating: 99it [01:29,  1.47it/s]Extractor Estimating: 100it [01:30,  1.48it/s]Extractor Estimating: 101it [01:30,  1.49it/s]Extractor Estimating: 102it [01:31,  1.53it/s]Extractor Estimating: 103it [01:32,  1.44it/s]Extractor Estimating: 104it [01:33,  1.47it/s]Extractor Estimating: 105it [01:33,  1.53it/s]Extractor Estimating: 106it [01:34,  1.57it/s]Extractor Estimating: 107it [01:34,  1.60it/s]Extractor Estimating: 108it [01:35,  1.50it/s]Extractor Estimating: 109it [01:36,  1.55it/s]Extractor Estimating: 110it [01:36,  1.65it/s]Extractor Estimating: 111it [01:37,  1.57it/s]Extractor Estimating: 112it [01:38,  1.57it/s]Extractor Estimating: 113it [01:38,  1.56it/s]Extractor Estimating: 114it [01:39,  1.60it/s]Extractor Estimating: 115it [01:39,  1.55it/s]Extractor Estimating: 116it [01:40,  1.55it/s]Extractor Estimating: 117it [01:41,  1.54it/s]Extractor Estimating: 118it [01:42,  1.29it/s]Extractor Estimating: 119it [01:42,  1.37it/s]Extractor Estimating: 120it [01:43,  1.44it/s]Extractor Estimating: 121it [01:44,  1.53it/s]Extractor Estimating: 122it [01:44,  1.59it/s]Extractor Estimating: 123it [01:45,  1.58it/s]Extractor Estimating: 124it [01:45,  1.64it/s]Extractor Estimating: 125it [01:46,  1.64it/s]Extractor Estimating: 126it [01:47,  1.68it/s]Extractor Estimating: 127it [01:47,  1.58it/s]Extractor Estimating: 128it [01:48,  1.38it/s]Extractor Estimating: 129it [01:49,  1.42it/s]Extractor Estimating: 130it [01:50,  1.44it/s]Extractor Estimating: 131it [01:50,  1.50it/s]Extractor Estimating: 132it [01:51,  1.46it/s]Extractor Estimating: 133it [01:52,  1.38it/s]Extractor Estimating: 134it [01:52,  1.43it/s]Extractor Estimating: 135it [01:53,  1.45it/s]Extractor Estimating: 136it [01:54,  1.52it/s]Extractor Estimating: 137it [01:54,  1.55it/s]Extractor Estimating: 138it [01:55,  1.36it/s]Extractor Estimating: 139it [01:56,  1.36it/s]Extractor Estimating: 140it [01:56,  1.44it/s]Extractor Estimating: 141it [01:57,  1.44it/s]Extractor Estimating: 142it [01:58,  1.47it/s]Extractor Estimating: 143it [01:59,  1.23it/s]Extractor Estimating: 144it [02:00,  1.28it/s]Extractor Estimating: 145it [02:00,  1.37it/s]Extractor Estimating: 146it [02:01,  1.40it/s]Extractor Estimating: 147it [02:02,  1.35it/s]Extractor Estimating: 148it [02:02,  1.37it/s]Extractor Estimating: 149it [02:03,  1.45it/s]Extractor Estimating: 150it [02:04,  1.48it/s]Extractor Estimating: 151it [02:04,  1.54it/s]Extractor Estimating: 152it [02:05,  1.34it/s]Extractor Estimating: 153it [02:06,  1.44it/s]Extractor Estimating: 154it [02:06,  1.58it/s]Extractor Estimating: 155it [02:07,  1.64it/s]Extractor Estimating: 156it [02:07,  1.64it/s]Extractor Estimating: 157it [02:08,  1.51it/s]Extractor Estimating: 158it [02:09,  1.56it/s]Extractor Estimating: 159it [02:09,  1.58it/s]Extractor Estimating: 160it [02:10,  1.67it/s]Extractor Estimating: 161it [02:11,  1.72it/s]Extractor Estimating: 162it [02:11,  1.51it/s]Extractor Estimating: 163it [02:12,  1.59it/s]Extractor Estimating: 164it [02:12,  1.64it/s]Extractor Estimating: 165it [02:13,  1.65it/s]Extractor Estimating: 166it [02:14,  1.67it/s]Extractor Estimating: 167it [02:15,  1.41it/s]Extractor Estimating: 168it [02:15,  1.53it/s]Extractor Estimating: 169it [02:16,  1.62it/s]Extractor Estimating: 170it [02:16,  1.63it/s]Extractor Estimating: 171it [02:19,  1.36s/it]Extractor Estimating: 172it [02:20,  1.21s/it]Extractor Estimating: 173it [02:21,  1.02s/it]Extractor Estimating: 174it [02:21,  1.13it/s]Extractor Estimating: 175it [02:22,  1.26it/s]Extractor Estimating: 176it [02:23,  1.35it/s]Extractor Estimating: 177it [02:23,  1.35it/s]Extractor Estimating: 178it [02:24,  1.46it/s]Extractor Estimating: 179it [02:25,  1.49it/s]Extractor Estimating: 180it [02:25,  1.39it/s]Extractor Estimating: 181it [02:26,  1.47it/s]Extractor Estimating: 182it [02:26,  1.58it/s]Extractor Estimating: 183it [02:27,  1.59it/s]Extractor Estimating: 184it [02:28,  1.53it/s]Extractor Estimating: 185it [02:28,  1.60it/s]Extractor Estimating: 186it [02:29,  1.59it/s]Extractor Estimating: 187it [02:30,  1.35it/s]Extractor Estimating: 188it [02:31,  1.43it/s]Extractor Estimating: 189it [02:31,  1.47it/s]Extractor Estimating: 190it [02:32,  1.52it/s]Extractor Estimating: 191it [02:32,  1.57it/s]Extractor Estimating: 192it [02:33,  1.46it/s]Extractor Estimating: 193it [02:34,  1.54it/s]Extractor Estimating: 194it [02:34,  1.59it/s]Extractor Estimating: 195it [02:35,  1.58it/s]Extractor Estimating: 196it [02:36,  1.60it/s]Extractor Estimating: 197it [02:36,  1.56it/s]Extractor Estimating: 198it [02:37,  1.57it/s]Extractor Estimating: 199it [02:38,  1.62it/s]Extractor Estimating: 200it [02:38,  1.61it/s]Extractor Estimating: 201it [02:39,  1.60it/s]Extractor Estimating: 202it [02:40,  1.20it/s]Extractor Estimating: 203it [02:41,  1.32it/s]Extractor Estimating: 204it [02:41,  1.37it/s]Extractor Estimating: 205it [02:42,  1.42it/s]Extractor Estimating: 206it [02:43,  1.43it/s]Extractor Estimating: 207it [02:43,  1.36it/s]Extractor Estimating: 208it [02:44,  1.40it/s]Extractor Estimating: 209it [02:45,  1.46it/s]Extractor Estimating: 210it [02:45,  1.48it/s]Extractor Estimating: 211it [02:46,  1.44it/s]Extractor Estimating: 212it [02:47,  1.40it/s]Extractor Estimating: 213it [02:48,  1.46it/s]Extractor Estimating: 214it [02:48,  1.50it/s]Extractor Estimating: 215it [02:49,  1.47it/s]Extractor Estimating: 216it [02:50,  1.42it/s]Extractor Estimating: 217it [02:50,  1.44it/s]Extractor Estimating: 218it [02:51,  1.45it/s]Extractor Estimating: 219it [02:52,  1.44it/s]Extractor Estimating: 220it [02:52,  1.49it/s]Extractor Estimating: 221it [02:53,  1.48it/s]Extractor Estimating: 222it [02:54,  1.43it/s]Extractor Estimating: 223it [02:54,  1.51it/s]Extractor Estimating: 224it [02:55,  1.51it/s]Extractor Estimating: 225it [02:56,  1.49it/s]Extractor Estimating: 226it [02:57,  1.30it/s]Extractor Estimating: 227it [02:58,  1.06it/s]Extractor Estimating: 228it [02:59,  1.19it/s]Extractor Estimating: 229it [02:59,  1.30it/s]Extractor Estimating: 230it [03:00,  1.35it/s]Extractor Estimating: 231it [03:01,  1.01it/s]Extractor Estimating: 232it [03:02,  1.13it/s]Extractor Estimating: 233it [03:03,  1.21it/s]Extractor Estimating: 234it [03:03,  1.34it/s]Extractor Estimating: 235it [03:04,  1.34it/s]Extractor Estimating: 236it [03:05,  1.41it/s]Extractor Estimating: 237it [03:05,  1.45it/s]Extractor Estimating: 238it [03:06,  1.48it/s]Extractor Estimating: 239it [03:07,  1.52it/s]Extractor Estimating: 240it [03:07,  1.49it/s]Extractor Estimating: 241it [03:08,  1.49it/s]Extractor Estimating: 242it [03:09,  1.53it/s]Extractor Estimating: 243it [03:09,  1.57it/s]Extractor Estimating: 244it [03:10,  1.54it/s]Extractor Estimating: 245it [03:11,  1.51it/s]Extractor Estimating: 246it [03:11,  1.51it/s]Extractor Estimating: 247it [03:12,  1.58it/s]Extractor Estimating: 248it [03:13,  1.52it/s]Extractor Estimating: 249it [03:13,  1.50it/s]Extractor Estimating: 250it [03:14,  1.49it/s]Extractor Estimating: 251it [03:15,  1.51it/s]Extractor Estimating: 252it [03:15,  1.51it/s]Extractor Estimating: 253it [03:16,  1.52it/s]Extractor Estimating: 254it [03:16,  1.57it/s]Extractor Estimating: 255it [03:18,  1.30it/s]Extractor Estimating: 256it [03:18,  1.36it/s]Extractor Estimating: 257it [03:19,  1.43it/s]Extractor Estimating: 258it [03:19,  1.49it/s]Extractor Estimating: 259it [03:20,  1.54it/s]Extractor Estimating: 260it [03:21,  1.48it/s]Extractor Estimating: 261it [03:21,  1.48it/s]Extractor Estimating: 262it [03:22,  1.38it/s]Extractor Estimating: 263it [03:23,  1.44it/s]Extractor Estimating: 264it [03:23,  1.49it/s]Extractor Estimating: 265it [03:24,  1.46it/s]Extractor Estimating: 266it [03:25,  1.46it/s]Extractor Estimating: 267it [03:25,  1.53it/s]Extractor Estimating: 268it [03:26,  1.60it/s]Extractor Estimating: 269it [03:27,  1.68it/s]Extractor Estimating: 270it [03:27,  1.55it/s]Extractor Estimating: 271it [03:28,  1.59it/s]Extractor Estimating: 272it [03:29,  1.58it/s]Extractor Estimating: 273it [03:29,  1.63it/s]Extractor Estimating: 274it [03:30,  1.60it/s]Extractor Estimating: 275it [03:30,  1.57it/s]Extractor Estimating: 276it [03:31,  1.47it/s]Extractor Estimating: 277it [03:32,  1.53it/s]Extractor Estimating: 278it [03:32,  1.57it/s]Extractor Estimating: 279it [03:33,  1.59it/s]Extractor Estimating: 280it [03:34,  1.56it/s]Extractor Estimating: 281it [03:35,  1.38it/s]Extractor Estimating: 282it [03:35,  1.42it/s]Extractor Estimating: 283it [03:36,  1.49it/s]Extractor Estimating: 284it [03:37,  1.50it/s]Extractor Estimating: 285it [03:37,  1.51it/s]Extractor Estimating: 286it [03:38,  1.38it/s]Extractor Estimating: 287it [03:39,  1.42it/s]Extractor Estimating: 288it [03:39,  1.45it/s]Extractor Estimating: 289it [03:40,  1.48it/s]Extractor Estimating: 290it [03:41,  1.48it/s]Extractor Estimating: 291it [03:41,  1.45it/s]Extractor Estimating: 292it [03:42,  1.44it/s]Extractor Estimating: 293it [03:43,  1.48it/s]Extractor Estimating: 294it [03:43,  1.50it/s]Extractor Estimating: 295it [03:44,  1.52it/s]Extractor Estimating: 296it [03:45,  1.42it/s]Extractor Estimating: 297it [03:46,  1.39it/s]Extractor Estimating: 298it [03:46,  1.41it/s]Extractor Estimating: 299it [03:47,  1.43it/s]Extractor Estimating: 300it [03:48,  1.50it/s]Extractor Estimating: 301it [03:48,  1.42it/s]Extractor Estimating: 302it [03:49,  1.50it/s]Extractor Estimating: 303it [03:50,  1.54it/s]Extractor Estimating: 304it [03:50,  1.60it/s]Extractor Estimating: 305it [03:51,  1.60it/s]Extractor Estimating: 306it [03:52,  1.35it/s]Extractor Estimating: 307it [03:52,  1.42it/s]Extractor Estimating: 308it [03:53,  1.48it/s]Extractor Estimating: 309it [03:53,  1.59it/s]Extractor Estimating: 310it [03:54,  1.64it/s]Extractor Estimating: 311it [03:55,  1.44it/s]Extractor Estimating: 312it [03:56,  1.50it/s]Extractor Estimating: 313it [03:56,  1.55it/s]Extractor Estimating: 314it [03:57,  1.57it/s]Extractor Estimating: 315it [03:57,  1.62it/s]Extractor Estimating: 316it [03:58,  1.56it/s]Extractor Estimating: 317it [03:59,  1.36it/s]Extractor Estimating: 318it [04:00,  1.44it/s]Extractor Estimating: 319it [04:00,  1.48it/s]Extractor Estimating: 320it [04:01,  1.52it/s]Extractor Estimating: 321it [04:02,  1.38it/s]Extractor Estimating: 322it [04:02,  1.47it/s]Extractor Estimating: 323it [04:03,  1.56it/s]Extractor Estimating: 324it [04:03,  1.60it/s]Extractor Estimating: 325it [04:04,  1.62it/s]Extractor Estimating: 326it [04:05,  1.46it/s]Extractor Estimating: 327it [04:06,  1.47it/s]Extractor Estimating: 328it [04:06,  1.47it/s]Extractor Estimating: 329it [04:07,  1.47it/s]Extractor Estimating: 330it [04:07,  1.53it/s]Extractor Estimating: 331it [04:08,  1.55it/s]Extractor Estimating: 332it [04:09,  1.64it/s]Extractor Estimating: 333it [04:09,  1.61it/s]Extractor Estimating: 334it [04:10,  1.59it/s]Extractor Estimating: 335it [04:11,  1.59it/s]Extractor Estimating: 336it [04:11,  1.54it/s]Extractor Estimating: 337it [04:12,  1.53it/s]Extractor Estimating: 338it [04:13,  1.52it/s]Extractor Estimating: 339it [04:13,  1.61it/s]Extractor Estimating: 340it [04:14,  1.60it/s]Extractor Estimating: 341it [04:15,  1.44it/s]Extractor Estimating: 342it [04:15,  1.54it/s]Extractor Estimating: 343it [04:16,  1.57it/s]Extractor Estimating: 344it [04:16,  1.59it/s]Extractor Estimating: 345it [04:17,  1.59it/s]Extractor Estimating: 346it [04:18,  1.42it/s]Extractor Estimating: 347it [04:19,  1.40it/s]Extractor Estimating: 348it [04:19,  1.47it/s]Extractor Estimating: 349it [04:20,  1.48it/s]Extractor Estimating: 350it [04:20,  1.54it/s]Extractor Estimating: 351it [04:22,  1.21it/s]Extractor Estimating: 352it [04:22,  1.29it/s]Extractor Estimating: 353it [04:23,  1.36it/s]Extractor Estimating: 354it [04:24,  1.46it/s]Extractor Estimating: 355it [04:24,  1.37it/s]Extractor Estimating: 356it [04:25,  1.46it/s]Extractor Estimating: 357it [04:26,  1.50it/s]Extractor Estimating: 358it [04:26,  1.53it/s]Extractor Estimating: 359it [04:27,  1.59it/s]Extractor Estimating: 360it [04:28,  1.44it/s]Extractor Estimating: 361it [04:28,  1.46it/s]Extractor Estimating: 362it [04:29,  1.51it/s]Extractor Estimating: 363it [04:30,  1.55it/s]Extractor Estimating: 364it [04:30,  1.56it/s]Extractor Estimating: 365it [04:31,  1.56it/s]Extractor Estimating: 366it [04:31,  1.59it/s]Extractor Estimating: 367it [04:32,  1.50it/s]Extractor Estimating: 368it [04:33,  1.53it/s]Extractor Estimating: 369it [04:34,  1.39it/s]Extractor Estimating: 370it [04:34,  1.45it/s]Extractor Estimating: 371it [04:35,  1.48it/s]Extractor Estimating: 372it [04:36,  1.48it/s]Extractor Estimating: 373it [04:36,  1.53it/s]Extractor Estimating: 374it [04:37,  1.60it/s]Extractor Estimating: 375it [04:37,  1.60it/s]Extractor Estimating: 376it [04:38,  1.61it/s]Extractor Estimating: 377it [04:39,  1.54it/s]Extractor Estimating: 378it [04:39,  1.55it/s]Extractor Estimating: 379it [04:40,  1.60it/s]Extractor Estimating: 380it [04:41,  1.63it/s]Extractor Estimating: 381it [04:41,  1.59it/s]Extractor Estimating: 382it [04:42,  1.56it/s]Extractor Estimating: 383it [04:42,  1.58it/s]Extractor Estimating: 384it [04:43,  1.55it/s]Extractor Estimating: 385it [04:44,  1.53it/s]Extractor Estimating: 386it [04:44,  1.54it/s]Extractor Estimating: 387it [04:45,  1.35it/s]Extractor Estimating: 388it [04:46,  1.43it/s]Extractor Estimating: 389it [04:47,  1.43it/s]Extractor Estimating: 390it [04:47,  1.45it/s]Extractor Estimating: 391it [04:48,  1.47it/s]Extractor Estimating: 392it [04:49,  1.31it/s]Extractor Estimating: 393it [04:50,  1.35it/s]Extractor Estimating: 394it [04:50,  1.43it/s]Extractor Estimating: 395it [04:51,  1.48it/s]Extractor Estimating: 396it [04:52,  1.44it/s]Extractor Estimating: 397it [04:52,  1.44it/s]Extractor Estimating: 398it [04:53,  1.45it/s]Extractor Estimating: 399it [04:54,  1.48it/s]Extractor Estimating: 400it [04:54,  1.52it/s]Extractor Estimating: 401it [04:55,  1.56it/s]Extractor Estimating: 402it [04:56,  1.50it/s]Extractor Estimating: 403it [04:56,  1.57it/s]Extractor Estimating: 404it [04:57,  1.56it/s]Extractor Estimating: 405it [04:57,  1.61it/s]Extractor Estimating: 406it [04:58,  1.63it/s]Extractor Estimating: 407it [04:59,  1.44it/s]Extractor Estimating: 408it [04:59,  1.51it/s]Extractor Estimating: 409it [05:00,  1.55it/s]Extractor Estimating: 410it [05:01,  1.57it/s]Extractor Estimating: 411it [05:01,  1.57it/s]Extractor Estimating: 412it [05:02,  1.61it/s]Extractor Estimating: 413it [05:03,  1.61it/s]Extractor Estimating: 414it [05:03,  1.50it/s]Extractor Estimating: 415it [05:04,  1.53it/s]Extractor Estimating: 416it [05:05,  1.51it/s]Extractor Estimating: 417it [05:05,  1.58it/s]Extractor Estimating: 418it [05:06,  1.55it/s]Extractor Estimating: 419it [05:07,  1.26it/s]Extractor Estimating: 420it [05:08,  1.34it/s]Extractor Estimating: 421it [05:08,  1.39it/s]Extractor Estimating: 422it [05:09,  1.40it/s]Extractor Estimating: 423it [05:10,  1.39it/s]Extractor Estimating: 424it [05:10,  1.47it/s]Extractor Estimating: 425it [05:11,  1.48it/s]Extractor Estimating: 426it [05:12,  1.52it/s]Extractor Estimating: 427it [05:12,  1.56it/s]Extractor Estimating: 428it [05:13,  1.50it/s]Extractor Estimating: 429it [05:13,  1.56it/s]Extractor Estimating: 430it [05:14,  1.53it/s]Extractor Estimating: 431it [05:15,  1.52it/s]Extractor Estimating: 432it [05:15,  1.54it/s]Extractor Estimating: 433it [05:16,  1.48it/s]Extractor Estimating: 434it [05:17,  1.46it/s]Extractor Estimating: 435it [05:18,  1.47it/s]Extractor Estimating: 436it [05:18,  1.52it/s]Extractor Estimating: 437it [05:19,  1.55it/s]Extractor Estimating: 438it [05:20,  1.46it/s]Extractor Estimating: 439it [05:20,  1.46it/s]Extractor Estimating: 440it [05:21,  1.34it/s]Extractor Estimating: 441it [05:22,  1.38it/s]Extractor Estimating: 442it [05:23,  1.25it/s]Extractor Estimating: 443it [05:23,  1.33it/s]Extractor Estimating: 444it [05:24,  1.37it/s]Extractor Estimating: 445it [05:25,  1.39it/s]Extractor Estimating: 446it [05:25,  1.42it/s]Extractor Estimating: 447it [05:26,  1.40it/s]Extractor Estimating: 448it [05:27,  1.40it/s]Extractor Estimating: 449it [05:28,  1.39it/s]Extractor Estimating: 450it [05:28,  1.47it/s]Extractor Estimating: 451it [05:29,  1.57it/s]Extractor Estimating: 452it [05:30,  1.50it/s]Extractor Estimating: 453it [05:30,  1.49it/s]Extractor Estimating: 454it [05:31,  1.36it/s]Extractor Estimating: 455it [05:32,  1.44it/s]Extractor Estimating: 456it [05:32,  1.50it/s]Extractor Estimating: 457it [05:33,  1.53it/s]Extractor Estimating: 458it [05:34,  1.12it/s]Extractor Estimating: 459it [05:35,  1.26it/s]Extractor Estimating: 460it [05:36,  1.37it/s]Extractor Estimating: 461it [05:36,  1.42it/s]Extractor Estimating: 462it [05:37,  1.37it/s]Extractor Estimating: 463it [05:38,  1.44it/s]Extractor Estimating: 464it [05:38,  1.49it/s]Extractor Estimating: 465it [05:39,  1.49it/s]Extractor Estimating: 466it [05:39,  1.59it/s]Extractor Estimating: 467it [05:40,  1.37it/s]Extractor Estimating: 468it [05:41,  1.46it/s]Extractor Estimating: 469it [05:42,  1.52it/s]Extractor Estimating: 470it [05:42,  1.56it/s]Extractor Estimating: 471it [05:43,  1.59it/s]Extractor Estimating: 472it [05:43,  1.48it/s]Extractor Estimating: 473it [05:44,  1.51it/s]Extractor Estimating: 474it [05:45,  1.55it/s]Extractor Estimating: 475it [05:45,  1.60it/s]Extractor Estimating: 476it [05:46,  1.58it/s]Extractor Estimating: 477it [05:47,  1.53it/s]Extractor Estimating: 478it [05:47,  1.56it/s]Extractor Estimating: 479it [05:48,  1.56it/s]Extractor Estimating: 480it [05:49,  1.55it/s]Extractor Estimating: 481it [05:49,  1.55it/s]Extractor Estimating: 482it [05:50,  1.32it/s]Extractor Estimating: 483it [05:51,  1.40it/s]Extractor Estimating: 484it [05:51,  1.49it/s]Extractor Estimating: 485it [05:52,  1.51it/s]Extractor Estimating: 486it [05:53,  1.55it/s]Extractor Estimating: 487it [05:53,  1.48it/s]Extractor Estimating: 488it [05:54,  1.50it/s]Extractor Estimating: 489it [05:55,  1.52it/s]Extractor Estimating: 490it [05:55,  1.58it/s]Extractor Estimating: 491it [05:56,  1.59it/s]Extractor Estimating: 492it [05:57,  1.55it/s]Extractor Estimating: 493it [05:57,  1.56it/s]Extractor Estimating: 494it [05:58,  1.56it/s]Extractor Estimating: 495it [05:58,  1.57it/s]Extractor Estimating: 496it [05:59,  1.56it/s]Extractor Estimating: 497it [06:00,  1.20it/s]Extractor Estimating: 498it [06:01,  1.31it/s]Extractor Estimating: 499it [06:02,  1.37it/s]Extractor Estimating: 500it [06:02,  1.50it/s]Extractor Estimating: 500it [06:02,  1.38it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9999 mean pseudo reward: 0.9155517590331473
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 30923
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31023, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31023, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.273, loss:1290.7649
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.980, loss:1247.4957
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.975, loss:1218.5662
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.975, loss:1151.2963
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.071, loss:1124.3957
>> valid entity prec:0.5151, rec:0.5288, f1:0.5219
>> valid relation prec:0.1857, rec:0.0075, f1:0.0144
>> valid relation with NER prec:0.1857, rec:0.0075, f1:0.0144
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.264, loss:1160.5492
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.981, loss:1162.6150
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.974, loss:1149.1004
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.971, loss:1117.9570
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.997, loss:1127.9282
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5577, rec:0.3812, f1:0.4528
>> valid relation prec:0.1915, rec:0.0299, f1:0.0517
>> valid relation with NER prec:0.1915, rec:0.0299, f1:0.0517
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.243, loss:1074.2817
g_step 1200, step 366, avg_time 0.982, loss:1129.1568
g_step 1300, step 49, avg_time 0.968, loss:1066.0182
g_step 1400, step 149, avg_time 0.995, loss:1048.7556
g_step 1500, step 249, avg_time 0.980, loss:1033.6048
>> valid entity prec:0.5744, rec:0.4987, f1:0.5339
>> valid relation prec:0.1276, rec:0.0158, f1:0.0281
>> valid relation with NER prec:0.1276, rec:0.0158, f1:0.0281
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.277, loss:1056.8194
g_step 1700, step 32, avg_time 0.977, loss:1032.2335
g_step 1800, step 132, avg_time 0.985, loss:995.1595
g_step 1900, step 232, avg_time 0.990, loss:959.5196
g_step 2000, step 332, avg_time 0.987, loss:1001.6920
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5359, rec:0.5151, f1:0.5253
>> valid relation prec:0.0639, rec:0.0066, f1:0.0120
>> valid relation with NER prec:0.0639, rec:0.0066, f1:0.0120
g_step 2100, step 15, avg_time 2.255, loss:975.3460
g_step 2200, step 115, avg_time 0.980, loss:953.2067
g_step 2300, step 215, avg_time 0.985, loss:927.2206
g_step 2400, step 315, avg_time 0.986, loss:945.6170
g_step 2500, step 415, avg_time 1.000, loss:926.8058
>> valid entity prec:0.5234, rec:0.5732, f1:0.5472
>> valid relation prec:0.0750, rec:0.0152, f1:0.0253
>> valid relation with NER prec:0.0750, rec:0.0152, f1:0.0253
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.311, loss:887.9033
g_step 2700, step 198, avg_time 0.987, loss:879.0369
g_step 2800, step 298, avg_time 0.980, loss:881.5595
g_step 2900, step 398, avg_time 0.996, loss:923.3943
g_step 3000, step 81, avg_time 1.001, loss:844.3712
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5573, rec:0.6445, f1:0.5977
>> valid relation prec:0.0684, rec:0.0115, f1:0.0197
>> valid relation with NER prec:0.0684, rec:0.0115, f1:0.0197
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 181, avg_time 2.275, loss:856.4335
g_step 3200, step 281, avg_time 0.984, loss:873.7113
g_step 3300, step 381, avg_time 0.975, loss:843.4607
g_step 3400, step 64, avg_time 0.989, loss:819.3723
g_step 3500, step 164, avg_time 0.983, loss:786.6348
>> valid entity prec:0.5303, rec:0.6058, f1:0.5655
>> valid relation prec:0.0549, rec:0.0129, f1:0.0209
>> valid relation with NER prec:0.0549, rec:0.0129, f1:0.0209
g_step 3600, step 264, avg_time 2.237, loss:845.3857
g_step 3700, step 364, avg_time 0.987, loss:823.1203
g_step 3800, step 47, avg_time 0.969, loss:779.2908
g_step 3900, step 147, avg_time 0.984, loss:787.8788
g_step 4000, step 247, avg_time 0.985, loss:810.2329
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5497, rec:0.7022, f1:0.6167
>> valid relation prec:0.0788, rec:0.0201, f1:0.0320
>> valid relation with NER prec:0.0788, rec:0.0201, f1:0.0320
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 347, avg_time 2.308, loss:794.8303
g_step 4200, step 30, avg_time 0.979, loss:770.1934
g_step 4300, step 130, avg_time 0.984, loss:739.6747
g_step 4400, step 230, avg_time 0.992, loss:751.2437
g_step 4500, step 330, avg_time 0.992, loss:757.7519
>> valid entity prec:0.5120, rec:0.6656, f1:0.5788
>> valid relation prec:0.0736, rec:0.0258, f1:0.0383
>> valid relation with NER prec:0.0736, rec:0.0258, f1:0.0383
g_step 4600, step 13, avg_time 2.257, loss:748.3588
g_step 4700, step 113, avg_time 0.992, loss:716.8540
g_step 4800, step 213, avg_time 0.978, loss:726.5505
g_step 4900, step 313, avg_time 0.995, loss:725.7351
g_step 5000, step 413, avg_time 0.991, loss:762.5636
learning rate was adjusted to 0.0008
>> valid entity prec:0.4980, rec:0.6720, f1:0.5721
>> valid relation prec:0.0605, rec:0.0161, f1:0.0254
>> valid relation with NER prec:0.0605, rec:0.0161, f1:0.0254
g_step 5100, step 96, avg_time 2.274, loss:653.5731
g_step 5200, step 196, avg_time 0.980, loss:704.8654
g_step 5300, step 296, avg_time 0.983, loss:713.5393
g_step 5400, step 396, avg_time 0.996, loss:732.6876
g_step 5500, step 79, avg_time 0.979, loss:670.2654
>> valid entity prec:0.5119, rec:0.6340, f1:0.5664
>> valid relation prec:0.0612, rec:0.0181, f1:0.0279
>> valid relation with NER prec:0.0612, rec:0.0181, f1:0.0279
g_step 5600, step 179, avg_time 2.260, loss:662.7996
g_step 5700, step 279, avg_time 0.979, loss:671.7032
g_step 5800, step 379, avg_time 0.984, loss:673.1801
g_step 5900, step 62, avg_time 0.988, loss:645.6569
g_step 6000, step 162, avg_time 0.987, loss:647.6378
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5999, rec:0.5614, f1:0.5800
>> valid relation prec:0.0540, rec:0.0103, f1:0.0174
>> valid relation with NER prec:0.0540, rec:0.0103, f1:0.0174
g_step 6100, step 262, avg_time 2.266, loss:653.0818
g_step 6200, step 362, avg_time 0.986, loss:657.7702
g_step 6300, step 45, avg_time 0.973, loss:636.3961
g_step 6400, step 145, avg_time 0.982, loss:616.2915
g_step 6500, step 245, avg_time 0.991, loss:614.5963
>> valid entity prec:0.5384, rec:0.6245, f1:0.5783
>> valid relation prec:0.0745, rec:0.0172, f1:0.0280
>> valid relation with NER prec:0.0745, rec:0.0172, f1:0.0280
g_step 6600, step 345, avg_time 2.254, loss:631.5296
g_step 6700, step 28, avg_time 0.986, loss:628.5745
g_step 6800, step 128, avg_time 0.987, loss:595.4064
g_step 6900, step 228, avg_time 0.986, loss:609.6821
g_step 7000, step 328, avg_time 0.990, loss:607.4672
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5476, rec:0.6005, f1:0.5728
>> valid relation prec:0.0203, rec:0.0063, f1:0.0096
>> valid relation with NER prec:0.0203, rec:0.0063, f1:0.0096
g_step 7100, step 11, avg_time 2.256, loss:630.1968
g_step 7200, step 111, avg_time 0.970, loss:575.5001
g_step 7300, step 211, avg_time 0.986, loss:567.7646
g_step 7400, step 311, avg_time 0.982, loss:595.4347
g_step 7500, step 411, avg_time 0.975, loss:598.4244
>> valid entity prec:0.5686, rec:0.6149, f1:0.5909
>> valid relation prec:0.0327, rec:0.0092, f1:0.0143
>> valid relation with NER prec:0.0327, rec:0.0092, f1:0.0143
g_step 7600, step 94, avg_time 2.261, loss:543.7852
g_step 7700, step 194, avg_time 0.989, loss:566.2458
g_step 7800, step 294, avg_time 0.981, loss:555.5482
g_step 7900, step 394, avg_time 0.984, loss:579.4061
g_step 8000, step 77, avg_time 0.984, loss:519.3638
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5378, rec:0.5394, f1:0.5386
>> valid relation prec:0.0420, rec:0.0118, f1:0.0184
>> valid relation with NER prec:0.0420, rec:0.0118, f1:0.0184
g_step 8100, step 177, avg_time 2.262, loss:534.3496
g_step 8200, step 277, avg_time 0.989, loss:554.7714
g_step 8300, step 377, avg_time 0.978, loss:554.6931
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:22:28 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:22:28 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-22-28_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:22:29 - WARNING - datasets.builder -   Using custom data configuration default-aba9d53b64646d76
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-aba9d53b64646d76/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:22:31,015 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:22:31,017 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:22:31,017 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:22:31,018 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:22:31,048 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:31,081 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:31,081 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:31,081 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:31,081 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:31,081 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:31,081 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:22:31,528 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:22:34,574 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:22:34,576 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-aba9d53b64646d76/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 21:22:34 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14e1ebc9e5f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:04,  2.23ba/s] 18%|        | 2/11 [00:00<00:02,  3.20ba/s] 27%|       | 3/11 [00:00<00:02,  3.70ba/s] 36%|      | 4/11 [00:01<00:01,  3.99ba/s] 45%|     | 5/11 [00:01<00:01,  4.18ba/s] 55%|    | 6/11 [00:01<00:01,  3.49ba/s] 64%|   | 7/11 [00:01<00:01,  3.78ba/s] 73%|  | 8/11 [00:02<00:00,  3.98ba/s] 82%| | 9/11 [00:02<00:00,  4.14ba/s] 91%| | 10/11 [00:02<00:00,  4.26ba/s]100%|| 11/11 [00:02<00:00,  4.23ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.06ba/s] 50%|     | 2/4 [00:00<00:00,  3.73ba/s] 75%|  | 3/4 [00:00<00:00,  4.03ba/s]100%|| 4/4 [00:00<00:00,  5.13ba/s]100%|| 4/4 [00:00<00:00,  4.48ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  6.02ba/s] 27%|       | 3/11 [00:00<00:00,  8.96ba/s] 45%|     | 5/11 [00:00<00:00,  9.88ba/s] 64%|   | 7/11 [00:00<00:00, 10.24ba/s] 82%| | 9/11 [00:00<00:00, 10.33ba/s]100%|| 11/11 [00:01<00:00, 12.30ba/s]100%|| 11/11 [00:01<00:00, 10.83ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  8.63ba/s] 75%|  | 3/4 [00:00<00:00, 10.34ba/s]100%|| 4/4 [00:00<00:00, 11.66ba/s]
[INFO|trainer.py:414] 2023-08-28 21:22:40,775 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:22:40,784 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:22:40,784 >>   Num examples = 10042
[INFO|trainer.py:1149] 2023-08-28 21:22:40,784 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:22:40,784 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:22:40,784 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:22:40,784 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:22:40,784 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:55,  3.33it/s]  0%|          | 2/785 [00:00<04:07,  3.17it/s]  0%|          | 3/785 [00:00<03:57,  3.29it/s]  1%|          | 4/785 [00:01<03:52,  3.35it/s]  1%|          | 5/785 [00:01<03:50,  3.39it/s]  1%|          | 6/785 [00:01<03:48,  3.42it/s]  1%|          | 7/785 [00:02<03:46,  3.43it/s]  1%|          | 8/785 [00:02<03:46,  3.42it/s]  1%|          | 9/785 [00:02<03:46,  3.42it/s]  1%|         | 10/785 [00:02<03:47,  3.41it/s]  1%|         | 11/785 [00:03<03:47,  3.41it/s]  2%|         | 12/785 [00:03<03:46,  3.41it/s]  2%|         | 13/785 [00:03<03:59,  3.22it/s]  2%|         | 14/785 [00:04<03:55,  3.28it/s]  2%|         | 15/785 [00:04<03:52,  3.32it/s]  2%|         | 16/785 [00:04<04:26,  2.89it/s]  2%|         | 17/785 [00:05<04:13,  3.03it/s]  2%|         | 18/785 [00:06<06:44,  1.89it/s]  2%|         | 19/785 [00:06<05:50,  2.19it/s]  3%|         | 20/785 [00:07<06:06,  2.09it/s]  3%|         | 21/785 [00:07<06:11,  2.05it/s]  3%|         | 22/785 [00:07<05:27,  2.33it/s]  3%|         | 23/785 [00:08<04:55,  2.58it/s]  3%|         | 24/785 [00:08<04:33,  2.78it/s]  3%|         | 25/785 [00:08<04:18,  2.94it/s]  3%|         | 26/785 [00:08<04:07,  3.07it/s]  3%|         | 27/785 [00:09<03:59,  3.16it/s]  4%|         | 28/785 [00:09<03:54,  3.23it/s]  4%|         | 29/785 [00:09<03:50,  3.28it/s]  4%|         | 30/785 [00:10<03:47,  3.32it/s]  4%|         | 31/785 [00:10<04:02,  3.11it/s]  4%|         | 32/785 [00:10<03:56,  3.19it/s]  4%|         | 33/785 [00:11<03:51,  3.25it/s]  4%|         | 34/785 [00:11<03:48,  3.29it/s]  4%|         | 35/785 [00:11<03:45,  3.33it/s]  5%|         | 36/785 [00:12<03:43,  3.35it/s]  5%|         | 37/785 [00:12<03:42,  3.36it/s]  5%|         | 38/785 [00:12<03:41,  3.37it/s]  5%|         | 39/785 [00:12<03:40,  3.38it/s]  5%|         | 40/785 [00:13<03:40,  3.39it/s]  5%|         | 41/785 [00:13<05:05,  2.44it/s]  5%|         | 42/785 [00:14<04:39,  2.66it/s]  5%|         | 43/785 [00:14<04:21,  2.84it/s]  6%|         | 44/785 [00:14<04:08,  2.99it/s]  6%|         | 45/785 [00:15<03:58,  3.10it/s]  6%|         | 46/785 [00:15<03:52,  3.18it/s]  6%|         | 47/785 [00:15<03:47,  3.24it/s]  6%|         | 48/785 [00:15<03:44,  3.28it/s]  6%|         | 49/785 [00:16<03:41,  3.32it/s]  6%|         | 50/785 [00:16<04:15,  2.88it/s]  6%|         | 51/785 [00:16<04:03,  3.02it/s]  7%|         | 52/785 [00:17<03:54,  3.12it/s]  7%|         | 53/785 [00:17<04:50,  2.52it/s]  7%|         | 54/785 [00:18<04:27,  2.73it/s]  7%|         | 55/785 [00:18<04:11,  2.90it/s]  7%|         | 56/785 [00:18<03:59,  3.04it/s]  7%|         | 57/785 [00:19<03:51,  3.14it/s]  7%|         | 58/785 [00:19<03:46,  3.22it/s]  8%|         | 59/785 [00:19<04:15,  2.84it/s]  8%|         | 60/785 [00:20<04:02,  2.99it/s]  8%|         | 61/785 [00:20<03:53,  3.10it/s]  8%|         | 62/785 [00:20<03:47,  3.18it/s]  8%|         | 63/785 [00:20<03:42,  3.24it/s]  8%|         | 64/785 [00:21<03:39,  3.28it/s]  8%|         | 65/785 [00:21<03:37,  3.32it/s]  8%|         | 66/785 [00:21<03:35,  3.34it/s]  9%|         | 67/785 [00:22<03:33,  3.36it/s]  9%|         | 68/785 [00:22<03:32,  3.37it/s]  9%|         | 69/785 [00:23<04:54,  2.43it/s]  9%|         | 70/785 [00:23<04:29,  2.66it/s]  9%|         | 71/785 [00:23<04:11,  2.84it/s]  9%|         | 72/785 [00:23<03:58,  2.99it/s]  9%|         | 73/785 [00:24<03:49,  3.10it/s]  9%|         | 74/785 [00:24<03:43,  3.18it/s] 10%|         | 75/785 [00:24<03:38,  3.24it/s] 10%|         | 76/785 [00:25<03:35,  3.29it/s] 10%|         | 77/785 [00:25<03:33,  3.32it/s] 10%|         | 78/785 [00:26<04:47,  2.46it/s] 10%|         | 79/785 [00:26<04:23,  2.68it/s] 10%|         | 80/785 [00:26<04:06,  2.86it/s] 10%|         | 81/785 [00:26<03:54,  3.00it/s] 10%|         | 82/785 [00:27<03:46,  3.10it/s] 11%|         | 83/785 [00:27<03:40,  3.18it/s] 11%|         | 84/785 [00:27<03:35,  3.25it/s] 11%|         | 85/785 [00:28<03:32,  3.29it/s] 11%|         | 86/785 [00:28<03:30,  3.32it/s] 11%|         | 87/785 [00:29<05:35,  2.08it/s] 11%|         | 88/785 [00:29<04:55,  2.36it/s] 11%|        | 89/785 [00:29<04:28,  2.59it/s] 11%|        | 90/785 [00:30<04:09,  2.79it/s] 12%|        | 91/785 [00:30<03:55,  2.94it/s] 12%|        | 92/785 [00:30<03:46,  3.06it/s] 12%|        | 93/785 [00:31<03:39,  3.15it/s] 12%|        | 94/785 [00:31<03:34,  3.22it/s] 12%|        | 95/785 [00:31<03:30,  3.27it/s] 12%|        | 96/785 [00:32<03:43,  3.08it/s] 12%|        | 97/785 [00:32<03:37,  3.16it/s] 12%|        | 98/785 [00:32<03:32,  3.23it/s] 13%|        | 99/785 [00:32<03:29,  3.27it/s] 13%|        | 100/785 [00:33<03:27,  3.31it/s] 13%|        | 101/785 [00:33<03:25,  3.33it/s] 13%|        | 102/785 [00:33<03:23,  3.35it/s] 13%|        | 103/785 [00:34<03:22,  3.36it/s] 13%|        | 104/785 [00:34<03:21,  3.37it/s] 13%|        | 105/785 [00:34<03:21,  3.38it/s] 14%|        | 106/785 [00:35<03:47,  2.99it/s] 14%|        | 107/785 [00:35<03:38,  3.10it/s] 14%|        | 108/785 [00:35<03:32,  3.18it/s] 14%|        | 109/785 [00:36<03:28,  3.25it/s] 14%|        | 110/785 [00:36<03:25,  3.29it/s] 14%|        | 111/785 [00:36<03:22,  3.32it/s] 14%|        | 112/785 [00:36<03:21,  3.34it/s] 14%|        | 113/785 [00:37<03:20,  3.36it/s] 15%|        | 114/785 [00:37<03:18,  3.37it/s] 15%|        | 115/785 [00:37<03:18,  3.38it/s] 15%|        | 116/785 [00:38<05:16,  2.11it/s] 15%|        | 117/785 [00:39<04:40,  2.38it/s] 15%|        | 118/785 [00:39<04:14,  2.62it/s] 15%|        | 119/785 [00:39<03:56,  2.81it/s] 15%|        | 120/785 [00:39<03:44,  2.97it/s] 15%|        | 121/785 [00:40<03:35,  3.08it/s] 16%|        | 122/785 [00:40<03:28,  3.17it/s] 16%|        | 123/785 [00:40<03:24,  3.24it/s] 16%|        | 124/785 [00:41<03:21,  3.28it/s] 16%|        | 125/785 [00:41<04:16,  2.57it/s] 16%|        | 126/785 [00:41<03:57,  2.77it/s] 16%|        | 127/785 [00:42<03:45,  2.92it/s] 16%|        | 128/785 [00:42<03:35,  3.05it/s] 16%|        | 129/785 [00:42<03:28,  3.14it/s] 17%|        | 130/785 [00:43<03:23,  3.21it/s] 17%|        | 131/785 [00:43<03:20,  3.27it/s] 17%|        | 132/785 [00:43<03:17,  3.30it/s] 17%|        | 133/785 [00:44<03:15,  3.33it/s] 17%|        | 134/785 [00:44<03:14,  3.35it/s] 17%|        | 135/785 [00:45<04:52,  2.22it/s] 17%|        | 136/785 [00:45<04:21,  2.48it/s] 17%|        | 137/785 [00:45<04:00,  2.70it/s] 18%|        | 138/785 [00:45<03:45,  2.87it/s] 18%|        | 139/785 [00:46<03:34,  3.02it/s] 18%|        | 140/785 [00:46<03:26,  3.12it/s] 18%|        | 141/785 [00:46<03:21,  3.20it/s] 18%|        | 142/785 [00:47<03:17,  3.26it/s] 18%|        | 143/785 [00:47<03:14,  3.30it/s] 18%|        | 144/785 [00:47<03:44,  2.85it/s] 18%|        | 145/785 [00:48<03:33,  2.99it/s] 19%|        | 146/785 [00:48<03:25,  3.10it/s] 19%|        | 147/785 [00:48<03:20,  3.19it/s] 19%|        | 148/785 [00:49<03:16,  3.25it/s] 19%|        | 149/785 [00:49<03:13,  3.29it/s] 19%|        | 150/785 [00:49<03:10,  3.34it/s] 19%|        | 151/785 [00:49<03:08,  3.37it/s] 19%|        | 152/785 [00:50<03:06,  3.39it/s] 19%|        | 153/785 [00:50<03:05,  3.41it/s] 20%|        | 154/785 [00:50<03:32,  2.97it/s] 20%|        | 155/785 [00:51<03:23,  3.09it/s] 20%|        | 156/785 [00:51<03:17,  3.19it/s] 20%|        | 157/785 [00:51<03:07,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 21:23:32,632 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:23:32,632 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:23:32,632 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.62it/s][A
  3%|         | 12/436 [00:00<00:08, 49.02it/s][A
  4%|         | 17/436 [00:00<00:08, 47.33it/s][A
  5%|         | 22/436 [00:00<00:08, 46.37it/s][A
  6%|         | 27/436 [00:00<00:08, 45.48it/s][A
  7%|         | 32/436 [00:00<00:09, 44.87it/s][A
  8%|         | 37/436 [00:00<00:08, 44.58it/s][A
 10%|         | 42/436 [00:00<00:08, 44.46it/s][A
 11%|         | 47/436 [00:01<00:08, 44.52it/s][A
 12%|        | 52/436 [00:01<00:08, 44.61it/s][A
 13%|        | 57/436 [00:01<00:08, 44.68it/s][A
 14%|        | 62/436 [00:01<00:12, 29.71it/s][A
 15%|        | 67/436 [00:01<00:11, 33.10it/s][A
 17%|        | 72/436 [00:01<00:10, 36.05it/s][A
 18%|        | 77/436 [00:01<00:09, 38.36it/s][A
 19%|        | 82/436 [00:01<00:08, 40.15it/s][A
 20%|        | 87/436 [00:02<00:08, 41.52it/s][A
 21%|        | 92/436 [00:02<00:23, 14.56it/s][A
 22%|       | 97/436 [00:03<00:18, 18.30it/s][A
 23%|       | 102/436 [00:03<00:15, 22.26it/s][A
 25%|       | 107/436 [00:03<00:12, 26.28it/s][A
 26%|       | 112/436 [00:03<00:10, 29.99it/s][A
 27%|       | 117/436 [00:03<00:09, 33.37it/s][A
 28%|       | 122/436 [00:03<00:08, 36.19it/s][A
 29%|       | 127/436 [00:03<00:08, 38.49it/s][A
 30%|       | 132/436 [00:03<00:07, 39.91it/s][A
 31%|      | 137/436 [00:03<00:07, 40.97it/s][A
 33%|      | 142/436 [00:04<00:07, 41.81it/s][A
 34%|      | 147/436 [00:04<00:06, 42.56it/s][A
 35%|      | 152/436 [00:04<00:06, 43.27it/s][A
 36%|      | 157/436 [00:04<00:10, 27.37it/s][A
 37%|      | 162/436 [00:04<00:08, 31.01it/s][A
 38%|      | 167/436 [00:04<00:07, 34.25it/s][A
 39%|      | 172/436 [00:04<00:07, 36.87it/s][A
 41%|      | 177/436 [00:05<00:06, 38.96it/s][A
 42%|     | 182/436 [00:05<00:06, 40.53it/s][A
 43%|     | 187/436 [00:05<00:05, 41.86it/s][A
 44%|     | 192/436 [00:05<00:05, 42.62it/s][A
 45%|     | 197/436 [00:05<00:05, 42.95it/s][A
 46%|     | 202/436 [00:05<00:05, 42.97it/s][A
 47%|     | 207/436 [00:05<00:05, 43.32it/s][A
 49%|     | 212/436 [00:05<00:05, 43.71it/s][A
 50%|     | 217/436 [00:05<00:04, 44.08it/s][A
 51%|     | 222/436 [00:06<00:04, 44.34it/s][A
 52%|    | 227/436 [00:06<00:04, 44.50it/s][A
 53%|    | 232/436 [00:06<00:04, 44.76it/s][A
 54%|    | 237/436 [00:06<00:04, 44.72it/s][A
 56%|    | 242/436 [00:06<00:04, 44.30it/s][A
 57%|    | 247/436 [00:06<00:04, 44.01it/s][A
 58%|    | 252/436 [00:06<00:04, 44.12it/s][A
 59%|    | 257/436 [00:06<00:04, 44.22it/s][A
 60%|    | 262/436 [00:07<00:03, 44.53it/s][A
 61%|    | 267/436 [00:07<00:03, 44.62it/s][A
 62%|   | 272/436 [00:07<00:03, 44.72it/s][A
 64%|   | 277/436 [00:07<00:03, 44.92it/s][A
 65%|   | 282/436 [00:07<00:05, 27.99it/s][A
 66%|   | 287/436 [00:07<00:04, 31.53it/s][A
 67%|   | 292/436 [00:07<00:04, 34.71it/s][A
 68%|   | 297/436 [00:08<00:03, 37.33it/s][A
 69%|   | 302/436 [00:08<00:03, 39.42it/s][A
 70%|   | 307/436 [00:08<00:03, 40.87it/s][A
 72%|  | 312/436 [00:08<00:02, 42.21it/s][A
 73%|  | 317/436 [00:08<00:02, 42.87it/s][A
 74%|  | 322/436 [00:08<00:02, 42.93it/s][A
 75%|  | 327/436 [00:08<00:02, 43.03it/s][A
 76%|  | 332/436 [00:08<00:02, 43.24it/s][A
 77%|  | 337/436 [00:08<00:02, 43.74it/s][A
 78%|  | 342/436 [00:09<00:02, 44.11it/s][A
 80%|  | 347/436 [00:09<00:02, 44.42it/s][A
 81%|  | 352/436 [00:09<00:01, 44.58it/s][A
 82%| | 357/436 [00:09<00:01, 44.81it/s][A
 83%| | 362/436 [00:09<00:01, 44.78it/s][A
 84%| | 367/436 [00:09<00:01, 44.53it/s][A
 85%| | 372/436 [00:09<00:01, 44.21it/s][A
 86%| | 377/436 [00:09<00:01, 44.14it/s][A
 88%| | 382/436 [00:09<00:01, 44.34it/s][A
 89%| | 387/436 [00:10<00:01, 44.53it/s][A
 90%| | 392/436 [00:10<00:00, 44.69it/s][A
 91%| | 397/436 [00:10<00:00, 44.80it/s][A
 92%|| 402/436 [00:10<00:00, 44.92it/s][A
 93%|| 407/436 [00:10<00:00, 29.11it/s][A
 94%|| 412/436 [00:10<00:00, 32.51it/s][A
 96%|| 417/436 [00:10<00:00, 35.39it/s][A
 97%|| 422/436 [00:11<00:00, 37.87it/s][A
 98%|| 427/436 [00:11<00:00, 39.70it/s][A
 99%|| 432/436 [00:11<00:00, 41.26it/s][A                                                 
                                                 [A 20%|        | 157/785 [01:03<03:07,  3.34it/s]
100%|| 436/436 [00:11<00:00, 41.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:23:45,279 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 21:23:45,568 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:24:02,767 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:24:03,828 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:24:04,054 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|        | 158/785 [02:02<3:43:53, 21.42s/it] 20%|        | 159/785 [02:02<2:37:32, 15.10s/it] 20%|        | 160/785 [02:03<1:51:01, 10.66s/it] 21%|        | 161/785 [02:03<1:18:30,  7.55s/it] 21%|        | 162/785 [02:03<55:47,  5.37s/it]   21%|        | 163/785 [02:04<39:53,  3.85s/it] 21%|        | 164/785 [02:04<28:47,  2.78s/it] 21%|        | 165/785 [02:04<21:20,  2.07s/it] 21%|        | 166/785 [02:05<18:07,  1.76s/it] 21%|       | 167/785 [02:06<15:32,  1.51s/it] 21%|       | 168/785 [02:07<12:29,  1.22s/it] 22%|       | 169/785 [02:07<09:47,  1.05it/s] 22%|       | 170/785 [02:07<07:44,  1.32it/s] 22%|       | 171/785 [02:08<06:18,  1.62it/s] 22%|       | 172/785 [02:08<05:18,  1.93it/s] 22%|       | 173/785 [02:08<04:36,  2.22it/s] 22%|       | 174/785 [02:09<04:06,  2.48it/s] 22%|       | 175/785 [02:09<04:01,  2.52it/s] 22%|       | 176/785 [02:09<03:42,  2.74it/s] 23%|       | 177/785 [02:10<03:28,  2.91it/s] 23%|       | 178/785 [02:10<03:19,  3.05it/s] 23%|       | 179/785 [02:10<03:12,  3.15it/s] 23%|       | 180/785 [02:10<03:07,  3.23it/s] 23%|       | 181/785 [02:11<03:03,  3.28it/s] 23%|       | 182/785 [02:11<03:01,  3.32it/s] 23%|       | 183/785 [02:11<02:59,  3.35it/s] 23%|       | 184/785 [02:12<02:58,  3.37it/s] 24%|       | 185/785 [02:12<03:49,  2.62it/s] 24%|       | 186/785 [02:12<03:32,  2.82it/s] 24%|       | 187/785 [02:13<03:21,  2.97it/s] 24%|       | 188/785 [02:13<03:12,  3.09it/s] 24%|       | 189/785 [02:13<03:07,  3.18it/s] 24%|       | 190/785 [02:14<03:02,  3.26it/s] 24%|       | 191/785 [02:14<02:58,  3.32it/s] 24%|       | 192/785 [02:14<02:56,  3.37it/s] 25%|       | 193/785 [02:14<02:54,  3.39it/s] 25%|       | 194/785 [02:15<02:53,  3.42it/s] 25%|       | 195/785 [02:15<03:26,  2.86it/s] 25%|       | 196/785 [02:16<03:15,  3.01it/s] 25%|       | 197/785 [02:16<03:08,  3.11it/s] 25%|       | 198/785 [02:16<03:03,  3.20it/s] 25%|       | 199/785 [02:16<02:59,  3.26it/s] 25%|       | 200/785 [02:17<02:57,  3.30it/s] 26%|       | 201/785 [02:17<02:55,  3.34it/s] 26%|       | 202/785 [02:17<02:53,  3.36it/s] 26%|       | 203/785 [02:18<03:23,  2.86it/s] 26%|       | 204/785 [02:18<03:26,  2.82it/s] 26%|       | 205/785 [02:18<03:14,  2.98it/s] 26%|       | 206/785 [02:19<03:07,  3.10it/s] 26%|       | 207/785 [02:19<03:01,  3.18it/s] 26%|       | 208/785 [02:19<02:57,  3.25it/s] 27%|       | 209/785 [02:20<02:54,  3.30it/s] 27%|       | 210/785 [02:20<02:52,  3.33it/s] 27%|       | 211/785 [02:20<02:51,  3.36it/s] 27%|       | 212/785 [02:20<02:49,  3.37it/s] 27%|       | 213/785 [02:21<02:48,  3.38it/s] 27%|       | 214/785 [02:21<03:37,  2.63it/s] 27%|       | 215/785 [02:22<03:21,  2.82it/s] 28%|       | 216/785 [02:22<03:11,  2.98it/s] 28%|       | 217/785 [02:22<03:03,  3.10it/s] 28%|       | 218/785 [02:23<02:57,  3.20it/s] 28%|       | 219/785 [02:23<02:52,  3.27it/s] 28%|       | 220/785 [02:23<02:49,  3.33it/s] 28%|       | 221/785 [02:23<02:47,  3.37it/s] 28%|       | 222/785 [02:24<02:45,  3.40it/s] 28%|       | 223/785 [02:24<02:44,  3.42it/s] 29%|       | 224/785 [02:24<02:56,  3.17it/s] 29%|       | 225/785 [02:25<02:52,  3.26it/s] 29%|       | 226/785 [02:25<02:48,  3.31it/s] 29%|       | 227/785 [02:25<02:46,  3.36it/s] 29%|       | 228/785 [02:25<02:44,  3.39it/s] 29%|       | 229/785 [02:26<02:43,  3.41it/s] 29%|       | 230/785 [02:26<02:42,  3.42it/s] 29%|       | 231/785 [02:26<02:41,  3.43it/s] 30%|       | 232/785 [02:27<02:40,  3.44it/s] 30%|       | 233/785 [02:27<02:40,  3.44it/s] 30%|       | 234/785 [02:27<02:39,  3.44it/s] 30%|       | 235/785 [02:28<02:53,  3.17it/s] 30%|       | 236/785 [02:28<02:48,  3.25it/s] 30%|       | 237/785 [02:28<02:45,  3.30it/s] 30%|       | 238/785 [02:28<02:43,  3.35it/s] 30%|       | 239/785 [02:29<02:41,  3.38it/s] 31%|       | 240/785 [02:29<02:40,  3.40it/s] 31%|       | 241/785 [02:29<02:39,  3.41it/s] 31%|       | 242/785 [02:30<02:38,  3.42it/s] 31%|       | 243/785 [02:30<02:37,  3.43it/s] 31%|       | 244/785 [02:30<02:37,  3.44it/s] 31%|       | 245/785 [02:31<02:36,  3.44it/s] 31%|      | 246/785 [02:31<03:33,  2.53it/s] 31%|      | 247/785 [02:31<03:16,  2.74it/s] 32%|      | 248/785 [02:32<03:03,  2.92it/s] 32%|      | 249/785 [02:32<02:55,  3.06it/s] 32%|      | 250/785 [02:32<02:49,  3.16it/s] 32%|      | 251/785 [02:33<02:44,  3.24it/s] 32%|      | 252/785 [02:33<02:41,  3.30it/s] 32%|      | 253/785 [02:33<02:39,  3.34it/s] 32%|      | 254/785 [02:33<02:37,  3.37it/s] 32%|      | 255/785 [02:34<02:36,  3.39it/s] 33%|      | 256/785 [02:34<03:15,  2.71it/s] 33%|      | 257/785 [02:35<03:02,  2.90it/s] 33%|      | 258/785 [02:35<02:53,  3.04it/s] 33%|      | 259/785 [02:35<02:46,  3.15it/s] 33%|      | 260/785 [02:35<02:44,  3.19it/s] 33%|      | 261/785 [02:36<02:40,  3.26it/s] 33%|      | 262/785 [02:36<02:37,  3.32it/s] 34%|      | 263/785 [02:36<02:35,  3.36it/s] 34%|      | 264/785 [02:37<02:33,  3.39it/s] 34%|      | 265/785 [02:37<02:32,  3.41it/s] 34%|      | 266/785 [02:37<02:31,  3.42it/s] 34%|      | 267/785 [02:38<02:31,  3.43it/s] 34%|      | 268/785 [02:38<02:30,  3.44it/s] 34%|      | 269/785 [02:38<02:29,  3.44it/s] 34%|      | 270/785 [02:38<02:42,  3.17it/s] 35%|      | 271/785 [02:39<02:38,  3.25it/s] 35%|      | 272/785 [02:39<02:35,  3.30it/s] 35%|      | 273/785 [02:39<02:33,  3.34it/s] 35%|      | 274/785 [02:40<02:31,  3.37it/s] 35%|      | 275/785 [02:40<02:30,  3.39it/s] 35%|      | 276/785 [02:40<02:29,  3.40it/s] 35%|      | 277/785 [02:41<02:28,  3.41it/s] 35%|      | 278/785 [02:41<02:28,  3.42it/s] 36%|      | 279/785 [02:41<02:27,  3.42it/s] 36%|      | 280/785 [02:41<02:27,  3.43it/s] 36%|      | 281/785 [02:42<02:53,  2.90it/s] 36%|      | 282/785 [02:42<02:45,  3.04it/s] 36%|      | 283/785 [02:42<02:39,  3.15it/s] 36%|      | 284/785 [02:43<02:34,  3.24it/s] 36%|      | 285/785 [02:43<02:31,  3.30it/s] 36%|      | 286/785 [02:43<02:29,  3.34it/s] 37%|      | 287/785 [02:44<02:27,  3.37it/s] 37%|      | 288/785 [02:44<02:26,  3.39it/s] 37%|      | 289/785 [02:44<02:25,  3.41it/s] 37%|      | 290/785 [02:44<02:24,  3.42it/s] 37%|      | 291/785 [02:45<03:26,  2.39it/s] 37%|      | 292/785 [02:45<03:07,  2.63it/s] 37%|      | 293/785 [02:46<02:53,  2.83it/s] 37%|      | 294/785 [02:46<02:44,  2.99it/s] 38%|      | 295/785 [02:46<02:37,  3.12it/s] 38%|      | 296/785 [02:47<02:32,  3.21it/s] 38%|      | 297/785 [02:47<02:28,  3.28it/s] 38%|      | 298/785 [02:47<02:26,  3.33it/s] 38%|      | 299/785 [02:47<02:24,  3.36it/s] 38%|      | 300/785 [02:48<02:48,  2.88it/s] 38%|      | 301/785 [02:48<02:39,  3.03it/s] 38%|      | 302/785 [02:49<02:33,  3.14it/s] 39%|      | 303/785 [02:49<02:29,  3.23it/s] 39%|      | 304/785 [02:49<02:26,  3.29it/s] 39%|      | 305/785 [02:49<02:23,  3.34it/s] 39%|      | 306/785 [02:50<02:22,  3.37it/s] 39%|      | 307/785 [02:50<02:20,  3.39it/s] 39%|      | 308/785 [02:50<02:19,  3.41it/s] 39%|      | 309/785 [02:51<02:19,  3.42it/s] 39%|      | 310/785 [02:52<04:06,  1.93it/s] 40%|      | 311/785 [02:52<03:33,  2.22it/s] 40%|      | 312/785 [02:52<03:10,  2.49it/s] 40%|      | 313/785 [02:52<02:54,  2.71it/s] 40%|      | 314/785 [02:53<02:39,  2.96it/s][INFO|trainer.py:2140] 2023-08-28 21:25:34,035 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:25:34,035 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:25:34,035 >>   Batch size = 8
{'eval_loss': 1.0336496829986572, 'eval_runtime': 11.3317, 'eval_samples_per_second': 307.278, 'eval_steps_per_second': 38.476, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.37it/s][A
  3%|         | 12/436 [00:00<00:08, 49.27it/s][A
  4%|         | 17/436 [00:00<00:08, 47.20it/s][A
  5%|         | 22/436 [00:00<00:09, 45.96it/s][A
  6%|         | 27/436 [00:00<00:09, 45.28it/s][A
  7%|         | 32/436 [00:00<00:09, 44.87it/s][A
  8%|         | 37/436 [00:01<00:08, 44.60it/s][A
 10%|         | 42/436 [00:01<00:19, 20.12it/s][A
 11%|         | 47/436 [00:01<00:16, 24.26it/s][A
 12%|        | 52/436 [00:01<00:13, 28.13it/s][A
 13%|        | 57/436 [00:01<00:11, 31.81it/s][A
 14%|        | 62/436 [00:01<00:10, 34.96it/s][A
 15%|        | 67/436 [00:01<00:09, 37.53it/s][A
 17%|        | 72/436 [00:02<00:09, 39.57it/s][A
 18%|        | 77/436 [00:02<00:08, 41.08it/s][A
 19%|        | 82/436 [00:02<00:08, 41.83it/s][A
 20%|        | 87/436 [00:02<00:08, 42.40it/s][A
 21%|        | 92/436 [00:02<00:07, 43.01it/s][A
 22%|       | 97/436 [00:02<00:07, 43.40it/s][A
 23%|       | 102/436 [00:02<00:07, 43.89it/s][A
 25%|       | 107/436 [00:02<00:07, 44.29it/s][A
 26%|       | 112/436 [00:02<00:07, 44.64it/s][A
 27%|       | 117/436 [00:03<00:07, 44.77it/s][A
 28%|       | 122/436 [00:03<00:07, 44.72it/s][A
 29%|       | 127/436 [00:03<00:06, 44.54it/s][A
 30%|       | 132/436 [00:03<00:06, 44.27it/s][A
 31%|      | 137/436 [00:03<00:06, 44.25it/s][A
 33%|      | 142/436 [00:03<00:06, 44.33it/s][A
 34%|      | 147/436 [00:03<00:06, 44.41it/s][A
 35%|      | 152/436 [00:03<00:06, 44.63it/s][A
 36%|      | 157/436 [00:03<00:07, 38.11it/s][A
 37%|      | 162/436 [00:04<00:06, 40.01it/s][A
 38%|      | 167/436 [00:04<00:06, 41.51it/s][A
 39%|      | 172/436 [00:04<00:06, 42.42it/s][A
 41%|      | 177/436 [00:04<00:05, 43.27it/s][A
 42%|     | 182/436 [00:04<00:05, 43.76it/s][A
 43%|     | 187/436 [00:04<00:05, 44.22it/s][A
 44%|     | 192/436 [00:04<00:05, 44.38it/s][A
 45%|     | 197/436 [00:04<00:05, 44.12it/s][A
 46%|     | 202/436 [00:04<00:05, 44.00it/s][A
 47%|     | 207/436 [00:05<00:05, 44.15it/s][A
 49%|     | 212/436 [00:05<00:05, 44.28it/s][A
 50%|     | 217/436 [00:05<00:04, 44.53it/s][A
 51%|     | 222/436 [00:05<00:04, 44.70it/s][A
 52%|    | 227/436 [00:05<00:04, 44.81it/s][A
 53%|    | 232/436 [00:05<00:04, 44.75it/s][A
 54%|    | 237/436 [00:05<00:04, 44.74it/s][A
 56%|    | 242/436 [00:05<00:04, 44.35it/s][A
 57%|    | 247/436 [00:06<00:04, 44.17it/s][A
 58%|    | 252/436 [00:06<00:04, 44.32it/s][A
 59%|    | 257/436 [00:06<00:04, 44.42it/s][A
 60%|    | 262/436 [00:06<00:03, 44.63it/s][A
 61%|    | 267/436 [00:06<00:03, 44.83it/s][A
 62%|   | 272/436 [00:06<00:03, 44.86it/s][A
 64%|   | 277/436 [00:06<00:03, 44.84it/s][A
 65%|   | 282/436 [00:06<00:03, 44.72it/s][A
 66%|   | 287/436 [00:06<00:03, 44.46it/s][A
 67%|   | 292/436 [00:07<00:03, 38.46it/s][A
 68%|   | 297/436 [00:07<00:03, 40.21it/s][A
 69%|   | 302/436 [00:07<00:03, 41.49it/s][A
 70%|   | 307/436 [00:07<00:03, 42.52it/s][A
 72%|  | 312/436 [00:07<00:02, 43.18it/s][A
 73%|  | 317/436 [00:07<00:02, 43.66it/s][A
 74%|  | 322/436 [00:07<00:02, 43.97it/s][A
 75%|  | 327/436 [00:07<00:02, 44.13it/s][A
 76%|  | 332/436 [00:07<00:02, 43.86it/s][A
 77%|  | 337/436 [00:08<00:02, 43.72it/s][A
 78%|  | 342/436 [00:08<00:02, 43.96it/s][A
 80%|  | 347/436 [00:08<00:02, 44.14it/s][A
 81%|  | 352/436 [00:08<00:01, 44.44it/s][A
 82%| | 357/436 [00:08<00:01, 44.66it/s][A
 83%| | 362/436 [00:08<00:01, 44.81it/s][A
 84%| | 367/436 [00:08<00:01, 44.90it/s][A
 85%| | 372/436 [00:08<00:01, 44.76it/s][A
 86%| | 377/436 [00:08<00:01, 44.59it/s][A
 88%| | 382/436 [00:09<00:01, 44.47it/s][A
 89%| | 387/436 [00:09<00:01, 44.03it/s][A
 90%| | 392/436 [00:09<00:00, 44.54it/s][A
 91%| | 397/436 [00:09<00:00, 44.65it/s][A
 92%|| 402/436 [00:09<00:00, 44.79it/s][A
 93%|| 407/436 [00:09<00:00, 44.83it/s][A
 94%|| 412/436 [00:09<00:00, 44.82it/s][A
 96%|| 417/436 [00:09<00:00, 44.69it/s][A
 97%|| 422/436 [00:10<00:00, 44.50it/s][A
 98%|| 427/436 [00:10<00:00, 32.87it/s][A
 99%|| 432/436 [00:10<00:00, 35.82it/s][A                                                 
                                                 [A 40%|      | 314/785 [03:03<02:39,  2.96it/s]
100%|| 436/436 [00:10<00:00, 35.82it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:25:45,391 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 21:25:46,205 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:26:05,283 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:26:07,060 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:26:07,293 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|      | 315/785 [04:00<2:40:01, 20.43s/it] 40%|      | 316/785 [04:00<1:52:40, 14.42s/it] 40%|      | 317/785 [04:01<1:19:23, 10.18s/it] 41%|      | 318/785 [04:01<56:08,  7.21s/it]   41%|      | 319/785 [04:01<39:54,  5.14s/it] 41%|      | 320/785 [04:02<28:33,  3.68s/it] 41%|      | 321/785 [04:02<20:37,  2.67s/it] 41%|      | 322/785 [04:02<15:05,  1.96s/it] 41%|      | 323/785 [04:02<11:13,  1.46s/it] 41%|     | 324/785 [04:03<08:30,  1.11s/it] 41%|     | 325/785 [04:03<06:37,  1.16it/s] 42%|     | 326/785 [04:04<06:44,  1.13it/s] 42%|     | 327/785 [04:04<05:23,  1.42it/s] 42%|     | 328/785 [04:06<07:32,  1.01it/s] 42%|     | 329/785 [04:06<05:55,  1.28it/s] 42%|     | 330/785 [04:08<07:23,  1.03it/s] 42%|     | 331/785 [04:08<05:49,  1.30it/s] 42%|     | 332/785 [04:08<04:44,  1.59it/s] 42%|     | 333/785 [04:09<03:58,  1.90it/s] 43%|     | 334/785 [04:09<03:26,  2.19it/s] 43%|     | 335/785 [04:09<03:03,  2.45it/s] 43%|     | 336/785 [04:09<02:47,  2.68it/s] 43%|     | 337/785 [04:10<02:36,  2.86it/s] 43%|     | 338/785 [04:10<02:28,  3.01it/s] 43%|     | 339/785 [04:10<02:23,  3.12it/s] 43%|     | 340/785 [04:11<03:02,  2.44it/s] 43%|     | 341/785 [04:11<02:46,  2.67it/s] 44%|     | 342/785 [04:12<02:35,  2.85it/s] 44%|     | 343/785 [04:12<02:27,  3.00it/s] 44%|     | 344/785 [04:12<02:21,  3.11it/s] 44%|     | 345/785 [04:12<02:17,  3.20it/s] 44%|     | 346/785 [04:13<02:14,  3.26it/s] 44%|     | 347/785 [04:13<02:12,  3.30it/s] 44%|     | 348/785 [04:13<02:11,  3.33it/s] 44%|     | 349/785 [04:14<02:09,  3.36it/s] 45%|     | 350/785 [04:14<02:18,  3.14it/s] 45%|     | 351/785 [04:14<02:15,  3.21it/s] 45%|     | 352/785 [04:15<02:12,  3.27it/s] 45%|     | 353/785 [04:15<02:10,  3.31it/s] 45%|     | 354/785 [04:15<02:08,  3.34it/s] 45%|     | 355/785 [04:15<02:08,  3.36it/s] 45%|     | 356/785 [04:16<02:07,  3.37it/s] 45%|     | 357/785 [04:16<02:06,  3.39it/s] 46%|     | 358/785 [04:16<02:05,  3.39it/s] 46%|     | 359/785 [04:17<02:05,  3.40it/s] 46%|     | 360/785 [04:17<02:20,  3.02it/s] 46%|     | 361/785 [04:17<02:15,  3.13it/s] 46%|     | 362/785 [04:18<02:11,  3.21it/s] 46%|     | 363/785 [04:18<02:37,  2.67it/s] 46%|     | 364/785 [04:18<02:27,  2.86it/s] 46%|     | 365/785 [04:19<02:19,  3.01it/s] 47%|     | 366/785 [04:19<02:14,  3.12it/s] 47%|     | 367/785 [04:19<02:10,  3.20it/s] 47%|     | 368/785 [04:20<02:07,  3.26it/s] 47%|     | 369/785 [04:20<02:05,  3.30it/s] 47%|     | 370/785 [04:20<02:32,  2.72it/s] 47%|     | 371/785 [04:21<02:23,  2.89it/s] 47%|     | 372/785 [04:21<02:16,  3.03it/s] 48%|     | 373/785 [04:21<02:11,  3.13it/s] 48%|     | 374/785 [04:22<02:07,  3.21it/s] 48%|     | 375/785 [04:22<02:05,  3.27it/s] 48%|     | 376/785 [04:22<02:03,  3.31it/s] 48%|     | 377/785 [04:22<02:01,  3.35it/s] 48%|     | 378/785 [04:23<02:00,  3.38it/s] 48%|     | 379/785 [04:23<01:59,  3.41it/s] 48%|     | 380/785 [04:23<02:10,  3.11it/s] 49%|     | 381/785 [04:24<02:05,  3.21it/s] 49%|     | 382/785 [04:24<02:02,  3.28it/s] 49%|     | 383/785 [04:24<02:00,  3.33it/s] 49%|     | 384/785 [04:25<01:59,  3.37it/s] 49%|     | 385/785 [04:25<01:57,  3.39it/s] 49%|     | 386/785 [04:25<01:56,  3.41it/s] 49%|     | 387/785 [04:25<01:56,  3.43it/s] 49%|     | 388/785 [04:26<01:55,  3.44it/s] 50%|     | 389/785 [04:26<01:55,  3.44it/s] 50%|     | 390/785 [04:26<01:54,  3.45it/s] 50%|     | 391/785 [04:27<01:59,  3.29it/s] 50%|     | 392/785 [04:27<01:58,  3.33it/s] 50%|     | 393/785 [04:27<01:56,  3.36it/s] 50%|     | 394/785 [04:28<01:55,  3.38it/s] 50%|     | 395/785 [04:28<01:54,  3.40it/s] 50%|     | 396/785 [04:28<01:54,  3.41it/s] 51%|     | 397/785 [04:28<01:53,  3.42it/s] 51%|     | 398/785 [04:29<01:53,  3.42it/s] 51%|     | 399/785 [04:29<01:52,  3.43it/s] 51%|     | 400/785 [04:29<01:52,  3.43it/s] 51%|     | 401/785 [04:30<01:51,  3.44it/s] 51%|     | 402/785 [04:30<02:05,  3.06it/s] 51%|    | 403/785 [04:30<02:00,  3.17it/s] 51%|    | 404/785 [04:31<01:57,  3.25it/s] 52%|    | 405/785 [04:31<01:54,  3.30it/s] 52%|    | 406/785 [04:31<01:53,  3.35it/s] 52%|    | 407/785 [04:31<01:51,  3.38it/s] 52%|    | 408/785 [04:32<01:50,  3.40it/s] 52%|    | 409/785 [04:32<01:50,  3.42it/s] 52%|    | 410/785 [04:32<01:49,  3.42it/s] 52%|    | 411/785 [04:33<01:49,  3.43it/s] 52%|    | 412/785 [04:33<02:25,  2.56it/s] 53%|    | 413/785 [04:33<02:14,  2.77it/s] 53%|    | 414/785 [04:34<02:06,  2.94it/s] 53%|    | 415/785 [04:34<02:00,  3.08it/s] 53%|    | 416/785 [04:34<01:56,  3.18it/s] 53%|    | 417/785 [04:35<01:53,  3.25it/s] 53%|    | 418/785 [04:35<01:50,  3.31it/s] 53%|    | 419/785 [04:35<01:49,  3.35it/s] 54%|    | 420/785 [04:36<01:47,  3.38it/s] 54%|    | 421/785 [04:36<01:46,  3.40it/s] 54%|    | 422/785 [04:36<02:29,  2.43it/s] 54%|    | 423/785 [04:37<02:16,  2.66it/s] 54%|    | 424/785 [04:37<02:06,  2.85it/s] 54%|    | 425/785 [04:37<01:59,  3.01it/s] 54%|    | 426/785 [04:38<01:54,  3.12it/s] 54%|    | 427/785 [04:38<01:51,  3.21it/s] 55%|    | 428/785 [04:38<01:58,  3.02it/s] 55%|    | 429/785 [04:39<01:53,  3.13it/s] 55%|    | 430/785 [04:39<01:50,  3.22it/s] 55%|    | 431/785 [04:39<01:47,  3.28it/s] 55%|    | 432/785 [04:39<01:45,  3.33it/s] 55%|    | 433/785 [04:40<01:44,  3.37it/s] 55%|    | 434/785 [04:40<01:43,  3.39it/s] 55%|    | 435/785 [04:40<01:42,  3.41it/s] 56%|    | 436/785 [04:41<01:42,  3.42it/s] 56%|    | 437/785 [04:41<01:41,  3.43it/s] 56%|    | 438/785 [04:41<02:04,  2.78it/s] 56%|    | 439/785 [04:42<01:57,  2.95it/s] 56%|    | 440/785 [04:42<01:51,  3.08it/s] 56%|    | 441/785 [04:42<01:47,  3.19it/s] 56%|    | 442/785 [04:43<01:45,  3.26it/s] 56%|    | 443/785 [04:43<01:43,  3.31it/s] 57%|    | 444/785 [04:43<01:41,  3.35it/s] 57%|    | 445/785 [04:43<01:40,  3.38it/s] 57%|    | 446/785 [04:44<01:39,  3.40it/s] 57%|    | 447/785 [04:44<01:39,  3.41it/s] 57%|    | 448/785 [04:44<01:51,  3.04it/s] 57%|    | 449/785 [04:45<01:46,  3.15it/s] 57%|    | 450/785 [04:45<01:43,  3.23it/s] 57%|    | 451/785 [04:45<01:41,  3.29it/s] 58%|    | 452/785 [04:46<01:39,  3.34it/s] 58%|    | 453/785 [04:46<01:38,  3.37it/s] 58%|    | 454/785 [04:46<01:37,  3.39it/s] 58%|    | 455/785 [04:47<01:36,  3.40it/s] 58%|    | 456/785 [04:47<01:36,  3.41it/s] 58%|    | 457/785 [04:47<01:35,  3.42it/s] 58%|    | 458/785 [04:47<01:45,  3.09it/s] 58%|    | 459/785 [04:48<01:42,  3.19it/s] 59%|    | 460/785 [04:48<01:39,  3.26it/s] 59%|    | 461/785 [04:48<01:37,  3.31it/s] 59%|    | 462/785 [04:49<01:36,  3.35it/s] 59%|    | 463/785 [04:49<01:35,  3.38it/s] 59%|    | 464/785 [04:49<01:34,  3.40it/s] 59%|    | 465/785 [04:50<01:33,  3.41it/s] 59%|    | 466/785 [04:50<01:33,  3.42it/s] 59%|    | 467/785 [04:50<01:32,  3.43it/s] 60%|    | 468/785 [04:51<01:56,  2.73it/s] 60%|    | 469/785 [04:51<01:48,  2.91it/s] 60%|    | 470/785 [04:51<01:43,  3.05it/s] 60%|    | 471/785 [04:51<01:36,  3.24it/s][INFO|trainer.py:2140] 2023-08-28 21:27:32,767 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:27:32,767 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:27:32,767 >>   Batch size = 8
{'eval_loss': 1.0291813611984253, 'eval_runtime': 10.4422, 'eval_samples_per_second': 333.453, 'eval_steps_per_second': 41.753, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.63it/s][A
  3%|         | 12/436 [00:00<00:08, 48.21it/s][A
  4%|         | 17/436 [00:00<00:09, 46.37it/s][A
  5%|         | 22/436 [00:00<00:09, 45.71it/s][A
  6%|         | 27/436 [00:00<00:09, 45.31it/s][A
  7%|         | 32/436 [00:00<00:09, 44.79it/s][A
  8%|         | 37/436 [00:00<00:08, 44.65it/s][A
 10%|         | 42/436 [00:00<00:08, 44.58it/s][A
 11%|         | 47/436 [00:01<00:08, 44.68it/s][A
 12%|        | 52/436 [00:01<00:08, 44.72it/s][A
 13%|        | 57/436 [00:01<00:08, 44.69it/s][A
 14%|        | 62/436 [00:01<00:08, 44.64it/s][A
 15%|        | 67/436 [00:01<00:08, 44.64it/s][A
 17%|        | 72/436 [00:01<00:08, 44.51it/s][A
 18%|        | 77/436 [00:01<00:08, 44.56it/s][A
 19%|        | 82/436 [00:01<00:11, 32.07it/s][A
 20%|        | 87/436 [00:02<00:09, 35.02it/s][A
 21%|        | 92/436 [00:02<00:09, 37.62it/s][A
 22%|       | 97/436 [00:02<00:08, 39.57it/s][A
 23%|       | 102/436 [00:02<00:08, 41.18it/s][A
 25%|       | 107/436 [00:02<00:15, 21.13it/s][A
 26%|       | 112/436 [00:03<00:12, 25.21it/s][A
 27%|       | 117/436 [00:03<00:11, 29.00it/s][A
 28%|       | 122/436 [00:03<00:09, 32.51it/s][A
 29%|       | 127/436 [00:03<00:08, 35.51it/s][A
 30%|       | 132/436 [00:03<00:08, 37.96it/s][A
 31%|      | 137/436 [00:03<00:07, 39.88it/s][A
 33%|      | 142/436 [00:03<00:07, 41.25it/s][A
 34%|      | 147/436 [00:03<00:06, 42.11it/s][A
 35%|      | 152/436 [00:03<00:06, 42.46it/s][A
 36%|      | 157/436 [00:04<00:06, 42.95it/s][A
 37%|      | 162/436 [00:04<00:06, 43.39it/s][A
 38%|      | 167/436 [00:04<00:06, 43.87it/s][A
 39%|      | 172/436 [00:04<00:05, 44.23it/s][A
 41%|      | 177/436 [00:04<00:05, 44.52it/s][A
 42%|     | 182/436 [00:04<00:05, 44.69it/s][A
 43%|     | 187/436 [00:04<00:05, 44.76it/s][A
 44%|     | 192/436 [00:04<00:05, 44.65it/s][A
 45%|     | 197/436 [00:05<00:07, 33.25it/s][A
 46%|     | 202/436 [00:05<00:06, 36.14it/s][A
 47%|     | 207/436 [00:05<00:05, 38.45it/s][A
 49%|     | 212/436 [00:05<00:05, 40.25it/s][A
 50%|     | 217/436 [00:05<00:05, 41.57it/s][A
 51%|     | 222/436 [00:05<00:05, 42.59it/s][A
 52%|    | 227/436 [00:05<00:04, 43.34it/s][A
 53%|    | 232/436 [00:05<00:04, 43.69it/s][A
 54%|    | 237/436 [00:05<00:04, 43.61it/s][A
 56%|    | 242/436 [00:06<00:04, 43.61it/s][A
 57%|    | 247/436 [00:06<00:04, 43.85it/s][A
 58%|    | 252/436 [00:06<00:04, 44.08it/s][A
 59%|    | 257/436 [00:06<00:04, 44.22it/s][A
 60%|    | 262/436 [00:06<00:03, 44.64it/s][A
 61%|    | 267/436 [00:06<00:03, 44.86it/s][A
 62%|   | 272/436 [00:06<00:03, 44.77it/s][A
 64%|   | 277/436 [00:06<00:03, 44.75it/s][A
 65%|   | 282/436 [00:06<00:03, 44.42it/s][A
 66%|   | 287/436 [00:07<00:03, 44.38it/s][A
 67%|   | 292/436 [00:07<00:03, 44.31it/s][A
 68%|   | 297/436 [00:07<00:03, 44.41it/s][A
 69%|   | 302/436 [00:07<00:03, 44.57it/s][A
 70%|   | 307/436 [00:07<00:02, 44.71it/s][A
 72%|  | 312/436 [00:07<00:02, 44.80it/s][A
 73%|  | 317/436 [00:07<00:02, 44.86it/s][A
 74%|  | 322/436 [00:07<00:02, 44.76it/s][A
 75%|  | 327/436 [00:08<00:02, 36.48it/s][A
 76%|  | 332/436 [00:08<00:02, 38.66it/s][A
 77%|  | 337/436 [00:08<00:02, 40.43it/s][A
 78%|  | 342/436 [00:08<00:02, 41.72it/s][A
 80%|  | 347/436 [00:08<00:02, 42.63it/s][A
 81%|  | 352/436 [00:08<00:01, 43.43it/s][A
 82%| | 357/436 [00:08<00:01, 43.96it/s][A
 83%| | 362/436 [00:08<00:01, 44.20it/s][A
 84%| | 367/436 [00:08<00:01, 43.91it/s][A
 85%| | 372/436 [00:09<00:01, 43.77it/s][A
 86%| | 377/436 [00:09<00:01, 43.94it/s][A
 88%| | 382/436 [00:09<00:01, 44.19it/s][A
 89%| | 387/436 [00:09<00:01, 44.58it/s][A
 90%| | 392/436 [00:09<00:00, 44.63it/s][A
 91%| | 397/436 [00:09<00:00, 44.74it/s][A
 92%|| 402/436 [00:09<00:00, 44.79it/s][A
 93%|| 407/436 [00:09<00:00, 44.59it/s][A
 94%|| 412/436 [00:09<00:00, 44.31it/s][A
 96%|| 417/436 [00:10<00:00, 44.01it/s][A
 97%|| 422/436 [00:10<00:00, 44.18it/s][A
 98%|| 427/436 [00:10<00:00, 44.37it/s][A
 99%|| 432/436 [00:10<00:00, 44.63it/s][A                                                 
                                                 [A 60%|    | 471/785 [05:02<01:36,  3.24it/s]
100%|| 436/436 [00:10<00:00, 44.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:27:43,846 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 21:27:46,337 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:28:09,374 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:28:10,545 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:28:11,172 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|    | 472/785 [05:51<1:34:53, 18.19s/it] 60%|    | 473/785 [05:52<1:06:44, 12.84s/it] 60%|    | 474/785 [05:52<47:01,  9.07s/it]   61%|    | 475/785 [05:52<33:15,  6.44s/it] 61%|    | 476/785 [05:53<23:39,  4.59s/it] 61%|    | 477/785 [05:53<16:57,  3.30s/it] 61%|    | 478/785 [05:53<12:17,  2.40s/it] 61%|    | 479/785 [05:53<09:01,  1.77s/it] 61%|    | 480/785 [05:54<06:44,  1.33s/it] 61%|   | 481/785 [05:54<05:26,  1.07s/it] 61%|   | 482/785 [05:55<04:14,  1.19it/s] 62%|   | 483/785 [05:55<03:28,  1.45it/s] 62%|   | 484/785 [05:55<02:51,  1.75it/s] 62%|   | 485/785 [05:55<02:25,  2.06it/s] 62%|   | 486/785 [05:56<02:08,  2.33it/s] 62%|   | 487/785 [05:56<01:55,  2.58it/s] 62%|   | 488/785 [05:56<01:46,  2.78it/s] 62%|   | 489/785 [05:57<01:40,  2.95it/s] 62%|   | 490/785 [05:57<01:36,  3.07it/s] 63%|   | 491/785 [05:57<01:32,  3.17it/s] 63%|   | 492/785 [05:58<01:30,  3.24it/s] 63%|   | 493/785 [05:58<01:28,  3.29it/s] 63%|   | 494/785 [05:58<01:34,  3.07it/s] 63%|   | 495/785 [05:58<01:31,  3.16it/s] 63%|   | 496/785 [05:59<01:29,  3.23it/s] 63%|   | 497/785 [05:59<01:27,  3.29it/s] 63%|   | 498/785 [05:59<01:26,  3.32it/s] 64%|   | 499/785 [06:00<01:25,  3.35it/s] 64%|   | 500/785 [06:00<01:24,  3.37it/s]                                                  64%|   | 500/785 [06:00<01:24,  3.37it/s] 64%|   | 501/785 [06:00<01:24,  3.38it/s] 64%|   | 502/785 [06:01<01:23,  3.39it/s] 64%|   | 503/785 [06:01<01:23,  3.40it/s] 64%|   | 504/785 [06:01<01:30,  3.10it/s] 64%|   | 505/785 [06:02<01:27,  3.19it/s] 64%|   | 506/785 [06:02<01:25,  3.25it/s] 65%|   | 507/785 [06:02<01:24,  3.30it/s] 65%|   | 508/785 [06:02<01:23,  3.33it/s] 65%|   | 509/785 [06:03<01:22,  3.35it/s] 65%|   | 510/785 [06:03<01:21,  3.37it/s] 65%|   | 511/785 [06:03<01:21,  3.38it/s] 65%|   | 512/785 [06:04<01:20,  3.39it/s] 65%|   | 513/785 [06:04<01:20,  3.40it/s] 65%|   | 514/785 [06:04<01:35,  2.84it/s] 66%|   | 515/785 [06:05<01:30,  3.00it/s] 66%|   | 516/785 [06:05<01:26,  3.13it/s] 66%|   | 517/785 [06:05<01:23,  3.22it/s] 66%|   | 518/785 [06:06<01:21,  3.29it/s] 66%|   | 519/785 [06:06<01:19,  3.34it/s] 66%|   | 520/785 [06:06<01:18,  3.37it/s] 66%|   | 521/785 [06:06<01:17,  3.40it/s] 66%|   | 522/785 [06:07<01:16,  3.42it/s] 67%|   | 523/785 [06:07<01:16,  3.43it/s] 67%|   | 524/785 [06:08<02:02,  2.12it/s] 67%|   | 525/785 [06:08<01:48,  2.40it/s] 67%|   | 526/785 [06:08<01:37,  2.64it/s] 67%|   | 527/785 [06:09<01:30,  2.85it/s] 67%|   | 528/785 [06:09<01:25,  3.01it/s] 67%|   | 529/785 [06:09<01:21,  3.13it/s] 68%|   | 530/785 [06:10<01:19,  3.22it/s] 68%|   | 531/785 [06:10<01:17,  3.29it/s] 68%|   | 532/785 [06:10<01:15,  3.33it/s] 68%|   | 533/785 [06:10<01:14,  3.36it/s] 68%|   | 534/785 [06:11<01:14,  3.39it/s] 68%|   | 535/785 [06:11<01:13,  3.41it/s] 68%|   | 536/785 [06:11<01:12,  3.42it/s] 68%|   | 537/785 [06:12<01:12,  3.43it/s] 69%|   | 538/785 [06:12<01:11,  3.44it/s] 69%|   | 539/785 [06:12<01:11,  3.44it/s] 69%|   | 540/785 [06:12<01:11,  3.45it/s] 69%|   | 541/785 [06:13<01:10,  3.45it/s] 69%|   | 542/785 [06:13<01:10,  3.45it/s] 69%|   | 543/785 [06:14<01:23,  2.89it/s] 69%|   | 544/785 [06:14<01:19,  3.03it/s] 69%|   | 545/785 [06:14<01:16,  3.14it/s] 70%|   | 546/785 [06:14<01:13,  3.23it/s] 70%|   | 547/785 [06:15<01:12,  3.30it/s] 70%|   | 548/785 [06:15<01:10,  3.34it/s] 70%|   | 549/785 [06:15<01:09,  3.37it/s] 70%|   | 550/785 [06:16<01:09,  3.40it/s] 70%|   | 551/785 [06:16<01:08,  3.41it/s] 70%|   | 552/785 [06:16<01:08,  3.42it/s] 70%|   | 553/785 [06:17<01:59,  1.94it/s] 71%|   | 554/785 [06:17<01:43,  2.23it/s] 71%|   | 555/785 [06:18<01:32,  2.50it/s] 71%|   | 556/785 [06:18<01:24,  2.72it/s] 71%|   | 557/785 [06:19<01:50,  2.07it/s] 71%|   | 558/785 [06:19<01:36,  2.35it/s] 71%|   | 559/785 [06:19<01:27,  2.60it/s] 71%|  | 560/785 [06:20<01:27,  2.58it/s] 71%|  | 561/785 [06:20<01:20,  2.79it/s] 72%|  | 562/785 [06:20<01:15,  2.96it/s] 72%|  | 563/785 [06:21<01:11,  3.08it/s] 72%|  | 564/785 [06:21<01:09,  3.18it/s] 72%|  | 565/785 [06:21<01:07,  3.26it/s] 72%|  | 566/785 [06:22<01:06,  3.31it/s] 72%|  | 567/785 [06:22<01:05,  3.34it/s] 72%|  | 568/785 [06:22<01:04,  3.37it/s] 72%|  | 569/785 [06:22<01:03,  3.40it/s] 73%|  | 570/785 [06:23<01:17,  2.79it/s] 73%|  | 571/785 [06:23<01:12,  2.96it/s] 73%|  | 572/785 [06:23<01:08,  3.09it/s] 73%|  | 573/785 [06:24<01:06,  3.19it/s] 73%|  | 574/785 [06:24<01:04,  3.27it/s] 73%|  | 575/785 [06:24<01:03,  3.32it/s] 73%|  | 576/785 [06:25<01:02,  3.36it/s] 74%|  | 577/785 [06:25<01:01,  3.39it/s] 74%|  | 578/785 [06:25<01:00,  3.41it/s] 74%|  | 579/785 [06:27<02:03,  1.66it/s] 74%|  | 580/785 [06:27<01:46,  1.92it/s] 74%|  | 581/785 [06:28<02:09,  1.57it/s] 74%|  | 582/785 [06:28<01:48,  1.88it/s] 74%|  | 583/785 [06:28<01:33,  2.17it/s] 74%|  | 584/785 [06:29<01:22,  2.44it/s] 75%|  | 585/785 [06:29<01:14,  2.67it/s] 75%|  | 586/785 [06:29<01:09,  2.87it/s] 75%|  | 587/785 [06:30<01:05,  3.01it/s] 75%|  | 588/785 [06:30<01:02,  3.13it/s] 75%|  | 589/785 [06:30<01:22,  2.39it/s] 75%|  | 590/785 [06:31<01:14,  2.63it/s] 75%|  | 591/785 [06:31<01:08,  2.83it/s] 75%|  | 592/785 [06:31<01:04,  2.98it/s] 76%|  | 593/785 [06:32<01:01,  3.11it/s] 76%|  | 594/785 [06:32<00:59,  3.20it/s] 76%|  | 595/785 [06:32<00:58,  3.27it/s] 76%|  | 596/785 [06:33<00:56,  3.32it/s] 76%|  | 597/785 [06:33<00:55,  3.36it/s] 76%|  | 598/785 [06:33<00:55,  3.38it/s] 76%|  | 599/785 [06:34<01:17,  2.41it/s] 76%|  | 600/785 [06:34<01:09,  2.64it/s] 77%|  | 601/785 [06:34<01:04,  2.84it/s] 77%|  | 602/785 [06:35<01:00,  3.00it/s] 77%|  | 603/785 [06:35<00:58,  3.12it/s] 77%|  | 604/785 [06:35<00:56,  3.21it/s] 77%|  | 605/785 [06:36<00:54,  3.28it/s] 77%|  | 606/785 [06:36<00:53,  3.33it/s] 77%|  | 607/785 [06:36<00:52,  3.36it/s] 77%|  | 608/785 [06:37<00:59,  2.98it/s] 78%|  | 609/785 [06:37<00:56,  3.11it/s] 78%|  | 610/785 [06:37<00:54,  3.20it/s] 78%|  | 611/785 [06:37<00:53,  3.27it/s] 78%|  | 612/785 [06:38<00:52,  3.32it/s] 78%|  | 613/785 [06:38<00:51,  3.36it/s] 78%|  | 614/785 [06:38<00:50,  3.39it/s] 78%|  | 615/785 [06:39<00:49,  3.41it/s] 78%|  | 616/785 [06:39<00:49,  3.42it/s] 79%|  | 617/785 [06:39<00:49,  3.42it/s] 79%|  | 618/785 [06:40<01:28,  1.89it/s] 79%|  | 619/785 [06:41<01:16,  2.18it/s] 79%|  | 620/785 [06:41<01:07,  2.45it/s] 79%|  | 621/785 [06:41<01:05,  2.49it/s] 79%|  | 622/785 [06:41<01:00,  2.72it/s] 79%|  | 623/785 [06:42<00:55,  2.90it/s] 79%|  | 624/785 [06:42<00:52,  3.04it/s] 80%|  | 625/785 [06:42<00:50,  3.15it/s] 80%|  | 626/785 [06:43<00:49,  3.23it/s] 80%|  | 627/785 [06:43<00:48,  3.29it/s] 80%|  | 628/785 [06:43<00:45,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:29:24,489 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:29:24,489 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:29:24,489 >>   Batch size = 8
{'eval_loss': 1.0339648723602295, 'eval_runtime': 10.4949, 'eval_samples_per_second': 331.78, 'eval_steps_per_second': 41.544, 'epoch': 3.0}
{'loss': 0.8264, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.36it/s][A
  3%|         | 12/436 [00:00<00:08, 49.11it/s][A
  4%|         | 17/436 [00:00<00:08, 47.31it/s][A
  5%|         | 22/436 [00:00<00:08, 46.38it/s][A
  6%|         | 27/436 [00:00<00:08, 45.63it/s][A
  7%|         | 32/436 [00:00<00:11, 33.73it/s][A
  8%|         | 37/436 [00:00<00:10, 36.64it/s][A
 10%|         | 42/436 [00:01<00:10, 38.99it/s][A
 11%|         | 47/436 [00:01<00:09, 40.67it/s][A
 12%|        | 52/436 [00:01<00:09, 42.00it/s][A
 13%|        | 57/436 [00:01<00:08, 42.79it/s][A
 14%|        | 62/436 [00:01<00:08, 43.57it/s][A
 15%|        | 67/436 [00:01<00:08, 43.88it/s][A
 17%|        | 72/436 [00:01<00:08, 43.68it/s][A
 18%|        | 77/436 [00:01<00:08, 43.59it/s][A
 19%|        | 82/436 [00:01<00:08, 43.71it/s][A
 20%|        | 87/436 [00:02<00:07, 44.13it/s][A
 21%|        | 92/436 [00:02<00:07, 44.44it/s][A
 22%|       | 97/436 [00:02<00:07, 44.68it/s][A
 23%|       | 102/436 [00:02<00:07, 44.73it/s][A
 25%|       | 107/436 [00:02<00:07, 44.79it/s][A
 26%|       | 112/436 [00:02<00:07, 44.70it/s][A
 27%|       | 117/436 [00:02<00:07, 44.30it/s][A
 28%|       | 122/436 [00:02<00:07, 44.11it/s][A
 29%|       | 127/436 [00:02<00:07, 44.13it/s][A
 30%|       | 132/436 [00:03<00:06, 44.47it/s][A
 31%|      | 137/436 [00:03<00:06, 44.70it/s][A
 33%|      | 142/436 [00:03<00:06, 44.72it/s][A
 34%|      | 147/436 [00:03<00:06, 44.73it/s][A
 35%|      | 152/436 [00:03<00:06, 44.85it/s][A
 36%|      | 157/436 [00:04<00:06, 44.76it/s][A
 37%|      | 162/436 [00:04<00:14, 18.61it/s][A
 38%|      | 167/436 [00:04<00:11, 22.57it/s][A
 39%|      | 172/436 [00:04<00:09, 26.54it/s][A
 41%|      | 177/436 [00:04<00:08, 30.26it/s][A
 42%|     | 182/436 [00:04<00:07, 33.60it/s][A
 43%|     | 187/436 [00:04<00:06, 36.34it/s][A
 44%|     | 192/436 [00:04<00:06, 38.64it/s][A
 45%|     | 197/436 [00:05<00:05, 40.29it/s][A
 46%|     | 202/436 [00:05<00:05, 41.22it/s][A
 47%|     | 207/436 [00:05<00:05, 41.84it/s][A
 49%|     | 212/436 [00:05<00:05, 42.54it/s][A
 50%|     | 217/436 [00:05<00:05, 43.13it/s][A
 51%|     | 222/436 [00:05<00:04, 43.72it/s][A
 52%|    | 227/436 [00:05<00:04, 44.19it/s][A
 53%|    | 232/436 [00:05<00:04, 44.41it/s][A
 54%|    | 237/436 [00:05<00:04, 44.62it/s][A
 56%|    | 242/436 [00:06<00:04, 44.58it/s][A
 57%|    | 247/436 [00:06<00:04, 44.29it/s][A
 58%|    | 252/436 [00:06<00:04, 44.11it/s][A
 59%|    | 257/436 [00:06<00:04, 44.15it/s][A
 60%|    | 262/436 [00:06<00:03, 44.38it/s][A
 61%|    | 267/436 [00:06<00:03, 44.51it/s][A
 62%|   | 272/436 [00:06<00:03, 44.63it/s][A
 64%|   | 277/436 [00:06<00:03, 41.54it/s][A
 65%|   | 282/436 [00:06<00:03, 42.56it/s][A
 66%|   | 287/436 [00:07<00:03, 43.24it/s][A
 67%|   | 292/436 [00:07<00:03, 43.37it/s][A
 68%|   | 297/436 [00:07<00:03, 43.62it/s][A
 69%|   | 302/436 [00:07<00:03, 43.70it/s][A
 70%|   | 307/436 [00:07<00:02, 44.02it/s][A
 72%|  | 312/436 [00:07<00:02, 44.22it/s][A
 73%|  | 317/436 [00:07<00:02, 44.02it/s][A
 74%|  | 322/436 [00:07<00:02, 44.17it/s][A
 75%|  | 327/436 [00:07<00:02, 44.29it/s][A
 76%|  | 332/436 [00:08<00:02, 44.33it/s][A
 77%|  | 337/436 [00:08<00:02, 44.15it/s][A
 78%|  | 342/436 [00:08<00:02, 44.12it/s][A
 80%|  | 347/436 [00:08<00:02, 44.18it/s][A
 81%|  | 352/436 [00:08<00:01, 44.33it/s][A
 82%| | 357/436 [00:08<00:01, 44.47it/s][A
 83%| | 362/436 [00:08<00:01, 44.54it/s][A
 84%| | 367/436 [00:08<00:01, 44.54it/s][A
 85%| | 372/436 [00:08<00:01, 44.56it/s][A
 86%| | 377/436 [00:09<00:01, 44.62it/s][A
 88%| | 382/436 [00:09<00:01, 44.46it/s][A
 89%| | 387/436 [00:09<00:01, 44.41it/s][A
 90%| | 392/436 [00:09<00:00, 44.33it/s][A
 91%| | 397/436 [00:09<00:00, 44.53it/s][A
 92%|| 402/436 [00:09<00:00, 44.65it/s][A
 93%|| 407/436 [00:09<00:00, 44.75it/s][A
 94%|| 412/436 [00:10<00:00, 31.64it/s][A
 96%|| 417/436 [00:10<00:00, 34.74it/s][A
 97%|| 422/436 [00:10<00:00, 37.27it/s][A
 98%|| 427/436 [00:10<00:00, 39.27it/s][A
 99%|| 432/436 [00:10<00:00, 40.83it/s][A                                                 
                                                 [A 80%|  | 628/785 [06:54<00:45,  3.41it/s]
100%|| 436/436 [00:10<00:00, 40.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:29:36,568 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 21:29:37,668 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:54,576 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:56,101 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:57,170 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|  | 629/785 [07:48<51:15, 19.72s/it] 80%|  | 630/785 [07:49<35:55, 13.91s/it] 80%|  | 631/785 [07:49<25:12,  9.82s/it] 81%|  | 632/785 [07:49<17:45,  6.96s/it] 81%|  | 633/785 [07:49<12:34,  4.96s/it] 81%|  | 634/785 [07:50<08:57,  3.56s/it] 81%|  | 635/785 [07:50<06:27,  2.58s/it] 81%|  | 636/785 [07:50<04:42,  1.89s/it] 81%|  | 637/785 [07:51<03:29,  1.41s/it] 81%| | 638/785 [07:51<02:38,  1.08s/it] 81%| | 639/785 [07:51<02:02,  1.19it/s] 82%| | 640/785 [07:52<01:38,  1.48it/s] 82%| | 641/785 [07:52<01:28,  1.63it/s] 82%| | 642/785 [07:52<01:13,  1.94it/s] 82%| | 643/785 [07:53<01:03,  2.23it/s] 82%| | 644/785 [07:53<00:56,  2.48it/s] 82%| | 645/785 [07:53<00:51,  2.71it/s] 82%| | 646/785 [07:53<00:48,  2.88it/s] 82%| | 647/785 [07:54<00:45,  3.02it/s] 83%| | 648/785 [07:54<00:43,  3.13it/s] 83%| | 649/785 [07:54<00:42,  3.21it/s] 83%| | 650/785 [07:55<00:41,  3.27it/s] 83%| | 651/785 [07:55<00:42,  3.13it/s] 83%| | 652/785 [07:56<00:51,  2.56it/s] 83%| | 653/785 [07:56<00:47,  2.77it/s] 83%| | 654/785 [07:56<00:44,  2.93it/s] 83%| | 655/785 [07:56<00:42,  3.06it/s] 84%| | 656/785 [07:57<00:40,  3.16it/s] 84%| | 657/785 [07:57<00:39,  3.24it/s] 84%| | 658/785 [07:57<00:38,  3.30it/s] 84%| | 659/785 [07:58<00:37,  3.35it/s] 84%| | 660/785 [07:58<00:36,  3.38it/s] 84%| | 661/785 [07:58<00:38,  3.20it/s] 84%| | 662/785 [07:58<00:37,  3.26it/s] 84%| | 663/785 [07:59<00:36,  3.32it/s] 85%| | 664/785 [07:59<00:36,  3.35it/s] 85%| | 665/785 [07:59<00:35,  3.39it/s] 85%| | 666/785 [08:00<00:34,  3.41it/s] 85%| | 667/785 [08:00<00:34,  3.42it/s] 85%| | 668/785 [08:00<00:34,  3.43it/s] 85%| | 669/785 [08:01<00:33,  3.44it/s] 85%| | 670/785 [08:01<00:33,  3.44it/s] 85%| | 671/785 [08:01<00:33,  3.45it/s] 86%| | 672/785 [08:02<00:39,  2.84it/s] 86%| | 673/785 [08:02<00:37,  3.00it/s] 86%| | 674/785 [08:02<00:35,  3.12it/s] 86%| | 675/785 [08:02<00:34,  3.21it/s] 86%| | 676/785 [08:03<00:33,  3.28it/s] 86%| | 677/785 [08:03<00:32,  3.33it/s] 86%| | 678/785 [08:03<00:31,  3.37it/s] 86%| | 679/785 [08:04<00:31,  3.40it/s] 87%| | 680/785 [08:04<00:30,  3.41it/s] 87%| | 681/785 [08:04<00:30,  3.43it/s] 87%| | 682/785 [08:05<00:31,  3.31it/s] 87%| | 683/785 [08:05<00:30,  3.35it/s] 87%| | 684/785 [08:05<00:29,  3.38it/s] 87%| | 685/785 [08:05<00:29,  3.40it/s] 87%| | 686/785 [08:06<00:29,  3.40it/s] 88%| | 687/785 [08:06<00:28,  3.42it/s] 88%| | 688/785 [08:06<00:28,  3.42it/s] 88%| | 689/785 [08:07<00:27,  3.44it/s] 88%| | 690/785 [08:07<00:27,  3.44it/s] 88%| | 691/785 [08:07<00:27,  3.45it/s] 88%| | 692/785 [08:07<00:26,  3.45it/s] 88%| | 693/785 [08:08<00:31,  2.91it/s] 88%| | 694/785 [08:08<00:29,  3.04it/s] 89%| | 695/785 [08:08<00:28,  3.15it/s] 89%| | 696/785 [08:09<00:27,  3.23it/s] 89%| | 697/785 [08:09<00:26,  3.29it/s] 89%| | 698/785 [08:09<00:26,  3.34it/s] 89%| | 699/785 [08:10<00:25,  3.37it/s] 89%| | 700/785 [08:10<00:25,  3.39it/s] 89%| | 701/785 [08:10<00:24,  3.41it/s] 89%| | 702/785 [08:11<00:24,  3.42it/s] 90%| | 703/785 [08:11<00:25,  3.21it/s] 90%| | 704/785 [08:11<00:24,  3.28it/s] 90%| | 705/785 [08:11<00:24,  3.33it/s] 90%| | 706/785 [08:12<00:23,  3.36it/s] 90%| | 707/785 [08:12<00:23,  3.39it/s] 90%| | 708/785 [08:12<00:22,  3.40it/s] 90%| | 709/785 [08:13<00:22,  3.42it/s] 90%| | 710/785 [08:13<00:21,  3.42it/s] 91%| | 711/785 [08:13<00:21,  3.43it/s] 91%| | 712/785 [08:13<00:21,  3.43it/s] 91%| | 713/785 [08:14<00:20,  3.44it/s] 91%| | 714/785 [08:14<00:20,  3.44it/s] 91%| | 715/785 [08:14<00:20,  3.44it/s] 91%| | 716/785 [08:15<00:20,  3.44it/s] 91%|| 717/785 [08:15<00:19,  3.44it/s] 91%|| 718/785 [08:15<00:19,  3.44it/s] 92%|| 719/785 [08:16<00:19,  3.44it/s] 92%|| 720/785 [08:16<00:18,  3.44it/s] 92%|| 721/785 [08:16<00:18,  3.44it/s] 92%|| 722/785 [08:17<00:21,  2.91it/s] 92%|| 723/785 [08:17<00:20,  3.06it/s] 92%|| 724/785 [08:17<00:19,  3.16it/s] 92%|| 725/785 [08:17<00:18,  3.25it/s] 92%|| 726/785 [08:18<00:17,  3.30it/s] 93%|| 727/785 [08:18<00:17,  3.35it/s] 93%|| 728/785 [08:18<00:16,  3.37it/s] 93%|| 729/785 [08:19<00:16,  3.40it/s] 93%|| 730/785 [08:19<00:21,  2.60it/s] 93%|| 731/785 [08:20<00:21,  2.53it/s] 93%|| 732/785 [08:20<00:19,  2.74it/s] 93%|| 733/785 [08:20<00:17,  2.92it/s] 94%|| 734/785 [08:20<00:16,  3.06it/s] 94%|| 735/785 [08:21<00:15,  3.17it/s] 94%|| 736/785 [08:21<00:15,  3.24it/s] 94%|| 737/785 [08:21<00:14,  3.30it/s] 94%|| 738/785 [08:22<00:14,  3.34it/s] 94%|| 739/785 [08:22<00:13,  3.37it/s] 94%|| 740/785 [08:22<00:13,  3.39it/s] 94%|| 741/785 [08:23<00:15,  2.88it/s] 95%|| 742/785 [08:23<00:14,  3.03it/s] 95%|| 743/785 [08:23<00:13,  3.14it/s] 95%|| 744/785 [08:24<00:12,  3.22it/s] 95%|| 745/785 [08:24<00:12,  3.29it/s] 95%|| 746/785 [08:24<00:11,  3.34it/s] 95%|| 747/785 [08:24<00:11,  3.37it/s] 95%|| 748/785 [08:25<00:10,  3.39it/s] 95%|| 749/785 [08:25<00:10,  3.41it/s] 96%|| 750/785 [08:25<00:10,  3.42it/s] 96%|| 751/785 [08:26<00:13,  2.49it/s] 96%|| 752/785 [08:26<00:12,  2.71it/s] 96%|| 753/785 [08:27<00:11,  2.90it/s] 96%|| 754/785 [08:27<00:12,  2.51it/s] 96%|| 755/785 [08:28<00:15,  1.88it/s] 96%|| 756/785 [08:28<00:13,  2.17it/s] 96%|| 757/785 [08:29<00:11,  2.44it/s] 97%|| 758/785 [08:29<00:15,  1.75it/s] 97%|| 759/785 [08:30<00:12,  2.05it/s] 97%|| 760/785 [08:30<00:10,  2.33it/s] 97%|| 761/785 [08:30<00:09,  2.58it/s] 97%|| 762/785 [08:31<00:08,  2.79it/s] 97%|| 763/785 [08:31<00:07,  2.96it/s] 97%|| 764/785 [08:31<00:06,  3.09it/s] 97%|| 765/785 [08:31<00:06,  3.19it/s] 98%|| 766/785 [08:32<00:08,  2.11it/s] 98%|| 767/785 [08:33<00:07,  2.39it/s] 98%|| 768/785 [08:33<00:06,  2.63it/s] 98%|| 769/785 [08:33<00:05,  2.83it/s] 98%|| 770/785 [08:33<00:05,  2.98it/s] 98%|| 771/785 [08:34<00:04,  3.11it/s] 98%|| 772/785 [08:34<00:04,  3.20it/s] 98%|| 773/785 [08:34<00:03,  3.27it/s] 99%|| 774/785 [08:35<00:03,  3.32it/s] 99%|| 775/785 [08:35<00:03,  3.14it/s] 99%|| 776/785 [08:35<00:02,  3.23it/s] 99%|| 777/785 [08:36<00:02,  3.29it/s] 99%|| 778/785 [08:36<00:02,  3.34it/s] 99%|| 779/785 [08:36<00:01,  3.37it/s] 99%|| 780/785 [08:36<00:01,  3.39it/s] 99%|| 781/785 [08:37<00:01,  3.41it/s]100%|| 782/785 [08:37<00:00,  3.42it/s]100%|| 783/785 [08:37<00:00,  3.43it/s]100%|| 784/785 [08:38<00:00,  3.44it/s]100%|| 785/785 [08:38<00:00,  3.53it/s][INFO|trainer.py:2140] 2023-08-28 21:31:19,326 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:31:19,327 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:31:19,327 >>   Batch size = 8
{'eval_loss': 1.0391796827316284, 'eval_runtime': 10.5782, 'eval_samples_per_second': 329.166, 'eval_steps_per_second': 41.217, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.18it/s][A
  3%|         | 12/436 [00:00<00:08, 48.96it/s][A
  4%|         | 17/436 [00:00<00:08, 47.42it/s][A
  5%|         | 22/436 [00:00<00:08, 46.56it/s][A
  6%|         | 27/436 [00:00<00:08, 46.12it/s][A
  7%|         | 32/436 [00:00<00:08, 45.75it/s][A
  8%|         | 37/436 [00:00<00:08, 45.58it/s][A
 10%|         | 42/436 [00:00<00:08, 45.03it/s][A
 11%|         | 47/436 [00:01<00:08, 44.55it/s][A
 12%|        | 52/436 [00:01<00:08, 44.13it/s][A
 13%|        | 57/436 [00:01<00:08, 44.14it/s][A
 14%|        | 62/436 [00:01<00:08, 44.29it/s][A
 15%|        | 67/436 [00:01<00:08, 44.49it/s][A
 17%|        | 72/436 [00:01<00:08, 44.68it/s][A
 18%|        | 77/436 [00:01<00:08, 44.79it/s][A
 19%|        | 82/436 [00:01<00:07, 44.95it/s][A
 20%|        | 87/436 [00:01<00:07, 44.71it/s][A
 21%|        | 92/436 [00:02<00:07, 44.44it/s][A
 22%|       | 97/436 [00:02<00:07, 44.13it/s][A
 23%|       | 102/436 [00:02<00:07, 44.19it/s][A
 25%|       | 107/436 [00:02<00:07, 44.30it/s][A
 26%|       | 112/436 [00:02<00:07, 44.52it/s][A
 27%|       | 117/436 [00:02<00:07, 44.74it/s][A
 28%|       | 122/436 [00:02<00:06, 44.88it/s][A
 29%|       | 127/436 [00:02<00:06, 44.93it/s][A
 30%|       | 132/436 [00:03<00:06, 44.88it/s][A
 31%|      | 137/436 [00:03<00:12, 23.19it/s][A
 33%|      | 142/436 [00:03<00:10, 27.12it/s][A
 34%|      | 147/436 [00:03<00:09, 30.79it/s][A
 35%|      | 152/436 [00:03<00:08, 34.07it/s][A
 36%|      | 157/436 [00:03<00:07, 36.79it/s][A
 37%|      | 162/436 [00:03<00:07, 38.93it/s][A
 38%|      | 167/436 [00:04<00:06, 40.67it/s][A
 39%|      | 172/436 [00:04<00:06, 41.79it/s][A
 41%|      | 177/436 [00:04<00:06, 42.20it/s][A
 42%|     | 182/436 [00:04<00:05, 42.69it/s][A
 43%|     | 187/436 [00:04<00:05, 43.21it/s][A
 44%|     | 192/436 [00:04<00:05, 43.72it/s][A
 45%|     | 197/436 [00:04<00:05, 44.16it/s][A
 46%|     | 202/436 [00:04<00:05, 44.31it/s][A
 47%|     | 207/436 [00:04<00:05, 44.61it/s][A
 49%|     | 212/436 [00:05<00:05, 44.61it/s][A
 50%|     | 217/436 [00:05<00:04, 44.46it/s][A
 51%|     | 222/436 [00:05<00:04, 44.25it/s][A
 52%|    | 227/436 [00:05<00:04, 44.10it/s][A
 53%|    | 232/436 [00:05<00:04, 44.18it/s][A
 54%|    | 237/436 [00:05<00:04, 44.43it/s][A
 56%|    | 242/436 [00:05<00:04, 44.61it/s][A
 57%|    | 247/436 [00:05<00:04, 44.75it/s][A
 58%|    | 252/436 [00:06<00:04, 44.81it/s][A
 59%|    | 257/436 [00:06<00:08, 21.28it/s][A
 60%|    | 262/436 [00:06<00:06, 25.26it/s][A
 61%|    | 267/436 [00:06<00:05, 29.12it/s][A
 62%|   | 272/436 [00:06<00:05, 32.60it/s][A
 64%|   | 277/436 [00:06<00:04, 35.59it/s][A
 65%|   | 282/436 [00:07<00:04, 37.99it/s][A
 66%|   | 287/436 [00:07<00:03, 39.92it/s][A
 67%|   | 292/436 [00:07<00:03, 41.21it/s][A
 68%|   | 297/436 [00:07<00:03, 41.92it/s][A
 69%|   | 302/436 [00:07<00:03, 42.44it/s][A
 70%|   | 307/436 [00:07<00:03, 42.94it/s][A
 72%|  | 312/436 [00:07<00:02, 43.39it/s][A
 73%|  | 317/436 [00:07<00:02, 43.91it/s][A
 74%|  | 322/436 [00:07<00:02, 44.32it/s][A
 75%|  | 327/436 [00:08<00:02, 44.57it/s][A
 76%|  | 332/436 [00:08<00:02, 44.69it/s][A
 77%|  | 337/436 [00:08<00:02, 44.67it/s][A
 78%|  | 342/436 [00:08<00:02, 44.45it/s][A
 80%|  | 347/436 [00:08<00:02, 44.18it/s][A
 81%|  | 352/436 [00:08<00:01, 44.13it/s][A
 82%| | 357/436 [00:08<00:01, 44.17it/s][A
 83%| | 362/436 [00:08<00:01, 44.31it/s][A
 84%| | 367/436 [00:08<00:01, 44.48it/s][A
 85%| | 372/436 [00:09<00:01, 44.55it/s][A
 86%| | 377/436 [00:09<00:02, 22.95it/s][A
 88%| | 382/436 [00:09<00:02, 26.89it/s][A
 89%| | 387/436 [00:09<00:01, 30.60it/s][A
 90%| | 392/436 [00:09<00:01, 33.88it/s][A
 91%| | 397/436 [00:09<00:01, 36.68it/s][A
 92%|| 402/436 [00:10<00:00, 38.86it/s][A
 93%|| 407/436 [00:10<00:00, 40.50it/s][A
 94%|| 412/436 [00:10<00:00, 41.69it/s][A
 96%|| 417/436 [00:10<00:00, 42.12it/s][A
 97%|| 422/436 [00:10<00:00, 42.56it/s][A
 98%|| 427/436 [00:10<00:00, 42.92it/s][A
 99%|| 432/436 [00:10<00:00, 43.42it/s][A                                                 
                                                 [A100%|| 785/785 [08:49<00:00,  3.53it/s]
100%|| 436/436 [00:10<00:00, 43.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:31:31,960 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 21:31:34,767 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:58,014 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:58,893 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:59,007 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:32:24,783 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:32:25,646 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314 (score: 1.0291813611984253).
                                                 100%|| 785/785 [10:27<00:00,  3.53it/s]100%|| 785/785 [10:27<00:00,  1.25it/s]
[INFO|trainer.py:1894] 2023-08-28 21:33:08,783 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 21:33:09,247 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:33:18,800 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:33:19,057 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:33:19,120 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:33:20,516 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:20,516 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:20,516 >>   train_loss               =     0.8119
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:20,516 >>   train_runtime            = 0:10:27.43
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:20,516 >>   train_samples            =      10042
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:20,516 >>   train_samples_per_second =     80.025
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:20,516 >>   train_steps_per_second   =      1.251
{'eval_loss': 1.0422749519348145, 'eval_runtime': 10.8632, 'eval_samples_per_second': 320.533, 'eval_steps_per_second': 40.136, 'epoch': 5.0}
{'train_runtime': 627.4304, 'train_samples_per_second': 80.025, 'train_steps_per_second': 1.251, 'train_loss': 0.8119406341747114, 'epoch': 5.0}
08/28/2023 21:33:21 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:33:21,533 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:33:21,533 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:33:21,533 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.59it/s]  3%|         | 12/436 [00:00<00:08, 49.30it/s]  4%|         | 17/436 [00:00<00:08, 47.66it/s]  5%|         | 22/436 [00:00<00:17, 23.33it/s]  6%|         | 27/436 [00:00<00:14, 28.08it/s]  7%|         | 31/436 [00:01<00:24, 16.34it/s]  8%|         | 35/436 [00:01<00:20, 19.39it/s]  9%|         | 40/436 [00:01<00:16, 24.10it/s] 10%|         | 45/436 [00:01<00:13, 28.44it/s] 11%|        | 50/436 [00:01<00:11, 32.22it/s] 13%|        | 55/436 [00:01<00:10, 35.41it/s] 14%|        | 60/436 [00:02<00:09, 37.97it/s] 15%|        | 65/436 [00:02<00:13, 27.24it/s] 16%|        | 70/436 [00:02<00:11, 31.34it/s] 17%|        | 75/436 [00:02<00:10, 34.56it/s] 18%|        | 80/436 [00:02<00:09, 37.13it/s] 19%|        | 85/436 [00:02<00:08, 39.28it/s] 21%|        | 90/436 [00:02<00:08, 40.84it/s] 22%|       | 95/436 [00:02<00:08, 42.14it/s] 23%|       | 100/436 [00:03<00:07, 42.90it/s] 24%|       | 105/436 [00:03<00:07, 43.22it/s] 25%|       | 110/436 [00:03<00:07, 43.39it/s] 26%|       | 115/436 [00:03<00:07, 43.81it/s] 28%|       | 120/436 [00:03<00:10, 31.48it/s] 29%|       | 125/436 [00:03<00:08, 34.67it/s] 30%|       | 130/436 [00:03<00:08, 37.25it/s] 31%|       | 135/436 [00:04<00:07, 39.35it/s] 32%|      | 140/436 [00:04<00:07, 40.95it/s] 33%|      | 145/436 [00:04<00:06, 42.05it/s] 34%|      | 150/436 [00:04<00:06, 42.96it/s] 36%|      | 155/436 [00:04<00:06, 43.50it/s] 37%|      | 160/436 [00:04<00:06, 43.49it/s] 38%|      | 165/436 [00:04<00:06, 43.68it/s] 39%|      | 170/436 [00:04<00:06, 44.00it/s] 40%|      | 175/436 [00:04<00:05, 44.29it/s] 41%|     | 180/436 [00:05<00:05, 44.53it/s] 42%|     | 185/436 [00:05<00:05, 44.64it/s] 44%|     | 190/436 [00:05<00:05, 44.85it/s] 45%|     | 195/436 [00:05<00:05, 44.94it/s] 46%|     | 200/436 [00:05<00:05, 44.80it/s] 47%|     | 205/436 [00:05<00:05, 44.53it/s] 48%|     | 210/436 [00:05<00:05, 44.43it/s] 49%|     | 215/436 [00:05<00:04, 44.48it/s] 50%|     | 220/436 [00:05<00:04, 44.39it/s] 52%|    | 225/436 [00:06<00:04, 44.69it/s] 53%|    | 230/436 [00:06<00:04, 44.86it/s] 54%|    | 235/436 [00:06<00:04, 44.91it/s] 55%|    | 240/436 [00:06<00:04, 44.92it/s] 56%|    | 245/436 [00:06<00:04, 44.68it/s] 57%|    | 250/436 [00:06<00:06, 27.60it/s] 58%|    | 255/436 [00:06<00:05, 31.24it/s] 60%|    | 260/436 [00:07<00:05, 34.44it/s] 61%|    | 265/436 [00:07<00:04, 37.03it/s] 62%|   | 270/436 [00:07<00:04, 39.17it/s] 63%|   | 275/436 [00:07<00:03, 40.73it/s] 64%|   | 280/436 [00:07<00:03, 42.02it/s] 65%|   | 285/436 [00:07<00:03, 42.83it/s] 67%|   | 290/436 [00:07<00:03, 43.03it/s] 68%|   | 295/436 [00:07<00:03, 43.26it/s] 69%|   | 300/436 [00:07<00:03, 43.67it/s] 70%|   | 305/436 [00:08<00:02, 44.07it/s] 71%|   | 310/436 [00:08<00:02, 44.32it/s] 72%|  | 315/436 [00:08<00:02, 44.61it/s] 73%|  | 320/436 [00:08<00:02, 44.66it/s] 75%|  | 325/436 [00:08<00:02, 44.69it/s] 76%|  | 330/436 [00:08<00:02, 44.71it/s] 77%|  | 335/436 [00:08<00:02, 44.54it/s] 78%|  | 340/436 [00:08<00:02, 44.34it/s] 79%|  | 345/436 [00:08<00:02, 44.39it/s] 80%|  | 350/436 [00:09<00:01, 44.39it/s] 81%| | 355/436 [00:09<00:01, 44.62it/s] 83%| | 360/436 [00:09<00:01, 44.69it/s] 84%| | 365/436 [00:09<00:01, 44.97it/s] 85%| | 370/436 [00:09<00:01, 45.13it/s] 86%| | 375/436 [00:09<00:01, 36.34it/s] 87%| | 380/436 [00:09<00:01, 38.71it/s] 88%| | 385/436 [00:09<00:01, 40.48it/s] 89%| | 390/436 [00:10<00:01, 41.82it/s] 91%| | 395/436 [00:10<00:00, 42.92it/s] 92%|| 400/436 [00:10<00:00, 43.56it/s] 93%|| 405/436 [00:10<00:00, 44.17it/s] 94%|| 410/436 [00:10<00:00, 44.48it/s] 95%|| 415/436 [00:10<00:00, 44.32it/s] 96%|| 420/436 [00:10<00:00, 44.26it/s] 97%|| 425/436 [00:10<00:00, 44.47it/s] 99%|| 430/436 [00:10<00:00, 44.64it/s]100%|| 435/436 [00:11<00:00, 44.93it/s]100%|| 436/436 [00:11<00:00, 39.41it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:33:32,616 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   eval_loss               =     1.0292
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   eval_runtime            = 0:00:11.08
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   eval_samples_per_second =    314.184
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   eval_steps_per_second   =     39.341
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:32,616 >>   perplexity              =     2.7988
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,024 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:56,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:33:56,790 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:33:56,791 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:57,623 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:58,694 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:58,695 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:03,342 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:03,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:03,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:03,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:03,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:34:04,622 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:34:04,623 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:05,571 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:05,746 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:05,746 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-157
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/checkpoint-471
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.39it/s]Extractor Predicting: 12it [00:07,  1.44it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:14,  1.24it/s]Extractor Predicting: 22it [00:14,  1.34it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:16,  1.38it/s]Extractor Predicting: 26it [00:17,  1.43it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:20,  1.50it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.40it/s]Extractor Predicting: 34it [00:23,  1.41it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.36it/s]Extractor Predicting: 39it [00:26,  1.40it/s]Extractor Predicting: 40it [00:27,  1.44it/s]Extractor Predicting: 41it [00:27,  1.47it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:29,  1.46it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:30,  1.50it/s]Extractor Predicting: 46it [00:31,  1.52it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.36it/s]Extractor Predicting: 49it [00:33,  1.38it/s]Extractor Predicting: 50it [00:34,  1.40it/s]Extractor Predicting: 51it [00:34,  1.43it/s]Extractor Predicting: 52it [00:35,  1.46it/s]Extractor Predicting: 53it [00:36,  1.42it/s]Extractor Predicting: 54it [00:36,  1.42it/s]Extractor Predicting: 55it [00:37,  1.45it/s]Extractor Predicting: 56it [00:38,  1.45it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.25it/s]Extractor Predicting: 59it [00:40,  1.31it/s]Extractor Predicting: 60it [00:41,  1.35it/s]Extractor Predicting: 61it [00:41,  1.41it/s]Extractor Predicting: 62it [00:42,  1.37it/s]Extractor Predicting: 63it [00:43,  1.37it/s]Extractor Predicting: 64it [00:44,  1.39it/s]Extractor Predicting: 65it [00:44,  1.46it/s]Extractor Predicting: 66it [00:45,  1.46it/s]Extractor Predicting: 67it [00:46,  1.27it/s]Extractor Predicting: 68it [00:47,  1.33it/s]Extractor Predicting: 69it [00:47,  1.39it/s]Extractor Predicting: 70it [00:48,  1.46it/s]Extractor Predicting: 71it [00:49,  1.49it/s]Extractor Predicting: 72it [00:50,  1.22it/s]Extractor Predicting: 73it [00:50,  1.34it/s]Extractor Predicting: 74it [00:51,  1.41it/s]Extractor Predicting: 75it [00:52,  1.43it/s]Extractor Predicting: 76it [00:52,  1.43it/s]Extractor Predicting: 77it [00:53,  1.44it/s]Extractor Predicting: 78it [00:54,  1.35it/s]Extractor Predicting: 79it [00:54,  1.40it/s]Extractor Predicting: 80it [00:55,  1.43it/s]Extractor Predicting: 81it [00:56,  1.45it/s]Extractor Predicting: 82it [00:57,  1.42it/s]Extractor Predicting: 83it [00:57,  1.46it/s]Extractor Predicting: 84it [00:58,  1.45it/s]Extractor Predicting: 85it [00:58,  1.51it/s]Extractor Predicting: 86it [00:59,  1.48it/s]Extractor Predicting: 87it [01:00,  1.49it/s]Extractor Predicting: 88it [01:01,  1.48it/s]Extractor Predicting: 89it [01:01,  1.47it/s]Extractor Predicting: 90it [01:02,  1.46it/s]Extractor Predicting: 91it [01:03,  1.44it/s]Extractor Predicting: 92it [01:03,  1.42it/s]Extractor Predicting: 93it [01:04,  1.47it/s]Extractor Predicting: 94it [01:05,  1.45it/s]Extractor Predicting: 95it [01:05,  1.50it/s]Extractor Predicting: 96it [01:06,  1.51it/s]Extractor Predicting: 97it [01:07,  1.47it/s]Extractor Predicting: 98it [01:07,  1.46it/s]Extractor Predicting: 99it [01:08,  1.43it/s]Extractor Predicting: 100it [01:09,  1.42it/s]Extractor Predicting: 101it [01:09,  1.46it/s]Extractor Predicting: 102it [01:10,  1.37it/s]Extractor Predicting: 103it [01:11,  1.39it/s]Extractor Predicting: 104it [01:12,  1.44it/s]Extractor Predicting: 105it [01:12,  1.46it/s]Extractor Predicting: 106it [01:13,  1.43it/s]Extractor Predicting: 107it [01:14,  1.37it/s]Extractor Predicting: 108it [01:14,  1.43it/s]Extractor Predicting: 109it [01:15,  1.49it/s]Extractor Predicting: 110it [01:16,  1.50it/s]Extractor Predicting: 111it [01:16,  1.56it/s]Extractor Predicting: 112it [01:17,  1.47it/s]Extractor Predicting: 113it [01:18,  1.49it/s]Extractor Predicting: 114it [01:18,  1.48it/s]Extractor Predicting: 115it [01:19,  1.48it/s]Extractor Predicting: 116it [01:20,  1.48it/s]Extractor Predicting: 117it [01:21,  1.24it/s]Extractor Predicting: 118it [01:22,  1.30it/s]Extractor Predicting: 119it [01:22,  1.36it/s]Extractor Predicting: 120it [01:23,  1.43it/s]Extractor Predicting: 121it [01:24,  1.36it/s]Extractor Predicting: 122it [01:24,  1.42it/s]Extractor Predicting: 123it [01:25,  1.41it/s]Extractor Predicting: 124it [01:26,  1.45it/s]Extractor Predicting: 125it [01:26,  1.47it/s]Extractor Predicting: 126it [01:27,  1.41it/s]Extractor Predicting: 127it [01:28,  1.43it/s]Extractor Predicting: 128it [01:28,  1.43it/s]Extractor Predicting: 129it [01:29,  1.45it/s]Extractor Predicting: 130it [01:30,  1.44it/s]Extractor Predicting: 131it [01:30,  1.45it/s]Extractor Predicting: 132it [01:31,  1.48it/s]Extractor Predicting: 133it [01:32,  1.45it/s]Extractor Predicting: 134it [01:33,  1.41it/s]Extractor Predicting: 135it [01:33,  1.43it/s]Extractor Predicting: 136it [01:34,  1.38it/s]Extractor Predicting: 137it [01:35,  1.41it/s]Extractor Predicting: 138it [01:35,  1.44it/s]Extractor Predicting: 139it [01:36,  1.43it/s]Extractor Predicting: 140it [01:37,  1.43it/s]Extractor Predicting: 141it [01:38,  1.41it/s]Extractor Predicting: 142it [01:38,  1.45it/s]Extractor Predicting: 143it [01:39,  1.51it/s]Extractor Predicting: 143it [01:39,  1.44it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:21,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:21,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:21,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:21,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:21,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:36:22,252 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:36:22,253 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:22,603 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:23,657 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:23,657 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:26,453 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:26,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:26,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:26,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:26,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:36:27,028 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:36:27,030 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:27,320 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:27,492 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:27,492 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3934010152284264,
  "recall": 0.04451464675473866,
  "score": 0.07997936016511868,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.50it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:17,  1.48it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:18,  1.52it/s]Extractor Predicting: 31it [00:19,  1.51it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:20,  1.48it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:22,  1.49it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.43it/s]Extractor Predicting: 39it [00:25,  1.47it/s]Extractor Predicting: 40it [00:25,  1.46it/s]Extractor Predicting: 41it [00:26,  1.49it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:28,  1.25it/s]Extractor Predicting: 44it [00:28,  1.32it/s]Extractor Predicting: 45it [00:29,  1.37it/s]Extractor Predicting: 46it [00:30,  1.41it/s]Extractor Predicting: 47it [00:30,  1.35it/s]Extractor Predicting: 48it [00:31,  1.39it/s]Extractor Predicting: 49it [00:32,  1.42it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.45it/s]Extractor Predicting: 53it [00:34,  1.44it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:39,  1.42it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:40,  1.46it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.55it/s]Extractor Predicting: 65it [00:42,  1.55it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.35it/s]Extractor Predicting: 72it [00:47,  1.40it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:49,  1.28it/s]Extractor Predicting: 75it [00:50,  1.37it/s]Extractor Predicting: 76it [00:50,  1.42it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:51,  1.55it/s]Extractor Predicting: 79it [00:52,  1.38it/s]Extractor Predicting: 80it [00:53,  1.41it/s]Extractor Predicting: 81it [00:54,  1.45it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:56,  1.46it/s]Extractor Predicting: 85it [00:56,  1.48it/s]Extractor Predicting: 86it [00:57,  1.51it/s]Extractor Predicting: 87it [00:58,  1.54it/s]Extractor Predicting: 88it [00:58,  1.54it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [01:00,  1.52it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:01,  1.51it/s]Extractor Predicting: 93it [01:02,  1.53it/s]Extractor Predicting: 94it [01:02,  1.39it/s]Extractor Predicting: 95it [01:03,  1.42it/s]Extractor Predicting: 96it [01:04,  1.44it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:05,  1.48it/s]Extractor Predicting: 99it [01:06,  1.47it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.52it/s]Extractor Predicting: 103it [01:08,  1.52it/s]Extractor Predicting: 104it [01:09,  1.52it/s]Extractor Predicting: 105it [01:10,  1.52it/s]Extractor Predicting: 106it [01:10,  1.52it/s]Extractor Predicting: 107it [01:11,  1.51it/s]Extractor Predicting: 108it [01:12,  1.53it/s]Extractor Predicting: 109it [01:12,  1.49it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:14,  1.49it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:15,  1.51it/s]Extractor Predicting: 114it [01:16,  1.31it/s]Extractor Predicting: 115it [01:17,  1.36it/s]Extractor Predicting: 116it [01:17,  1.45it/s]Extractor Predicting: 117it [01:18,  1.56it/s]Extractor Predicting: 118it [01:18,  1.62it/s]Extractor Predicting: 119it [01:19,  1.59it/s]Extractor Predicting: 120it [01:20,  1.61it/s]Extractor Predicting: 121it [01:20,  1.60it/s]Extractor Predicting: 122it [01:21,  1.59it/s]Extractor Predicting: 123it [01:22,  1.58it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.58it/s]Extractor Predicting: 126it [01:23,  1.60it/s]Extractor Predicting: 127it [01:24,  1.63it/s]Extractor Predicting: 128it [01:25,  1.67it/s]Extractor Predicting: 129it [01:25,  1.64it/s]Extractor Predicting: 130it [01:26,  1.64it/s]Extractor Predicting: 131it [01:26,  1.67it/s]Extractor Predicting: 132it [01:27,  1.67it/s]Extractor Predicting: 133it [01:28,  1.67it/s]Extractor Predicting: 134it [01:28,  1.54it/s]Extractor Predicting: 135it [01:29,  1.60it/s]Extractor Predicting: 136it [01:29,  1.66it/s]Extractor Predicting: 137it [01:30,  1.68it/s]Extractor Predicting: 138it [01:31,  1.67it/s]Extractor Predicting: 139it [01:32,  1.28it/s]Extractor Predicting: 140it [01:32,  1.39it/s]Extractor Predicting: 141it [01:33,  1.44it/s]Extractor Predicting: 142it [01:34,  1.49it/s]Extractor Predicting: 143it [01:35,  1.22it/s]Extractor Predicting: 144it [01:35,  1.31it/s]Extractor Predicting: 145it [01:36,  1.39it/s]Extractor Predicting: 146it [01:37,  1.47it/s]Extractor Predicting: 147it [01:37,  1.46it/s]Extractor Predicting: 148it [01:38,  1.49it/s]Extractor Predicting: 149it [01:39,  1.58it/s]Extractor Predicting: 150it [01:39,  1.59it/s]Extractor Predicting: 151it [01:40,  1.38it/s]Extractor Predicting: 152it [01:41,  1.47it/s]Extractor Predicting: 153it [01:41,  1.56it/s]Extractor Predicting: 154it [01:42,  1.60it/s]Extractor Predicting: 155it [01:42,  1.63it/s]Extractor Predicting: 156it [01:43,  1.54it/s]Extractor Predicting: 157it [01:44,  1.57it/s]Extractor Predicting: 158it [01:44,  1.61it/s]Extractor Predicting: 159it [01:45,  1.59it/s]Extractor Predicting: 160it [01:46,  1.62it/s]Extractor Predicting: 161it [01:46,  1.53it/s]Extractor Predicting: 162it [01:47,  1.56it/s]Extractor Predicting: 163it [01:48,  1.59it/s]Extractor Predicting: 164it [01:48,  1.60it/s]Extractor Predicting: 165it [01:49,  1.63it/s]Extractor Predicting: 166it [01:50,  1.48it/s]Extractor Predicting: 167it [01:50,  1.53it/s]Extractor Predicting: 168it [01:51,  1.53it/s]Extractor Predicting: 169it [01:51,  1.56it/s]Extractor Predicting: 170it [01:52,  1.55it/s]Extractor Predicting: 171it [01:53,  1.49it/s]Extractor Predicting: 172it [01:53,  1.58it/s]Extractor Predicting: 173it [01:54,  1.54it/s]Extractor Predicting: 174it [01:55,  1.54it/s]Extractor Predicting: 175it [01:55,  1.53it/s]Extractor Predicting: 176it [01:56,  1.41it/s]Extractor Predicting: 177it [01:57,  1.29it/s]Extractor Predicting: 178it [01:58,  1.34it/s]Extractor Predicting: 179it [01:58,  1.40it/s]Extractor Predicting: 180it [01:59,  1.40it/s]Extractor Predicting: 181it [02:00,  1.43it/s]Extractor Predicting: 182it [02:01,  1.45it/s]Extractor Predicting: 183it [02:01,  1.47it/s]Extractor Predicting: 184it [02:02,  1.47it/s]Extractor Predicting: 185it [02:03,  1.41it/s]Extractor Predicting: 186it [02:03,  1.44it/s]Extractor Predicting: 187it [02:04,  1.44it/s]Extractor Predicting: 188it [02:05,  1.50it/s]Extractor Predicting: 189it [02:05,  1.51it/s]Extractor Predicting: 190it [02:06,  1.46it/s]Extractor Predicting: 191it [02:07,  1.45it/s]Extractor Predicting: 192it [02:07,  1.47it/s]Extractor Predicting: 193it [02:08,  1.45it/s]Extractor Predicting: 194it [02:09,  1.46it/s]Extractor Predicting: 195it [02:09,  1.41it/s]Extractor Predicting: 196it [02:10,  1.45it/s]Extractor Predicting: 197it [02:11,  1.45it/s]Extractor Predicting: 198it [02:11,  1.46it/s]Extractor Predicting: 199it [02:12,  1.47it/s]Extractor Predicting: 200it [02:13,  1.47it/s]Extractor Predicting: 201it [02:14,  1.45it/s]Extractor Predicting: 202it [02:14,  1.46it/s]Extractor Predicting: 203it [02:15,  1.49it/s]Extractor Predicting: 204it [02:16,  1.52it/s]Extractor Predicting: 205it [02:16,  1.57it/s]Extractor Predicting: 206it [02:17,  1.50it/s]Extractor Predicting: 207it [02:17,  1.51it/s]Extractor Predicting: 208it [02:18,  1.52it/s]Extractor Predicting: 209it [02:19,  1.52it/s]Extractor Predicting: 210it [02:19,  1.55it/s]Extractor Predicting: 211it [02:20,  1.45it/s]Extractor Predicting: 212it [02:21,  1.49it/s]Extractor Predicting: 213it [02:21,  1.55it/s]Extractor Predicting: 214it [02:22,  1.55it/s]Extractor Predicting: 215it [02:23,  1.52it/s]Extractor Predicting: 216it [02:23,  1.48it/s]Extractor Predicting: 217it [02:24,  1.51it/s]Extractor Predicting: 218it [02:25,  1.56it/s]Extractor Predicting: 219it [02:25,  1.60it/s]Extractor Predicting: 220it [02:26,  1.58it/s]Extractor Predicting: 221it [02:27,  1.55it/s]Extractor Predicting: 222it [02:27,  1.59it/s]Extractor Predicting: 223it [02:28,  1.58it/s]Extractor Predicting: 224it [02:28,  1.59it/s]Extractor Predicting: 225it [02:29,  1.60it/s]Extractor Predicting: 226it [02:30,  1.49it/s]Extractor Predicting: 227it [02:30,  1.50it/s]Extractor Predicting: 228it [02:32,  1.27it/s]Extractor Predicting: 229it [02:32,  1.34it/s]Extractor Predicting: 230it [02:33,  1.39it/s]Extractor Predicting: 231it [02:33,  1.44it/s]Extractor Predicting: 232it [02:34,  1.39it/s]Extractor Predicting: 233it [02:35,  1.49it/s]Extractor Predicting: 234it [02:35,  1.54it/s]Extractor Predicting: 235it [02:36,  1.58it/s]Extractor Predicting: 236it [02:37,  1.64it/s]Extractor Predicting: 237it [02:37,  1.58it/s]Extractor Predicting: 238it [02:38,  1.61it/s]Extractor Predicting: 239it [02:38,  1.64it/s]Extractor Predicting: 240it [02:39,  1.68it/s]Extractor Predicting: 241it [02:40,  1.68it/s]Extractor Predicting: 242it [02:41,  1.45it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:42,  1.59it/s]Extractor Predicting: 245it [02:42,  1.69it/s]Extractor Predicting: 246it [02:43,  1.75it/s]Extractor Predicting: 247it [02:43,  1.65it/s]Extractor Predicting: 248it [02:44,  1.63it/s]Extractor Predicting: 249it [02:45,  1.67it/s]Extractor Predicting: 250it [02:45,  1.69it/s]Extractor Predicting: 251it [02:46,  1.73it/s]Extractor Predicting: 252it [02:46,  1.59it/s]Extractor Predicting: 253it [02:47,  1.63it/s]Extractor Predicting: 254it [02:48,  1.64it/s]Extractor Predicting: 255it [02:48,  1.69it/s]Extractor Predicting: 256it [02:49,  1.71it/s]Extractor Predicting: 257it [02:49,  1.69it/s]Extractor Predicting: 258it [02:50,  1.71it/s]Extractor Predicting: 259it [02:50,  1.74it/s]Extractor Predicting: 260it [02:51,  1.73it/s]Extractor Predicting: 261it [02:52,  1.65it/s]Extractor Predicting: 262it [02:52,  1.66it/s]Extractor Predicting: 263it [02:53,  1.50it/s]Extractor Predicting: 264it [02:54,  1.49it/s]Extractor Predicting: 265it [02:54,  1.50it/s]Extractor Predicting: 266it [02:55,  1.53it/s]Extractor Predicting: 267it [02:56,  1.56it/s]Extractor Predicting: 268it [02:56,  1.49it/s]Extractor Predicting: 269it [02:57,  1.48it/s]Extractor Predicting: 270it [02:58,  1.47it/s]Extractor Predicting: 271it [02:58,  1.47it/s]Extractor Predicting: 272it [02:59,  1.51it/s]Extractor Predicting: 273it [03:00,  1.49it/s]Extractor Predicting: 274it [03:00,  1.49it/s]Extractor Predicting: 275it [03:01,  1.50it/s]Extractor Predicting: 276it [03:02,  1.51it/s]Extractor Predicting: 277it [03:02,  1.50it/s]Extractor Predicting: 278it [03:03,  1.50it/s]Extractor Predicting: 279it [03:04,  1.54it/s]Extractor Predicting: 280it [03:04,  1.52it/s]Extractor Predicting: 281it [03:05,  1.43it/s]Extractor Predicting: 282it [03:06,  1.45it/s]Extractor Predicting: 283it [03:07,  1.47it/s]Extractor Predicting: 284it [03:07,  1.48it/s]Extractor Predicting: 285it [03:08,  1.50it/s]Extractor Predicting: 286it [03:09,  1.42it/s]Extractor Predicting: 287it [03:10,  1.28it/s]Extractor Predicting: 288it [03:10,  1.38it/s]Extractor Predicting: 289it [03:11,  1.43it/s]Extractor Predicting: 290it [03:11,  1.48it/s]Extractor Predicting: 291it [03:12,  1.39it/s]Extractor Predicting: 292it [03:13,  1.44it/s]Extractor Predicting: 293it [03:14,  1.48it/s]Extractor Predicting: 294it [03:14,  1.52it/s]Extractor Predicting: 295it [03:15,  1.54it/s]Extractor Predicting: 296it [03:16,  1.40it/s]Extractor Predicting: 297it [03:16,  1.46it/s]Extractor Predicting: 298it [03:17,  1.53it/s]Extractor Predicting: 299it [03:17,  1.53it/s]Extractor Predicting: 300it [03:18,  1.55it/s]Extractor Predicting: 301it [03:19,  1.50it/s]Extractor Predicting: 302it [03:19,  1.51it/s]Extractor Predicting: 303it [03:20,  1.52it/s]Extractor Predicting: 304it [03:21,  1.53it/s]Extractor Predicting: 305it [03:21,  1.53it/s]Extractor Predicting: 306it [03:22,  1.40it/s]Extractor Predicting: 307it [03:23,  1.45it/s]Extractor Predicting: 308it [03:24,  1.47it/s]Extractor Predicting: 309it [03:24,  1.47it/s]Extractor Predicting: 310it [03:25,  1.51it/s]Extractor Predicting: 311it [03:26,  1.45it/s]Extractor Predicting: 312it [03:26,  1.49it/s]Extractor Predicting: 313it [03:27,  1.51it/s]Extractor Predicting: 314it [03:28,  1.52it/s]Extractor Predicting: 315it [03:28,  1.54it/s]Extractor Predicting: 316it [03:29,  1.46it/s]Extractor Predicting: 317it [03:30,  1.51it/s]Extractor Predicting: 318it [03:30,  1.56it/s]Extractor Predicting: 319it [03:31,  1.57it/s]Extractor Predicting: 320it [03:31,  1.55it/s]Extractor Predicting: 321it [03:32,  1.49it/s]Extractor Predicting: 322it [03:33,  1.40it/s]Extractor Predicting: 323it [03:34,  1.47it/s]Extractor Predicting: 324it [03:34,  1.51it/s]Extractor Predicting: 325it [03:35,  1.52it/s]Extractor Predicting: 326it [03:35,  1.56it/s]Extractor Predicting: 327it [03:36,  1.43it/s]Extractor Predicting: 328it [03:37,  1.47it/s]Extractor Predicting: 329it [03:38,  1.50it/s]Extractor Predicting: 330it [03:38,  1.52it/s]Extractor Predicting: 331it [03:39,  1.53it/s]Extractor Predicting: 332it [03:40,  1.34it/s]Extractor Predicting: 333it [03:40,  1.38it/s]Extractor Predicting: 334it [03:41,  1.46it/s]Extractor Predicting: 335it [03:42,  1.48it/s]Extractor Predicting: 336it [03:42,  1.53it/s]Extractor Predicting: 337it [03:43,  1.46it/s]Extractor Predicting: 338it [03:44,  1.50it/s]Extractor Predicting: 339it [03:44,  1.53it/s]Extractor Predicting: 340it [03:45,  1.53it/s]Extractor Predicting: 341it [03:46,  1.56it/s]Extractor Predicting: 342it [03:46,  1.56it/s]Extractor Predicting: 343it [03:47,  1.57it/s]Extractor Predicting: 344it [03:47,  1.61it/s]Extractor Predicting: 345it [03:48,  1.58it/s]Extractor Predicting: 346it [03:49,  1.57it/s]Extractor Predicting: 347it [03:50,  1.43it/s]Extractor Predicting: 348it [03:50,  1.44it/s]Extractor Predicting: 349it [03:51,  1.49it/s]Extractor Predicting: 350it [03:52,  1.49it/s]Extractor Predicting: 351it [03:52,  1.52it/s]Extractor Predicting: 352it [03:53,  1.49it/s]Extractor Predicting: 353it [03:54,  1.52it/s]Extractor Predicting: 354it [03:54,  1.53it/s]Extractor Predicting: 355it [03:55,  1.53it/s]Extractor Predicting: 356it [03:55,  1.55it/s]Extractor Predicting: 357it [03:56,  1.46it/s]Extractor Predicting: 358it [03:57,  1.50it/s]Extractor Predicting: 359it [03:57,  1.52it/s]Extractor Predicting: 360it [03:58,  1.53it/s]Extractor Predicting: 361it [03:59,  1.54it/s]Extractor Predicting: 362it [03:59,  1.50it/s]Extractor Predicting: 363it [04:00,  1.52it/s]Extractor Predicting: 364it [04:01,  1.57it/s]Extractor Predicting: 365it [04:01,  1.56it/s]Extractor Predicting: 366it [04:02,  1.57it/s]Extractor Predicting: 367it [04:03,  1.52it/s]Extractor Predicting: 368it [04:03,  1.54it/s]Extractor Predicting: 369it [04:04,  1.44it/s]Extractor Predicting: 370it [04:05,  1.47it/s]Extractor Predicting: 371it [04:05,  1.50it/s]Extractor Predicting: 372it [04:06,  1.53it/s]Extractor Predicting: 373it [04:07,  1.54it/s]Extractor Predicting: 374it [04:07,  1.50it/s]Extractor Predicting: 375it [04:08,  1.52it/s]Extractor Predicting: 376it [04:09,  1.52it/s]Extractor Predicting: 377it [04:09,  1.58it/s]Extractor Predicting: 378it [04:10,  1.59it/s]Extractor Predicting: 379it [04:11,  1.43it/s]Extractor Predicting: 380it [04:11,  1.46it/s]Extractor Predicting: 381it [04:12,  1.50it/s]Extractor Predicting: 382it [04:13,  1.52it/s]Extractor Predicting: 383it [04:13,  1.52it/s]Extractor Predicting: 384it [04:14,  1.49it/s]Extractor Predicting: 385it [04:15,  1.52it/s]Extractor Predicting: 386it [04:15,  1.52it/s]Extractor Predicting: 387it [04:16,  1.52it/s]Extractor Predicting: 388it [04:17,  1.48it/s]Extractor Predicting: 389it [04:17,  1.50it/s]Extractor Predicting: 390it [04:18,  1.51it/s]Extractor Predicting: 391it [04:19,  1.54it/s]Extractor Predicting: 392it [04:19,  1.56it/s]Extractor Predicting: 393it [04:20,  1.56it/s]Extractor Predicting: 394it [04:20,  1.54it/s]Extractor Predicting: 395it [04:21,  1.56it/s]Extractor Predicting: 396it [04:22,  1.37it/s]Extractor Predicting: 397it [04:23,  1.42it/s]Extractor Predicting: 398it [04:23,  1.45it/s]Extractor Predicting: 399it [04:24,  1.49it/s]Extractor Predicting: 400it [04:25,  1.49it/s]Extractor Predicting: 401it [04:25,  1.52it/s]Extractor Predicting: 402it [04:26,  1.53it/s]Extractor Predicting: 403it [04:27,  1.52it/s]Extractor Predicting: 404it [04:27,  1.43it/s]Extractor Predicting: 405it [04:28,  1.47it/s]Extractor Predicting: 406it [04:29,  1.51it/s]Extractor Predicting: 407it [04:29,  1.53it/s]Extractor Predicting: 408it [04:30,  1.53it/s]Extractor Predicting: 409it [04:31,  1.56it/s]Extractor Predicting: 410it [04:31,  1.57it/s]Extractor Predicting: 411it [04:32,  1.56it/s]Extractor Predicting: 412it [04:32,  1.59it/s]Extractor Predicting: 413it [04:33,  1.52it/s]Extractor Predicting: 414it [04:34,  1.53it/s]Extractor Predicting: 415it [04:34,  1.52it/s]Extractor Predicting: 416it [04:35,  1.37it/s]Extractor Predicting: 417it [04:36,  1.43it/s]Extractor Predicting: 418it [04:37,  1.49it/s]Extractor Predicting: 419it [04:37,  1.53it/s]Extractor Predicting: 420it [04:38,  1.50it/s]Extractor Predicting: 421it [04:39,  1.51it/s]Extractor Predicting: 422it [04:39,  1.54it/s]Extractor Predicting: 423it [04:40,  1.56it/s]Extractor Predicting: 424it [04:40,  1.57it/s]Extractor Predicting: 425it [04:41,  1.55it/s]Extractor Predicting: 426it [04:42,  1.59it/s]Extractor Predicting: 427it [04:42,  1.58it/s]Extractor Predicting: 428it [04:43,  1.54it/s]Extractor Predicting: 429it [04:43,  1.77it/s]Extractor Predicting: 429it [04:43,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:35,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:35,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:35,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:35,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:35,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:41:36,125 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:41:36,126 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:36,685 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:37,780 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:37,780 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:43,101 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:43,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:43,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:43,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:43,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:41:44,239 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:41:44,281 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:44,891 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:45,064 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:45,064 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.39809296781883197,
  "recall": 0.0649679050768333,
  "score": 0.11170568561872908,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 5it [00:03,  1.53it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:41:52,620 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:52,621 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:41:52,741 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:52,742 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:41:53,076 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:42:23,109 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:42:23,253 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:42:24,239 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:42:24,241 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:42:24,330 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:24,398 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:24,398 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:24,398 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:24,398 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:24,398 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:42:24,398 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.009174311926605505,
  "score": 0.018099547511312215,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:42:25,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:36,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:38,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:21, 16.93s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:42,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:55,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:32, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:56,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:00,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:49<04:43, 16.67s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:14,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:15,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:16,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:25,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:04<04:18, 16.16s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:29,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:31,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:36,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:36,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:38,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:41,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:43,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:46,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:21<04:06, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:46,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:47,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:48,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:48,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:51,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:53,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:59,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:36<03:41, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:01,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:03,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:03,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:09,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:15,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:16,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:18,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:19,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:54<03:37, 16.75s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:20,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:26,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:27,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:27,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:28,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:29,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:30,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:30,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:31,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:32,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:32,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:33,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:33,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:34,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:35,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:10<03:17, 16.44s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:35,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:36,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:37,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:38,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:38,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:39,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:40,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:41,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:42,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:42,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:43,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:44,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:44,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:45,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:46,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:46,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:47,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:48,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:49,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:49,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:50,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:51,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:51,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:27<03:00, 16.45s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:52,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:53,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:53,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:54,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:55,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:56,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:56,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:57,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:58,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:58,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:59,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:59,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:00,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:01,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:02,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:02,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:03,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:04,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:05,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:05,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:06,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:41<02:39, 15.91s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:07,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:07,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:08,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:09,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:09,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:10,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:11,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:12,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:12,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:13,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:14,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:15,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:15,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:16,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:16,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:18,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:18,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:19,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:19,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:20,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:21,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:22,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:23,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:58<02:24, 16.09s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:23,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:24,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:25,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:25,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:26,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:27,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:27,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:28,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:29,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:29,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:30,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:31,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:31,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:32,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:33,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:33,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:34,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:35,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:35,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:36,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:37,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:38,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:38,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:39,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:14<02:09, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:40,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:40,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:41,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:42,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:43,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:43,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:44,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:44,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:45,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:46,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:46,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:47,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:48,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:48,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:49,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:50,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:51,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:52,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:52,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:53,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:54,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:55,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:30<01:52, 16.03s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:55,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:56,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:57,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:58,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:58,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:59,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:00,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:00,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:01,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:02,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:02,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:03,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:04,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:04,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:05,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:06,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:06,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:07,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:08,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:08,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:09,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:10,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:45<01:34, 15.76s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:10,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:11,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:12,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:12,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:13,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:14,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:15,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:16,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:16,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:17,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:18,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:19,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:19,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:20,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:21,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:21,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:22,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:23,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:23,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:24,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:25,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:26,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:27,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:02<01:20, 16.10s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:27,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:28,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:29,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:29,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:30,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:31,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:31,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:32,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:33,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:33,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:34,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:36,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:37,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:37,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:38,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:39,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:39,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:40,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:40,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:41,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:42,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:43,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:18<01:04, 16.09s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:43,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:44,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:45,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:45,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:46,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:47,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:48,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:48,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:49,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:51,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:52,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:53,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:53,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:54,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:55,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:56,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:56,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:57,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:58,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:58,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:59,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:35<00:48, 16.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:47:00,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:01,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:01,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:02,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:03,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:03,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:04,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:05,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:05,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:06,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:07,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:07,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:08,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:08,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:09,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:10,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:10,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:11,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:11,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:12,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:13,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:13,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [04:49<00:31, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 21:47:14,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:14,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:15,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:16,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:17,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:17,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:18,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:19,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:19,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:20,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:20,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:21,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:22,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:22,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:23,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:23,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:24,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:25,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:25,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:26,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:27,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:27,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:28,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:29,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:04<00:15, 15.60s/it][WARNING|generation_utils.py:914] 2023-08-28 21:47:30,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:30,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:31,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:32,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:32,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:33,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:33,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:34,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:35,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:36,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:36,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:37,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:38,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:38,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:39,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:40,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:40,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:41,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:41,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:42,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:43,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:47:44,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:19<00:00, 15.28s/it]Generating: 100%|| 20/20 [05:19<00:00, 15.97s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:58,090 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:58,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:58,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:58,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:58,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:47:59,216 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:47:59,217 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:48:00,489 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:48:01,585 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:48:01,665 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:06,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:06,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:06,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:06,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:48:06,696 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:48:07,442 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:48:07,443 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:48:08,917 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:48:09,091 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:48:09,092 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 544, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8934659090909091, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8206521739130435, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9122023809523809, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : competition class . Context : Following his promotion to the senior class class , he won the French Grand Prix of 1968 and the Belgian Grand Prix of 1969 . Head Entity : French Grand Prix of 1968 , Tail Entity : junior class .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : competition class .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : has part .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : headquarters location . Context : Later in 2008 , the U.S. Department of Justice announced that it would release a criminal investigation into allegations that President Barack Obama had ordered the killings under the guise of an internal investigation . Head Entity : President Barack Obama , Tail Entity : State Department of Justice .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.80078125, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8707386363636364, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : winner . Context : Later in the year , the competition was decided by a four - match tournament that included six wins and eleven losses . Head Entity : seven , Tail Entity : winner .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.8059895833333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : work location .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 15308
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15408, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.61it/s]Extractor Estimating: 2it [00:01,  1.56it/s]Extractor Estimating: 3it [00:01,  1.58it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.64it/s]Extractor Estimating: 7it [00:04,  1.54it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.53it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:10,  1.62it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:11,  1.54it/s]Extractor Estimating: 19it [00:11,  1.56it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:13,  1.56it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:15,  1.62it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.53it/s]Extractor Estimating: 27it [00:17,  1.55it/s]Extractor Estimating: 28it [00:17,  1.57it/s]Extractor Estimating: 29it [00:18,  1.53it/s]Extractor Estimating: 30it [00:19,  1.54it/s]Extractor Estimating: 31it [00:19,  1.50it/s]Extractor Estimating: 32it [00:20,  1.54it/s]Extractor Estimating: 33it [00:20,  1.57it/s]Extractor Estimating: 34it [00:21,  1.47it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:22,  1.61it/s]Extractor Estimating: 37it [00:23,  1.64it/s]Extractor Estimating: 38it [00:24,  1.62it/s]Extractor Estimating: 39it [00:24,  1.58it/s]Extractor Estimating: 40it [00:25,  1.55it/s]Extractor Estimating: 41it [00:26,  1.58it/s]Extractor Estimating: 42it [00:26,  1.53it/s]Extractor Estimating: 43it [00:27,  1.56it/s]Extractor Estimating: 44it [00:27,  1.56it/s]Extractor Estimating: 45it [00:29,  1.31it/s]Extractor Estimating: 46it [00:29,  1.38it/s]Extractor Estimating: 47it [00:30,  1.47it/s]Extractor Estimating: 48it [00:30,  1.48it/s]Extractor Estimating: 49it [00:31,  1.34it/s]Extractor Estimating: 50it [00:32,  1.40it/s]Extractor Estimating: 51it [00:33,  1.44it/s]Extractor Estimating: 52it [00:33,  1.49it/s]Extractor Estimating: 53it [00:34,  1.49it/s]Extractor Estimating: 54it [00:35,  1.46it/s]Extractor Estimating: 55it [00:35,  1.55it/s]Extractor Estimating: 56it [00:36,  1.55it/s]Extractor Estimating: 57it [00:37,  1.46it/s]Extractor Estimating: 58it [00:37,  1.47it/s]Extractor Estimating: 59it [00:38,  1.47it/s]Extractor Estimating: 60it [00:39,  1.51it/s]Extractor Estimating: 61it [00:39,  1.50it/s]Extractor Estimating: 62it [00:40,  1.44it/s]Extractor Estimating: 63it [00:41,  1.45it/s]Extractor Estimating: 64it [00:41,  1.48it/s]Extractor Estimating: 65it [00:42,  1.38it/s]Extractor Estimating: 66it [00:43,  1.40it/s]Extractor Estimating: 67it [00:44,  1.41it/s]Extractor Estimating: 68it [00:44,  1.46it/s]Extractor Estimating: 69it [00:45,  1.50it/s]Extractor Estimating: 70it [00:45,  1.49it/s]Extractor Estimating: 71it [00:46,  1.51it/s]Extractor Estimating: 72it [00:47,  1.29it/s]Extractor Estimating: 73it [00:48,  1.40it/s]Extractor Estimating: 74it [00:48,  1.48it/s]Extractor Estimating: 75it [00:49,  1.50it/s]Extractor Estimating: 76it [00:50,  1.49it/s]Extractor Estimating: 77it [00:50,  1.47it/s]Extractor Estimating: 78it [00:51,  1.46it/s]Extractor Estimating: 79it [00:52,  1.31it/s]Extractor Estimating: 80it [00:53,  1.30it/s]Extractor Estimating: 81it [00:53,  1.36it/s]Extractor Estimating: 82it [00:54,  1.45it/s]Extractor Estimating: 83it [00:55,  1.51it/s]Extractor Estimating: 84it [00:55,  1.51it/s]Extractor Estimating: 85it [00:56,  1.52it/s]Extractor Estimating: 86it [00:57,  1.52it/s]Extractor Estimating: 87it [00:57,  1.50it/s]Extractor Estimating: 88it [00:58,  1.46it/s]Extractor Estimating: 89it [00:59,  1.53it/s]Extractor Estimating: 90it [00:59,  1.51it/s]Extractor Estimating: 91it [01:00,  1.55it/s]Extractor Estimating: 92it [01:01,  1.49it/s]Extractor Estimating: 93it [01:01,  1.50it/s]Extractor Estimating: 94it [01:02,  1.53it/s]Extractor Estimating: 95it [01:03,  1.43it/s]Extractor Estimating: 96it [01:03,  1.44it/s]Extractor Estimating: 97it [01:04,  1.46it/s]Extractor Estimating: 98it [01:05,  1.20it/s]Extractor Estimating: 99it [01:06,  1.28it/s]Extractor Estimating: 100it [01:07,  1.29it/s]Extractor Estimating: 101it [01:07,  1.36it/s]Extractor Estimating: 102it [01:08,  1.37it/s]Extractor Estimating: 103it [01:09,  1.46it/s]Extractor Estimating: 104it [01:09,  1.48it/s]Extractor Estimating: 105it [01:10,  1.63it/s]Extractor Estimating: 106it [01:10,  1.64it/s]Extractor Estimating: 107it [01:11,  1.63it/s]Extractor Estimating: 108it [01:12,  1.63it/s]Extractor Estimating: 109it [01:12,  1.57it/s]Extractor Estimating: 110it [01:13,  1.61it/s]Extractor Estimating: 111it [01:14,  1.52it/s]Extractor Estimating: 112it [01:14,  1.51it/s]Extractor Estimating: 113it [01:15,  1.54it/s]Extractor Estimating: 114it [01:16,  1.53it/s]Extractor Estimating: 115it [01:16,  1.59it/s]Extractor Estimating: 116it [01:17,  1.52it/s]Extractor Estimating: 117it [01:17,  1.61it/s]Extractor Estimating: 118it [01:18,  1.58it/s]Extractor Estimating: 119it [01:19,  1.57it/s]Extractor Estimating: 120it [01:19,  1.60it/s]Extractor Estimating: 121it [01:20,  1.54it/s]Extractor Estimating: 122it [01:21,  1.49it/s]Extractor Estimating: 123it [01:21,  1.57it/s]Extractor Estimating: 124it [01:22,  1.41it/s]Extractor Estimating: 125it [01:23,  1.46it/s]Extractor Estimating: 126it [01:23,  1.50it/s]Extractor Estimating: 127it [01:24,  1.51it/s]Extractor Estimating: 128it [01:25,  1.49it/s]Extractor Estimating: 129it [01:25,  1.46it/s]Extractor Estimating: 130it [01:26,  1.50it/s]Extractor Estimating: 131it [01:27,  1.49it/s]Extractor Estimating: 132it [01:28,  1.38it/s]Extractor Estimating: 133it [01:28,  1.41it/s]Extractor Estimating: 134it [01:29,  1.21it/s]Extractor Estimating: 135it [01:30,  1.32it/s]Extractor Estimating: 136it [01:31,  1.36it/s]Extractor Estimating: 137it [01:31,  1.38it/s]Extractor Estimating: 138it [01:32,  1.39it/s]Extractor Estimating: 139it [01:33,  1.40it/s]Extractor Estimating: 140it [01:33,  1.50it/s]Extractor Estimating: 141it [01:34,  1.44it/s]Extractor Estimating: 142it [01:35,  1.45it/s]Extractor Estimating: 143it [01:35,  1.45it/s]Extractor Estimating: 144it [01:36,  1.47it/s]Extractor Estimating: 145it [01:37,  1.50it/s]Extractor Estimating: 146it [01:37,  1.51it/s]Extractor Estimating: 147it [01:38,  1.47it/s]Extractor Estimating: 148it [01:39,  1.38it/s]Extractor Estimating: 149it [01:40,  1.47it/s]Extractor Estimating: 150it [01:40,  1.45it/s]Extractor Estimating: 151it [01:41,  1.52it/s]Extractor Estimating: 152it [01:42,  1.41it/s]Extractor Estimating: 153it [01:42,  1.46it/s]Extractor Estimating: 154it [01:43,  1.51it/s]Extractor Estimating: 155it [01:43,  1.54it/s]Extractor Estimating: 156it [01:44,  1.55it/s]Extractor Estimating: 157it [01:45,  1.61it/s]Extractor Estimating: 158it [01:45,  1.59it/s]Extractor Estimating: 159it [01:46,  1.50it/s]Extractor Estimating: 160it [01:47,  1.58it/s]Extractor Estimating: 161it [01:47,  1.63it/s]Extractor Estimating: 162it [01:48,  1.58it/s]Extractor Estimating: 163it [01:49,  1.55it/s]Extractor Estimating: 164it [01:49,  1.49it/s]Extractor Estimating: 165it [01:50,  1.56it/s]Extractor Estimating: 166it [01:50,  1.57it/s]Extractor Estimating: 167it [01:51,  1.59it/s]Extractor Estimating: 168it [01:52,  1.50it/s]Extractor Estimating: 169it [01:53,  1.50it/s]Extractor Estimating: 170it [01:53,  1.54it/s]Extractor Estimating: 171it [01:54,  1.56it/s]Extractor Estimating: 172it [01:54,  1.60it/s]Extractor Estimating: 173it [01:55,  1.52it/s]Extractor Estimating: 174it [01:56,  1.53it/s]Extractor Estimating: 175it [01:56,  1.55it/s]Extractor Estimating: 176it [01:57,  1.60it/s]Extractor Estimating: 177it [01:58,  1.55it/s]Extractor Estimating: 178it [01:58,  1.59it/s]Extractor Estimating: 179it [01:59,  1.64it/s]Extractor Estimating: 180it [01:59,  1.68it/s]Extractor Estimating: 181it [02:00,  1.60it/s]Extractor Estimating: 182it [02:01,  1.65it/s]Extractor Estimating: 183it [02:01,  1.71it/s]Extractor Estimating: 184it [02:02,  1.67it/s]Extractor Estimating: 185it [02:02,  1.68it/s]Extractor Estimating: 186it [02:03,  1.63it/s]Extractor Estimating: 187it [02:04,  1.64it/s]Extractor Estimating: 188it [02:04,  1.66it/s]Extractor Estimating: 189it [02:05,  1.61it/s]Extractor Estimating: 190it [02:05,  1.66it/s]Extractor Estimating: 191it [02:06,  1.50it/s]Extractor Estimating: 192it [02:07,  1.52it/s]Extractor Estimating: 193it [02:07,  1.63it/s]Extractor Estimating: 194it [02:08,  1.63it/s]Extractor Estimating: 195it [02:09,  1.64it/s]Extractor Estimating: 196it [02:09,  1.66it/s]Extractor Estimating: 197it [02:10,  1.52it/s]Extractor Estimating: 198it [02:11,  1.52it/s]Extractor Estimating: 199it [02:11,  1.55it/s]Extractor Estimating: 200it [02:12,  1.52it/s]Extractor Estimating: 201it [02:13,  1.33it/s]Extractor Estimating: 202it [02:14,  1.34it/s]Extractor Estimating: 203it [02:14,  1.40it/s]Extractor Estimating: 204it [02:15,  1.42it/s]Extractor Estimating: 205it [02:16,  1.36it/s]Extractor Estimating: 206it [02:16,  1.41it/s]Extractor Estimating: 207it [02:17,  1.41it/s]Extractor Estimating: 208it [02:18,  1.45it/s]Extractor Estimating: 209it [02:18,  1.44it/s]Extractor Estimating: 210it [02:19,  1.46it/s]Extractor Estimating: 211it [02:20,  1.48it/s]Extractor Estimating: 212it [02:20,  1.47it/s]Extractor Estimating: 213it [02:21,  1.30it/s]Extractor Estimating: 214it [02:22,  1.37it/s]Extractor Estimating: 215it [02:23,  1.40it/s]Extractor Estimating: 216it [02:24,  1.38it/s]Extractor Estimating: 217it [02:24,  1.44it/s]Extractor Estimating: 218it [02:25,  1.46it/s]Extractor Estimating: 219it [02:26,  1.37it/s]Extractor Estimating: 220it [02:26,  1.39it/s]Extractor Estimating: 221it [02:27,  1.37it/s]Extractor Estimating: 222it [02:28,  1.41it/s]Extractor Estimating: 223it [02:29,  1.25it/s]Extractor Estimating: 224it [02:29,  1.33it/s]Extractor Estimating: 225it [02:30,  1.42it/s]Extractor Estimating: 226it [02:31,  1.45it/s]Extractor Estimating: 227it [02:31,  1.47it/s]Extractor Estimating: 228it [02:32,  1.40it/s]Extractor Estimating: 229it [02:33,  1.42it/s]Extractor Estimating: 230it [02:33,  1.50it/s]Extractor Estimating: 231it [02:34,  1.48it/s]Extractor Estimating: 232it [02:35,  1.53it/s]Extractor Estimating: 233it [02:35,  1.48it/s]Extractor Estimating: 234it [02:36,  1.52it/s]Extractor Estimating: 235it [02:37,  1.51it/s]Extractor Estimating: 236it [02:37,  1.40it/s]Extractor Estimating: 237it [02:38,  1.44it/s]Extractor Estimating: 238it [02:39,  1.46it/s]Extractor Estimating: 239it [02:40,  1.43it/s]Extractor Estimating: 240it [02:40,  1.44it/s]Extractor Estimating: 241it [02:41,  1.48it/s]Extractor Estimating: 242it [02:41,  1.55it/s]Extractor Estimating: 243it [02:42,  1.48it/s]Extractor Estimating: 244it [02:43,  1.33it/s]Extractor Estimating: 245it [02:44,  1.42it/s]Extractor Estimating: 246it [02:44,  1.45it/s]Extractor Estimating: 247it [02:45,  1.48it/s]Extractor Estimating: 248it [02:46,  1.50it/s]Extractor Estimating: 249it [02:46,  1.49it/s]Extractor Estimating: 250it [02:47,  1.53it/s]Extractor Estimating: 251it [02:48,  1.45it/s]Extractor Estimating: 252it [02:48,  1.42it/s]Extractor Estimating: 253it [02:49,  1.47it/s]Extractor Estimating: 254it [02:50,  1.48it/s]Extractor Estimating: 255it [02:50,  1.45it/s]Extractor Estimating: 256it [02:51,  1.50it/s]Extractor Estimating: 257it [02:52,  1.49it/s]Extractor Estimating: 258it [02:52,  1.50it/s]Extractor Estimating: 259it [02:53,  1.42it/s]Extractor Estimating: 260it [02:54,  1.44it/s]Extractor Estimating: 261it [02:55,  1.43it/s]Extractor Estimating: 262it [02:55,  1.47it/s]Extractor Estimating: 263it [02:56,  1.54it/s]Extractor Estimating: 264it [02:57,  1.50it/s]Extractor Estimating: 265it [02:57,  1.54it/s]Extractor Estimating: 266it [02:58,  1.37it/s]Extractor Estimating: 267it [02:59,  1.40it/s]Extractor Estimating: 268it [02:59,  1.45it/s]Extractor Estimating: 269it [03:00,  1.46it/s]Extractor Estimating: 270it [03:01,  1.49it/s]Extractor Estimating: 271it [03:01,  1.47it/s]Extractor Estimating: 272it [03:02,  1.44it/s]Extractor Estimating: 273it [03:03,  1.42it/s]Extractor Estimating: 274it [03:04,  1.39it/s]Extractor Estimating: 275it [03:04,  1.42it/s]Extractor Estimating: 276it [03:05,  1.44it/s]Extractor Estimating: 277it [03:06,  1.40it/s]Extractor Estimating: 278it [03:06,  1.47it/s]Extractor Estimating: 279it [03:07,  1.49it/s]Extractor Estimating: 280it [03:07,  1.57it/s]Extractor Estimating: 281it [03:08,  1.55it/s]Extractor Estimating: 282it [03:09,  1.53it/s]Extractor Estimating: 283it [03:09,  1.55it/s]Extractor Estimating: 284it [03:10,  1.52it/s]Extractor Estimating: 285it [03:11,  1.43it/s]Extractor Estimating: 286it [03:12,  1.46it/s]Extractor Estimating: 287it [03:12,  1.49it/s]Extractor Estimating: 288it [03:13,  1.52it/s]Extractor Estimating: 289it [03:13,  1.53it/s]Extractor Estimating: 290it [03:14,  1.33it/s]Extractor Estimating: 291it [03:15,  1.42it/s]Extractor Estimating: 292it [03:16,  1.44it/s]Extractor Estimating: 293it [03:17,  1.37it/s]Extractor Estimating: 294it [03:17,  1.43it/s]Extractor Estimating: 295it [03:18,  1.45it/s]Extractor Estimating: 296it [03:18,  1.47it/s]Extractor Estimating: 297it [03:19,  1.47it/s]Extractor Estimating: 298it [03:20,  1.48it/s]Extractor Estimating: 299it [03:21,  1.45it/s]Extractor Estimating: 300it [03:21,  1.49it/s]Extractor Estimating: 301it [03:22,  1.50it/s]Extractor Estimating: 302it [03:23,  1.50it/s]Extractor Estimating: 303it [03:23,  1.59it/s]Extractor Estimating: 304it [03:24,  1.64it/s]Extractor Estimating: 305it [03:24,  1.70it/s]Extractor Estimating: 306it [03:25,  1.48it/s]Extractor Estimating: 307it [03:26,  1.54it/s]Extractor Estimating: 308it [03:26,  1.55it/s]Extractor Estimating: 309it [03:27,  1.53it/s]Extractor Estimating: 310it [03:28,  1.56it/s]Extractor Estimating: 311it [03:28,  1.59it/s]Extractor Estimating: 312it [03:29,  1.60it/s]Extractor Estimating: 313it [03:29,  1.59it/s]Extractor Estimating: 314it [03:30,  1.51it/s]Extractor Estimating: 315it [03:31,  1.59it/s]Extractor Estimating: 316it [03:31,  1.61it/s]Extractor Estimating: 317it [03:32,  1.65it/s]Extractor Estimating: 318it [03:32,  1.64it/s]Extractor Estimating: 319it [03:33,  1.62it/s]Extractor Estimating: 320it [03:34,  1.59it/s]Extractor Estimating: 321it [03:34,  1.61it/s]Extractor Estimating: 322it [03:35,  1.62it/s]Extractor Estimating: 323it [03:36,  1.42it/s]Extractor Estimating: 324it [03:37,  1.45it/s]Extractor Estimating: 325it [03:38,  1.27it/s]Extractor Estimating: 326it [03:38,  1.38it/s]Extractor Estimating: 327it [03:39,  1.46it/s]Extractor Estimating: 328it [03:39,  1.56it/s]Extractor Estimating: 329it [03:40,  1.67it/s]Extractor Estimating: 330it [03:40,  1.58it/s]Extractor Estimating: 331it [03:41,  1.59it/s]Extractor Estimating: 332it [03:42,  1.50it/s]Extractor Estimating: 333it [03:43,  1.48it/s]Extractor Estimating: 334it [03:43,  1.54it/s]Extractor Estimating: 335it [03:44,  1.57it/s]Extractor Estimating: 336it [03:44,  1.59it/s]Extractor Estimating: 337it [03:45,  1.63it/s]Extractor Estimating: 338it [03:46,  1.55it/s]Extractor Estimating: 339it [03:46,  1.56it/s]Extractor Estimating: 340it [03:47,  1.58it/s]Extractor Estimating: 341it [03:48,  1.57it/s]Extractor Estimating: 342it [03:48,  1.55it/s]Extractor Estimating: 343it [03:49,  1.55it/s]Extractor Estimating: 344it [03:50,  1.56it/s]Extractor Estimating: 345it [03:50,  1.57it/s]Extractor Estimating: 346it [03:51,  1.55it/s]Extractor Estimating: 347it [03:51,  1.60it/s]Extractor Estimating: 348it [03:52,  1.65it/s]Extractor Estimating: 349it [03:52,  1.70it/s]Extractor Estimating: 350it [03:53,  1.52it/s]Extractor Estimating: 351it [03:54,  1.48it/s]Extractor Estimating: 352it [03:55,  1.51it/s]Extractor Estimating: 353it [03:55,  1.50it/s]Extractor Estimating: 354it [03:56,  1.37it/s]Extractor Estimating: 355it [03:57,  1.44it/s]Extractor Estimating: 356it [03:58,  1.42it/s]Extractor Estimating: 357it [03:58,  1.41it/s]Extractor Estimating: 358it [03:59,  1.45it/s]Extractor Estimating: 359it [04:00,  1.48it/s]Extractor Estimating: 360it [04:00,  1.47it/s]Extractor Estimating: 361it [04:01,  1.47it/s]Extractor Estimating: 362it [04:02,  1.45it/s]Extractor Estimating: 363it [04:02,  1.44it/s]Extractor Estimating: 364it [04:03,  1.43it/s]Extractor Estimating: 365it [04:04,  1.44it/s]Extractor Estimating: 366it [04:04,  1.48it/s]Extractor Estimating: 367it [04:05,  1.49it/s]Extractor Estimating: 368it [04:06,  1.51it/s]Extractor Estimating: 369it [04:07,  1.39it/s]Extractor Estimating: 370it [04:07,  1.43it/s]Extractor Estimating: 371it [04:08,  1.44it/s]Extractor Estimating: 372it [04:08,  1.48it/s]Extractor Estimating: 373it [04:09,  1.48it/s]Extractor Estimating: 374it [04:10,  1.34it/s]Extractor Estimating: 375it [04:11,  1.40it/s]Extractor Estimating: 376it [04:11,  1.45it/s]Extractor Estimating: 377it [04:12,  1.49it/s]Extractor Estimating: 378it [04:13,  1.55it/s]Extractor Estimating: 379it [04:13,  1.36it/s]Extractor Estimating: 380it [04:14,  1.41it/s]Extractor Estimating: 381it [04:15,  1.44it/s]Extractor Estimating: 382it [04:15,  1.51it/s]Extractor Estimating: 383it [04:16,  1.53it/s]Extractor Estimating: 384it [04:17,  1.57it/s]Extractor Estimating: 385it [04:18,  1.30it/s]Extractor Estimating: 386it [04:18,  1.40it/s]Extractor Estimating: 387it [04:19,  1.42it/s]Extractor Estimating: 388it [04:20,  1.46it/s]Extractor Estimating: 389it [04:20,  1.48it/s]Extractor Estimating: 390it [04:21,  1.47it/s]Extractor Estimating: 391it [04:22,  1.50it/s]Extractor Estimating: 392it [04:22,  1.53it/s]Extractor Estimating: 393it [04:23,  1.57it/s]Extractor Estimating: 394it [04:23,  1.58it/s]Extractor Estimating: 395it [04:24,  1.31it/s]Extractor Estimating: 396it [04:25,  1.34it/s]Extractor Estimating: 397it [04:26,  1.40it/s]Extractor Estimating: 398it [04:26,  1.45it/s]Extractor Estimating: 399it [04:27,  1.33it/s]Extractor Estimating: 400it [04:28,  1.44it/s]Extractor Estimating: 401it [04:29,  1.47it/s]Extractor Estimating: 402it [04:29,  1.55it/s]Extractor Estimating: 403it [04:30,  1.53it/s]Extractor Estimating: 404it [04:31,  1.50it/s]Extractor Estimating: 405it [04:31,  1.52it/s]Extractor Estimating: 406it [04:32,  1.57it/s]Extractor Estimating: 407it [04:32,  1.61it/s]Extractor Estimating: 408it [04:33,  1.59it/s]Extractor Estimating: 409it [04:34,  1.50it/s]Extractor Estimating: 410it [04:34,  1.52it/s]Extractor Estimating: 411it [04:35,  1.54it/s]Extractor Estimating: 412it [04:36,  1.54it/s]Extractor Estimating: 413it [04:36,  1.60it/s]Extractor Estimating: 414it [04:37,  1.44it/s]Extractor Estimating: 415it [04:38,  1.48it/s]Extractor Estimating: 416it [04:38,  1.54it/s]Extractor Estimating: 417it [04:39,  1.56it/s]Extractor Estimating: 418it [04:40,  1.56it/s]Extractor Estimating: 419it [04:40,  1.46it/s]Extractor Estimating: 420it [04:41,  1.49it/s]Extractor Estimating: 421it [04:42,  1.54it/s]Extractor Estimating: 422it [04:42,  1.50it/s]Extractor Estimating: 423it [04:43,  1.53it/s]Extractor Estimating: 424it [04:44,  1.43it/s]Extractor Estimating: 425it [04:44,  1.52it/s]Extractor Estimating: 426it [04:45,  1.36it/s]Extractor Estimating: 427it [04:46,  1.30it/s]Extractor Estimating: 428it [04:47,  1.33it/s]Extractor Estimating: 429it [04:47,  1.39it/s]Extractor Estimating: 430it [04:48,  1.32it/s]Extractor Estimating: 431it [04:49,  1.36it/s]Extractor Estimating: 432it [04:50,  1.37it/s]Extractor Estimating: 433it [04:50,  1.35it/s]Extractor Estimating: 434it [04:51,  1.43it/s]Extractor Estimating: 435it [04:52,  1.43it/s]Extractor Estimating: 436it [04:52,  1.43it/s]Extractor Estimating: 437it [04:53,  1.50it/s]Extractor Estimating: 438it [04:54,  1.41it/s]Extractor Estimating: 439it [04:54,  1.46it/s]Extractor Estimating: 440it [04:55,  1.50it/s]Extractor Estimating: 441it [04:56,  1.47it/s]Extractor Estimating: 442it [04:56,  1.53it/s]Extractor Estimating: 443it [04:57,  1.57it/s]Extractor Estimating: 444it [04:58,  1.56it/s]Extractor Estimating: 445it [04:58,  1.50it/s]Extractor Estimating: 446it [04:59,  1.36it/s]Extractor Estimating: 447it [05:00,  1.42it/s]Extractor Estimating: 448it [05:00,  1.51it/s]Extractor Estimating: 449it [05:01,  1.51it/s]Extractor Estimating: 450it [05:02,  1.54it/s]Extractor Estimating: 451it [05:02,  1.56it/s]Extractor Estimating: 452it [05:03,  1.58it/s]Extractor Estimating: 453it [05:04,  1.46it/s]Extractor Estimating: 454it [05:04,  1.52it/s]Extractor Estimating: 455it [05:05,  1.60it/s]Extractor Estimating: 456it [05:06,  1.46it/s]Extractor Estimating: 457it [05:06,  1.50it/s]Extractor Estimating: 458it [05:07,  1.50it/s]Extractor Estimating: 459it [05:08,  1.59it/s]Extractor Estimating: 460it [05:08,  1.60it/s]Extractor Estimating: 461it [05:09,  1.59it/s]Extractor Estimating: 462it [05:09,  1.60it/s]Extractor Estimating: 463it [05:10,  1.62it/s]Extractor Estimating: 464it [05:11,  1.62it/s]Extractor Estimating: 465it [05:11,  1.64it/s]Extractor Estimating: 466it [05:12,  1.61it/s]Extractor Estimating: 467it [05:12,  1.67it/s]Extractor Estimating: 468it [05:13,  1.64it/s]Extractor Estimating: 469it [05:14,  1.54it/s]Extractor Estimating: 470it [05:14,  1.57it/s]Extractor Estimating: 471it [05:15,  1.58it/s]Extractor Estimating: 472it [05:16,  1.60it/s]Extractor Estimating: 473it [05:16,  1.62it/s]Extractor Estimating: 474it [05:17,  1.61it/s]Extractor Estimating: 475it [05:18,  1.59it/s]Extractor Estimating: 476it [05:18,  1.54it/s]Extractor Estimating: 477it [05:19,  1.29it/s]Extractor Estimating: 478it [05:20,  1.34it/s]Extractor Estimating: 479it [05:21,  1.42it/s]Extractor Estimating: 480it [05:21,  1.30it/s]Extractor Estimating: 481it [05:22,  1.37it/s]Extractor Estimating: 482it [05:23,  1.40it/s]Extractor Estimating: 483it [05:23,  1.48it/s]Extractor Estimating: 484it [05:24,  1.53it/s]Extractor Estimating: 485it [05:25,  1.33it/s]Extractor Estimating: 486it [05:26,  1.39it/s]Extractor Estimating: 487it [05:26,  1.41it/s]Extractor Estimating: 488it [05:27,  1.44it/s]Extractor Estimating: 489it [05:28,  1.46it/s]Extractor Estimating: 490it [05:28,  1.50it/s]Extractor Estimating: 491it [05:29,  1.47it/s]Extractor Estimating: 492it [05:30,  1.41it/s]Extractor Estimating: 493it [05:30,  1.41it/s]Extractor Estimating: 494it [05:31,  1.47it/s]Extractor Estimating: 495it [05:32,  1.44it/s]Extractor Estimating: 496it [05:32,  1.45it/s]Extractor Estimating: 497it [05:33,  1.47it/s]Extractor Estimating: 498it [05:34,  1.46it/s]Extractor Estimating: 499it [05:34,  1.48it/s]Extractor Estimating: 500it [05:35,  1.66it/s]Extractor Estimating: 500it [05:35,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:19,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:19,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:19,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:19,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:19,305 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:54:19,705 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:54:19,706 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:54:20,062 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:54:21,153 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:54:21,153 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:26,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:26,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:26,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:26,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:54:26,599 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:54:27,287 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:54:27,288 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:54:28,262 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:54:28,433 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:54:28,434 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:48:30,080 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:48:30,102 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9984 mean pseudo reward: 0.9314213238226968
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 28455
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28555, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28555, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.018, loss:800.3453
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.996, loss:762.0852
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.010, loss:756.5557
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.003, loss:770.3836
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.004, loss:742.5555
>> valid entity prec:0.5462, rec:0.6543, f1:0.5954
>> valid relation prec:0.0116, rec:0.0034, f1:0.0053
>> valid relation with NER prec:0.0116, rec:0.0034, f1:0.0053
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.319, loss:727.6833
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.997, loss:774.3225
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.016, loss:786.7476
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.001, loss:763.5611
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.009, loss:758.4311
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5745, rec:0.5922, f1:0.5832
>> valid relation prec:0.0623, rec:0.0161, f1:0.0256
>> valid relation with NER prec:0.0623, rec:0.0161, f1:0.0256
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.326, loss:796.1492
g_step 1200, step 368, avg_time 1.006, loss:796.5281
g_step 1300, step 52, avg_time 1.019, loss:771.1646
g_step 1400, step 152, avg_time 0.993, loss:757.5498
g_step 1500, step 252, avg_time 1.007, loss:734.3850
>> valid entity prec:0.5456, rec:0.6388, f1:0.5885
>> valid relation prec:0.0393, rec:0.0101, f1:0.0160
>> valid relation with NER prec:0.0393, rec:0.0101, f1:0.0160
g_step 1600, step 352, avg_time 2.291, loss:749.9282
g_step 1700, step 36, avg_time 1.009, loss:754.9072
g_step 1800, step 136, avg_time 1.012, loss:708.6291
g_step 1900, step 236, avg_time 1.006, loss:719.8903
g_step 2000, step 336, avg_time 1.004, loss:742.7055
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6013, rec:0.5527, f1:0.5760
>> valid relation prec:0.0519, rec:0.0112, f1:0.0184
>> valid relation with NER prec:0.0519, rec:0.0112, f1:0.0184
g_step 2100, step 20, avg_time 2.290, loss:730.2218
g_step 2200, step 120, avg_time 1.003, loss:679.6150
g_step 2300, step 220, avg_time 0.998, loss:686.1625
g_step 2400, step 320, avg_time 1.001, loss:722.3265
g_step 2500, step 4, avg_time 1.011, loss:685.3907
>> valid entity prec:0.5424, rec:0.6485, f1:0.5907
>> valid relation prec:0.0287, rec:0.0069, f1:0.0111
>> valid relation with NER prec:0.0287, rec:0.0069, f1:0.0111
g_step 2600, step 104, avg_time 2.275, loss:649.3529
g_step 2700, step 204, avg_time 1.006, loss:652.4597
g_step 2800, step 304, avg_time 1.007, loss:672.0194
g_step 2900, step 404, avg_time 1.009, loss:696.9718
g_step 3000, step 88, avg_time 1.004, loss:637.5624
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5403, rec:0.5918, f1:0.5649
>> valid relation prec:0.0432, rec:0.0132, f1:0.0202
>> valid relation with NER prec:0.0432, rec:0.0132, f1:0.0202
g_step 3100, step 188, avg_time 2.294, loss:633.5884
g_step 3200, step 288, avg_time 1.008, loss:645.5405
g_step 3300, step 388, avg_time 0.998, loss:636.8426
g_step 3400, step 72, avg_time 1.003, loss:613.2676
g_step 3500, step 172, avg_time 0.997, loss:610.2779
>> valid entity prec:0.5909, rec:0.4997, f1:0.5415
>> valid relation prec:0.0889, rec:0.0187, f1:0.0309
>> valid relation with NER prec:0.0889, rec:0.0187, f1:0.0309
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 272, avg_time 2.284, loss:615.9666
g_step 3700, step 372, avg_time 1.006, loss:624.2230
g_step 3800, step 56, avg_time 1.009, loss:590.0479
g_step 3900, step 156, avg_time 1.006, loss:569.8443
g_step 4000, step 256, avg_time 1.013, loss:605.9373
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5626, rec:0.5879, f1:0.5750
>> valid relation prec:0.0136, rec:0.0037, f1:0.0059
>> valid relation with NER prec:0.0136, rec:0.0037, f1:0.0059
g_step 4100, step 356, avg_time 2.267, loss:615.8485
g_step 4200, step 40, avg_time 1.000, loss:581.0745
g_step 4300, step 140, avg_time 0.996, loss:553.7102
g_step 4400, step 240, avg_time 1.020, loss:573.6426
g_step 4500, step 340, avg_time 1.001, loss:576.2788
>> valid entity prec:0.5310, rec:0.5734, f1:0.5514
>> valid relation prec:0.0605, rec:0.0178, f1:0.0275
>> valid relation with NER prec:0.0605, rec:0.0178, f1:0.0275
g_step 4600, step 24, avg_time 2.276, loss:574.0721
g_step 4700, step 124, avg_time 1.015, loss:527.5149
g_step 4800, step 224, avg_time 1.008, loss:554.2524
g_step 4900, step 324, avg_time 0.994, loss:536.7589
g_step 5000, step 8, avg_time 0.994, loss:591.6400
learning rate was adjusted to 0.0008
>> valid entity prec:0.5756, rec:0.4916, f1:0.5303
>> valid relation prec:0.0511, rec:0.0121, f1:0.0195
>> valid relation with NER prec:0.0511, rec:0.0121, f1:0.0195
g_step 5100, step 108, avg_time 2.273, loss:510.5158
g_step 5200, step 208, avg_time 1.008, loss:538.5777
g_step 5300, step 308, avg_time 1.011, loss:558.9707
g_step 5400, step 408, avg_time 1.010, loss:552.3798
g_step 5500, step 92, avg_time 1.017, loss:498.3775
>> valid entity prec:0.5994, rec:0.5253, f1:0.5599
>> valid relation prec:0.0523, rec:0.0158, f1:0.0243
>> valid relation with NER prec:0.0523, rec:0.0158, f1:0.0243
g_step 5600, step 192, avg_time 2.280, loss:502.6680
g_step 5700, step 292, avg_time 1.003, loss:535.8162
g_step 5800, step 392, avg_time 0.999, loss:555.8388
g_step 5900, step 76, avg_time 1.010, loss:474.4550
g_step 6000, step 176, avg_time 1.006, loss:493.9702
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5415, rec:0.5301, f1:0.5357
>> valid relation prec:0.0479, rec:0.0144, f1:0.0221
>> valid relation with NER prec:0.0479, rec:0.0144, f1:0.0221
g_step 6100, step 276, avg_time 2.280, loss:510.5097
g_step 6200, step 376, avg_time 1.000, loss:514.8354
g_step 6300, step 60, avg_time 1.022, loss:482.8276
g_step 6400, step 160, avg_time 1.004, loss:472.1305
g_step 6500, step 260, avg_time 1.002, loss:475.3501
>> valid entity prec:0.5347, rec:0.6006, f1:0.5658
>> valid relation prec:0.0373, rec:0.0132, f1:0.0195
>> valid relation with NER prec:0.0373, rec:0.0132, f1:0.0195
g_step 6600, step 360, avg_time 2.293, loss:506.6724
g_step 6700, step 44, avg_time 0.994, loss:476.6635
g_step 6800, step 144, avg_time 1.009, loss:455.1176
g_step 6900, step 244, avg_time 1.008, loss:470.7789
g_step 7000, step 344, avg_time 0.996, loss:483.0576
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5539, rec:0.6385, f1:0.5932
>> valid relation prec:0.0613, rec:0.0235, f1:0.0340
>> valid relation with NER prec:0.0613, rec:0.0235, f1:0.0340
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 7100, step 28, avg_time 2.316, loss:462.1240
g_step 7200, step 128, avg_time 1.018, loss:427.4206
g_step 7300, step 228, avg_time 1.014, loss:457.2410
g_step 7400, step 328, avg_time 0.997, loss:463.0125
g_step 7500, step 12, avg_time 1.003, loss:455.7525
>> valid entity prec:0.5449, rec:0.5415, f1:0.5432
>> valid relation prec:0.0593, rec:0.0210, f1:0.0310
>> valid relation with NER prec:0.0593, rec:0.0210, f1:0.0310
g_step 7600, step 112, avg_time 2.281, loss:428.8201
g_step 7700, step 212, avg_time 1.009, loss:453.1522
g_step 7800, step 312, avg_time 1.004, loss:428.5915
g_step 7900, step 412, avg_time 1.002, loss:456.9430
g_step 8000, step 96, avg_time 1.006, loss:402.4704
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5628, rec:0.5727, f1:0.5677
>> valid relation prec:0.0347, rec:0.0109, f1:0.0166
>> valid relation with NER prec:0.0347, rec:0.0109, f1:0.0166
g_step 8100, step 196, avg_time 2.284, loss:422.1930
g_step 8200, step 296, avg_time 1.001, loss:441.6328
g_step 8300, step 396, avg_time 1.002, loss:453.4981
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:48:30 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:48:30 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-48-30_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:48:31 - WARNING - datasets.builder -   Using custom data configuration default-66454784a6ea03ea
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-66454784a6ea03ea/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:48:32,622 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:48:32,624 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:48:32,624 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:48:32,625 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:48:32,685 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:32,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:32,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:32,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:32,711 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:32,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:48:32,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:48:32,972 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:48:36,211 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:48:36,270 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-66454784a6ea03ea/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:04,  2.46ba/s] 18%|        | 2/11 [00:00<00:02,  3.43ba/s] 27%|       | 3/11 [00:00<00:02,  3.87ba/s] 36%|      | 4/11 [00:01<00:01,  4.13ba/s] 45%|     | 5/11 [00:01<00:01,  4.29ba/s] 55%|    | 6/11 [00:01<00:01,  4.39ba/s] 64%|   | 7/11 [00:01<00:00,  4.44ba/s] 73%|  | 8/11 [00:01<00:00,  4.48ba/s] 82%| | 9/11 [00:02<00:00,  4.51ba/s] 91%| | 10/11 [00:02<00:00,  3.78ba/s]100%|| 11/11 [00:02<00:00,  4.37ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.58ba/s] 50%|     | 2/4 [00:00<00:00,  3.39ba/s] 75%|  | 3/4 [00:00<00:00,  3.80ba/s]100%|| 4/4 [00:00<00:00,  4.90ba/s]100%|| 4/4 [00:00<00:00,  4.18ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  3.66ba/s] 27%|       | 3/11 [00:00<00:01,  7.33ba/s] 45%|     | 5/11 [00:00<00:00,  8.95ba/s] 64%|   | 7/11 [00:00<00:00,  9.75ba/s] 82%| | 9/11 [00:00<00:00, 10.27ba/s]100%|| 11/11 [00:01<00:00, 10.23ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.59ba/s] 50%|     | 2/4 [00:00<00:00,  5.62ba/s]100%|| 4/4 [00:00<00:00,  9.26ba/s]100%|| 4/4 [00:00<00:00,  7.70ba/s]
[INFO|trainer.py:414] 2023-08-29 00:48:45,270 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:48:45,280 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:48:45,280 >>   Num examples = 10011
[INFO|trainer.py:1149] 2023-08-29 00:48:45,280 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:48:45,280 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:48:45,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:48:45,280 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:48:45,280 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:01<18:38,  1.44s/it]  0%|          | 2/780 [00:02<15:36,  1.20s/it]  0%|          | 3/780 [00:03<12:26,  1.04it/s]  1%|          | 4/780 [00:03<10:00,  1.29it/s]  1%|          | 5/780 [00:04<08:55,  1.45it/s]  1%|          | 6/780 [00:04<07:10,  1.80it/s]  1%|          | 7/780 [00:04<06:25,  2.01it/s]  1%|          | 8/780 [00:05<05:47,  2.22it/s]  1%|          | 9/780 [00:05<05:23,  2.39it/s]  1%|         | 10/780 [00:05<05:21,  2.39it/s]  1%|         | 11/780 [00:06<04:52,  2.63it/s]  2%|         | 12/780 [00:06<04:31,  2.83it/s]  2%|         | 13/780 [00:06<04:44,  2.69it/s]  2%|         | 14/780 [00:07<04:35,  2.78it/s]  2%|         | 15/780 [00:07<04:20,  2.94it/s]  2%|         | 16/780 [00:07<04:09,  3.07it/s]  2%|         | 17/780 [00:08<04:01,  3.16it/s]  2%|         | 18/780 [00:08<03:55,  3.24it/s]  2%|         | 19/780 [00:08<03:50,  3.30it/s]  3%|         | 20/780 [00:09<03:52,  3.27it/s]  3%|         | 21/780 [00:09<03:48,  3.32it/s]  3%|         | 22/780 [00:09<03:45,  3.36it/s]  3%|         | 23/780 [00:09<03:43,  3.39it/s]  3%|         | 24/780 [00:10<03:41,  3.41it/s]  3%|         | 25/780 [00:10<03:40,  3.42it/s]  3%|         | 26/780 [00:10<03:39,  3.43it/s]  3%|         | 27/780 [00:11<03:39,  3.44it/s]  4%|         | 28/780 [00:11<03:38,  3.44it/s]  4%|         | 29/780 [00:11<03:38,  3.44it/s]  4%|         | 30/780 [00:11<03:37,  3.45it/s]  4%|         | 31/780 [00:12<03:41,  3.38it/s]  4%|         | 32/780 [00:12<03:39,  3.40it/s]  4%|         | 33/780 [00:12<03:38,  3.42it/s]  4%|         | 34/780 [00:13<03:37,  3.43it/s]  4%|         | 35/780 [00:13<03:36,  3.43it/s]  5%|         | 36/780 [00:13<03:36,  3.44it/s]  5%|         | 37/780 [00:14<03:35,  3.44it/s]  5%|         | 38/780 [00:14<03:35,  3.45it/s]  5%|         | 39/780 [00:14<03:34,  3.45it/s]  5%|         | 40/780 [00:14<03:34,  3.45it/s]  5%|         | 41/780 [00:15<03:34,  3.45it/s]  5%|         | 42/780 [00:15<03:45,  3.28it/s]  6%|         | 43/780 [00:15<03:41,  3.33it/s]  6%|         | 44/780 [00:16<03:38,  3.37it/s]  6%|         | 45/780 [00:16<03:36,  3.39it/s]  6%|         | 46/780 [00:16<03:35,  3.41it/s]  6%|         | 47/780 [00:16<03:34,  3.42it/s]  6%|         | 48/780 [00:17<03:33,  3.43it/s]  6%|         | 49/780 [00:17<03:32,  3.44it/s]  6%|         | 50/780 [00:17<03:32,  3.44it/s]  7%|         | 51/780 [00:18<03:31,  3.44it/s]  7%|         | 52/780 [00:18<03:43,  3.26it/s]  7%|         | 53/780 [00:18<03:39,  3.31it/s]  7%|         | 54/780 [00:19<04:54,  2.47it/s]  7%|         | 55/780 [00:19<04:28,  2.70it/s]  7%|         | 56/780 [00:19<04:10,  2.89it/s]  7%|         | 57/780 [00:20<03:58,  3.04it/s]  7%|         | 58/780 [00:20<03:49,  3.15it/s]  8%|         | 59/780 [00:20<03:48,  3.15it/s]  8%|         | 60/780 [00:21<03:42,  3.23it/s]  8%|         | 61/780 [00:21<03:37,  3.30it/s]  8%|         | 62/780 [00:21<03:34,  3.34it/s]  8%|         | 63/780 [00:22<03:32,  3.37it/s]  8%|         | 64/780 [00:22<03:30,  3.40it/s]  8%|         | 65/780 [00:22<03:29,  3.41it/s]  8%|         | 66/780 [00:22<03:28,  3.42it/s]  9%|         | 67/780 [00:23<03:27,  3.43it/s]  9%|         | 68/780 [00:23<03:26,  3.44it/s]  9%|         | 69/780 [00:23<03:26,  3.44it/s]  9%|         | 70/780 [00:24<03:34,  3.31it/s]  9%|         | 71/780 [00:24<03:31,  3.35it/s]  9%|         | 72/780 [00:24<03:29,  3.38it/s]  9%|         | 73/780 [00:24<03:28,  3.40it/s]  9%|         | 74/780 [00:25<03:26,  3.42it/s] 10%|         | 75/780 [00:25<03:25,  3.42it/s] 10%|         | 76/780 [00:25<03:25,  3.43it/s] 10%|         | 77/780 [00:26<03:24,  3.44it/s] 10%|         | 78/780 [00:26<03:24,  3.44it/s] 10%|         | 79/780 [00:26<03:23,  3.44it/s] 10%|         | 80/780 [00:27<03:23,  3.44it/s] 10%|         | 81/780 [00:27<03:28,  3.35it/s] 11%|         | 82/780 [00:27<03:26,  3.38it/s] 11%|         | 83/780 [00:27<03:24,  3.40it/s] 11%|         | 84/780 [00:28<03:23,  3.41it/s] 11%|         | 85/780 [00:28<03:22,  3.43it/s] 11%|         | 86/780 [00:28<03:22,  3.43it/s] 11%|         | 87/780 [00:29<03:21,  3.44it/s] 11%|        | 88/780 [00:29<03:21,  3.44it/s] 11%|        | 89/780 [00:29<03:20,  3.44it/s] 12%|        | 90/780 [00:29<03:20,  3.44it/s] 12%|        | 91/780 [00:30<03:20,  3.44it/s] 12%|        | 92/780 [00:30<03:29,  3.29it/s] 12%|        | 93/780 [00:30<03:25,  3.34it/s] 12%|        | 94/780 [00:31<03:23,  3.37it/s] 12%|        | 95/780 [00:31<03:21,  3.39it/s] 12%|        | 96/780 [00:31<03:20,  3.41it/s] 12%|        | 97/780 [00:32<03:19,  3.42it/s] 13%|        | 98/780 [00:32<03:18,  3.43it/s] 13%|        | 99/780 [00:32<03:18,  3.43it/s] 13%|        | 100/780 [00:32<03:17,  3.44it/s] 13%|        | 101/780 [00:33<03:17,  3.44it/s] 13%|        | 102/780 [00:33<03:16,  3.44it/s] 13%|        | 103/780 [00:33<03:25,  3.29it/s] 13%|        | 104/780 [00:34<03:22,  3.33it/s] 13%|        | 105/780 [00:34<03:20,  3.36it/s] 14%|        | 106/780 [00:34<03:19,  3.38it/s] 14%|        | 107/780 [00:34<03:17,  3.41it/s] 14%|        | 108/780 [00:35<03:16,  3.42it/s] 14%|        | 109/780 [00:35<03:15,  3.43it/s] 14%|        | 110/780 [00:35<03:15,  3.43it/s] 14%|        | 111/780 [00:36<03:14,  3.44it/s] 14%|        | 112/780 [00:36<03:14,  3.44it/s] 14%|        | 113/780 [00:36<03:13,  3.44it/s] 15%|        | 114/780 [00:37<03:20,  3.33it/s] 15%|        | 115/780 [00:37<03:17,  3.36it/s] 15%|        | 116/780 [00:37<03:16,  3.39it/s] 15%|        | 117/780 [00:37<03:14,  3.40it/s] 15%|        | 118/780 [00:38<03:13,  3.42it/s] 15%|        | 119/780 [00:38<03:12,  3.43it/s] 15%|        | 120/780 [00:38<03:12,  3.44it/s] 16%|        | 121/780 [00:39<03:11,  3.44it/s] 16%|        | 122/780 [00:39<03:11,  3.44it/s] 16%|        | 123/780 [00:39<03:10,  3.44it/s] 16%|        | 124/780 [00:39<03:10,  3.45it/s] 16%|        | 125/780 [00:40<03:13,  3.39it/s] 16%|        | 126/780 [00:40<03:12,  3.41it/s] 16%|        | 127/780 [00:40<03:11,  3.42it/s] 16%|        | 128/780 [00:41<03:10,  3.43it/s] 17%|        | 129/780 [00:41<03:09,  3.43it/s] 17%|        | 130/780 [00:41<03:09,  3.44it/s] 17%|        | 131/780 [00:41<03:08,  3.44it/s] 17%|        | 132/780 [00:42<03:08,  3.44it/s] 17%|        | 133/780 [00:42<03:08,  3.44it/s] 17%|        | 134/780 [00:42<03:07,  3.44it/s] 17%|        | 135/780 [00:43<03:07,  3.44it/s] 17%|        | 136/780 [00:43<03:10,  3.38it/s] 18%|        | 137/780 [00:43<03:09,  3.40it/s] 18%|        | 138/780 [00:44<03:08,  3.41it/s] 18%|        | 139/780 [00:44<03:07,  3.42it/s] 18%|        | 140/780 [00:44<03:07,  3.42it/s] 18%|        | 141/780 [00:44<03:06,  3.43it/s] 18%|        | 142/780 [00:45<03:05,  3.44it/s] 18%|        | 143/780 [00:45<03:05,  3.44it/s] 18%|        | 144/780 [00:45<03:04,  3.44it/s] 19%|        | 145/780 [00:46<03:04,  3.44it/s] 19%|        | 146/780 [00:46<03:04,  3.44it/s] 19%|        | 147/780 [00:46<03:11,  3.30it/s] 19%|        | 148/780 [00:46<03:09,  3.34it/s] 19%|        | 149/780 [00:47<03:07,  3.37it/s] 19%|        | 150/780 [00:47<03:05,  3.39it/s] 19%|        | 151/780 [00:47<03:04,  3.41it/s] 19%|        | 152/780 [00:48<03:03,  3.41it/s] 20%|        | 153/780 [00:48<03:03,  3.42it/s] 20%|        | 154/780 [00:48<03:02,  3.43it/s] 20%|        | 155/780 [00:49<03:02,  3.43it/s] 20%|        | 156/780 [00:49<03:01,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:49:34,696 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:49:34,696 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:49:34,696 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.19it/s][A
  3%|         | 12/436 [00:00<00:08, 48.11it/s][A
  4%|         | 17/436 [00:00<00:09, 46.40it/s][A
  5%|         | 22/436 [00:00<00:09, 45.78it/s][A
  6%|         | 27/436 [00:00<00:09, 45.19it/s][A
  7%|         | 32/436 [00:00<00:08, 45.01it/s][A
  8%|         | 37/436 [00:00<00:08, 44.94it/s][A
 10%|         | 42/436 [00:00<00:08, 44.70it/s][A
 11%|         | 47/436 [00:01<00:08, 44.82it/s][A
 12%|        | 52/436 [00:01<00:08, 44.86it/s][A
 13%|        | 57/436 [00:01<00:08, 44.83it/s][A
 14%|        | 62/436 [00:01<00:08, 44.79it/s][A
 15%|        | 67/436 [00:01<00:08, 44.72it/s][A
 17%|        | 72/436 [00:01<00:08, 44.63it/s][A
 18%|        | 77/436 [00:01<00:08, 44.64it/s][A
 19%|        | 82/436 [00:01<00:07, 44.53it/s][A
 20%|        | 87/436 [00:01<00:07, 44.73it/s][A
 21%|        | 92/436 [00:02<00:08, 40.85it/s][A
 22%|       | 97/436 [00:02<00:08, 42.12it/s][A
 23%|       | 102/436 [00:02<00:07, 42.92it/s][A
 25%|       | 107/436 [00:02<00:07, 43.59it/s][A
 26%|       | 112/436 [00:02<00:07, 44.00it/s][A
 27%|       | 117/436 [00:02<00:07, 44.28it/s][A
 28%|       | 122/436 [00:02<00:07, 44.47it/s][A
 29%|       | 127/436 [00:02<00:06, 44.50it/s][A
 30%|       | 132/436 [00:02<00:06, 44.15it/s][A
 31%|      | 137/436 [00:03<00:06, 44.18it/s][A
 33%|      | 142/436 [00:03<00:06, 44.39it/s][A
 34%|      | 147/436 [00:03<00:06, 44.64it/s][A
 35%|      | 152/436 [00:03<00:06, 44.83it/s][A
 36%|      | 157/436 [00:03<00:06, 44.94it/s][A
 37%|      | 162/436 [00:03<00:06, 44.80it/s][A
 38%|      | 167/436 [00:03<00:05, 44.87it/s][A
 39%|      | 172/436 [00:03<00:05, 44.55it/s][A
 41%|      | 177/436 [00:03<00:05, 44.44it/s][A
 42%|     | 182/436 [00:04<00:05, 44.44it/s][A
 43%|     | 187/436 [00:04<00:05, 44.40it/s][A
 44%|     | 192/436 [00:04<00:05, 44.63it/s][A
 45%|     | 197/436 [00:04<00:05, 44.78it/s][A
 46%|     | 202/436 [00:04<00:05, 44.76it/s][A
 47%|     | 207/436 [00:04<00:05, 44.87it/s][A
 49%|     | 212/436 [00:04<00:05, 44.77it/s][A
 50%|     | 217/436 [00:04<00:04, 44.60it/s][A
 51%|     | 222/436 [00:05<00:04, 44.56it/s][A
 52%|    | 227/436 [00:05<00:05, 40.40it/s][A
 53%|    | 232/436 [00:05<00:04, 41.85it/s][A
 54%|    | 237/436 [00:05<00:04, 42.82it/s][A
 56%|    | 242/436 [00:05<00:04, 43.55it/s][A
 57%|    | 247/436 [00:05<00:04, 44.03it/s][A
 58%|    | 252/436 [00:05<00:04, 44.30it/s][A
 59%|    | 257/436 [00:05<00:04, 44.28it/s][A
 60%|    | 262/436 [00:05<00:03, 44.22it/s][A
 61%|    | 267/436 [00:06<00:03, 43.93it/s][A
 62%|   | 272/436 [00:06<00:03, 43.97it/s][A
 64%|   | 277/436 [00:06<00:03, 44.22it/s][A
 65%|   | 282/436 [00:06<00:03, 44.53it/s][A
 66%|   | 287/436 [00:06<00:03, 44.69it/s][A
 67%|   | 292/436 [00:06<00:03, 44.83it/s][A
 68%|   | 297/436 [00:06<00:03, 44.93it/s][A
 69%|   | 302/436 [00:06<00:02, 44.89it/s][A
 70%|   | 307/436 [00:06<00:02, 44.52it/s][A
 72%|  | 312/436 [00:07<00:02, 44.26it/s][A
 73%|  | 317/436 [00:07<00:02, 44.19it/s][A
 74%|  | 322/436 [00:07<00:02, 44.31it/s][A
 75%|  | 327/436 [00:07<00:02, 44.55it/s][A
 76%|  | 332/436 [00:07<00:02, 44.66it/s][A
 77%|  | 337/436 [00:07<00:02, 44.86it/s][A
 78%|  | 342/436 [00:07<00:02, 45.03it/s][A
 80%|  | 347/436 [00:07<00:01, 44.73it/s][A
 81%|  | 352/436 [00:07<00:01, 44.61it/s][A
 82%| | 357/436 [00:08<00:01, 44.44it/s][A
 83%| | 362/436 [00:08<00:01, 41.64it/s][A
 84%| | 367/436 [00:08<00:01, 42.67it/s][A
 85%| | 372/436 [00:08<00:01, 43.44it/s][A
 86%| | 377/436 [00:08<00:01, 43.86it/s][A
 88%| | 382/436 [00:08<00:01, 44.34it/s][A
 89%| | 387/436 [00:08<00:01, 44.49it/s][A
 90%| | 392/436 [00:08<00:00, 44.43it/s][A
 91%| | 397/436 [00:08<00:00, 44.35it/s][A
 92%|| 402/436 [00:09<00:00, 43.97it/s][A
 93%|| 407/436 [00:09<00:00, 44.15it/s][A
 94%|| 412/436 [00:09<00:00, 44.43it/s][A
 96%|| 417/436 [00:09<00:00, 44.53it/s][A
 97%|| 422/436 [00:09<00:00, 44.80it/s][A
 98%|| 427/436 [00:09<00:00, 44.88it/s][A
 99%|| 432/436 [00:09<00:00, 45.00it/s][A                                                 
                                                 [A 20%|        | 156/780 [00:59<03:01,  3.43it/s]
100%|| 436/436 [00:09<00:00, 45.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:49:44,921 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 00:49:45,287 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:49:48,289 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:49:48,417 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:49:48,463 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:09<1:05:51,  6.34s/it] 20%|        | 158/780 [01:10<46:59,  4.53s/it]   20%|        | 159/780 [01:10<33:45,  3.26s/it] 21%|        | 160/780 [01:10<24:30,  2.37s/it] 21%|        | 161/780 [01:10<18:01,  1.75s/it] 21%|        | 162/780 [01:11<13:30,  1.31s/it] 21%|        | 163/780 [01:11<10:20,  1.01s/it] 21%|        | 164/780 [01:11<08:08,  1.26it/s] 21%|        | 165/780 [01:12<06:35,  1.56it/s] 21%|       | 166/780 [01:12<05:30,  1.86it/s] 21%|       | 167/780 [01:12<04:44,  2.15it/s] 22%|       | 168/780 [01:13<04:13,  2.42it/s] 22%|       | 169/780 [01:13<03:54,  2.60it/s] 22%|       | 170/780 [01:13<03:37,  2.80it/s] 22%|       | 171/780 [01:13<03:25,  2.96it/s] 22%|       | 172/780 [01:14<03:17,  3.08it/s] 22%|       | 173/780 [01:14<03:11,  3.17it/s] 22%|       | 174/780 [01:14<03:07,  3.24it/s] 22%|       | 175/780 [01:15<03:04,  3.28it/s] 23%|       | 176/780 [01:15<03:02,  3.32it/s] 23%|       | 177/780 [01:15<03:00,  3.34it/s] 23%|       | 178/780 [01:15<02:59,  3.36it/s] 23%|       | 179/780 [01:16<02:58,  3.37it/s] 23%|       | 180/780 [01:16<03:04,  3.26it/s] 23%|       | 181/780 [01:16<03:01,  3.30it/s] 23%|       | 182/780 [01:17<02:59,  3.33it/s] 23%|       | 183/780 [01:17<02:58,  3.35it/s] 24%|       | 184/780 [01:17<02:57,  3.37it/s] 24%|       | 185/780 [01:18<02:56,  3.37it/s] 24%|       | 186/780 [01:18<02:55,  3.38it/s] 24%|       | 187/780 [01:18<02:54,  3.39it/s] 24%|       | 188/780 [01:19<03:10,  3.10it/s] 24%|       | 189/780 [01:19<03:15,  3.02it/s] 24%|       | 190/780 [01:19<03:08,  3.13it/s] 24%|       | 191/780 [01:19<03:03,  3.21it/s] 25%|       | 192/780 [01:20<03:00,  3.27it/s] 25%|       | 193/780 [01:20<02:57,  3.30it/s] 25%|       | 194/780 [01:20<02:56,  3.33it/s] 25%|       | 195/780 [01:21<02:54,  3.35it/s] 25%|       | 196/780 [01:21<02:53,  3.37it/s] 25%|       | 197/780 [01:21<02:52,  3.38it/s] 25%|       | 198/780 [01:22<02:51,  3.38it/s] 26%|       | 199/780 [01:22<02:55,  3.31it/s] 26%|       | 200/780 [01:22<02:53,  3.34it/s] 26%|       | 201/780 [01:22<02:52,  3.36it/s] 26%|       | 202/780 [01:23<02:51,  3.37it/s] 26%|       | 203/780 [01:23<02:50,  3.38it/s] 26%|       | 204/780 [01:23<02:50,  3.39it/s] 26%|       | 205/780 [01:24<02:49,  3.40it/s] 26%|       | 206/780 [01:24<02:48,  3.41it/s] 27%|       | 207/780 [01:24<02:47,  3.42it/s] 27%|       | 208/780 [01:24<02:46,  3.43it/s] 27%|       | 209/780 [01:25<02:45,  3.44it/s] 27%|       | 210/780 [01:25<02:50,  3.34it/s] 27%|       | 211/780 [01:25<02:48,  3.37it/s] 27%|       | 212/780 [01:26<02:47,  3.39it/s] 27%|       | 213/780 [01:26<02:46,  3.41it/s] 27%|       | 214/780 [01:26<02:45,  3.42it/s] 28%|       | 215/780 [01:27<02:44,  3.43it/s] 28%|       | 216/780 [01:27<02:44,  3.43it/s] 28%|       | 217/780 [01:27<02:43,  3.44it/s] 28%|       | 218/780 [01:27<02:43,  3.44it/s] 28%|       | 219/780 [01:28<02:42,  3.45it/s] 28%|       | 220/780 [01:28<02:42,  3.45it/s] 28%|       | 221/780 [01:28<02:48,  3.33it/s] 28%|       | 222/780 [01:29<02:45,  3.36it/s] 29%|       | 223/780 [01:29<02:44,  3.39it/s] 29%|       | 224/780 [01:29<02:43,  3.41it/s] 29%|       | 225/780 [01:29<02:42,  3.42it/s] 29%|       | 226/780 [01:30<02:41,  3.43it/s] 29%|       | 227/780 [01:30<02:41,  3.43it/s] 29%|       | 228/780 [01:30<02:40,  3.44it/s] 29%|       | 229/780 [01:31<02:40,  3.44it/s] 29%|       | 230/780 [01:31<02:39,  3.45it/s] 30%|       | 231/780 [01:31<02:39,  3.45it/s] 30%|       | 232/780 [01:32<02:42,  3.38it/s] 30%|       | 233/780 [01:32<02:41,  3.40it/s] 30%|       | 234/780 [01:32<02:39,  3.41it/s] 30%|       | 235/780 [01:32<02:39,  3.42it/s] 30%|       | 236/780 [01:33<02:38,  3.43it/s] 30%|       | 237/780 [01:33<02:38,  3.43it/s] 31%|       | 238/780 [01:33<02:37,  3.44it/s] 31%|       | 239/780 [01:34<02:37,  3.44it/s] 31%|       | 240/780 [01:34<02:36,  3.44it/s] 31%|       | 241/780 [01:34<02:36,  3.44it/s] 31%|       | 242/780 [01:34<02:36,  3.45it/s] 31%|       | 243/780 [01:35<02:42,  3.30it/s] 31%|      | 244/780 [01:35<02:40,  3.35it/s] 31%|      | 245/780 [01:35<02:38,  3.38it/s] 32%|      | 246/780 [01:36<02:37,  3.40it/s] 32%|      | 247/780 [01:36<02:36,  3.41it/s] 32%|      | 248/780 [01:36<02:35,  3.43it/s] 32%|      | 249/780 [01:36<02:34,  3.43it/s] 32%|      | 250/780 [01:37<02:34,  3.44it/s] 32%|      | 251/780 [01:37<02:33,  3.44it/s] 32%|      | 252/780 [01:37<02:33,  3.45it/s] 32%|      | 253/780 [01:38<02:33,  3.44it/s] 33%|      | 254/780 [01:38<02:37,  3.33it/s] 33%|      | 255/780 [01:38<02:36,  3.36it/s] 33%|      | 256/780 [01:39<02:34,  3.39it/s] 33%|      | 257/780 [01:39<02:33,  3.40it/s] 33%|      | 258/780 [01:39<02:32,  3.42it/s] 33%|      | 259/780 [01:39<02:32,  3.42it/s] 33%|      | 260/780 [01:40<02:31,  3.43it/s] 33%|      | 261/780 [01:40<02:30,  3.44it/s] 34%|      | 262/780 [01:40<02:30,  3.44it/s] 34%|      | 263/780 [01:41<02:30,  3.44it/s] 34%|      | 264/780 [01:41<02:29,  3.45it/s] 34%|      | 265/780 [01:41<02:36,  3.28it/s] 34%|      | 266/780 [01:42<02:34,  3.33it/s] 34%|      | 267/780 [01:42<02:32,  3.37it/s] 34%|      | 268/780 [01:42<02:31,  3.39it/s] 34%|      | 269/780 [01:42<02:29,  3.41it/s] 35%|      | 270/780 [01:43<02:29,  3.42it/s] 35%|      | 271/780 [01:43<02:28,  3.43it/s] 35%|      | 272/780 [01:43<02:27,  3.43it/s] 35%|      | 273/780 [01:44<02:27,  3.44it/s] 35%|      | 274/780 [01:44<02:27,  3.44it/s] 35%|      | 275/780 [01:44<02:26,  3.44it/s] 35%|      | 276/780 [01:44<02:30,  3.34it/s] 36%|      | 277/780 [01:45<02:29,  3.37it/s] 36%|      | 278/780 [01:45<02:27,  3.40it/s] 36%|      | 279/780 [01:45<02:26,  3.41it/s] 36%|      | 280/780 [01:46<02:26,  3.42it/s] 36%|      | 281/780 [01:46<02:25,  3.43it/s] 36%|      | 282/780 [01:46<02:24,  3.44it/s] 36%|      | 283/780 [01:46<02:24,  3.44it/s] 36%|      | 284/780 [01:47<02:24,  3.44it/s] 37%|      | 285/780 [01:47<02:23,  3.44it/s] 37%|      | 286/780 [01:47<02:23,  3.44it/s] 37%|      | 287/780 [01:48<02:27,  3.33it/s] 37%|      | 288/780 [01:48<02:26,  3.37it/s] 37%|      | 289/780 [01:48<02:24,  3.39it/s] 37%|      | 290/780 [01:49<02:23,  3.41it/s] 37%|      | 291/780 [01:49<02:22,  3.42it/s] 37%|      | 292/780 [01:49<02:22,  3.43it/s] 38%|      | 293/780 [01:49<02:21,  3.43it/s] 38%|      | 294/780 [01:50<02:21,  3.44it/s] 38%|      | 295/780 [01:50<02:20,  3.44it/s] 38%|      | 296/780 [01:50<02:20,  3.44it/s] 38%|      | 297/780 [01:51<02:20,  3.45it/s] 38%|      | 298/780 [01:51<02:19,  3.44it/s] 38%|      | 299/780 [01:51<02:19,  3.44it/s] 38%|      | 300/780 [01:51<02:19,  3.44it/s] 39%|      | 301/780 [01:52<02:19,  3.44it/s] 39%|      | 302/780 [01:52<02:18,  3.44it/s] 39%|      | 303/780 [01:52<02:18,  3.44it/s] 39%|      | 304/780 [01:53<02:22,  3.34it/s] 39%|      | 305/780 [01:53<02:20,  3.37it/s] 39%|      | 306/780 [01:53<02:19,  3.39it/s] 39%|      | 307/780 [01:53<02:18,  3.41it/s] 39%|      | 308/780 [01:54<02:17,  3.42it/s] 40%|      | 309/780 [01:54<02:17,  3.43it/s] 40%|      | 310/780 [01:54<02:16,  3.43it/s] 40%|      | 311/780 [01:55<02:16,  3.44it/s] 40%|      | 312/780 [01:55<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 00:50:40,847 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:50:40,847 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:50:40,847 >>   Batch size = 8
{'eval_loss': 1.0460652112960815, 'eval_runtime': 9.8433, 'eval_samples_per_second': 353.743, 'eval_steps_per_second': 44.294, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.09it/s][A
  3%|         | 12/436 [00:00<00:08, 48.30it/s][A
  4%|         | 17/436 [00:00<00:10, 40.96it/s][A
  5%|         | 22/436 [00:00<00:09, 42.39it/s][A
  6%|         | 27/436 [00:00<00:09, 43.40it/s][A
  7%|         | 32/436 [00:00<00:09, 43.83it/s][A
  8%|         | 37/436 [00:00<00:09, 43.89it/s][A
 10%|         | 42/436 [00:00<00:08, 44.23it/s][A
 11%|         | 47/436 [00:01<00:08, 44.54it/s][A
 12%|        | 52/436 [00:01<00:08, 44.66it/s][A
 13%|        | 57/436 [00:01<00:08, 44.31it/s][A
 14%|        | 62/436 [00:01<00:08, 44.26it/s][A
 15%|        | 67/436 [00:01<00:08, 44.50it/s][A
 17%|        | 72/436 [00:01<00:08, 44.63it/s][A
 18%|        | 77/436 [00:01<00:08, 44.64it/s][A
 19%|        | 82/436 [00:01<00:07, 44.61it/s][A
 20%|        | 87/436 [00:01<00:07, 44.71it/s][A
 21%|        | 92/436 [00:02<00:07, 44.87it/s][A
 22%|       | 97/436 [00:02<00:07, 44.72it/s][A
 23%|       | 102/436 [00:02<00:07, 44.72it/s][A
 25%|       | 107/436 [00:02<00:07, 44.57it/s][A
 26%|       | 112/436 [00:02<00:07, 44.60it/s][A
 27%|       | 117/436 [00:02<00:07, 44.70it/s][A
 28%|       | 122/436 [00:02<00:07, 44.63it/s][A
 29%|       | 127/436 [00:02<00:06, 44.57it/s][A
 30%|       | 132/436 [00:02<00:06, 44.64it/s][A
 31%|      | 137/436 [00:03<00:06, 44.76it/s][A
 33%|      | 142/436 [00:03<00:06, 44.75it/s][A
 34%|      | 147/436 [00:03<00:06, 44.62it/s][A
 35%|      | 152/436 [00:03<00:06, 40.85it/s][A
 36%|      | 157/436 [00:03<00:06, 42.07it/s][A
 37%|      | 162/436 [00:03<00:06, 43.07it/s][A
 38%|      | 167/436 [00:03<00:06, 43.62it/s][A
 39%|      | 172/436 [00:03<00:06, 43.97it/s][A
 41%|      | 177/436 [00:04<00:05, 44.23it/s][A
 42%|     | 182/436 [00:04<00:05, 44.46it/s][A
 43%|     | 187/436 [00:04<00:05, 44.53it/s][A
 44%|     | 192/436 [00:04<00:05, 44.26it/s][A
 45%|     | 197/436 [00:04<00:05, 44.18it/s][A
 46%|     | 202/436 [00:04<00:05, 44.42it/s][A
 47%|     | 207/436 [00:04<00:05, 44.56it/s][A
 49%|     | 212/436 [00:04<00:05, 44.70it/s][A
 50%|     | 217/436 [00:04<00:04, 44.64it/s][A
 51%|     | 222/436 [00:05<00:04, 44.77it/s][A
 52%|    | 227/436 [00:05<00:04, 44.86it/s][A
 53%|    | 232/436 [00:05<00:04, 44.71it/s][A
 54%|    | 237/436 [00:05<00:04, 44.52it/s][A
 56%|    | 242/436 [00:05<00:04, 44.47it/s][A
 57%|    | 247/436 [00:05<00:04, 44.58it/s][A
 58%|    | 252/436 [00:05<00:04, 44.67it/s][A
 59%|    | 257/436 [00:05<00:03, 44.77it/s][A
 60%|    | 262/436 [00:05<00:03, 44.73it/s][A
 61%|    | 267/436 [00:06<00:03, 44.81it/s][A
 62%|   | 272/436 [00:06<00:03, 44.83it/s][A
 64%|   | 277/436 [00:06<00:03, 44.71it/s][A
 65%|   | 282/436 [00:06<00:03, 44.64it/s][A
 66%|   | 287/436 [00:06<00:03, 41.28it/s][A
 67%|   | 292/436 [00:06<00:03, 42.33it/s][A
 68%|   | 297/436 [00:06<00:03, 43.06it/s][A
 69%|   | 302/436 [00:06<00:03, 43.74it/s][A
 70%|   | 307/436 [00:06<00:02, 44.18it/s][A
 72%|  | 312/436 [00:07<00:02, 44.46it/s][A
 73%|  | 317/436 [00:07<00:02, 44.48it/s][A
 74%|  | 322/436 [00:07<00:02, 44.53it/s][A
 75%|  | 327/436 [00:07<00:02, 44.29it/s][A
 76%|  | 332/436 [00:07<00:02, 44.20it/s][A
 77%|  | 337/436 [00:07<00:02, 44.37it/s][A
 78%|  | 342/436 [00:07<00:02, 44.50it/s][A
 80%|  | 347/436 [00:07<00:01, 44.70it/s][A
 81%|  | 352/436 [00:07<00:01, 44.84it/s][A
 82%| | 357/436 [00:08<00:01, 44.99it/s][A
 83%| | 362/436 [00:08<00:01, 44.89it/s][A
 84%| | 367/436 [00:08<00:01, 44.66it/s][A
 85%| | 372/436 [00:08<00:01, 44.56it/s][A
 86%| | 377/436 [00:08<00:01, 44.44it/s][A
 88%| | 382/436 [00:08<00:01, 44.42it/s][A
 89%| | 387/436 [00:08<00:01, 44.65it/s][A
 90%| | 392/436 [00:08<00:00, 44.74it/s][A
 91%| | 397/436 [00:08<00:00, 44.87it/s][A
 92%|| 402/436 [00:09<00:00, 44.89it/s][A
 93%|| 407/436 [00:09<00:00, 44.86it/s][A
 94%|| 412/436 [00:09<00:00, 44.71it/s][A
 96%|| 417/436 [00:09<00:00, 44.55it/s][A
 97%|| 422/436 [00:09<00:00, 40.88it/s][A
 98%|| 427/436 [00:09<00:00, 42.12it/s][A
 99%|| 432/436 [00:09<00:00, 43.06it/s][A                                                 
                                                 [A 40%|      | 312/780 [02:05<02:16,  3.44it/s]
100%|| 436/436 [00:09<00:00, 43.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:50:50,921 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 00:50:51,121 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:50:54,666 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:50:54,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:50:54,843 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:18<55:23,  7.12s/it] 40%|      | 314/780 [02:18<39:36,  5.10s/it] 40%|      | 315/780 [02:19<28:59,  3.74s/it] 41%|      | 316/780 [02:19<20:55,  2.71s/it] 41%|      | 317/780 [02:20<15:18,  1.98s/it] 41%|      | 318/780 [02:20<11:22,  1.48s/it] 41%|      | 319/780 [02:20<08:37,  1.12s/it] 41%|      | 320/780 [02:20<06:41,  1.14it/s] 41%|      | 321/780 [02:21<05:21,  1.43it/s] 41%|     | 322/780 [02:21<04:24,  1.73it/s] 41%|     | 323/780 [02:21<03:45,  2.03it/s] 42%|     | 324/780 [02:22<03:17,  2.31it/s] 42%|     | 325/780 [02:22<02:58,  2.55it/s] 42%|     | 326/780 [02:22<02:44,  2.76it/s] 42%|     | 327/780 [02:22<02:35,  2.92it/s] 42%|     | 328/780 [02:23<02:28,  3.05it/s] 42%|     | 329/780 [02:23<02:23,  3.14it/s] 42%|     | 330/780 [02:24<02:37,  2.85it/s] 42%|     | 331/780 [02:24<02:29,  3.00it/s] 43%|     | 332/780 [02:24<02:24,  3.11it/s] 43%|     | 333/780 [02:24<02:20,  3.19it/s] 43%|     | 334/780 [02:25<02:17,  3.25it/s] 43%|     | 335/780 [02:25<02:15,  3.29it/s] 43%|     | 336/780 [02:25<02:13,  3.32it/s] 43%|     | 337/780 [02:26<02:12,  3.34it/s] 43%|     | 338/780 [02:26<02:11,  3.35it/s] 43%|     | 339/780 [02:26<02:11,  3.36it/s] 44%|     | 340/780 [02:27<02:19,  3.14it/s] 44%|     | 341/780 [02:27<02:16,  3.21it/s] 44%|     | 342/780 [02:27<02:13,  3.27it/s] 44%|     | 343/780 [02:27<02:12,  3.31it/s] 44%|     | 344/780 [02:28<02:10,  3.33it/s] 44%|     | 345/780 [02:28<02:09,  3.35it/s] 44%|     | 346/780 [02:28<02:08,  3.37it/s] 44%|     | 347/780 [02:29<02:08,  3.37it/s] 45%|     | 348/780 [02:29<02:07,  3.38it/s] 45%|     | 349/780 [02:29<02:07,  3.39it/s] 45%|     | 350/780 [02:29<02:10,  3.30it/s] 45%|     | 351/780 [02:30<02:08,  3.33it/s] 45%|     | 352/780 [02:30<02:07,  3.34it/s] 45%|     | 353/780 [02:30<02:07,  3.36it/s] 45%|     | 354/780 [02:31<02:06,  3.37it/s] 46%|     | 355/780 [02:31<02:05,  3.38it/s] 46%|     | 356/780 [02:31<02:05,  3.38it/s] 46%|     | 357/780 [02:32<02:05,  3.38it/s] 46%|     | 358/780 [02:32<02:04,  3.38it/s] 46%|     | 359/780 [02:32<02:04,  3.39it/s] 46%|     | 360/780 [02:32<02:03,  3.39it/s] 46%|     | 361/780 [02:33<02:09,  3.23it/s] 46%|     | 362/780 [02:33<02:07,  3.28it/s] 47%|     | 363/780 [02:33<02:05,  3.31it/s] 47%|     | 364/780 [02:34<02:04,  3.34it/s] 47%|     | 365/780 [02:34<02:03,  3.36it/s] 47%|     | 366/780 [02:34<02:02,  3.37it/s] 47%|     | 367/780 [02:35<02:02,  3.38it/s] 47%|     | 368/780 [02:35<02:01,  3.39it/s] 47%|     | 369/780 [02:35<02:01,  3.39it/s] 47%|     | 370/780 [02:35<02:00,  3.39it/s] 48%|     | 371/780 [02:36<02:00,  3.39it/s] 48%|     | 372/780 [02:36<02:03,  3.29it/s] 48%|     | 373/780 [02:36<02:02,  3.32it/s] 48%|     | 374/780 [02:37<02:01,  3.34it/s] 48%|     | 375/780 [02:37<02:00,  3.36it/s] 48%|     | 376/780 [02:37<01:59,  3.37it/s] 48%|     | 377/780 [02:38<01:59,  3.38it/s] 48%|     | 378/780 [02:38<01:58,  3.38it/s] 49%|     | 379/780 [02:38<01:58,  3.39it/s] 49%|     | 380/780 [02:38<01:57,  3.39it/s] 49%|     | 381/780 [02:39<01:57,  3.39it/s] 49%|     | 382/780 [02:39<01:57,  3.39it/s] 49%|     | 383/780 [02:39<02:00,  3.31it/s] 49%|     | 384/780 [02:40<01:58,  3.33it/s] 49%|     | 385/780 [02:40<01:57,  3.36it/s] 49%|     | 386/780 [02:40<01:56,  3.38it/s] 50%|     | 387/780 [02:40<01:55,  3.40it/s] 50%|     | 388/780 [02:41<01:54,  3.41it/s] 50%|     | 389/780 [02:41<01:54,  3.42it/s] 50%|     | 390/780 [02:41<01:53,  3.43it/s] 50%|     | 391/780 [02:42<01:53,  3.43it/s] 50%|     | 392/780 [02:42<01:52,  3.44it/s] 50%|     | 393/780 [02:42<01:52,  3.44it/s] 51%|     | 394/780 [02:43<01:54,  3.37it/s] 51%|     | 395/780 [02:43<01:53,  3.39it/s] 51%|     | 396/780 [02:43<01:52,  3.41it/s] 51%|     | 397/780 [02:43<01:52,  3.42it/s] 51%|     | 398/780 [02:44<01:51,  3.43it/s] 51%|     | 399/780 [02:44<01:51,  3.43it/s] 51%|    | 400/780 [02:44<01:50,  3.44it/s] 51%|    | 401/780 [02:45<01:50,  3.44it/s] 52%|    | 402/780 [02:45<01:49,  3.44it/s] 52%|    | 403/780 [02:45<01:49,  3.44it/s] 52%|    | 404/780 [02:45<01:49,  3.44it/s] 52%|    | 405/780 [02:46<01:51,  3.36it/s] 52%|    | 406/780 [02:46<01:50,  3.39it/s] 52%|    | 407/780 [02:46<01:49,  3.41it/s] 52%|    | 408/780 [02:47<01:48,  3.42it/s] 52%|    | 409/780 [02:47<01:48,  3.43it/s] 53%|    | 410/780 [02:47<01:47,  3.43it/s] 53%|    | 411/780 [02:47<01:47,  3.43it/s] 53%|    | 412/780 [02:48<01:47,  3.44it/s] 53%|    | 413/780 [02:48<01:46,  3.44it/s] 53%|    | 414/780 [02:48<01:46,  3.44it/s] 53%|    | 415/780 [02:49<01:46,  3.44it/s] 53%|    | 416/780 [02:49<01:47,  3.38it/s] 53%|    | 417/780 [02:49<01:46,  3.40it/s] 54%|    | 418/780 [02:50<01:46,  3.41it/s] 54%|    | 419/780 [02:50<01:45,  3.42it/s] 54%|    | 420/780 [02:50<01:45,  3.43it/s] 54%|    | 421/780 [02:50<01:44,  3.43it/s] 54%|    | 422/780 [02:51<01:44,  3.44it/s] 54%|    | 423/780 [02:51<02:18,  2.58it/s] 54%|    | 424/780 [02:52<02:07,  2.79it/s] 54%|    | 425/780 [02:52<01:59,  2.96it/s] 55%|    | 426/780 [02:52<01:54,  3.09it/s] 55%|    | 427/780 [02:52<01:50,  3.19it/s] 55%|    | 428/780 [02:53<01:47,  3.26it/s] 55%|    | 429/780 [02:53<01:45,  3.31it/s] 55%|    | 430/780 [02:53<01:44,  3.35it/s] 55%|    | 431/780 [02:54<01:43,  3.38it/s] 55%|    | 432/780 [02:54<01:42,  3.40it/s] 56%|    | 433/780 [02:54<01:44,  3.32it/s] 56%|    | 434/780 [02:55<01:42,  3.36it/s] 56%|    | 435/780 [02:55<01:41,  3.39it/s] 56%|    | 436/780 [02:55<01:41,  3.40it/s] 56%|    | 437/780 [02:55<01:40,  3.41it/s] 56%|    | 438/780 [02:56<01:39,  3.42it/s] 56%|    | 439/780 [02:56<01:39,  3.43it/s] 56%|    | 440/780 [02:56<01:38,  3.43it/s] 57%|    | 441/780 [02:57<01:38,  3.44it/s] 57%|    | 442/780 [02:57<01:38,  3.44it/s] 57%|    | 443/780 [02:57<01:37,  3.44it/s] 57%|    | 444/780 [02:57<01:42,  3.28it/s] 57%|    | 445/780 [02:58<01:40,  3.33it/s] 57%|    | 446/780 [02:58<01:39,  3.36it/s] 57%|    | 447/780 [02:58<01:38,  3.39it/s] 57%|    | 448/780 [02:59<01:37,  3.41it/s] 58%|    | 449/780 [02:59<01:37,  3.41it/s] 58%|    | 450/780 [02:59<01:36,  3.41it/s] 58%|    | 451/780 [03:00<01:36,  3.40it/s] 58%|    | 452/780 [03:00<01:36,  3.40it/s] 58%|    | 453/780 [03:00<01:36,  3.40it/s] 58%|    | 454/780 [03:00<01:35,  3.40it/s] 58%|    | 455/780 [03:01<01:41,  3.22it/s] 58%|    | 456/780 [03:01<01:39,  3.27it/s] 59%|    | 457/780 [03:01<01:37,  3.31it/s] 59%|    | 458/780 [03:02<01:36,  3.33it/s] 59%|    | 459/780 [03:02<01:35,  3.35it/s] 59%|    | 460/780 [03:02<01:35,  3.37it/s] 59%|    | 461/780 [03:03<01:34,  3.37it/s] 59%|    | 462/780 [03:03<01:34,  3.38it/s] 59%|    | 463/780 [03:03<01:33,  3.39it/s] 59%|    | 464/780 [03:03<01:33,  3.39it/s] 60%|    | 465/780 [03:04<01:36,  3.26it/s] 60%|    | 466/780 [03:04<01:35,  3.30it/s] 60%|    | 467/780 [03:04<01:33,  3.35it/s] 60%|    | 468/780 [03:05<01:32,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 00:51:50,524 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:51:50,524 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:51:50,524 >>   Batch size = 8
{'eval_loss': 1.0586755275726318, 'eval_runtime': 9.8702, 'eval_samples_per_second': 352.78, 'eval_steps_per_second': 44.173, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.95it/s][A
  3%|         | 12/436 [00:00<00:08, 48.05it/s][A
  4%|         | 17/436 [00:00<00:09, 46.46it/s][A
  5%|         | 22/436 [00:00<00:09, 45.77it/s][A
  6%|         | 27/436 [00:00<00:09, 45.26it/s][A
  7%|         | 32/436 [00:00<00:08, 45.04it/s][A
  8%|         | 37/436 [00:00<00:08, 44.91it/s][A
 10%|         | 42/436 [00:00<00:08, 44.74it/s][A
 11%|         | 47/436 [00:01<00:08, 44.86it/s][A
 12%|        | 52/436 [00:01<00:08, 44.77it/s][A
 13%|        | 57/436 [00:01<00:08, 44.87it/s][A
 14%|        | 62/436 [00:01<00:08, 44.68it/s][A
 15%|        | 67/436 [00:01<00:08, 44.69it/s][A
 17%|        | 72/436 [00:01<00:08, 44.59it/s][A
 18%|        | 77/436 [00:01<00:08, 44.53it/s][A
 19%|        | 82/436 [00:01<00:08, 43.58it/s][A
 20%|        | 87/436 [00:01<00:07, 43.99it/s][A
 21%|        | 92/436 [00:02<00:07, 44.26it/s][A
 22%|       | 97/436 [00:02<00:07, 44.52it/s][A
 23%|       | 102/436 [00:02<00:07, 44.55it/s][A
 25%|       | 107/436 [00:02<00:07, 44.59it/s][A
 26%|       | 112/436 [00:02<00:07, 44.56it/s][A
 27%|       | 117/436 [00:02<00:07, 44.55it/s][A
 28%|       | 122/436 [00:02<00:07, 44.35it/s][A
 29%|       | 127/436 [00:02<00:06, 44.46it/s][A
 30%|       | 132/436 [00:02<00:06, 44.52it/s][A
 31%|      | 137/436 [00:03<00:06, 44.67it/s][A
 33%|      | 142/436 [00:03<00:06, 44.77it/s][A
 34%|      | 147/436 [00:03<00:06, 44.86it/s][A
 35%|      | 152/436 [00:03<00:06, 44.76it/s][A
 36%|      | 157/436 [00:03<00:06, 44.63it/s][A
 37%|      | 162/436 [00:03<00:06, 44.56it/s][A
 38%|      | 167/436 [00:03<00:06, 44.36it/s][A
 39%|      | 172/436 [00:03<00:05, 44.38it/s][A
 41%|      | 177/436 [00:03<00:05, 44.57it/s][A
 42%|     | 182/436 [00:04<00:05, 44.63it/s][A
 43%|     | 187/436 [00:04<00:05, 44.73it/s][A
 44%|     | 192/436 [00:04<00:05, 44.82it/s][A
 45%|     | 197/436 [00:04<00:05, 44.74it/s][A
 46%|     | 202/436 [00:04<00:05, 44.76it/s][A
 47%|     | 207/436 [00:04<00:05, 44.54it/s][A
 49%|     | 212/436 [00:04<00:05, 44.45it/s][A
 50%|     | 217/436 [00:04<00:05, 43.54it/s][A
 51%|     | 222/436 [00:04<00:04, 44.06it/s][A
 52%|    | 227/436 [00:05<00:04, 44.41it/s][A
 53%|    | 232/436 [00:05<00:04, 44.53it/s][A
 54%|    | 237/436 [00:05<00:04, 44.62it/s][A
 56%|    | 242/436 [00:05<00:04, 44.56it/s][A
 57%|    | 247/436 [00:05<00:04, 44.58it/s][A
 58%|    | 252/436 [00:05<00:04, 44.53it/s][A
 59%|    | 257/436 [00:05<00:04, 44.39it/s][A
 60%|    | 262/436 [00:05<00:03, 44.38it/s][A
 61%|    | 267/436 [00:05<00:03, 44.56it/s][A
 62%|   | 272/436 [00:06<00:03, 44.65it/s][A
 64%|   | 277/436 [00:06<00:03, 44.70it/s][A
 65%|   | 282/436 [00:06<00:03, 44.59it/s][A
 66%|   | 287/436 [00:06<00:03, 44.52it/s][A
 67%|   | 292/436 [00:06<00:03, 40.94it/s][A
 68%|   | 297/436 [00:06<00:03, 42.17it/s][A
 69%|   | 302/436 [00:06<00:03, 42.96it/s][A
 70%|   | 307/436 [00:06<00:02, 43.45it/s][A
 72%|  | 312/436 [00:07<00:02, 43.84it/s][A
 73%|  | 317/436 [00:07<00:02, 44.22it/s][A
 74%|  | 322/436 [00:07<00:02, 44.33it/s][A
 75%|  | 327/436 [00:07<00:02, 44.46it/s][A
 76%|  | 332/436 [00:07<00:02, 44.19it/s][A
 77%|  | 337/436 [00:07<00:02, 44.27it/s][A
 78%|  | 342/436 [00:07<00:02, 44.44it/s][A
 80%|  | 347/436 [00:07<00:01, 44.62it/s][A
 81%|  | 352/436 [00:07<00:01, 44.66it/s][A
 82%| | 357/436 [00:08<00:01, 44.73it/s][A
 83%| | 362/436 [00:08<00:01, 44.87it/s][A
 84%| | 367/436 [00:08<00:01, 44.75it/s][A
 85%| | 372/436 [00:08<00:01, 44.63it/s][A
 86%| | 377/436 [00:08<00:01, 44.51it/s][A
 88%| | 382/436 [00:08<00:01, 44.51it/s][A
 89%| | 387/436 [00:08<00:01, 44.54it/s][A
 90%| | 392/436 [00:08<00:00, 44.69it/s][A
 91%| | 397/436 [00:08<00:00, 44.67it/s][A
 92%|| 402/436 [00:09<00:00, 44.80it/s][A
 93%|| 407/436 [00:09<00:00, 44.80it/s][A
 94%|| 412/436 [00:09<00:00, 44.79it/s][A
 96%|| 417/436 [00:09<00:00, 44.59it/s][A
 97%|| 422/436 [00:09<00:00, 44.51it/s][A
 98%|| 427/436 [00:09<00:00, 44.20it/s][A
 99%|| 432/436 [00:09<00:00, 44.31it/s][A                                                 
                                                 [A 60%|    | 468/780 [03:15<01:32,  3.37it/s]
100%|| 436/436 [00:09<00:00, 44.31it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:52:00,481 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:52:00,610 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:52:07,353 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:52:07,709 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:52:07,907 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:33<44:50,  8.65s/it] 60%|    | 470/780 [03:33<31:47,  6.15s/it] 60%|    | 471/780 [03:33<22:37,  4.39s/it] 61%|    | 472/780 [03:34<16:14,  3.16s/it] 61%|    | 473/780 [03:34<11:47,  2.30s/it] 61%|    | 474/780 [03:34<08:40,  1.70s/it] 61%|    | 475/780 [03:35<06:29,  1.28s/it] 61%|    | 476/780 [03:35<04:58,  1.02it/s] 61%|    | 477/780 [03:35<03:55,  1.29it/s] 61%|   | 478/780 [03:35<03:10,  1.58it/s] 61%|   | 479/780 [03:36<02:39,  1.89it/s] 62%|   | 480/780 [03:36<02:17,  2.18it/s] 62%|   | 481/780 [03:36<02:05,  2.38it/s] 62%|   | 482/780 [03:37<01:53,  2.62it/s] 62%|   | 483/780 [03:37<01:45,  2.81it/s] 62%|   | 484/780 [03:37<01:39,  2.97it/s] 62%|   | 485/780 [03:38<01:35,  3.08it/s] 62%|   | 486/780 [03:38<01:32,  3.17it/s] 62%|   | 487/780 [03:38<01:30,  3.24it/s] 63%|   | 488/780 [03:38<01:28,  3.28it/s] 63%|   | 489/780 [03:39<01:27,  3.32it/s] 63%|   | 490/780 [03:39<01:26,  3.34it/s] 63%|   | 491/780 [03:39<01:25,  3.36it/s] 63%|   | 492/780 [03:40<01:28,  3.27it/s] 63%|   | 493/780 [03:40<01:26,  3.31it/s] 63%|   | 494/780 [03:40<01:25,  3.34it/s] 63%|   | 495/780 [03:40<01:24,  3.37it/s] 64%|   | 496/780 [03:41<01:23,  3.40it/s] 64%|   | 497/780 [03:41<01:22,  3.41it/s] 64%|   | 498/780 [03:41<01:22,  3.43it/s] 64%|   | 499/780 [03:42<01:21,  3.43it/s] 64%|   | 500/780 [03:42<01:21,  3.44it/s]                                                  64%|   | 500/780 [03:42<01:21,  3.44it/s] 64%|   | 501/780 [03:42<01:21,  3.44it/s] 64%|   | 502/780 [03:43<01:20,  3.44it/s] 64%|   | 503/780 [03:43<01:20,  3.45it/s] 65%|   | 504/780 [03:43<01:20,  3.45it/s] 65%|   | 505/780 [03:43<01:19,  3.45it/s] 65%|   | 506/780 [03:44<01:19,  3.45it/s] 65%|   | 507/780 [03:44<01:19,  3.45it/s] 65%|   | 508/780 [03:44<01:18,  3.45it/s] 65%|   | 509/780 [03:45<01:18,  3.45it/s] 65%|   | 510/780 [03:45<01:18,  3.45it/s] 66%|   | 511/780 [03:45<01:17,  3.45it/s] 66%|   | 512/780 [03:45<01:20,  3.32it/s] 66%|   | 513/780 [03:46<01:19,  3.36it/s] 66%|   | 514/780 [03:46<01:18,  3.39it/s] 66%|   | 515/780 [03:46<01:17,  3.41it/s] 66%|   | 516/780 [03:47<01:17,  3.42it/s] 66%|   | 517/780 [03:47<01:16,  3.43it/s] 66%|   | 518/780 [03:47<01:16,  3.44it/s] 67%|   | 519/780 [03:47<01:15,  3.44it/s] 67%|   | 520/780 [03:48<01:15,  3.45it/s] 67%|   | 521/780 [03:48<01:15,  3.45it/s] 67%|   | 522/780 [03:48<01:14,  3.45it/s] 67%|   | 523/780 [03:49<01:16,  3.37it/s] 67%|   | 524/780 [03:49<01:15,  3.40it/s] 67%|   | 525/780 [03:49<01:14,  3.41it/s] 67%|   | 526/780 [03:50<01:14,  3.42it/s] 68%|   | 527/780 [03:50<01:13,  3.43it/s] 68%|   | 528/780 [03:50<01:13,  3.44it/s] 68%|   | 529/780 [03:50<01:12,  3.44it/s] 68%|   | 530/780 [03:51<01:12,  3.45it/s] 68%|   | 531/780 [03:51<01:12,  3.45it/s] 68%|   | 532/780 [03:51<01:11,  3.45it/s] 68%|   | 533/780 [03:52<01:11,  3.45it/s] 68%|   | 534/780 [03:52<01:12,  3.37it/s] 69%|   | 535/780 [03:52<01:12,  3.40it/s] 69%|   | 536/780 [03:52<01:11,  3.41it/s] 69%|   | 537/780 [03:53<01:10,  3.43it/s] 69%|   | 538/780 [03:53<01:10,  3.43it/s] 69%|   | 539/780 [03:53<01:10,  3.44it/s] 69%|   | 540/780 [03:54<01:09,  3.44it/s] 69%|   | 541/780 [03:54<01:09,  3.45it/s] 69%|   | 542/780 [03:54<01:09,  3.45it/s] 70%|   | 543/780 [03:54<01:08,  3.45it/s] 70%|   | 544/780 [03:55<01:08,  3.45it/s] 70%|   | 545/780 [03:55<01:10,  3.34it/s] 70%|   | 546/780 [03:55<01:09,  3.37it/s] 70%|   | 547/780 [03:56<01:08,  3.40it/s] 70%|   | 548/780 [03:56<01:07,  3.41it/s] 70%|   | 549/780 [03:56<01:07,  3.43it/s] 71%|   | 550/780 [03:57<01:06,  3.43it/s] 71%|   | 551/780 [03:57<01:06,  3.44it/s] 71%|   | 552/780 [03:57<01:06,  3.44it/s] 71%|   | 553/780 [03:57<01:05,  3.44it/s] 71%|   | 554/780 [03:58<01:05,  3.45it/s] 71%|   | 555/780 [03:58<01:05,  3.45it/s] 71%|  | 556/780 [03:58<01:08,  3.28it/s] 71%|  | 557/780 [03:59<01:07,  3.33it/s] 72%|  | 558/780 [03:59<01:05,  3.36it/s] 72%|  | 559/780 [03:59<01:05,  3.39it/s] 72%|  | 560/780 [03:59<01:04,  3.40it/s] 72%|  | 561/780 [04:00<01:03,  3.42it/s] 72%|  | 562/780 [04:00<01:03,  3.43it/s] 72%|  | 563/780 [04:00<01:03,  3.44it/s] 72%|  | 564/780 [04:01<01:02,  3.44it/s] 72%|  | 565/780 [04:01<01:02,  3.44it/s] 73%|  | 566/780 [04:01<01:02,  3.45it/s] 73%|  | 567/780 [04:02<01:04,  3.32it/s] 73%|  | 568/780 [04:02<01:03,  3.36it/s] 73%|  | 569/780 [04:02<01:02,  3.39it/s] 73%|  | 570/780 [04:02<01:01,  3.41it/s] 73%|  | 571/780 [04:03<01:01,  3.42it/s] 73%|  | 572/780 [04:03<01:00,  3.43it/s] 73%|  | 573/780 [04:03<01:00,  3.44it/s] 74%|  | 574/780 [04:04<00:59,  3.44it/s] 74%|  | 575/780 [04:04<00:59,  3.45it/s] 74%|  | 576/780 [04:04<00:59,  3.45it/s] 74%|  | 577/780 [04:04<00:58,  3.45it/s] 74%|  | 578/780 [04:05<01:02,  3.24it/s] 74%|  | 579/780 [04:05<01:00,  3.30it/s] 74%|  | 580/780 [04:05<00:59,  3.34it/s] 74%|  | 581/780 [04:06<00:58,  3.37it/s] 75%|  | 582/780 [04:06<00:58,  3.40it/s] 75%|  | 583/780 [04:06<00:57,  3.41it/s] 75%|  | 584/780 [04:07<00:57,  3.42it/s] 75%|  | 585/780 [04:07<00:56,  3.43it/s] 75%|  | 586/780 [04:07<00:56,  3.44it/s] 75%|  | 587/780 [04:07<00:56,  3.44it/s] 75%|  | 588/780 [04:08<00:55,  3.44it/s] 76%|  | 589/780 [04:08<00:57,  3.31it/s] 76%|  | 590/780 [04:08<00:56,  3.35it/s] 76%|  | 591/780 [04:09<00:55,  3.38it/s] 76%|  | 592/780 [04:09<00:55,  3.40it/s] 76%|  | 593/780 [04:09<00:54,  3.41it/s] 76%|  | 594/780 [04:09<00:54,  3.42it/s] 76%|  | 595/780 [04:10<00:53,  3.43it/s] 76%|  | 596/780 [04:10<00:53,  3.44it/s] 77%|  | 597/780 [04:10<00:53,  3.44it/s] 77%|  | 598/780 [04:11<00:52,  3.44it/s] 77%|  | 599/780 [04:11<00:52,  3.44it/s] 77%|  | 600/780 [04:11<00:53,  3.34it/s] 77%|  | 601/780 [04:12<00:53,  3.37it/s] 77%|  | 602/780 [04:12<00:52,  3.39it/s] 77%|  | 603/780 [04:12<00:51,  3.41it/s] 77%|  | 604/780 [04:12<00:51,  3.42it/s] 78%|  | 605/780 [04:13<00:51,  3.42it/s] 78%|  | 606/780 [04:13<00:50,  3.43it/s] 78%|  | 607/780 [04:13<00:50,  3.43it/s] 78%|  | 608/780 [04:14<00:50,  3.44it/s] 78%|  | 609/780 [04:14<00:49,  3.44it/s] 78%|  | 610/780 [04:14<00:49,  3.45it/s] 78%|  | 611/780 [04:14<00:49,  3.45it/s] 78%|  | 612/780 [04:15<00:48,  3.45it/s] 79%|  | 613/780 [04:15<00:48,  3.45it/s] 79%|  | 614/780 [04:15<00:48,  3.45it/s] 79%|  | 615/780 [04:16<00:47,  3.45it/s] 79%|  | 616/780 [04:16<00:49,  3.32it/s] 79%|  | 617/780 [04:16<00:51,  3.19it/s] 79%|  | 618/780 [04:17<00:49,  3.26it/s] 79%|  | 619/780 [04:17<00:48,  3.32it/s] 79%|  | 620/780 [04:17<00:47,  3.35it/s] 80%|  | 621/780 [04:17<00:47,  3.38it/s] 80%|  | 622/780 [04:18<00:46,  3.40it/s] 80%|  | 623/780 [04:18<00:45,  3.41it/s] 80%|  | 624/780 [04:18<00:45,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 00:53:04,205 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:53:04,205 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:53:04,205 >>   Batch size = 8
{'eval_loss': 1.0714119672775269, 'eval_runtime': 9.808, 'eval_samples_per_second': 355.016, 'eval_steps_per_second': 44.453, 'epoch': 3.0}
{'loss': 0.7323, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.64it/s][A
  3%|         | 12/436 [00:00<00:08, 48.06it/s][A
  4%|         | 17/436 [00:00<00:09, 42.26it/s][A
  5%|         | 22/436 [00:00<00:09, 43.35it/s][A
  6%|         | 27/436 [00:00<00:09, 43.94it/s][A
  7%|         | 32/436 [00:00<00:09, 44.16it/s][A
  8%|         | 37/436 [00:00<00:09, 44.27it/s][A
 10%|         | 42/436 [00:00<00:08, 44.32it/s][A
 11%|         | 47/436 [00:01<00:08, 44.62it/s][A
 12%|        | 52/436 [00:01<00:08, 44.71it/s][A
 13%|        | 57/436 [00:01<00:08, 44.25it/s][A
 14%|        | 62/436 [00:01<00:08, 44.45it/s][A
 15%|        | 67/436 [00:01<00:08, 44.50it/s][A
 17%|        | 72/436 [00:01<00:08, 44.64it/s][A
 18%|        | 77/436 [00:01<00:08, 44.66it/s][A
 19%|        | 82/436 [00:01<00:07, 44.61it/s][A
 20%|        | 87/436 [00:01<00:07, 44.56it/s][A
 21%|        | 92/436 [00:02<00:07, 44.61it/s][A
 22%|       | 97/436 [00:02<00:07, 44.59it/s][A
 23%|       | 102/436 [00:02<00:07, 44.49it/s][A
 25%|       | 107/436 [00:02<00:07, 44.33it/s][A
 26%|       | 112/436 [00:02<00:07, 44.45it/s][A
 27%|       | 117/436 [00:02<00:07, 44.70it/s][A
 28%|       | 122/436 [00:02<00:07, 44.77it/s][A
 29%|       | 127/436 [00:02<00:06, 44.76it/s][A
 30%|       | 132/436 [00:02<00:06, 44.71it/s][A
 31%|      | 137/436 [00:03<00:06, 44.57it/s][A
 33%|      | 142/436 [00:03<00:06, 44.57it/s][A
 34%|      | 147/436 [00:03<00:06, 44.50it/s][A
 35%|      | 152/436 [00:03<00:06, 40.80it/s][A
 36%|      | 157/436 [00:03<00:06, 42.09it/s][A
 37%|      | 162/436 [00:03<00:06, 42.98it/s][A
 38%|      | 167/436 [00:03<00:06, 43.72it/s][A
 39%|      | 172/436 [00:03<00:05, 44.17it/s][A
 41%|      | 177/436 [00:03<00:05, 44.32it/s][A
 42%|     | 182/436 [00:04<00:05, 44.34it/s][A
 43%|     | 187/436 [00:04<00:05, 44.27it/s][A
 44%|     | 192/436 [00:04<00:05, 44.00it/s][A
 45%|     | 197/436 [00:04<00:05, 44.10it/s][A
 46%|     | 202/436 [00:04<00:05, 44.34it/s][A
 47%|     | 207/436 [00:04<00:05, 44.56it/s][A
 49%|     | 212/436 [00:04<00:05, 44.72it/s][A
 50%|     | 217/436 [00:05<00:11, 19.13it/s][A
 51%|     | 222/436 [00:05<00:09, 23.29it/s][A
 52%|    | 227/436 [00:05<00:07, 27.27it/s][A
 53%|    | 232/436 [00:05<00:06, 30.98it/s][A
 54%|    | 237/436 [00:05<00:05, 34.22it/s][A
 56%|    | 242/436 [00:05<00:05, 36.91it/s][A
 57%|    | 247/436 [00:06<00:04, 39.03it/s][A
 58%|    | 252/436 [00:06<00:04, 40.65it/s][A
 59%|    | 257/436 [00:06<00:04, 41.64it/s][A
 60%|    | 262/436 [00:06<00:04, 42.16it/s][A
 61%|    | 267/436 [00:06<00:04, 39.41it/s][A
 62%|   | 272/436 [00:06<00:04, 40.98it/s][A
 64%|   | 277/436 [00:06<00:03, 42.25it/s][A
 65%|   | 282/436 [00:06<00:03, 43.16it/s][A
 66%|   | 287/436 [00:06<00:03, 43.78it/s][A
 67%|   | 292/436 [00:07<00:03, 44.24it/s][A
 68%|   | 297/436 [00:07<00:03, 44.34it/s][A
 69%|   | 302/436 [00:07<00:03, 44.46it/s][A
 70%|   | 307/436 [00:07<00:02, 44.10it/s][A
 72%|  | 312/436 [00:07<00:02, 44.03it/s][A
 73%|  | 317/436 [00:07<00:02, 44.21it/s][A
 74%|  | 322/436 [00:07<00:02, 44.36it/s][A
 75%|  | 327/436 [00:07<00:02, 44.65it/s][A
 76%|  | 332/436 [00:07<00:02, 44.90it/s][A
 77%|  | 337/436 [00:08<00:02, 45.03it/s][A
 78%|  | 342/436 [00:08<00:02, 45.07it/s][A
 80%|  | 347/436 [00:08<00:01, 44.70it/s][A
 81%|  | 352/436 [00:08<00:01, 44.35it/s][A
 82%| | 357/436 [00:08<00:01, 44.32it/s][A
 83%| | 362/436 [00:08<00:01, 44.26it/s][A
 84%| | 367/436 [00:08<00:01, 44.49it/s][A
 85%| | 372/436 [00:08<00:01, 44.67it/s][A
 86%| | 377/436 [00:08<00:01, 44.76it/s][A
 88%| | 382/436 [00:09<00:01, 44.96it/s][A
 89%| | 387/436 [00:09<00:01, 44.91it/s][A
 90%| | 392/436 [00:09<00:00, 44.86it/s][A
 91%| | 397/436 [00:09<00:00, 44.57it/s][A
 92%|| 402/436 [00:09<00:00, 42.39it/s][A
 93%|| 407/436 [00:09<00:00, 43.17it/s][A
 94%|| 412/436 [00:09<00:00, 43.57it/s][A
 96%|| 417/436 [00:09<00:00, 44.11it/s][A
 97%|| 422/436 [00:10<00:00, 44.44it/s][A
 98%|| 427/436 [00:10<00:00, 44.59it/s][A
 99%|| 432/436 [00:10<00:00, 44.63it/s][A                                                 
                                                 [A 80%|  | 624/780 [04:29<00:45,  3.42it/s]
100%|| 436/436 [00:10<00:00, 44.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:53:14,753 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 00:53:14,976 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:53:17,834 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:53:18,087 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:53:18,227 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:40<17:31,  6.78s/it] 80%|  | 626/780 [04:41<12:26,  4.85s/it] 80%|  | 627/780 [04:41<08:52,  3.48s/it] 81%|  | 628/780 [04:41<06:23,  2.52s/it] 81%|  | 629/780 [04:41<04:40,  1.85s/it] 81%|  | 630/780 [04:42<03:27,  1.39s/it] 81%|  | 631/780 [04:42<02:37,  1.06s/it] 81%|  | 632/780 [04:42<02:02,  1.21it/s] 81%|  | 633/780 [04:43<01:38,  1.50it/s] 81%| | 634/780 [04:43<01:21,  1.80it/s] 81%| | 635/780 [04:43<01:09,  2.09it/s] 82%| | 636/780 [04:43<01:00,  2.37it/s] 82%| | 637/780 [04:44<00:54,  2.61it/s] 82%| | 638/780 [04:44<00:50,  2.80it/s] 82%| | 639/780 [04:44<00:47,  2.96it/s] 82%| | 640/780 [04:45<00:45,  3.08it/s] 82%| | 641/780 [04:45<00:43,  3.17it/s] 82%| | 642/780 [04:45<00:42,  3.23it/s] 82%| | 643/780 [04:46<00:41,  3.28it/s] 83%| | 644/780 [04:46<00:40,  3.32it/s] 83%| | 645/780 [04:46<00:40,  3.34it/s] 83%| | 646/780 [04:46<00:39,  3.36it/s] 83%| | 647/780 [04:47<00:40,  3.29it/s] 83%| | 648/780 [04:47<00:39,  3.32it/s] 83%| | 649/780 [04:47<00:39,  3.35it/s] 83%| | 650/780 [04:48<00:38,  3.36it/s] 83%| | 651/780 [04:48<00:38,  3.38it/s] 84%| | 652/780 [04:48<00:37,  3.39it/s] 84%| | 653/780 [04:49<00:37,  3.39it/s] 84%| | 654/780 [04:49<00:37,  3.39it/s] 84%| | 655/780 [04:49<00:36,  3.40it/s] 84%| | 656/780 [04:49<00:36,  3.40it/s] 84%| | 657/780 [04:50<00:36,  3.40it/s] 84%| | 658/780 [04:50<00:36,  3.33it/s] 84%| | 659/780 [04:50<00:36,  3.35it/s] 85%| | 660/780 [04:51<00:35,  3.37it/s] 85%| | 661/780 [04:51<00:35,  3.38it/s] 85%| | 662/780 [04:51<00:34,  3.38it/s] 85%| | 663/780 [04:51<00:34,  3.39it/s] 85%| | 664/780 [04:52<00:34,  3.39it/s] 85%| | 665/780 [04:52<00:33,  3.39it/s] 85%| | 666/780 [04:52<00:33,  3.40it/s] 86%| | 667/780 [04:53<00:33,  3.40it/s] 86%| | 668/780 [04:53<00:32,  3.40it/s] 86%| | 669/780 [04:53<00:33,  3.35it/s] 86%| | 670/780 [04:54<00:32,  3.36it/s] 86%| | 671/780 [04:54<00:32,  3.38it/s] 86%| | 672/780 [04:54<00:31,  3.40it/s] 86%| | 673/780 [04:54<00:31,  3.42it/s] 86%| | 674/780 [04:55<00:30,  3.43it/s] 87%| | 675/780 [04:55<00:30,  3.43it/s] 87%| | 676/780 [04:55<00:30,  3.44it/s] 87%| | 677/780 [04:56<00:29,  3.44it/s] 87%| | 678/780 [04:56<00:29,  3.44it/s] 87%| | 679/780 [04:56<00:29,  3.45it/s] 87%| | 680/780 [04:56<00:30,  3.32it/s] 87%| | 681/780 [04:57<00:29,  3.36it/s] 87%| | 682/780 [04:57<00:28,  3.38it/s] 88%| | 683/780 [04:57<00:28,  3.41it/s] 88%| | 684/780 [04:58<00:28,  3.42it/s] 88%| | 685/780 [04:58<00:27,  3.43it/s] 88%| | 686/780 [04:58<00:27,  3.44it/s] 88%| | 687/780 [04:59<00:27,  3.44it/s] 88%| | 688/780 [04:59<00:26,  3.44it/s] 88%| | 689/780 [04:59<00:26,  3.45it/s] 88%| | 690/780 [04:59<00:26,  3.45it/s] 89%| | 691/780 [05:00<00:27,  3.25it/s] 89%| | 692/780 [05:00<00:26,  3.31it/s] 89%| | 693/780 [05:00<00:25,  3.35it/s] 89%| | 694/780 [05:01<00:25,  3.38it/s] 89%| | 695/780 [05:01<00:25,  3.40it/s] 89%| | 696/780 [05:01<00:24,  3.41it/s] 89%| | 697/780 [05:01<00:24,  3.42it/s] 89%| | 698/780 [05:02<00:23,  3.43it/s] 90%| | 699/780 [05:02<00:23,  3.44it/s] 90%| | 700/780 [05:02<00:23,  3.44it/s] 90%| | 701/780 [05:03<00:22,  3.44it/s] 90%| | 702/780 [05:03<00:23,  3.31it/s] 90%| | 703/780 [05:03<00:22,  3.35it/s] 90%| | 704/780 [05:04<00:22,  3.38it/s] 90%| | 705/780 [05:04<00:22,  3.40it/s] 91%| | 706/780 [05:04<00:21,  3.41it/s] 91%| | 707/780 [05:04<00:21,  3.42it/s] 91%| | 708/780 [05:05<00:20,  3.43it/s] 91%| | 709/780 [05:05<00:20,  3.44it/s] 91%| | 710/780 [05:05<00:20,  3.44it/s] 91%| | 711/780 [05:06<00:20,  3.44it/s] 91%|| 712/780 [05:06<00:19,  3.44it/s] 91%|| 713/780 [05:06<00:21,  3.17it/s] 92%|| 714/780 [05:07<00:20,  3.25it/s] 92%|| 715/780 [05:07<00:19,  3.31it/s] 92%|| 716/780 [05:07<00:19,  3.35it/s] 92%|| 717/780 [05:07<00:18,  3.38it/s] 92%|| 718/780 [05:08<00:18,  3.40it/s] 92%|| 719/780 [05:08<00:17,  3.42it/s] 92%|| 720/780 [05:08<00:17,  3.43it/s] 92%|| 721/780 [05:09<00:17,  3.43it/s] 93%|| 722/780 [05:09<00:16,  3.44it/s] 93%|| 723/780 [05:09<00:16,  3.44it/s] 93%|| 724/780 [05:09<00:17,  3.28it/s] 93%|| 725/780 [05:10<00:16,  3.33it/s] 93%|| 726/780 [05:10<00:16,  3.36it/s] 93%|| 727/780 [05:10<00:15,  3.38it/s] 93%|| 728/780 [05:11<00:15,  3.40it/s] 93%|| 729/780 [05:11<00:14,  3.42it/s] 94%|| 730/780 [05:11<00:14,  3.43it/s] 94%|| 731/780 [05:12<00:14,  3.43it/s] 94%|| 732/780 [05:12<00:13,  3.44it/s] 94%|| 733/780 [05:12<00:13,  3.44it/s] 94%|| 734/780 [05:12<00:13,  3.45it/s] 94%|| 735/780 [05:13<00:13,  3.38it/s] 94%|| 736/780 [05:13<00:12,  3.40it/s] 94%|| 737/780 [05:13<00:12,  3.42it/s] 95%|| 738/780 [05:14<00:12,  3.42it/s] 95%|| 739/780 [05:14<00:11,  3.43it/s] 95%|| 740/780 [05:14<00:11,  3.44it/s] 95%|| 741/780 [05:14<00:11,  3.44it/s] 95%|| 742/780 [05:15<00:11,  3.44it/s] 95%|| 743/780 [05:15<00:10,  3.44it/s] 95%|| 744/780 [05:15<00:10,  3.45it/s] 96%|| 745/780 [05:16<00:10,  3.45it/s] 96%|| 746/780 [05:16<00:09,  3.45it/s] 96%|| 747/780 [05:16<00:09,  3.44it/s] 96%|| 748/780 [05:16<00:09,  3.37it/s] 96%|| 749/780 [05:17<00:09,  3.40it/s] 96%|| 750/780 [05:17<00:08,  3.41it/s] 96%|| 751/780 [05:17<00:08,  3.30it/s] 96%|| 752/780 [05:18<00:08,  3.34it/s] 97%|| 753/780 [05:18<00:08,  3.37it/s] 97%|| 754/780 [05:18<00:07,  3.39it/s] 97%|| 755/780 [05:19<00:07,  3.41it/s] 97%|| 756/780 [05:19<00:07,  3.42it/s] 97%|| 757/780 [05:19<00:06,  3.43it/s] 97%|| 758/780 [05:19<00:06,  3.43it/s] 97%|| 759/780 [05:20<00:06,  3.44it/s] 97%|| 760/780 [05:20<00:05,  3.44it/s] 98%|| 761/780 [05:20<00:05,  3.44it/s] 98%|| 762/780 [05:21<00:05,  3.27it/s] 98%|| 763/780 [05:21<00:05,  3.33it/s] 98%|| 764/780 [05:21<00:04,  3.36it/s] 98%|| 765/780 [05:21<00:04,  3.38it/s] 98%|| 766/780 [05:22<00:04,  3.40it/s] 98%|| 767/780 [05:22<00:03,  3.41it/s] 98%|| 768/780 [05:22<00:03,  3.42it/s] 99%|| 769/780 [05:23<00:03,  3.43it/s] 99%|| 770/780 [05:23<00:02,  3.43it/s] 99%|| 771/780 [05:23<00:02,  3.44it/s] 99%|| 772/780 [05:24<00:03,  2.19it/s] 99%|| 773/780 [05:24<00:03,  2.33it/s] 99%|| 774/780 [05:25<00:02,  2.58it/s] 99%|| 775/780 [05:25<00:01,  2.79it/s] 99%|| 776/780 [05:25<00:01,  2.96it/s]100%|| 777/780 [05:26<00:00,  3.09it/s]100%|| 778/780 [05:26<00:00,  3.19it/s]100%|| 779/780 [05:26<00:00,  3.26it/s]100%|| 780/780 [05:26<00:00,  3.32it/s][INFO|trainer.py:2140] 2023-08-29 00:54:12,254 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:54:12,254 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:54:12,254 >>   Batch size = 8
{'eval_loss': 1.0766510963439941, 'eval_runtime': 10.3437, 'eval_samples_per_second': 336.629, 'eval_steps_per_second': 42.151, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.65it/s][A
  3%|         | 12/436 [00:00<00:08, 48.26it/s][A
  4%|         | 17/436 [00:00<00:08, 46.62it/s][A
  5%|         | 22/436 [00:00<00:09, 45.78it/s][A
  6%|         | 27/436 [00:00<00:09, 45.25it/s][A
  7%|         | 32/436 [00:00<00:08, 44.91it/s][A
  8%|         | 37/436 [00:00<00:10, 37.38it/s][A
 10%|         | 42/436 [00:00<00:09, 39.60it/s][A
 11%|         | 47/436 [00:01<00:09, 41.30it/s][A
 12%|        | 52/436 [00:01<00:09, 42.38it/s][A
 13%|        | 57/436 [00:01<00:08, 43.29it/s][A
 14%|        | 62/436 [00:01<00:08, 43.86it/s][A
 15%|        | 67/436 [00:01<00:08, 44.26it/s][A
 17%|        | 72/436 [00:01<00:08, 44.44it/s][A
 18%|        | 77/436 [00:01<00:08, 44.14it/s][A
 19%|        | 82/436 [00:01<00:08, 43.86it/s][A
 20%|        | 87/436 [00:01<00:07, 43.95it/s][A
 21%|        | 92/436 [00:02<00:07, 44.17it/s][A
 22%|       | 97/436 [00:02<00:07, 44.47it/s][A
 23%|       | 102/436 [00:02<00:07, 44.72it/s][A
 25%|       | 107/436 [00:02<00:07, 44.86it/s][A
 26%|       | 112/436 [00:02<00:07, 44.87it/s][A
 27%|       | 117/436 [00:02<00:07, 44.65it/s][A
 28%|       | 122/436 [00:02<00:07, 44.33it/s][A
 29%|       | 127/436 [00:02<00:06, 44.18it/s][A
 30%|       | 132/436 [00:03<00:06, 44.22it/s][A
 31%|      | 137/436 [00:03<00:06, 44.33it/s][A
 33%|      | 142/436 [00:03<00:06, 44.52it/s][A
 34%|      | 147/436 [00:03<00:06, 44.80it/s][A
 35%|      | 152/436 [00:03<00:06, 44.92it/s][A
 36%|      | 157/436 [00:03<00:06, 45.00it/s][A
 37%|      | 162/436 [00:03<00:06, 44.80it/s][A
 38%|      | 167/436 [00:03<00:06, 44.50it/s][A
 39%|      | 172/436 [00:03<00:06, 40.33it/s][A
 41%|      | 177/436 [00:04<00:06, 41.73it/s][A
 42%|     | 182/436 [00:04<00:05, 42.73it/s][A
 43%|     | 187/436 [00:04<00:06, 40.99it/s][A
 44%|     | 192/436 [00:04<00:05, 42.18it/s][A
 45%|     | 197/436 [00:04<00:05, 42.93it/s][A
 46%|     | 202/436 [00:04<00:05, 43.65it/s][A
 47%|     | 207/436 [00:04<00:05, 43.84it/s][A
 49%|     | 212/436 [00:04<00:05, 43.79it/s][A
 50%|     | 217/436 [00:04<00:04, 43.86it/s][A
 51%|     | 222/436 [00:05<00:04, 44.08it/s][A
 52%|    | 227/436 [00:05<00:04, 44.18it/s][A
 53%|    | 232/436 [00:05<00:04, 44.35it/s][A
 54%|    | 237/436 [00:05<00:04, 44.66it/s][A
 56%|    | 242/436 [00:05<00:04, 44.82it/s][A
 57%|    | 247/436 [00:05<00:04, 44.89it/s][A
 58%|    | 252/436 [00:05<00:04, 44.64it/s][A
 59%|    | 257/436 [00:05<00:04, 44.41it/s][A
 60%|    | 262/436 [00:05<00:03, 44.35it/s][A
 61%|    | 267/436 [00:06<00:03, 44.32it/s][A
 62%|   | 272/436 [00:06<00:03, 44.39it/s][A
 64%|   | 277/436 [00:06<00:03, 44.50it/s][A
 65%|   | 282/436 [00:06<00:03, 44.78it/s][A
 66%|   | 287/436 [00:06<00:03, 44.83it/s][A
 67%|   | 292/436 [00:06<00:03, 44.93it/s][A
 68%|   | 297/436 [00:06<00:03, 44.81it/s][A
 69%|   | 302/436 [00:06<00:03, 44.62it/s][A
 70%|   | 307/436 [00:06<00:03, 42.41it/s][A
 72%|  | 312/436 [00:07<00:02, 43.07it/s][A
 73%|  | 317/436 [00:07<00:02, 43.60it/s][A
 74%|  | 322/436 [00:07<00:02, 43.98it/s][A
 75%|  | 327/436 [00:07<00:02, 44.38it/s][A
 76%|  | 332/436 [00:07<00:02, 44.55it/s][A
 77%|  | 337/436 [00:07<00:02, 44.64it/s][A
 78%|  | 342/436 [00:07<00:02, 44.52it/s][A
 80%|  | 347/436 [00:07<00:02, 44.28it/s][A
 81%|  | 352/436 [00:08<00:01, 44.28it/s][A
 82%| | 357/436 [00:08<00:01, 44.49it/s][A
 83%| | 362/436 [00:08<00:01, 44.58it/s][A
 84%| | 367/436 [00:08<00:01, 44.81it/s][A
 85%| | 372/436 [00:08<00:01, 44.73it/s][A
 86%| | 377/436 [00:08<00:01, 44.85it/s][A
 88%| | 382/436 [00:08<00:01, 44.83it/s][A
 89%| | 387/436 [00:08<00:01, 44.58it/s][A
 90%| | 392/436 [00:08<00:00, 44.47it/s][A
 91%| | 397/436 [00:09<00:00, 44.48it/s][A
 92%|| 402/436 [00:09<00:00, 44.41it/s][A
 93%|| 407/436 [00:09<00:00, 44.61it/s][A
 94%|| 412/436 [00:09<00:00, 44.70it/s][A
 96%|| 417/436 [00:09<00:00, 44.84it/s][A
 97%|| 422/436 [00:09<00:00, 44.89it/s][A
 98%|| 427/436 [00:09<00:00, 44.81it/s][A
 99%|| 432/436 [00:09<00:00, 44.56it/s][A                                                 
                                                 [A100%|| 780/780 [05:36<00:00,  3.32it/s]
100%|| 436/436 [00:09<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:54:22,463 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 00:54:22,683 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:54:25,955 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:54:26,072 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:54:26,136 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:54:32,241 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:54:32,241 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156 (score: 1.0460652112960815).
                                                 100%|| 780/780 [05:54<00:00,  3.32it/s]100%|| 780/780 [05:54<00:00,  2.20it/s]
[INFO|trainer.py:1894] 2023-08-29 00:54:40,223 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 00:54:40,353 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:54:43,186 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:54:43,305 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:54:43,385 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:54:43,876 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:43,877 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:43,877 >>   train_loss               =     0.7208
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:43,877 >>   train_runtime            = 0:05:54.91
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:43,877 >>   train_samples            =      10011
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:43,877 >>   train_samples_per_second =    141.035
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:43,877 >>   train_steps_per_second   =      2.198
{'eval_loss': 1.0845474004745483, 'eval_runtime': 9.8992, 'eval_samples_per_second': 351.745, 'eval_steps_per_second': 44.044, 'epoch': 5.0}
{'train_runtime': 354.9109, 'train_samples_per_second': 141.035, 'train_steps_per_second': 2.198, 'train_loss': 0.7208462446163862, 'epoch': 5.0}
08/29/2023 00:54:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:54:44,154 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:54:44,154 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:54:44,154 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.24it/s]  3%|         | 12/436 [00:00<00:08, 49.02it/s]  4%|         | 17/436 [00:00<00:08, 47.52it/s]  5%|         | 22/436 [00:00<00:08, 46.66it/s]  6%|         | 27/436 [00:00<00:08, 46.21it/s]  7%|         | 32/436 [00:00<00:08, 45.90it/s]  8%|         | 37/436 [00:00<00:08, 45.60it/s] 10%|         | 42/436 [00:00<00:08, 45.22it/s] 11%|         | 47/436 [00:01<00:08, 44.68it/s] 12%|        | 52/436 [00:01<00:08, 44.56it/s] 13%|        | 57/436 [00:01<00:08, 44.68it/s] 14%|        | 62/436 [00:01<00:08, 44.81it/s] 15%|        | 67/436 [00:01<00:08, 44.95it/s] 17%|        | 72/436 [00:01<00:08, 45.02it/s] 18%|        | 77/436 [00:01<00:07, 45.06it/s] 19%|        | 82/436 [00:01<00:07, 45.03it/s] 20%|        | 87/436 [00:01<00:07, 44.81it/s] 21%|        | 92/436 [00:02<00:07, 44.48it/s] 22%|       | 97/436 [00:02<00:07, 44.51it/s] 23%|       | 102/436 [00:02<00:08, 39.85it/s] 25%|       | 107/436 [00:02<00:07, 41.42it/s] 26%|       | 112/436 [00:02<00:07, 42.51it/s] 27%|       | 117/436 [00:02<00:07, 43.38it/s] 28%|       | 122/436 [00:02<00:07, 43.82it/s] 29%|       | 127/436 [00:02<00:06, 44.26it/s] 30%|       | 132/436 [00:02<00:06, 44.47it/s] 31%|      | 137/436 [00:03<00:06, 44.53it/s] 33%|      | 142/436 [00:03<00:06, 44.21it/s] 34%|      | 147/436 [00:03<00:06, 44.15it/s] 35%|      | 152/436 [00:03<00:06, 44.22it/s] 36%|      | 157/436 [00:03<00:06, 44.53it/s] 37%|      | 162/436 [00:03<00:06, 44.73it/s] 38%|      | 167/436 [00:03<00:06, 44.78it/s] 39%|      | 172/436 [00:03<00:05, 44.94it/s] 41%|      | 177/436 [00:03<00:05, 45.02it/s] 42%|     | 182/436 [00:04<00:05, 44.79it/s] 43%|     | 187/436 [00:04<00:05, 44.52it/s] 44%|     | 192/436 [00:04<00:05, 44.50it/s] 45%|     | 197/436 [00:04<00:05, 44.47it/s] 46%|     | 202/436 [00:04<00:05, 44.66it/s] 47%|     | 207/436 [00:04<00:05, 44.75it/s] 49%|     | 212/436 [00:04<00:04, 44.87it/s] 50%|     | 217/436 [00:04<00:04, 44.93it/s] 51%|     | 222/436 [00:04<00:04, 44.96it/s] 52%|    | 227/436 [00:05<00:04, 44.83it/s] 53%|    | 232/436 [00:05<00:04, 44.71it/s] 54%|    | 237/436 [00:05<00:04, 41.29it/s] 56%|    | 242/436 [00:05<00:04, 42.42it/s] 57%|    | 247/436 [00:05<00:04, 43.22it/s] 58%|    | 252/436 [00:05<00:04, 43.63it/s] 59%|    | 257/436 [00:05<00:04, 44.17it/s] 60%|    | 262/436 [00:05<00:03, 44.35it/s] 61%|    | 267/436 [00:06<00:03, 44.57it/s] 62%|   | 272/436 [00:06<00:03, 44.54it/s] 64%|   | 277/436 [00:06<00:03, 44.26it/s] 65%|   | 282/436 [00:06<00:03, 44.24it/s] 66%|   | 287/436 [00:06<00:03, 44.44it/s] 67%|   | 292/436 [00:06<00:03, 44.68it/s] 68%|   | 297/436 [00:06<00:03, 44.77it/s] 69%|   | 302/436 [00:06<00:02, 44.77it/s] 70%|   | 307/436 [00:06<00:02, 44.88it/s] 72%|  | 312/436 [00:07<00:02, 44.86it/s] 73%|  | 317/436 [00:07<00:02, 44.66it/s] 74%|  | 322/436 [00:07<00:02, 44.47it/s] 75%|  | 327/436 [00:07<00:02, 44.37it/s] 76%|  | 332/436 [00:07<00:02, 44.59it/s] 77%|  | 337/436 [00:07<00:02, 44.68it/s] 78%|  | 342/436 [00:07<00:02, 44.84it/s] 80%|  | 347/436 [00:07<00:01, 44.86it/s] 81%|  | 352/436 [00:07<00:01, 44.89it/s] 82%| | 357/436 [00:08<00:01, 44.78it/s] 83%| | 362/436 [00:08<00:01, 44.71it/s] 84%| | 367/436 [00:08<00:01, 44.55it/s] 85%| | 372/436 [00:08<00:01, 40.84it/s] 86%| | 377/436 [00:08<00:01, 42.20it/s] 88%| | 382/436 [00:08<00:01, 43.10it/s] 89%| | 387/436 [00:08<00:01, 43.71it/s] 90%| | 392/436 [00:08<00:00, 44.22it/s] 91%| | 397/436 [00:08<00:00, 44.41it/s] 92%|| 402/436 [00:09<00:00, 44.57it/s] 93%|| 407/436 [00:09<00:00, 44.48it/s] 94%|| 412/436 [00:09<00:00, 44.15it/s] 96%|| 417/436 [00:09<00:00, 44.11it/s] 97%|| 422/436 [00:09<00:00, 44.37it/s] 98%|| 427/436 [00:09<00:00, 44.52it/s] 99%|| 432/436 [00:09<00:00, 44.67it/s]100%|| 436/436 [00:09<00:00, 44.41it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:54:53,989 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   eval_loss               =     1.0461
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   eval_runtime            = 0:00:09.83
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   eval_samples_per_second =    354.062
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   eval_steps_per_second   =     44.334
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:54:53,989 >>   perplexity              =     2.8464
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:03,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:03,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:03,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:03,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:03,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:55:04,238 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:55:04,239 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:04,525 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:05,607 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:05,607 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:08,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:08,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:08,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:08,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:08,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:55:08,405 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:55:08,406 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:08,684 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:08,842 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:08,842 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.58it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.48it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:31,  1.47it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:35,  1.47it/s]Extractor Predicting: 56it [00:36,  1.46it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:37,  1.45it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.44it/s]Extractor Predicting: 64it [00:42,  1.44it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.49it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.53it/s]Extractor Predicting: 69it [00:45,  1.53it/s]Extractor Predicting: 70it [00:45,  1.58it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:47,  1.58it/s]Extractor Predicting: 73it [00:47,  1.61it/s]Extractor Predicting: 74it [00:48,  1.62it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:49,  1.54it/s]Extractor Predicting: 77it [00:50,  1.55it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:52,  1.53it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:54,  1.50it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:56,  1.54it/s]Extractor Predicting: 88it [00:57,  1.50it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:58,  1.47it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:00,  1.51it/s]Extractor Predicting: 94it [01:01,  1.47it/s]Extractor Predicting: 95it [01:02,  1.38it/s]Extractor Predicting: 96it [01:03,  1.43it/s]Extractor Predicting: 97it [01:03,  1.44it/s]Extractor Predicting: 98it [01:04,  1.45it/s]Extractor Predicting: 99it [01:05,  1.43it/s]Extractor Predicting: 100it [01:05,  1.42it/s]Extractor Predicting: 101it [01:06,  1.44it/s]Extractor Predicting: 102it [01:07,  1.44it/s]Extractor Predicting: 103it [01:08,  1.44it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.45it/s]Extractor Predicting: 107it [01:10,  1.47it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:11,  1.53it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:13,  1.58it/s]Extractor Predicting: 112it [01:13,  1.57it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.51it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:17,  1.48it/s]Extractor Predicting: 119it [01:18,  1.49it/s]Extractor Predicting: 120it [01:19,  1.53it/s]Extractor Predicting: 121it [01:19,  1.51it/s]Extractor Predicting: 122it [01:20,  1.53it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:21,  1.50it/s]Extractor Predicting: 125it [01:22,  1.51it/s]Extractor Predicting: 126it [01:23,  1.52it/s]Extractor Predicting: 127it [01:23,  1.51it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:25,  1.50it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:26,  1.50it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:27,  1.48it/s]Extractor Predicting: 134it [01:28,  1.44it/s]Extractor Predicting: 135it [01:29,  1.46it/s]Extractor Predicting: 136it [01:29,  1.47it/s]Extractor Predicting: 137it [01:30,  1.48it/s]Extractor Predicting: 138it [01:31,  1.49it/s]Extractor Predicting: 139it [01:31,  1.47it/s]Extractor Predicting: 140it [01:32,  1.42it/s]Extractor Predicting: 141it [01:33,  1.43it/s]Extractor Predicting: 142it [01:34,  1.47it/s]Extractor Predicting: 143it [01:34,  1.52it/s]Extractor Predicting: 143it [01:34,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:01,775 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:01,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:01,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:01,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:01,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:57:02,443 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:57:02,444 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:57:03,057 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:57:04,100 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:57:04,100 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:06,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:06,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:06,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:06,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:57:06,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:57:07,173 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:57:07,174 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:57:07,457 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:57:07,617 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:57:07,617 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2558139534883721,
  "recall": 0.04738655944859276,
  "score": 0.07996123091834262,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.77it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:21,  1.52it/s]Extractor Predicting: 36it [00:22,  1.49it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:23,  1.49it/s]Extractor Predicting: 39it [00:24,  1.51it/s]Extractor Predicting: 40it [00:25,  1.50it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:27,  1.50it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:28,  1.51it/s]Extractor Predicting: 47it [00:29,  1.51it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:30,  1.51it/s]Extractor Predicting: 50it [00:31,  1.52it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:33,  1.52it/s]Extractor Predicting: 54it [00:34,  1.53it/s]Extractor Predicting: 55it [00:34,  1.51it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:36,  1.56it/s]Extractor Predicting: 59it [00:37,  1.51it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.58it/s]Extractor Predicting: 64it [00:40,  1.55it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:42,  1.51it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:44,  1.54it/s]Extractor Predicting: 71it [00:45,  1.54it/s]Extractor Predicting: 72it [00:45,  1.53it/s]Extractor Predicting: 73it [00:46,  1.52it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:47,  1.54it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:49,  1.63it/s]Extractor Predicting: 79it [00:50,  1.59it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:51,  1.54it/s]Extractor Predicting: 82it [00:52,  1.57it/s]Extractor Predicting: 83it [00:52,  1.56it/s]Extractor Predicting: 84it [00:53,  1.53it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:54,  1.54it/s]Extractor Predicting: 87it [00:55,  1.56it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:56,  1.60it/s]Extractor Predicting: 90it [00:57,  1.60it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:00,  1.52it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:02,  1.52it/s]Extractor Predicting: 99it [01:03,  1.52it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.52it/s]Extractor Predicting: 108it [01:09,  1.53it/s]Extractor Predicting: 109it [01:09,  1.53it/s]Extractor Predicting: 110it [01:10,  1.52it/s]Extractor Predicting: 111it [01:11,  1.52it/s]Extractor Predicting: 112it [01:11,  1.50it/s]Extractor Predicting: 113it [01:12,  1.52it/s]Extractor Predicting: 114it [01:13,  1.53it/s]Extractor Predicting: 115it [01:13,  1.52it/s]Extractor Predicting: 116it [01:14,  1.57it/s]Extractor Predicting: 117it [01:14,  1.65it/s]Extractor Predicting: 118it [01:15,  1.69it/s]Extractor Predicting: 119it [01:16,  1.68it/s]Extractor Predicting: 120it [01:16,  1.67it/s]Extractor Predicting: 121it [01:17,  1.46it/s]Extractor Predicting: 122it [01:18,  1.49it/s]Extractor Predicting: 123it [01:18,  1.51it/s]Extractor Predicting: 124it [01:19,  1.57it/s]Extractor Predicting: 125it [01:19,  1.65it/s]Extractor Predicting: 126it [01:20,  1.65it/s]Extractor Predicting: 127it [01:21,  1.67it/s]Extractor Predicting: 128it [01:21,  1.70it/s]Extractor Predicting: 129it [01:22,  1.68it/s]Extractor Predicting: 130it [01:22,  1.67it/s]Extractor Predicting: 131it [01:23,  1.69it/s]Extractor Predicting: 132it [01:24,  1.69it/s]Extractor Predicting: 133it [01:24,  1.69it/s]Extractor Predicting: 134it [01:25,  1.70it/s]Extractor Predicting: 135it [01:25,  1.71it/s]Extractor Predicting: 136it [01:26,  1.75it/s]Extractor Predicting: 137it [01:26,  1.75it/s]Extractor Predicting: 138it [01:27,  1.71it/s]Extractor Predicting: 139it [01:28,  1.67it/s]Extractor Predicting: 140it [01:28,  1.69it/s]Extractor Predicting: 141it [01:29,  1.65it/s]Extractor Predicting: 142it [01:30,  1.65it/s]Extractor Predicting: 143it [01:30,  1.67it/s]Extractor Predicting: 144it [01:31,  1.64it/s]Extractor Predicting: 145it [01:31,  1.64it/s]Extractor Predicting: 146it [01:32,  1.66it/s]Extractor Predicting: 147it [01:33,  1.65it/s]Extractor Predicting: 148it [01:33,  1.63it/s]Extractor Predicting: 149it [01:34,  1.69it/s]Extractor Predicting: 150it [01:34,  1.66it/s]Extractor Predicting: 151it [01:35,  1.64it/s]Extractor Predicting: 152it [01:36,  1.67it/s]Extractor Predicting: 153it [01:36,  1.72it/s]Extractor Predicting: 154it [01:37,  1.71it/s]Extractor Predicting: 155it [01:37,  1.71it/s]Extractor Predicting: 156it [01:38,  1.69it/s]Extractor Predicting: 157it [01:39,  1.68it/s]Extractor Predicting: 158it [01:39,  1.69it/s]Extractor Predicting: 159it [01:40,  1.64it/s]Extractor Predicting: 160it [01:40,  1.65it/s]Extractor Predicting: 161it [01:41,  1.66it/s]Extractor Predicting: 162it [01:42,  1.61it/s]Extractor Predicting: 163it [01:42,  1.62it/s]Extractor Predicting: 164it [01:43,  1.63it/s]Extractor Predicting: 165it [01:43,  1.65it/s]Extractor Predicting: 166it [01:44,  1.66it/s]Extractor Predicting: 167it [01:45,  1.63it/s]Extractor Predicting: 168it [01:45,  1.60it/s]Extractor Predicting: 169it [01:46,  1.62it/s]Extractor Predicting: 170it [01:47,  1.59it/s]Extractor Predicting: 171it [01:47,  1.58it/s]Extractor Predicting: 172it [01:48,  1.64it/s]Extractor Predicting: 173it [01:48,  1.59it/s]Extractor Predicting: 174it [01:49,  1.57it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:50,  1.56it/s]Extractor Predicting: 177it [01:51,  1.54it/s]Extractor Predicting: 178it [01:52,  1.52it/s]Extractor Predicting: 179it [01:52,  1.53it/s]Extractor Predicting: 180it [01:53,  1.57it/s]Extractor Predicting: 181it [01:54,  1.55it/s]Extractor Predicting: 182it [01:54,  1.53it/s]Extractor Predicting: 183it [01:55,  1.53it/s]Extractor Predicting: 184it [01:56,  1.51it/s]Extractor Predicting: 185it [01:56,  1.51it/s]Extractor Predicting: 186it [01:57,  1.51it/s]Extractor Predicting: 187it [01:58,  1.50it/s]Extractor Predicting: 188it [01:58,  1.54it/s]Extractor Predicting: 189it [01:59,  1.53it/s]Extractor Predicting: 190it [02:00,  1.54it/s]Extractor Predicting: 191it [02:00,  1.51it/s]Extractor Predicting: 192it [02:01,  1.51it/s]Extractor Predicting: 193it [02:02,  1.49it/s]Extractor Predicting: 194it [02:02,  1.49it/s]Extractor Predicting: 195it [02:03,  1.49it/s]Extractor Predicting: 196it [02:04,  1.51it/s]Extractor Predicting: 197it [02:04,  1.48it/s]Extractor Predicting: 198it [02:05,  1.49it/s]Extractor Predicting: 199it [02:06,  1.50it/s]Extractor Predicting: 200it [02:06,  1.49it/s]Extractor Predicting: 201it [02:07,  1.50it/s]Extractor Predicting: 202it [02:08,  1.51it/s]Extractor Predicting: 203it [02:08,  1.53it/s]Extractor Predicting: 204it [02:09,  1.53it/s]Extractor Predicting: 205it [02:09,  1.58it/s]Extractor Predicting: 206it [02:10,  1.57it/s]Extractor Predicting: 207it [02:11,  1.56it/s]Extractor Predicting: 208it [02:11,  1.56it/s]Extractor Predicting: 209it [02:12,  1.53it/s]Extractor Predicting: 210it [02:13,  1.56it/s]Extractor Predicting: 211it [02:13,  1.61it/s]Extractor Predicting: 212it [02:14,  1.61it/s]Extractor Predicting: 213it [02:14,  1.63it/s]Extractor Predicting: 214it [02:15,  1.59it/s]Extractor Predicting: 215it [02:16,  1.56it/s]Extractor Predicting: 216it [02:16,  1.57it/s]Extractor Predicting: 217it [02:17,  1.58it/s]Extractor Predicting: 218it [02:18,  1.61it/s]Extractor Predicting: 219it [02:18,  1.63it/s]Extractor Predicting: 220it [02:19,  1.60it/s]Extractor Predicting: 221it [02:19,  1.63it/s]Extractor Predicting: 222it [02:20,  1.65it/s]Extractor Predicting: 223it [02:21,  1.62it/s]Extractor Predicting: 224it [02:21,  1.62it/s]Extractor Predicting: 225it [02:22,  1.62it/s]Extractor Predicting: 226it [02:23,  1.57it/s]Extractor Predicting: 227it [02:23,  1.56it/s]Extractor Predicting: 228it [02:24,  1.57it/s]Extractor Predicting: 229it [02:25,  1.55it/s]Extractor Predicting: 230it [02:25,  1.55it/s]Extractor Predicting: 231it [02:26,  1.56it/s]Extractor Predicting: 232it [02:26,  1.60it/s]Extractor Predicting: 233it [02:27,  1.65it/s]Extractor Predicting: 234it [02:28,  1.64it/s]Extractor Predicting: 235it [02:28,  1.65it/s]Extractor Predicting: 236it [02:29,  1.70it/s]Extractor Predicting: 237it [02:29,  1.70it/s]Extractor Predicting: 238it [02:30,  1.71it/s]Extractor Predicting: 239it [02:31,  1.70it/s]Extractor Predicting: 240it [02:31,  1.71it/s]Extractor Predicting: 241it [02:32,  1.71it/s]Extractor Predicting: 242it [02:32,  1.70it/s]Extractor Predicting: 243it [02:33,  1.75it/s]Extractor Predicting: 244it [02:34,  1.52it/s]Extractor Predicting: 245it [02:34,  1.62it/s]Extractor Predicting: 246it [02:35,  1.70it/s]Extractor Predicting: 247it [02:35,  1.68it/s]Extractor Predicting: 248it [02:36,  1.66it/s]Extractor Predicting: 249it [02:37,  1.67it/s]Extractor Predicting: 250it [02:37,  1.68it/s]Extractor Predicting: 251it [02:38,  1.72it/s]Extractor Predicting: 252it [02:38,  1.74it/s]Extractor Predicting: 253it [02:39,  1.74it/s]Extractor Predicting: 254it [02:39,  1.71it/s]Extractor Predicting: 255it [02:40,  1.71it/s]Extractor Predicting: 256it [02:41,  1.71it/s]Extractor Predicting: 257it [02:41,  1.75it/s]Extractor Predicting: 258it [02:42,  1.75it/s]Extractor Predicting: 259it [02:42,  1.77it/s]Extractor Predicting: 260it [02:43,  1.75it/s]Extractor Predicting: 261it [02:44,  1.65it/s]Extractor Predicting: 262it [02:44,  1.66it/s]Extractor Predicting: 263it [02:45,  1.56it/s]Extractor Predicting: 264it [02:46,  1.53it/s]Extractor Predicting: 265it [02:46,  1.53it/s]Extractor Predicting: 266it [02:47,  1.51it/s]Extractor Predicting: 267it [02:47,  1.55it/s]Extractor Predicting: 268it [02:48,  1.56it/s]Extractor Predicting: 269it [02:49,  1.53it/s]Extractor Predicting: 270it [02:49,  1.51it/s]Extractor Predicting: 271it [02:50,  1.49it/s]Extractor Predicting: 272it [02:51,  1.52it/s]Extractor Predicting: 273it [02:51,  1.52it/s]Extractor Predicting: 274it [02:52,  1.52it/s]Extractor Predicting: 275it [02:53,  1.52it/s]Extractor Predicting: 276it [02:53,  1.52it/s]Extractor Predicting: 277it [02:54,  1.51it/s]Extractor Predicting: 278it [02:55,  1.51it/s]Extractor Predicting: 279it [02:55,  1.53it/s]Extractor Predicting: 280it [02:56,  1.51it/s]Extractor Predicting: 281it [02:57,  1.50it/s]Extractor Predicting: 282it [02:57,  1.50it/s]Extractor Predicting: 283it [02:58,  1.51it/s]Extractor Predicting: 284it [02:59,  1.50it/s]Extractor Predicting: 285it [02:59,  1.51it/s]Extractor Predicting: 286it [03:00,  1.51it/s]Extractor Predicting: 287it [03:01,  1.48it/s]Extractor Predicting: 288it [03:01,  1.53it/s]Extractor Predicting: 289it [03:02,  1.54it/s]Extractor Predicting: 290it [03:03,  1.56it/s]Extractor Predicting: 291it [03:03,  1.58it/s]Extractor Predicting: 292it [03:04,  1.56it/s]Extractor Predicting: 293it [03:05,  1.57it/s]Extractor Predicting: 294it [03:05,  1.59it/s]Extractor Predicting: 295it [03:06,  1.60it/s]Extractor Predicting: 296it [03:06,  1.58it/s]Extractor Predicting: 297it [03:07,  1.58it/s]Extractor Predicting: 298it [03:08,  1.62it/s]Extractor Predicting: 299it [03:08,  1.59it/s]Extractor Predicting: 300it [03:09,  1.60it/s]Extractor Predicting: 301it [03:10,  1.59it/s]Extractor Predicting: 302it [03:10,  1.55it/s]Extractor Predicting: 303it [03:11,  1.56it/s]Extractor Predicting: 304it [03:12,  1.55it/s]Extractor Predicting: 305it [03:12,  1.54it/s]Extractor Predicting: 306it [03:13,  1.56it/s]Extractor Predicting: 307it [03:13,  1.55it/s]Extractor Predicting: 308it [03:14,  1.55it/s]Extractor Predicting: 309it [03:15,  1.53it/s]Extractor Predicting: 310it [03:15,  1.54it/s]Extractor Predicting: 311it [03:16,  1.55it/s]Extractor Predicting: 312it [03:17,  1.55it/s]Extractor Predicting: 313it [03:17,  1.56it/s]Extractor Predicting: 314it [03:18,  1.54it/s]Extractor Predicting: 315it [03:19,  1.57it/s]Extractor Predicting: 316it [03:19,  1.56it/s]Extractor Predicting: 317it [03:20,  1.59it/s]Extractor Predicting: 318it [03:20,  1.61it/s]Extractor Predicting: 319it [03:21,  1.59it/s]Extractor Predicting: 320it [03:22,  1.57it/s]Extractor Predicting: 321it [03:22,  1.57it/s]Extractor Predicting: 322it [03:23,  1.57it/s]Extractor Predicting: 323it [03:24,  1.59it/s]Extractor Predicting: 324it [03:24,  1.60it/s]Extractor Predicting: 325it [03:25,  1.59it/s]Extractor Predicting: 326it [03:26,  1.60it/s]Extractor Predicting: 327it [03:26,  1.58it/s]Extractor Predicting: 328it [03:27,  1.57it/s]Extractor Predicting: 329it [03:27,  1.57it/s]Extractor Predicting: 330it [03:28,  1.57it/s]Extractor Predicting: 331it [03:29,  1.56it/s]Extractor Predicting: 332it [03:29,  1.58it/s]Extractor Predicting: 333it [03:30,  1.55it/s]Extractor Predicting: 334it [03:31,  1.59it/s]Extractor Predicting: 335it [03:31,  1.58it/s]Extractor Predicting: 336it [03:32,  1.60it/s]Extractor Predicting: 337it [03:33,  1.57it/s]Extractor Predicting: 338it [03:33,  1.58it/s]Extractor Predicting: 339it [03:34,  1.59it/s]Extractor Predicting: 340it [03:34,  1.57it/s]Extractor Predicting: 341it [03:35,  1.60it/s]Extractor Predicting: 342it [03:36,  1.59it/s]Extractor Predicting: 343it [03:36,  1.60it/s]Extractor Predicting: 344it [03:37,  1.63it/s]Extractor Predicting: 345it [03:38,  1.60it/s]Extractor Predicting: 346it [03:38,  1.59it/s]Extractor Predicting: 347it [03:39,  1.59it/s]Extractor Predicting: 348it [03:39,  1.56it/s]Extractor Predicting: 349it [03:40,  1.56it/s]Extractor Predicting: 350it [03:41,  1.55it/s]Extractor Predicting: 351it [03:41,  1.56it/s]Extractor Predicting: 352it [03:42,  1.52it/s]Extractor Predicting: 353it [03:43,  1.53it/s]Extractor Predicting: 354it [03:43,  1.54it/s]Extractor Predicting: 355it [03:44,  1.54it/s]Extractor Predicting: 356it [03:45,  1.56it/s]Extractor Predicting: 357it [03:45,  1.51it/s]Extractor Predicting: 358it [03:46,  1.53it/s]Extractor Predicting: 359it [03:47,  1.55it/s]Extractor Predicting: 360it [03:48,  1.38it/s]Extractor Predicting: 361it [03:48,  1.42it/s]Extractor Predicting: 362it [03:49,  1.49it/s]Extractor Predicting: 363it [03:49,  1.51it/s]Extractor Predicting: 364it [03:50,  1.56it/s]Extractor Predicting: 365it [03:51,  1.55it/s]Extractor Predicting: 366it [03:51,  1.52it/s]Extractor Predicting: 367it [03:52,  1.53it/s]Extractor Predicting: 368it [03:53,  1.55it/s]Extractor Predicting: 369it [03:53,  1.52it/s]Extractor Predicting: 370it [03:54,  1.53it/s]Extractor Predicting: 371it [03:55,  1.54it/s]Extractor Predicting: 372it [03:55,  1.56it/s]Extractor Predicting: 373it [03:56,  1.55it/s]Extractor Predicting: 374it [03:56,  1.56it/s]Extractor Predicting: 375it [03:57,  1.56it/s]Extractor Predicting: 376it [03:58,  1.54it/s]Extractor Predicting: 377it [03:58,  1.60it/s]Extractor Predicting: 378it [03:59,  1.60it/s]Extractor Predicting: 379it [04:00,  1.63it/s]Extractor Predicting: 380it [04:00,  1.60it/s]Extractor Predicting: 381it [04:01,  1.60it/s]Extractor Predicting: 382it [04:02,  1.58it/s]Extractor Predicting: 383it [04:02,  1.55it/s]Extractor Predicting: 384it [04:03,  1.54it/s]Extractor Predicting: 385it [04:03,  1.55it/s]Extractor Predicting: 386it [04:04,  1.54it/s]Extractor Predicting: 387it [04:05,  1.53it/s]Extractor Predicting: 388it [04:06,  1.49it/s]Extractor Predicting: 389it [04:06,  1.51it/s]Extractor Predicting: 390it [04:07,  1.49it/s]Extractor Predicting: 391it [04:07,  1.52it/s]Extractor Predicting: 392it [04:08,  1.55it/s]Extractor Predicting: 393it [04:09,  1.55it/s]Extractor Predicting: 394it [04:09,  1.56it/s]Extractor Predicting: 395it [04:10,  1.56it/s]Extractor Predicting: 396it [04:11,  1.54it/s]Extractor Predicting: 397it [04:11,  1.55it/s]Extractor Predicting: 398it [04:12,  1.52it/s]Extractor Predicting: 399it [04:13,  1.56it/s]Extractor Predicting: 400it [04:13,  1.54it/s]Extractor Predicting: 401it [04:14,  1.52it/s]Extractor Predicting: 402it [04:15,  1.53it/s]Extractor Predicting: 403it [04:15,  1.52it/s]Extractor Predicting: 404it [04:16,  1.55it/s]Extractor Predicting: 405it [04:16,  1.56it/s]Extractor Predicting: 406it [04:17,  1.57it/s]Extractor Predicting: 407it [04:18,  1.57it/s]Extractor Predicting: 408it [04:18,  1.56it/s]Extractor Predicting: 409it [04:19,  1.58it/s]Extractor Predicting: 410it [04:20,  1.58it/s]Extractor Predicting: 411it [04:20,  1.58it/s]Extractor Predicting: 412it [04:21,  1.60it/s]Extractor Predicting: 413it [04:22,  1.59it/s]Extractor Predicting: 414it [04:22,  1.56it/s]Extractor Predicting: 415it [04:23,  1.54it/s]Extractor Predicting: 416it [04:23,  1.58it/s]Extractor Predicting: 417it [04:24,  1.58it/s]Extractor Predicting: 418it [04:25,  1.61it/s]Extractor Predicting: 419it [04:25,  1.62it/s]Extractor Predicting: 420it [04:26,  1.63it/s]Extractor Predicting: 421it [04:27,  1.59it/s]Extractor Predicting: 422it [04:27,  1.57it/s]Extractor Predicting: 423it [04:28,  1.58it/s]Extractor Predicting: 424it [04:28,  1.58it/s]Extractor Predicting: 425it [04:29,  1.57it/s]Extractor Predicting: 426it [04:30,  1.61it/s]Extractor Predicting: 427it [04:30,  1.60it/s]Extractor Predicting: 428it [04:31,  1.55it/s]Extractor Predicting: 429it [04:31,  1.79it/s]Extractor Predicting: 429it [04:31,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:52,230 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:52,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:52,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:52,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:52,251 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:01:52,864 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:01:52,865 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:01:53,451 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:01:54,537 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:01:54,537 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:57,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:57,707 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:57,707 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:57,707 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:57,707 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:01:58,385 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:01:58,386 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:01:59,018 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:01:59,240 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:01:59,240 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.36669242658423495,
  "recall": 0.09229721843999222,
  "score": 0.14747474747474748,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.72it/s]Extractor Predicting: 5it [00:03,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-29 01:02:05,841 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:02:05,842 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:02:05,993 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:02:05,994 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 01:02:06,050 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:02:15,977 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 01:02:15,996 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 01:02:16,237 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:02:16,238 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:02:16,291 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:16,347 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:16,347 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:16,348 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:16,348 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:16,348 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:16,348 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3076923076923077,
  "recall": 0.01834862385321101,
  "score": 0.03463203463203463,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 01:02:16,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:17,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:17,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:18,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:19,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:20,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:20,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:21,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:22,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:22,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:23,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:23,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:24,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:25,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:25,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:26,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:26,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:27,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:28,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:28,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:29,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:13<04:09, 13.16s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:29,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:30,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:31,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:31,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:32,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:32,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:34,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:35,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:35,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:36,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:36,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:37,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:38,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:38,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:39,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:39,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:40,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:41,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:41,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:42,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:26<03:55, 13.08s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:42,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:43,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:44,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:45,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:45,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:46,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:47,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:47,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:48,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:49,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:49,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:50,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:51,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:51,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:52,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:53,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:54,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:54,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:55,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:56,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:57,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:57,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:41<04:02, 14.26s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:58,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:59,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:59,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:00,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:00,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:01,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:02,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:02,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:03,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:03,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:04,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:05,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:06,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:06,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:07,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:07,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:08,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:09,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:09,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:10,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:54<03:36, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:10,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:11,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:12,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:13,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:13,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:14,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:14,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:15,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:16,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:17,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:17,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:18,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:19,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:20,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:20,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:21,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:22,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:22,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:23,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:24,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:24,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:09<03:29, 13.97s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:25,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:26,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:26,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:27,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:28,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:28,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:29,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:30,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:30,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:31,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:31,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:32,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:33,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:33,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:34,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:34,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:35,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:36,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:36,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:37,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:21<03:07, 13.42s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:38,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:38,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:39,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:39,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:40,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:41,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:42,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:42,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:43,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:44,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:44,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:45,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:46,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:46,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:47,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:48,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:48,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:49,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:49,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:50,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:51,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:51,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:52,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:53,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:37<03:03, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:53,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:54,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:54,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:55,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:56,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:56,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:57,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:58,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:58,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:59,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:00,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:00,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:01,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:01,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:02,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:02,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:03,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:04,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:04,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:05,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:06,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:06,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:07,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:07,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:51<02:51, 14.31s/it][WARNING|generation_utils.py:914] 2023-08-29 01:04:08,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:08,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:09,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:10,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:10,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:11,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:12,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:12,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:13,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:14,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:14,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:15,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:16,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:17,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:17,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:18,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:19,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:19,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:20,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:21,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:22,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:22,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:23,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:07<02:42, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-29 01:04:24,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:24,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:25,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:26,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:26,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:27,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:27,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:28,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:29,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:30,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:30,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:31,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:32,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:32,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:33,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:34,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:34,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:35,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:36,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:36,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:37,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:21<02:24, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-29 01:04:37,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:38,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:39,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:39,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:40,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:40,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:41,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:42,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:42,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:43,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:44,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:44,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:45,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:46,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:46,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:47,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:48,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:48,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:49,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:49,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:50,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:34<02:06, 14.09s/it][WARNING|generation_utils.py:914] 2023-08-29 01:04:51,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:51,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:52,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:53,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:53,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:54,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:55,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:55,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:56,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:57,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:57,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:58,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:59,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:04:59,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:00,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:01,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:01,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:02,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:03,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:03,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:04,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:05,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:05,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:49<01:56, 14.52s/it][WARNING|generation_utils.py:914] 2023-08-29 01:05:06,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:07,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:07,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:08,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:09,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:09,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:10,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:11,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:12,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:12,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:13,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:14,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:14,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:15,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:16,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:16,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:17,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:18,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:18,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:19,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:20,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:04<01:41, 14.56s/it][WARNING|generation_utils.py:914] 2023-08-29 01:05:21,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:21,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:22,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:23,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:23,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:24,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:25,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:25,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:26,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:27,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:27,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:28,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:29,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:29,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:30,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:30,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:31,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:32,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:32,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:33,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:34,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:18<01:25, 14.32s/it][WARNING|generation_utils.py:914] 2023-08-29 01:05:35,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:35,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:36,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:36,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:37,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:38,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:38,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:39,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:40,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:41,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:41,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:42,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:43,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:44,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:44,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:45,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:45,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:46,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:47,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:47,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:48,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:48,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:32<01:11, 14.40s/it][WARNING|generation_utils.py:914] 2023-08-29 01:05:49,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:50,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:50,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:51,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:52,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:52,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:53,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:54,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:55,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:55,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:56,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:57,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:58,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:58,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:59,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:05:59,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:00,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:01,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:01,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:02,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:03,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:03,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:47<00:58, 14.57s/it][WARNING|generation_utils.py:914] 2023-08-29 01:06:04,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:05,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:06,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:06,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:07,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:08,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:08,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:09,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:10,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:10,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:11,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:12,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:12,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:13,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:14,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:15,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:15,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:16,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:17,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:17,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:18,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:19,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:03<00:44, 14.81s/it][WARNING|generation_utils.py:914] 2023-08-29 01:06:19,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:20,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:21,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:21,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:22,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:22,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:23,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:24,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:24,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:25,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:26,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:26,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:27,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:28,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:28,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:29,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:29,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:30,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:31,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:31,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:32,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [04:16<00:28, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-29 01:06:32,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:33,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:33,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:34,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:35,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:36,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:36,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:37,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:38,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:38,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:39,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:39,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:40,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:41,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:41,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:42,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:42,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:43,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:44,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:44,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:45,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:45,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:46,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:30<00:14, 14.28s/it][WARNING|generation_utils.py:914] 2023-08-29 01:06:47,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:47,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:48,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:49,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:49,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:50,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:51,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:51,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:52,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:53,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:53,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:54,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:55,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:55,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:56,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:57,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:57,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:58,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:58,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:06:59,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:43<00:00, 13.88s/it]Generating: 100%|| 20/20 [04:43<00:00, 14.17s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:08,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:08,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:08,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:08,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:08,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:07:09,103 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:07:09,104 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:07:09,822 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:07:10,907 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:07:10,907 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:13,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:13,737 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:13,737 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:13,737 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:13,737 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:07:14,498 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:07:14,499 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:07:15,220 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:07:15,392 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:07:15,392 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9241071428571429, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8579545454545454, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.80078125, 'errors': {'', "('Elizaveta Moreno de Mendoza', 'country of citizenship', '', 'Elizaveta Moreno - Fbregas became the fourth daughter of actor , actress , and director Elizaveta Moreno de Mendoza .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : field of work . Context : Later in the year , the University of New Brunswick partnered with the United States Postal Service to launch the First Postal Service Museum to showcase the first Postal Service mail collection . Head Entity : first Postal Service Museum , Tail Entity : United States .\n']
['Relation : field of work . Context : Later in the year , the University of New Brunswick partnered with the United States Postal Service to launch the First Postal Service Museum to showcase the first Postal Service mail collection . Head Entity : first Postal Service Museum , Tail Entity : United States .\n', 'Relation : field of work . Context : After he completed the Ph.D. in zoology at the University of Sheffield , he was assigned to the team that would place the first film of the season at the film festival in January . Head Entity : November in , Tail Entity : film festival .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : has part .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8464673913043478, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9107142857142857, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : record label . Context : Later in 2008 , he signed with The New Yorker magazine to produce " The Little Bites " , a follow up to 2010 " One Night Stand " , released by Columbia Pictures , titled " The Love Story " . Head Entity : Two Night Stand , Tail Entity : Columbia Pictures .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 13234
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13334, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.73it/s]Extractor Estimating: 2it [00:01,  1.67it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.64it/s]Extractor Estimating: 5it [00:02,  1.70it/s]Extractor Estimating: 6it [00:03,  1.70it/s]Extractor Estimating: 7it [00:04,  1.78it/s]Extractor Estimating: 8it [00:04,  1.74it/s]Extractor Estimating: 9it [00:05,  1.80it/s]Extractor Estimating: 10it [00:05,  1.78it/s]Extractor Estimating: 11it [00:06,  1.79it/s]Extractor Estimating: 12it [00:06,  1.72it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.71it/s]Extractor Estimating: 15it [00:08,  1.69it/s]Extractor Estimating: 16it [00:09,  1.62it/s]Extractor Estimating: 17it [00:10,  1.62it/s]Extractor Estimating: 18it [00:10,  1.60it/s]Extractor Estimating: 19it [00:11,  1.68it/s]Extractor Estimating: 20it [00:11,  1.73it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:12,  1.68it/s]Extractor Estimating: 23it [00:13,  1.70it/s]Extractor Estimating: 24it [00:14,  1.78it/s]Extractor Estimating: 25it [00:14,  1.78it/s]Extractor Estimating: 26it [00:15,  1.74it/s]Extractor Estimating: 27it [00:15,  1.72it/s]Extractor Estimating: 28it [00:16,  1.58it/s]Extractor Estimating: 29it [00:17,  1.63it/s]Extractor Estimating: 30it [00:17,  1.61it/s]Extractor Estimating: 31it [00:18,  1.65it/s]Extractor Estimating: 32it [00:19,  1.56it/s]Extractor Estimating: 33it [00:19,  1.60it/s]Extractor Estimating: 34it [00:20,  1.53it/s]Extractor Estimating: 35it [00:21,  1.55it/s]Extractor Estimating: 36it [00:21,  1.55it/s]Extractor Estimating: 37it [00:22,  1.62it/s]Extractor Estimating: 38it [00:22,  1.65it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:24,  1.63it/s]Extractor Estimating: 41it [00:24,  1.61it/s]Extractor Estimating: 42it [00:25,  1.64it/s]Extractor Estimating: 43it [00:25,  1.65it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:26,  1.70it/s]Extractor Estimating: 46it [00:27,  1.69it/s]Extractor Estimating: 47it [00:28,  1.69it/s]Extractor Estimating: 48it [00:28,  1.66it/s]Extractor Estimating: 49it [00:29,  1.64it/s]Extractor Estimating: 50it [00:30,  1.60it/s]Extractor Estimating: 51it [00:30,  1.61it/s]Extractor Estimating: 52it [00:31,  1.57it/s]Extractor Estimating: 53it [00:31,  1.59it/s]Extractor Estimating: 54it [00:32,  1.62it/s]Extractor Estimating: 55it [00:33,  1.66it/s]Extractor Estimating: 56it [00:33,  1.67it/s]Extractor Estimating: 57it [00:34,  1.63it/s]Extractor Estimating: 58it [00:35,  1.62it/s]Extractor Estimating: 59it [00:35,  1.65it/s]Extractor Estimating: 60it [00:36,  1.67it/s]Extractor Estimating: 61it [00:36,  1.61it/s]Extractor Estimating: 62it [00:37,  1.50it/s]Extractor Estimating: 63it [00:38,  1.54it/s]Extractor Estimating: 64it [00:38,  1.61it/s]Extractor Estimating: 65it [00:39,  1.69it/s]Extractor Estimating: 66it [00:40,  1.59it/s]Extractor Estimating: 67it [00:40,  1.61it/s]Extractor Estimating: 68it [00:41,  1.65it/s]Extractor Estimating: 69it [00:41,  1.68it/s]Extractor Estimating: 70it [00:42,  1.69it/s]Extractor Estimating: 71it [00:42,  1.69it/s]Extractor Estimating: 72it [00:43,  1.61it/s]Extractor Estimating: 73it [00:44,  1.63it/s]Extractor Estimating: 74it [00:44,  1.64it/s]Extractor Estimating: 75it [00:45,  1.68it/s]Extractor Estimating: 76it [00:46,  1.62it/s]Extractor Estimating: 77it [00:46,  1.61it/s]Extractor Estimating: 78it [00:47,  1.64it/s]Extractor Estimating: 79it [00:47,  1.58it/s]Extractor Estimating: 80it [00:48,  1.58it/s]Extractor Estimating: 81it [00:49,  1.56it/s]Extractor Estimating: 82it [00:49,  1.52it/s]Extractor Estimating: 83it [00:50,  1.54it/s]Extractor Estimating: 84it [00:51,  1.52it/s]Extractor Estimating: 85it [00:51,  1.54it/s]Extractor Estimating: 86it [00:52,  1.55it/s]Extractor Estimating: 87it [00:53,  1.57it/s]Extractor Estimating: 88it [00:53,  1.53it/s]Extractor Estimating: 89it [00:54,  1.51it/s]Extractor Estimating: 90it [00:55,  1.52it/s]Extractor Estimating: 91it [00:55,  1.51it/s]Extractor Estimating: 92it [00:56,  1.47it/s]Extractor Estimating: 93it [00:57,  1.40it/s]Extractor Estimating: 94it [00:57,  1.43it/s]Extractor Estimating: 95it [00:58,  1.51it/s]Extractor Estimating: 96it [00:59,  1.52it/s]Extractor Estimating: 97it [00:59,  1.56it/s]Extractor Estimating: 98it [01:00,  1.54it/s]Extractor Estimating: 99it [01:01,  1.55it/s]Extractor Estimating: 100it [01:01,  1.58it/s]Extractor Estimating: 101it [01:02,  1.59it/s]Extractor Estimating: 102it [01:02,  1.63it/s]Extractor Estimating: 103it [01:03,  1.61it/s]Extractor Estimating: 104it [01:04,  1.59it/s]Extractor Estimating: 105it [01:04,  1.68it/s]Extractor Estimating: 106it [01:05,  1.75it/s]Extractor Estimating: 107it [01:05,  1.74it/s]Extractor Estimating: 108it [01:06,  1.74it/s]Extractor Estimating: 109it [01:06,  1.74it/s]Extractor Estimating: 110it [01:07,  1.77it/s]Extractor Estimating: 111it [01:08,  1.53it/s]Extractor Estimating: 112it [01:08,  1.57it/s]Extractor Estimating: 113it [01:09,  1.61it/s]Extractor Estimating: 114it [01:10,  1.51it/s]Extractor Estimating: 115it [01:10,  1.51it/s]Extractor Estimating: 116it [01:11,  1.59it/s]Extractor Estimating: 117it [01:12,  1.55it/s]Extractor Estimating: 118it [01:12,  1.63it/s]Extractor Estimating: 119it [01:13,  1.60it/s]Extractor Estimating: 120it [01:13,  1.65it/s]Extractor Estimating: 121it [01:14,  1.61it/s]Extractor Estimating: 122it [01:15,  1.67it/s]Extractor Estimating: 123it [01:15,  1.60it/s]Extractor Estimating: 124it [01:16,  1.59it/s]Extractor Estimating: 125it [01:17,  1.61it/s]Extractor Estimating: 126it [01:17,  1.61it/s]Extractor Estimating: 127it [01:18,  1.59it/s]Extractor Estimating: 128it [01:19,  1.55it/s]Extractor Estimating: 129it [01:19,  1.52it/s]Extractor Estimating: 130it [01:20,  1.51it/s]Extractor Estimating: 131it [01:21,  1.55it/s]Extractor Estimating: 132it [01:21,  1.59it/s]Extractor Estimating: 133it [01:22,  1.59it/s]Extractor Estimating: 134it [01:22,  1.63it/s]Extractor Estimating: 135it [01:23,  1.66it/s]Extractor Estimating: 136it [01:24,  1.62it/s]Extractor Estimating: 137it [01:24,  1.61it/s]Extractor Estimating: 138it [01:25,  1.60it/s]Extractor Estimating: 139it [01:25,  1.59it/s]Extractor Estimating: 140it [01:26,  1.59it/s]Extractor Estimating: 141it [01:27,  1.61it/s]Extractor Estimating: 142it [01:27,  1.51it/s]Extractor Estimating: 143it [01:28,  1.50it/s]Extractor Estimating: 144it [01:29,  1.55it/s]Extractor Estimating: 145it [01:29,  1.59it/s]Extractor Estimating: 146it [01:30,  1.60it/s]Extractor Estimating: 147it [01:31,  1.54it/s]Extractor Estimating: 148it [01:31,  1.56it/s]Extractor Estimating: 149it [01:32,  1.60it/s]Extractor Estimating: 150it [01:33,  1.57it/s]Extractor Estimating: 151it [01:33,  1.65it/s]Extractor Estimating: 152it [01:34,  1.67it/s]Extractor Estimating: 153it [01:34,  1.71it/s]Extractor Estimating: 154it [01:35,  1.65it/s]Extractor Estimating: 155it [01:36,  1.54it/s]Extractor Estimating: 156it [01:36,  1.60it/s]Extractor Estimating: 157it [01:37,  1.60it/s]Extractor Estimating: 158it [01:37,  1.68it/s]Extractor Estimating: 159it [01:38,  1.67it/s]Extractor Estimating: 160it [01:39,  1.66it/s]Extractor Estimating: 161it [01:39,  1.67it/s]Extractor Estimating: 162it [01:40,  1.67it/s]Extractor Estimating: 163it [01:40,  1.69it/s]Extractor Estimating: 164it [01:41,  1.61it/s]Extractor Estimating: 165it [01:42,  1.64it/s]Extractor Estimating: 166it [01:42,  1.67it/s]Extractor Estimating: 167it [01:43,  1.68it/s]Extractor Estimating: 168it [01:43,  1.72it/s]Extractor Estimating: 169it [01:44,  1.67it/s]Extractor Estimating: 170it [01:45,  1.64it/s]Extractor Estimating: 171it [01:45,  1.72it/s]Extractor Estimating: 172it [01:46,  1.71it/s]Extractor Estimating: 173it [01:46,  1.73it/s]Extractor Estimating: 174it [01:47,  1.74it/s]Extractor Estimating: 175it [01:47,  1.78it/s]Extractor Estimating: 176it [01:48,  1.78it/s]Extractor Estimating: 177it [01:48,  1.79it/s]Extractor Estimating: 178it [01:49,  1.70it/s]Extractor Estimating: 179it [01:50,  1.71it/s]Extractor Estimating: 180it [01:50,  1.73it/s]Extractor Estimating: 181it [01:51,  1.63it/s]Extractor Estimating: 182it [01:52,  1.60it/s]Extractor Estimating: 183it [01:52,  1.58it/s]Extractor Estimating: 184it [01:53,  1.66it/s]Extractor Estimating: 185it [01:53,  1.66it/s]Extractor Estimating: 186it [01:54,  1.73it/s]Extractor Estimating: 187it [01:54,  1.74it/s]Extractor Estimating: 188it [01:55,  1.78it/s]Extractor Estimating: 189it [01:56,  1.75it/s]Extractor Estimating: 190it [01:56,  1.76it/s]Extractor Estimating: 191it [01:57,  1.73it/s]Extractor Estimating: 192it [01:57,  1.72it/s]Extractor Estimating: 193it [01:58,  1.77it/s]Extractor Estimating: 194it [01:58,  1.80it/s]Extractor Estimating: 195it [01:59,  1.71it/s]Extractor Estimating: 196it [02:00,  1.66it/s]Extractor Estimating: 197it [02:00,  1.70it/s]Extractor Estimating: 198it [02:01,  1.78it/s]Extractor Estimating: 199it [02:01,  1.72it/s]Extractor Estimating: 200it [02:02,  1.75it/s]Extractor Estimating: 201it [02:03,  1.72it/s]Extractor Estimating: 202it [02:03,  1.68it/s]Extractor Estimating: 203it [02:04,  1.61it/s]Extractor Estimating: 204it [02:05,  1.55it/s]Extractor Estimating: 205it [02:05,  1.53it/s]Extractor Estimating: 206it [02:06,  1.52it/s]Extractor Estimating: 207it [02:07,  1.48it/s]Extractor Estimating: 208it [02:07,  1.46it/s]Extractor Estimating: 209it [02:08,  1.36it/s]Extractor Estimating: 210it [02:09,  1.42it/s]Extractor Estimating: 211it [02:09,  1.50it/s]Extractor Estimating: 212it [02:10,  1.51it/s]Extractor Estimating: 213it [02:11,  1.54it/s]Extractor Estimating: 214it [02:11,  1.51it/s]Extractor Estimating: 215it [02:12,  1.48it/s]Extractor Estimating: 216it [02:13,  1.49it/s]Extractor Estimating: 217it [02:13,  1.51it/s]Extractor Estimating: 218it [02:14,  1.50it/s]Extractor Estimating: 219it [02:15,  1.48it/s]Extractor Estimating: 220it [02:15,  1.50it/s]Extractor Estimating: 221it [02:16,  1.51it/s]Extractor Estimating: 222it [02:17,  1.51it/s]Extractor Estimating: 223it [02:17,  1.53it/s]Extractor Estimating: 224it [02:18,  1.50it/s]Extractor Estimating: 225it [02:19,  1.53it/s]Extractor Estimating: 226it [02:19,  1.51it/s]Extractor Estimating: 227it [02:20,  1.49it/s]Extractor Estimating: 228it [02:21,  1.56it/s]Extractor Estimating: 229it [02:21,  1.58it/s]Extractor Estimating: 230it [02:22,  1.59it/s]Extractor Estimating: 231it [02:22,  1.55it/s]Extractor Estimating: 232it [02:23,  1.58it/s]Extractor Estimating: 233it [02:24,  1.53it/s]Extractor Estimating: 234it [02:24,  1.59it/s]Extractor Estimating: 235it [02:25,  1.59it/s]Extractor Estimating: 236it [02:25,  1.67it/s]Extractor Estimating: 237it [02:26,  1.53it/s]Extractor Estimating: 238it [02:27,  1.50it/s]Extractor Estimating: 239it [02:28,  1.53it/s]Extractor Estimating: 240it [02:28,  1.52it/s]Extractor Estimating: 241it [02:29,  1.51it/s]Extractor Estimating: 242it [02:30,  1.55it/s]Extractor Estimating: 243it [02:30,  1.57it/s]Extractor Estimating: 244it [02:31,  1.61it/s]Extractor Estimating: 245it [02:31,  1.64it/s]Extractor Estimating: 246it [02:32,  1.60it/s]Extractor Estimating: 247it [02:33,  1.56it/s]Extractor Estimating: 248it [02:33,  1.55it/s]Extractor Estimating: 249it [02:34,  1.61it/s]Extractor Estimating: 250it [02:34,  1.63it/s]Extractor Estimating: 251it [02:35,  1.59it/s]Extractor Estimating: 252it [02:36,  1.62it/s]Extractor Estimating: 253it [02:36,  1.60it/s]Extractor Estimating: 254it [02:37,  1.62it/s]Extractor Estimating: 255it [02:38,  1.60it/s]Extractor Estimating: 256it [02:38,  1.57it/s]Extractor Estimating: 257it [02:39,  1.60it/s]Extractor Estimating: 258it [02:40,  1.59it/s]Extractor Estimating: 259it [02:40,  1.56it/s]Extractor Estimating: 260it [02:41,  1.56it/s]Extractor Estimating: 261it [02:41,  1.59it/s]Extractor Estimating: 262it [02:42,  1.53it/s]Extractor Estimating: 263it [02:43,  1.55it/s]Extractor Estimating: 264it [02:43,  1.56it/s]Extractor Estimating: 265it [02:44,  1.53it/s]Extractor Estimating: 266it [02:45,  1.53it/s]Extractor Estimating: 267it [02:45,  1.51it/s]Extractor Estimating: 268it [02:46,  1.55it/s]Extractor Estimating: 269it [02:47,  1.56it/s]Extractor Estimating: 270it [02:47,  1.55it/s]Extractor Estimating: 271it [02:48,  1.54it/s]Extractor Estimating: 272it [02:49,  1.55it/s]Extractor Estimating: 273it [02:49,  1.52it/s]Extractor Estimating: 274it [02:50,  1.51it/s]Extractor Estimating: 275it [02:51,  1.52it/s]Extractor Estimating: 276it [02:51,  1.50it/s]Extractor Estimating: 277it [02:52,  1.53it/s]Extractor Estimating: 278it [02:53,  1.51it/s]Extractor Estimating: 279it [02:53,  1.51it/s]Extractor Estimating: 280it [02:54,  1.54it/s]Extractor Estimating: 281it [02:55,  1.52it/s]Extractor Estimating: 282it [02:55,  1.50it/s]Extractor Estimating: 283it [02:56,  1.55it/s]Extractor Estimating: 284it [02:56,  1.55it/s]Extractor Estimating: 285it [02:57,  1.54it/s]Extractor Estimating: 286it [02:58,  1.56it/s]Extractor Estimating: 287it [02:58,  1.61it/s]Extractor Estimating: 288it [02:59,  1.68it/s]Extractor Estimating: 289it [03:00,  1.66it/s]Extractor Estimating: 290it [03:00,  1.69it/s]Extractor Estimating: 291it [03:01,  1.64it/s]Extractor Estimating: 292it [03:02,  1.50it/s]Extractor Estimating: 293it [03:02,  1.53it/s]Extractor Estimating: 294it [03:03,  1.53it/s]Extractor Estimating: 295it [03:03,  1.55it/s]Extractor Estimating: 296it [03:04,  1.56it/s]Extractor Estimating: 297it [03:05,  1.58it/s]Extractor Estimating: 298it [03:05,  1.57it/s]Extractor Estimating: 299it [03:06,  1.55it/s]Extractor Estimating: 300it [03:07,  1.60it/s]Extractor Estimating: 301it [03:07,  1.67it/s]Extractor Estimating: 302it [03:08,  1.67it/s]Extractor Estimating: 303it [03:08,  1.69it/s]Extractor Estimating: 304it [03:09,  1.62it/s]Extractor Estimating: 305it [03:09,  1.70it/s]Extractor Estimating: 306it [03:10,  1.70it/s]Extractor Estimating: 307it [03:11,  1.73it/s]Extractor Estimating: 308it [03:11,  1.72it/s]Extractor Estimating: 309it [03:12,  1.72it/s]Extractor Estimating: 310it [03:12,  1.74it/s]Extractor Estimating: 311it [03:13,  1.68it/s]Extractor Estimating: 312it [03:14,  1.69it/s]Extractor Estimating: 313it [03:14,  1.71it/s]Extractor Estimating: 314it [03:15,  1.73it/s]Extractor Estimating: 315it [03:15,  1.67it/s]Extractor Estimating: 316it [03:16,  1.67it/s]Extractor Estimating: 317it [03:16,  1.72it/s]Extractor Estimating: 318it [03:17,  1.72it/s]Extractor Estimating: 319it [03:18,  1.76it/s]Extractor Estimating: 320it [03:18,  1.70it/s]Extractor Estimating: 321it [03:19,  1.73it/s]Extractor Estimating: 322it [03:19,  1.76it/s]Extractor Estimating: 323it [03:20,  1.71it/s]Extractor Estimating: 324it [03:21,  1.70it/s]Extractor Estimating: 325it [03:21,  1.66it/s]Extractor Estimating: 326it [03:22,  1.67it/s]Extractor Estimating: 327it [03:22,  1.67it/s]Extractor Estimating: 328it [03:23,  1.68it/s]Extractor Estimating: 329it [03:24,  1.72it/s]Extractor Estimating: 330it [03:24,  1.75it/s]Extractor Estimating: 331it [03:25,  1.75it/s]Extractor Estimating: 332it [03:25,  1.75it/s]Extractor Estimating: 333it [03:26,  1.78it/s]Extractor Estimating: 334it [03:26,  1.71it/s]Extractor Estimating: 335it [03:27,  1.74it/s]Extractor Estimating: 336it [03:27,  1.75it/s]Extractor Estimating: 337it [03:28,  1.73it/s]Extractor Estimating: 338it [03:29,  1.73it/s]Extractor Estimating: 339it [03:29,  1.71it/s]Extractor Estimating: 340it [03:30,  1.73it/s]Extractor Estimating: 341it [03:30,  1.74it/s]Extractor Estimating: 342it [03:31,  1.78it/s]Extractor Estimating: 343it [03:31,  1.79it/s]Extractor Estimating: 344it [03:32,  1.80it/s]Extractor Estimating: 345it [03:33,  1.71it/s]Extractor Estimating: 346it [03:33,  1.70it/s]Extractor Estimating: 347it [03:34,  1.73it/s]Extractor Estimating: 348it [03:34,  1.81it/s]Extractor Estimating: 349it [03:35,  1.78it/s]Extractor Estimating: 350it [03:36,  1.75it/s]Extractor Estimating: 351it [03:36,  1.72it/s]Extractor Estimating: 352it [03:37,  1.65it/s]Extractor Estimating: 353it [03:37,  1.70it/s]Extractor Estimating: 354it [03:38,  1.64it/s]Extractor Estimating: 355it [03:39,  1.60it/s]Extractor Estimating: 356it [03:39,  1.67it/s]Extractor Estimating: 357it [03:40,  1.64it/s]Extractor Estimating: 358it [03:40,  1.58it/s]Extractor Estimating: 359it [03:41,  1.57it/s]Extractor Estimating: 360it [03:42,  1.59it/s]Extractor Estimating: 361it [03:42,  1.62it/s]Extractor Estimating: 362it [03:43,  1.57it/s]Extractor Estimating: 363it [03:44,  1.55it/s]Extractor Estimating: 364it [03:44,  1.60it/s]Extractor Estimating: 365it [03:45,  1.56it/s]Extractor Estimating: 366it [03:46,  1.58it/s]Extractor Estimating: 367it [03:46,  1.59it/s]Extractor Estimating: 368it [03:47,  1.55it/s]Extractor Estimating: 369it [03:47,  1.58it/s]Extractor Estimating: 370it [03:48,  1.63it/s]Extractor Estimating: 371it [03:49,  1.60it/s]Extractor Estimating: 372it [03:49,  1.62it/s]Extractor Estimating: 373it [03:50,  1.63it/s]Extractor Estimating: 374it [03:51,  1.59it/s]Extractor Estimating: 375it [03:51,  1.61it/s]Extractor Estimating: 376it [03:52,  1.62it/s]Extractor Estimating: 377it [03:52,  1.62it/s]Extractor Estimating: 378it [03:53,  1.63it/s]Extractor Estimating: 379it [03:54,  1.62it/s]Extractor Estimating: 380it [03:54,  1.61it/s]Extractor Estimating: 381it [03:55,  1.41it/s]Extractor Estimating: 382it [03:56,  1.44it/s]Extractor Estimating: 383it [03:56,  1.51it/s]Extractor Estimating: 384it [03:57,  1.51it/s]Extractor Estimating: 385it [03:58,  1.52it/s]Extractor Estimating: 386it [03:58,  1.53it/s]Extractor Estimating: 387it [03:59,  1.47it/s]Extractor Estimating: 388it [04:00,  1.49it/s]Extractor Estimating: 389it [04:00,  1.49it/s]Extractor Estimating: 390it [04:01,  1.50it/s]Extractor Estimating: 391it [04:02,  1.52it/s]Extractor Estimating: 392it [04:02,  1.59it/s]Extractor Estimating: 393it [04:03,  1.62it/s]Extractor Estimating: 394it [04:04,  1.54it/s]Extractor Estimating: 395it [04:04,  1.50it/s]Extractor Estimating: 396it [04:05,  1.58it/s]Extractor Estimating: 397it [04:05,  1.59it/s]Extractor Estimating: 398it [04:06,  1.58it/s]Extractor Estimating: 399it [04:07,  1.56it/s]Extractor Estimating: 400it [04:07,  1.60it/s]Extractor Estimating: 401it [04:08,  1.60it/s]Extractor Estimating: 402it [04:09,  1.60it/s]Extractor Estimating: 403it [04:09,  1.57it/s]Extractor Estimating: 404it [04:10,  1.61it/s]Extractor Estimating: 405it [04:10,  1.64it/s]Extractor Estimating: 406it [04:11,  1.68it/s]Extractor Estimating: 407it [04:12,  1.67it/s]Extractor Estimating: 408it [04:12,  1.68it/s]Extractor Estimating: 409it [04:13,  1.66it/s]Extractor Estimating: 410it [04:14,  1.59it/s]Extractor Estimating: 411it [04:14,  1.65it/s]Extractor Estimating: 412it [04:15,  1.60it/s]Extractor Estimating: 413it [04:15,  1.63it/s]Extractor Estimating: 414it [04:16,  1.62it/s]Extractor Estimating: 415it [04:17,  1.60it/s]Extractor Estimating: 416it [04:17,  1.60it/s]Extractor Estimating: 417it [04:18,  1.63it/s]Extractor Estimating: 418it [04:18,  1.57it/s]Extractor Estimating: 419it [04:19,  1.58it/s]Extractor Estimating: 420it [04:20,  1.60it/s]Extractor Estimating: 421it [04:20,  1.58it/s]Extractor Estimating: 422it [04:21,  1.61it/s]Extractor Estimating: 423it [04:22,  1.61it/s]Extractor Estimating: 424it [04:22,  1.62it/s]Extractor Estimating: 425it [04:23,  1.52it/s]Extractor Estimating: 426it [04:24,  1.58it/s]Extractor Estimating: 427it [04:24,  1.60it/s]Extractor Estimating: 428it [04:25,  1.59it/s]Extractor Estimating: 429it [04:25,  1.62it/s]Extractor Estimating: 430it [04:26,  1.64it/s]Extractor Estimating: 431it [04:27,  1.64it/s]Extractor Estimating: 432it [04:27,  1.61it/s]Extractor Estimating: 433it [04:28,  1.65it/s]Extractor Estimating: 434it [04:28,  1.60it/s]Extractor Estimating: 435it [04:29,  1.65it/s]Extractor Estimating: 436it [04:30,  1.59it/s]Extractor Estimating: 437it [04:30,  1.53it/s]Extractor Estimating: 438it [04:31,  1.56it/s]Extractor Estimating: 439it [04:32,  1.52it/s]Extractor Estimating: 440it [04:32,  1.52it/s]Extractor Estimating: 441it [04:33,  1.54it/s]Extractor Estimating: 442it [04:34,  1.58it/s]Extractor Estimating: 443it [04:34,  1.58it/s]Extractor Estimating: 444it [04:35,  1.58it/s]Extractor Estimating: 445it [04:35,  1.62it/s]Extractor Estimating: 446it [04:36,  1.63it/s]Extractor Estimating: 447it [04:37,  1.65it/s]Extractor Estimating: 448it [04:37,  1.59it/s]Extractor Estimating: 449it [04:38,  1.56it/s]Extractor Estimating: 450it [04:39,  1.61it/s]Extractor Estimating: 451it [04:39,  1.63it/s]Extractor Estimating: 452it [04:40,  1.65it/s]Extractor Estimating: 453it [04:40,  1.55it/s]Extractor Estimating: 454it [04:41,  1.56it/s]Extractor Estimating: 455it [04:42,  1.61it/s]Extractor Estimating: 456it [04:42,  1.61it/s]Extractor Estimating: 457it [04:43,  1.70it/s]Extractor Estimating: 458it [04:43,  1.68it/s]Extractor Estimating: 459it [04:44,  1.74it/s]Extractor Estimating: 460it [04:45,  1.75it/s]Extractor Estimating: 461it [04:45,  1.72it/s]Extractor Estimating: 462it [04:46,  1.73it/s]Extractor Estimating: 463it [04:46,  1.66it/s]Extractor Estimating: 464it [04:47,  1.61it/s]Extractor Estimating: 465it [04:48,  1.73it/s]Extractor Estimating: 466it [04:48,  1.73it/s]Extractor Estimating: 467it [04:49,  1.79it/s]Extractor Estimating: 468it [04:49,  1.69it/s]Extractor Estimating: 469it [04:50,  1.53it/s]Extractor Estimating: 470it [04:51,  1.48it/s]Extractor Estimating: 471it [04:51,  1.54it/s]Extractor Estimating: 472it [04:52,  1.54it/s]Extractor Estimating: 473it [04:53,  1.62it/s]Extractor Estimating: 474it [04:53,  1.67it/s]Extractor Estimating: 475it [04:54,  1.63it/s]Extractor Estimating: 476it [04:54,  1.65it/s]Extractor Estimating: 477it [04:55,  1.56it/s]Extractor Estimating: 478it [04:56,  1.53it/s]Extractor Estimating: 479it [04:56,  1.48it/s]Extractor Estimating: 480it [04:57,  1.52it/s]Extractor Estimating: 481it [04:58,  1.49it/s]Extractor Estimating: 482it [04:58,  1.50it/s]Extractor Estimating: 483it [04:59,  1.49it/s]Extractor Estimating: 484it [05:00,  1.51it/s]Extractor Estimating: 485it [05:00,  1.49it/s]Extractor Estimating: 486it [05:01,  1.52it/s]Extractor Estimating: 487it [05:02,  1.55it/s]Extractor Estimating: 488it [05:03,  1.44it/s]Extractor Estimating: 489it [05:03,  1.45it/s]Extractor Estimating: 490it [05:04,  1.46it/s]Extractor Estimating: 491it [05:04,  1.51it/s]Extractor Estimating: 492it [05:05,  1.51it/s]Extractor Estimating: 493it [05:06,  1.47it/s]Extractor Estimating: 494it [05:06,  1.54it/s]Extractor Estimating: 495it [05:07,  1.57it/s]Extractor Estimating: 496it [05:08,  1.54it/s]Extractor Estimating: 497it [05:08,  1.52it/s]Extractor Estimating: 498it [05:09,  1.58it/s]Extractor Estimating: 499it [05:10,  1.58it/s]Extractor Estimating: 500it [05:10,  1.84it/s]Extractor Estimating: 500it [05:10,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:46,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:46,266 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:46,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:46,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:46,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:12:47,451 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:12:47,452 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:12:48,144 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:12:49,367 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:12:49,475 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:52,552 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:52,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:52,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:52,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:52,570 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:12:53,378 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:12:53,380 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:12:54,039 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:12:54,317 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:12:54,317 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 04:05:15,662 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 04:05:15,730 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9986 mean pseudo reward: 0.9481835875213824
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 25498
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25598, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25598, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.998, loss:665.0195
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.016, loss:648.9380
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.006, loss:624.7863
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.017, loss:634.3135
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.008, loss:580.3703
>> valid entity prec:0.5438, rec:0.6410, f1:0.5884
>> valid relation prec:0.0355, rec:0.0095, f1:0.0150
>> valid relation with NER prec:0.0355, rec:0.0095, f1:0.0150
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.287, loss:607.8963
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.006, loss:629.8142
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.012, loss:626.3240
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.002, loss:580.4953
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.007, loss:633.9056
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5668, rec:0.5683, f1:0.5675
>> valid relation prec:0.0188, rec:0.0046, f1:0.0074
>> valid relation with NER prec:0.0188, rec:0.0046, f1:0.0074
g_step 1100, step 266, avg_time 2.280, loss:639.1202
g_step 1200, step 366, avg_time 1.002, loss:629.8420
g_step 1300, step 49, avg_time 1.006, loss:618.5462
g_step 1400, step 149, avg_time 1.001, loss:607.6563
g_step 1500, step 249, avg_time 0.994, loss:612.1310
>> valid entity prec:0.5669, rec:0.5824, f1:0.5746
>> valid relation prec:0.0500, rec:0.0152, f1:0.0233
>> valid relation with NER prec:0.0500, rec:0.0152, f1:0.0233
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.285, loss:628.7422
g_step 1700, step 32, avg_time 0.992, loss:626.4324
g_step 1800, step 132, avg_time 0.997, loss:585.0763
g_step 1900, step 232, avg_time 1.005, loss:596.2299
g_step 2000, step 332, avg_time 1.011, loss:601.1996
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5554, rec:0.5256, f1:0.5401
>> valid relation prec:0.0472, rec:0.0118, f1:0.0188
>> valid relation with NER prec:0.0472, rec:0.0118, f1:0.0188
g_step 2100, step 15, avg_time 2.272, loss:581.8711
g_step 2200, step 115, avg_time 1.010, loss:539.3122
g_step 2300, step 215, avg_time 1.012, loss:575.7603
g_step 2400, step 315, avg_time 0.998, loss:553.0728
g_step 2500, step 415, avg_time 0.994, loss:589.1220
>> valid entity prec:0.5563, rec:0.5776, f1:0.5668
>> valid relation prec:0.0756, rec:0.0210, f1:0.0328
>> valid relation with NER prec:0.0756, rec:0.0210, f1:0.0328
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.259, loss:524.9939
g_step 2700, step 198, avg_time 1.008, loss:542.2021
g_step 2800, step 298, avg_time 1.001, loss:569.3835
g_step 2900, step 398, avg_time 1.004, loss:570.5458
g_step 3000, step 81, avg_time 0.998, loss:508.6683
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5785, rec:0.4957, f1:0.5339
>> valid relation prec:0.0515, rec:0.0132, f1:0.0210
>> valid relation with NER prec:0.0515, rec:0.0132, f1:0.0210
g_step 3100, step 181, avg_time 2.281, loss:514.7910
g_step 3200, step 281, avg_time 1.003, loss:527.7341
g_step 3300, step 381, avg_time 1.003, loss:529.5464
g_step 3400, step 64, avg_time 0.977, loss:507.9470
g_step 3500, step 164, avg_time 1.013, loss:497.2077
>> valid entity prec:0.4664, rec:0.5565, f1:0.5075
>> valid relation prec:0.0210, rec:0.0063, f1:0.0097
>> valid relation with NER prec:0.0210, rec:0.0063, f1:0.0097
g_step 3600, step 264, avg_time 2.264, loss:508.9907
g_step 3700, step 364, avg_time 1.011, loss:529.9830
g_step 3800, step 47, avg_time 1.005, loss:497.7905
g_step 3900, step 147, avg_time 1.004, loss:475.4418
g_step 4000, step 247, avg_time 1.020, loss:508.8967
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5229, rec:0.6284, f1:0.5708
>> valid relation prec:0.0424, rec:0.0158, f1:0.0230
>> valid relation with NER prec:0.0424, rec:0.0158, f1:0.0230
g_step 4100, step 347, avg_time 2.272, loss:493.1034
g_step 4200, step 30, avg_time 0.988, loss:489.6587
g_step 4300, step 130, avg_time 1.008, loss:447.0168
g_step 4400, step 230, avg_time 1.006, loss:479.8839
g_step 4500, step 330, avg_time 1.011, loss:488.5858
>> valid entity prec:0.5645, rec:0.5763, f1:0.5703
>> valid relation prec:0.0397, rec:0.0144, f1:0.0211
>> valid relation with NER prec:0.0397, rec:0.0144, f1:0.0211
g_step 4600, step 13, avg_time 2.259, loss:495.7521
g_step 4700, step 113, avg_time 0.996, loss:439.4482
g_step 4800, step 213, avg_time 1.007, loss:473.2999
g_step 4900, step 313, avg_time 1.012, loss:448.5371
g_step 5000, step 413, avg_time 1.001, loss:470.3815
learning rate was adjusted to 0.0008
>> valid entity prec:0.5273, rec:0.5795, f1:0.5522
>> valid relation prec:0.0348, rec:0.0146, f1:0.0206
>> valid relation with NER prec:0.0348, rec:0.0146, f1:0.0206
g_step 5100, step 96, avg_time 2.268, loss:412.0455
g_step 5200, step 196, avg_time 0.990, loss:427.5253
g_step 5300, step 296, avg_time 1.011, loss:450.9827
g_step 5400, step 396, avg_time 1.000, loss:461.0947
g_step 5500, step 79, avg_time 0.986, loss:420.7878
>> valid entity prec:0.5596, rec:0.5525, f1:0.5560
>> valid relation prec:0.0511, rec:0.0167, f1:0.0251
>> valid relation with NER prec:0.0511, rec:0.0167, f1:0.0251
g_step 5600, step 179, avg_time 2.280, loss:445.2549
g_step 5700, step 279, avg_time 1.006, loss:434.8861
g_step 5800, step 379, avg_time 1.007, loss:461.7992
g_step 5900, step 62, avg_time 0.985, loss:409.9928
g_step 6000, step 162, avg_time 1.010, loss:411.6712
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5528, rec:0.5524, f1:0.5526
>> valid relation prec:0.0389, rec:0.0123, f1:0.0188
>> valid relation with NER prec:0.0389, rec:0.0123, f1:0.0188
g_step 6100, step 262, avg_time 2.279, loss:426.0330
g_step 6200, step 362, avg_time 1.000, loss:406.5396
g_step 6300, step 45, avg_time 0.990, loss:403.4442
g_step 6400, step 145, avg_time 0.999, loss:394.7323
g_step 6500, step 245, avg_time 1.003, loss:422.5457
>> valid entity prec:0.5435, rec:0.5436, f1:0.5435
>> valid relation prec:0.0640, rec:0.0215, f1:0.0322
>> valid relation with NER prec:0.0640, rec:0.0215, f1:0.0322
g_step 6600, step 345, avg_time 2.264, loss:417.1539
g_step 6700, step 28, avg_time 1.002, loss:406.6395
g_step 6800, step 128, avg_time 0.985, loss:390.3583
g_step 6900, step 228, avg_time 0.968, loss:396.8348
g_step 7000, step 328, avg_time 0.964, loss:404.7385
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5694, rec:0.5843, f1:0.5767
>> valid relation prec:0.0487, rec:0.0204, f1:0.0287
>> valid relation with NER prec:0.0487, rec:0.0204, f1:0.0287
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 7100, step 11, avg_time 2.208, loss:398.7112
g_step 7200, step 111, avg_time 0.967, loss:369.7566
g_step 7300, step 211, avg_time 0.962, loss:374.4756
g_step 7400, step 311, avg_time 0.972, loss:380.8422
g_step 7500, step 411, avg_time 0.984, loss:392.5764
>> valid entity prec:0.5785, rec:0.5055, f1:0.5396
>> valid relation prec:0.0551, rec:0.0158, f1:0.0245
>> valid relation with NER prec:0.0551, rec:0.0158, f1:0.0245
g_step 7600, step 94, avg_time 2.208, loss:350.3129
g_step 7700, step 194, avg_time 0.965, loss:364.8730
g_step 7800, step 294, avg_time 0.986, loss:381.9117
g_step 7900, step 394, avg_time 0.959, loss:393.0114
g_step 8000, step 77, avg_time 0.954, loss:360.3752
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5469, rec:0.5360, f1:0.5414
>> valid relation prec:0.0475, rec:0.0169, f1:0.0250
>> valid relation with NER prec:0.0475, rec:0.0169, f1:0.0250
g_step 8100, step 177, avg_time 2.206, loss:344.6237
g_step 8200, step 277, avg_time 0.977, loss:350.4020
g_step 8300, step 377, avg_time 0.977, loss:372.4614
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:05:15 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:05:15 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-05-15_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:05:16 - WARNING - datasets.builder -   Using custom data configuration default-a43c9457f4941d4c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a43c9457f4941d4c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:05:19,834 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:05:19,875 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:05:19,875 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:05:19,876 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:05:20,037 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:05:20,122 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:05:20,122 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:05:20,122 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:05:20,122 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:05:20,122 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:05:20,122 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:05:20,671 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:05:23,799 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:05:23,854 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a43c9457f4941d4c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.63ba/s] 20%|        | 2/10 [00:00<00:02,  3.63ba/s] 30%|       | 3/10 [00:00<00:01,  4.12ba/s] 40%|      | 4/10 [00:00<00:01,  4.36ba/s] 50%|     | 5/10 [00:01<00:01,  4.48ba/s] 60%|    | 6/10 [00:01<00:00,  4.60ba/s] 70%|   | 7/10 [00:01<00:00,  4.66ba/s] 80%|  | 8/10 [00:01<00:00,  4.71ba/s] 90%| | 9/10 [00:02<00:00,  4.76ba/s]100%|| 10/10 [00:02<00:00,  4.78ba/s]100%|| 10/10 [00:02<00:00,  4.46ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.34ba/s] 50%|     | 2/4 [00:00<00:00,  3.25ba/s] 75%|  | 3/4 [00:00<00:00,  3.58ba/s]100%|| 4/4 [00:01<00:00,  4.71ba/s]100%|| 4/4 [00:01<00:00,  3.97ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.52ba/s] 30%|       | 3/10 [00:00<00:00,  8.96ba/s] 50%|     | 5/10 [00:00<00:00, 10.16ba/s] 70%|   | 7/10 [00:00<00:00, 10.72ba/s] 90%| | 9/10 [00:00<00:00, 10.94ba/s]100%|| 10/10 [00:00<00:00, 10.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.98ba/s] 75%|  | 3/4 [00:00<00:00,  9.33ba/s]100%|| 4/4 [00:00<00:00, 10.40ba/s]
[INFO|trainer.py:414] 2023-08-29 04:05:29,477 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:05:29,555 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:05:29,555 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 04:05:29,555 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:05:29,555 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:05:29,555 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:05:29,555 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:05:29,555 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:53,  3.33it/s]  0%|          | 2/780 [00:00<03:48,  3.41it/s]  0%|          | 3/780 [00:00<03:56,  3.29it/s]  1%|          | 4/780 [00:01<03:52,  3.34it/s]  1%|          | 5/780 [00:01<03:50,  3.37it/s]  1%|          | 6/780 [00:01<03:48,  3.38it/s]  1%|          | 7/780 [00:02<03:47,  3.39it/s]  1%|          | 8/780 [00:02<03:47,  3.40it/s]  1%|          | 9/780 [00:02<03:46,  3.40it/s]  1%|         | 10/780 [00:02<03:45,  3.41it/s]  1%|         | 11/780 [00:03<03:45,  3.41it/s]  2%|         | 12/780 [00:03<03:45,  3.41it/s]  2%|         | 13/780 [00:03<03:44,  3.42it/s]  2%|         | 14/780 [00:04<03:50,  3.33it/s]  2%|         | 15/780 [00:04<03:48,  3.35it/s]  2%|         | 16/780 [00:04<03:46,  3.37it/s]  2%|         | 17/780 [00:05<03:45,  3.38it/s]  2%|         | 18/780 [00:05<03:44,  3.39it/s]  2%|         | 19/780 [00:05<03:43,  3.40it/s]  3%|         | 20/780 [00:05<03:43,  3.40it/s]  3%|         | 21/780 [00:06<03:42,  3.41it/s]  3%|         | 22/780 [00:06<03:42,  3.41it/s]  3%|         | 23/780 [00:06<03:41,  3.41it/s]  3%|         | 24/780 [00:07<03:41,  3.41it/s]  3%|         | 25/780 [00:07<03:46,  3.34it/s]  3%|         | 26/780 [00:07<03:44,  3.36it/s]  3%|         | 27/780 [00:07<03:43,  3.37it/s]  4%|         | 28/780 [00:08<03:42,  3.39it/s]  4%|         | 29/780 [00:08<03:41,  3.39it/s]  4%|         | 30/780 [00:08<03:40,  3.40it/s]  4%|         | 31/780 [00:09<03:40,  3.40it/s]  4%|         | 32/780 [00:09<03:39,  3.41it/s]  4%|         | 33/780 [00:09<03:39,  3.41it/s]  4%|         | 34/780 [00:10<03:38,  3.41it/s]  4%|         | 35/780 [00:10<03:38,  3.41it/s]  5%|         | 36/780 [00:10<03:43,  3.33it/s]  5%|         | 37/780 [00:10<03:41,  3.35it/s]  5%|         | 38/780 [00:11<03:40,  3.37it/s]  5%|         | 39/780 [00:11<03:39,  3.38it/s]  5%|         | 40/780 [00:11<03:38,  3.39it/s]  5%|         | 41/780 [00:12<03:37,  3.40it/s]  5%|         | 42/780 [00:12<03:36,  3.40it/s]  6%|         | 43/780 [00:12<03:36,  3.41it/s]  6%|         | 44/780 [00:12<03:35,  3.41it/s]  6%|         | 45/780 [00:13<03:35,  3.41it/s]  6%|         | 46/780 [00:13<03:35,  3.41it/s]  6%|         | 47/780 [00:13<03:39,  3.34it/s]  6%|         | 48/780 [00:14<03:37,  3.36it/s]  6%|         | 49/780 [00:14<03:36,  3.38it/s]  6%|         | 50/780 [00:14<03:35,  3.39it/s]  7%|         | 51/780 [00:15<03:34,  3.40it/s]  7%|         | 52/780 [00:15<03:33,  3.40it/s]  7%|         | 53/780 [00:15<03:33,  3.40it/s]  7%|         | 54/780 [00:15<03:33,  3.41it/s]  7%|         | 55/780 [00:16<03:32,  3.41it/s]  7%|         | 56/780 [00:16<03:32,  3.41it/s]  7%|         | 57/780 [00:16<03:31,  3.41it/s]  7%|         | 58/780 [00:17<03:39,  3.28it/s]  8%|         | 59/780 [00:17<03:37,  3.32it/s]  8%|         | 60/780 [00:17<03:35,  3.35it/s]  8%|         | 61/780 [00:18<03:33,  3.37it/s]  8%|         | 62/780 [00:18<03:32,  3.38it/s]  8%|         | 63/780 [00:18<03:31,  3.39it/s]  8%|         | 64/780 [00:18<03:30,  3.39it/s]  8%|         | 65/780 [00:19<03:30,  3.40it/s]  8%|         | 66/780 [00:19<03:29,  3.40it/s]  9%|         | 67/780 [00:19<03:29,  3.41it/s]  9%|         | 68/780 [00:20<03:28,  3.41it/s]  9%|         | 69/780 [00:20<03:28,  3.41it/s]  9%|         | 70/780 [00:20<03:28,  3.41it/s]  9%|         | 71/780 [00:20<03:27,  3.41it/s]  9%|         | 72/780 [00:21<03:27,  3.41it/s]  9%|         | 73/780 [00:21<03:33,  3.31it/s]  9%|         | 74/780 [00:21<03:31,  3.34it/s] 10%|         | 75/780 [00:22<03:30,  3.35it/s] 10%|         | 76/780 [00:22<03:29,  3.37it/s] 10%|         | 77/780 [00:22<03:27,  3.38it/s] 10%|         | 78/780 [00:23<03:27,  3.39it/s] 10%|         | 79/780 [00:23<03:26,  3.40it/s] 10%|         | 80/780 [00:23<03:25,  3.40it/s] 10%|         | 81/780 [00:23<03:25,  3.41it/s] 11%|         | 82/780 [00:24<03:24,  3.41it/s] 11%|         | 83/780 [00:24<03:24,  3.41it/s] 11%|         | 84/780 [00:24<03:30,  3.31it/s] 11%|         | 85/780 [00:25<03:28,  3.34it/s] 11%|         | 86/780 [00:25<03:26,  3.36it/s] 11%|         | 87/780 [00:25<03:25,  3.37it/s] 11%|        | 88/780 [00:26<03:24,  3.38it/s] 11%|        | 89/780 [00:26<03:24,  3.38it/s] 12%|        | 90/780 [00:26<03:23,  3.39it/s] 12%|        | 91/780 [00:26<03:23,  3.39it/s] 12%|        | 92/780 [00:27<03:22,  3.40it/s] 12%|        | 93/780 [00:27<03:22,  3.40it/s] 12%|        | 94/780 [00:27<03:21,  3.41it/s] 12%|        | 95/780 [00:28<03:30,  3.25it/s] 12%|        | 96/780 [00:28<03:27,  3.29it/s] 12%|        | 97/780 [00:28<03:25,  3.33it/s] 13%|        | 98/780 [00:28<03:23,  3.35it/s] 13%|        | 99/780 [00:29<03:22,  3.37it/s] 13%|        | 100/780 [00:29<03:21,  3.38it/s] 13%|        | 101/780 [00:29<03:20,  3.39it/s] 13%|        | 102/780 [00:30<03:19,  3.39it/s] 13%|        | 103/780 [00:30<03:19,  3.40it/s] 13%|        | 104/780 [00:30<03:18,  3.40it/s] 13%|        | 105/780 [00:31<03:18,  3.40it/s] 14%|        | 106/780 [00:31<03:24,  3.30it/s] 14%|        | 107/780 [00:31<03:21,  3.33it/s] 14%|        | 108/780 [00:31<03:20,  3.35it/s] 14%|        | 109/780 [00:32<03:18,  3.37it/s] 14%|        | 110/780 [00:32<03:18,  3.38it/s] 14%|        | 111/780 [00:32<03:17,  3.39it/s] 14%|        | 112/780 [00:33<03:17,  3.39it/s] 14%|        | 113/780 [00:33<03:16,  3.40it/s] 15%|        | 114/780 [00:33<03:16,  3.40it/s] 15%|        | 115/780 [00:34<03:15,  3.40it/s] 15%|        | 116/780 [00:34<03:15,  3.40it/s] 15%|        | 117/780 [00:34<03:20,  3.31it/s] 15%|        | 118/780 [00:34<03:18,  3.34it/s] 15%|        | 119/780 [00:35<03:16,  3.36it/s] 15%|        | 120/780 [00:35<03:15,  3.37it/s] 16%|        | 121/780 [00:35<03:14,  3.38it/s] 16%|        | 122/780 [00:36<03:14,  3.39it/s] 16%|        | 123/780 [00:36<03:13,  3.40it/s] 16%|        | 124/780 [00:36<03:13,  3.40it/s] 16%|        | 125/780 [00:36<03:12,  3.40it/s] 16%|        | 126/780 [00:37<03:12,  3.40it/s] 16%|        | 127/780 [00:37<03:11,  3.40it/s] 16%|        | 128/780 [00:37<03:15,  3.34it/s] 17%|        | 129/780 [00:38<03:14,  3.35it/s] 17%|        | 130/780 [00:38<03:12,  3.37it/s] 17%|        | 131/780 [00:38<03:12,  3.38it/s] 17%|        | 132/780 [00:39<03:11,  3.39it/s] 17%|        | 133/780 [00:39<03:11,  3.38it/s] 17%|        | 134/780 [00:39<03:10,  3.39it/s] 17%|        | 135/780 [00:39<03:10,  3.39it/s] 17%|        | 136/780 [00:40<03:09,  3.40it/s] 18%|        | 137/780 [00:40<03:09,  3.40it/s] 18%|        | 138/780 [00:40<03:08,  3.40it/s] 18%|        | 139/780 [00:41<03:10,  3.36it/s] 18%|        | 140/780 [00:41<03:09,  3.37it/s] 18%|        | 141/780 [00:41<03:08,  3.38it/s] 18%|        | 142/780 [00:42<03:08,  3.39it/s] 18%|        | 143/780 [00:42<03:07,  3.39it/s] 18%|        | 144/780 [00:42<03:07,  3.40it/s] 19%|        | 145/780 [00:42<03:06,  3.40it/s] 19%|        | 146/780 [00:43<03:06,  3.40it/s] 19%|        | 147/780 [00:43<03:06,  3.40it/s] 19%|        | 148/780 [00:43<03:05,  3.40it/s] 19%|        | 149/780 [00:44<03:05,  3.40it/s] 19%|        | 150/780 [00:44<03:12,  3.27it/s] 19%|        | 151/780 [00:44<03:09,  3.31it/s] 19%|        | 152/780 [00:44<03:08,  3.34it/s] 20%|        | 153/780 [00:45<03:06,  3.36it/s] 20%|        | 154/780 [00:45<03:05,  3.37it/s] 20%|        | 155/780 [00:45<03:04,  3.38it/s] 20%|        | 156/780 [00:46<03:04,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 04:06:15,752 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:06:15,752 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 04:06:15,752 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.66it/s][A
  3%|         | 12/436 [00:00<00:08, 48.52it/s][A
  4%|         | 17/436 [00:00<00:08, 46.98it/s][A
  5%|         | 22/436 [00:00<00:08, 46.05it/s][A
  6%|         | 27/436 [00:00<00:09, 45.42it/s][A
  7%|         | 32/436 [00:00<00:08, 45.19it/s][A
  8%|         | 37/436 [00:00<00:08, 44.95it/s][A
 10%|         | 42/436 [00:00<00:08, 44.83it/s][A
 11%|         | 47/436 [00:01<00:09, 41.12it/s][A
 12%|        | 52/436 [00:01<00:09, 42.30it/s][A
 13%|        | 57/436 [00:01<00:08, 43.22it/s][A
 14%|        | 62/436 [00:01<00:08, 43.90it/s][A
 15%|        | 67/436 [00:01<00:08, 44.20it/s][A
 17%|        | 72/436 [00:01<00:08, 44.46it/s][A
 18%|        | 77/436 [00:01<00:08, 44.50it/s][A
 19%|        | 82/436 [00:01<00:07, 44.44it/s][A
 20%|        | 87/436 [00:01<00:07, 44.18it/s][A
 21%|        | 92/436 [00:02<00:07, 44.23it/s][A
 22%|       | 97/436 [00:02<00:07, 44.42it/s][A
 23%|       | 102/436 [00:02<00:07, 44.69it/s][A
 25%|       | 107/436 [00:02<00:07, 44.80it/s][A
 26%|       | 112/436 [00:02<00:07, 45.00it/s][A
 27%|       | 117/436 [00:02<00:07, 45.03it/s][A
 28%|       | 122/436 [00:02<00:06, 44.87it/s][A
 29%|       | 127/436 [00:02<00:07, 41.23it/s][A
 30%|       | 132/436 [00:02<00:07, 42.35it/s][A
 31%|      | 137/436 [00:03<00:06, 43.08it/s][A
 33%|      | 142/436 [00:03<00:06, 43.59it/s][A
 34%|      | 147/436 [00:03<00:06, 44.08it/s][A
 35%|      | 152/436 [00:03<00:06, 44.41it/s][A
 36%|      | 157/436 [00:03<00:06, 44.63it/s][A
 37%|      | 162/436 [00:03<00:06, 44.70it/s][A
 38%|      | 167/436 [00:03<00:06, 44.31it/s][A
 39%|      | 172/436 [00:03<00:05, 44.23it/s][A
 41%|      | 177/436 [00:03<00:05, 44.51it/s][A
 42%|     | 182/436 [00:04<00:05, 44.60it/s][A
 43%|     | 187/436 [00:04<00:05, 44.80it/s][A
 44%|     | 192/436 [00:04<00:05, 44.92it/s][A
 45%|     | 197/436 [00:04<00:05, 45.00it/s][A
 46%|     | 202/436 [00:04<00:05, 44.91it/s][A
 47%|     | 207/436 [00:04<00:05, 44.75it/s][A
 49%|     | 212/436 [00:04<00:05, 44.56it/s][A
 50%|     | 217/436 [00:04<00:04, 44.49it/s][A
 51%|     | 222/436 [00:04<00:04, 44.55it/s][A
 52%|    | 227/436 [00:05<00:04, 44.64it/s][A
 53%|    | 232/436 [00:05<00:04, 44.84it/s][A
 54%|    | 237/436 [00:05<00:04, 44.92it/s][A
 56%|    | 242/436 [00:05<00:04, 45.00it/s][A
 57%|    | 247/436 [00:05<00:04, 40.08it/s][A
 58%|    | 253/436 [00:05<00:04, 43.19it/s][A
 59%|    | 258/436 [00:05<00:04, 40.76it/s][A
 60%|    | 263/436 [00:05<00:04, 42.06it/s][A
 61%|   | 268/436 [00:06<00:03, 42.88it/s][A
 63%|   | 273/436 [00:06<00:03, 43.61it/s][A
 64%|   | 278/436 [00:06<00:03, 44.10it/s][A
 65%|   | 283/436 [00:06<00:03, 44.40it/s][A
 66%|   | 288/436 [00:06<00:03, 44.29it/s][A
 67%|   | 293/436 [00:06<00:03, 44.14it/s][A
 68%|   | 298/436 [00:06<00:03, 44.07it/s][A
 69%|   | 303/436 [00:06<00:03, 44.26it/s][A
 71%|   | 308/436 [00:06<00:02, 44.51it/s][A
 72%|  | 313/436 [00:07<00:02, 44.66it/s][A
 73%|  | 318/436 [00:07<00:02, 44.81it/s][A
 74%|  | 323/436 [00:07<00:02, 44.94it/s][A
 75%|  | 328/436 [00:07<00:02, 45.00it/s][A
 76%|  | 333/436 [00:07<00:02, 44.87it/s][A
 78%|  | 338/436 [00:07<00:02, 44.54it/s][A
 79%|  | 343/436 [00:07<00:02, 44.38it/s][A
 80%|  | 348/436 [00:07<00:01, 44.41it/s][A
 81%|  | 353/436 [00:07<00:01, 44.58it/s][A
 82%| | 358/436 [00:08<00:01, 44.81it/s][A
 83%| | 363/436 [00:08<00:01, 44.87it/s][A
 84%| | 368/436 [00:08<00:01, 45.03it/s][A
 86%| | 373/436 [00:08<00:01, 44.84it/s][A
 87%| | 378/436 [00:08<00:01, 44.79it/s][A
 88%| | 383/436 [00:08<00:01, 44.68it/s][A
 89%| | 388/436 [00:08<00:01, 44.56it/s][A
 90%| | 393/436 [00:08<00:00, 43.36it/s][A
 91%|| 398/436 [00:08<00:00, 43.88it/s][A
 92%|| 403/436 [00:09<00:00, 44.27it/s][A
 94%|| 408/436 [00:09<00:00, 44.58it/s][A
 95%|| 413/436 [00:09<00:00, 44.70it/s][A
 96%|| 418/436 [00:09<00:00, 44.78it/s][A
 97%|| 423/436 [00:09<00:00, 44.68it/s][A
 98%|| 428/436 [00:09<00:00, 44.56it/s][A
 99%|| 433/436 [00:09<00:00, 44.34it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.34it/s][A 20%|        | 156/780 [00:56<03:04,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:06:25,825 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 04:06:25,980 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:06:28,666 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:06:28,811 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:06:28,901 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:06<1:04:48,  6.24s/it] 20%|        | 158/780 [01:06<46:14,  4.46s/it]   20%|        | 159/780 [01:06<33:13,  3.21s/it] 21%|        | 160/780 [01:07<24:08,  2.34s/it] 21%|        | 161/780 [01:07<17:46,  1.72s/it] 21%|        | 162/780 [01:07<13:19,  1.29s/it] 21%|        | 163/780 [01:08<10:13,  1.01it/s] 21%|        | 164/780 [01:08<08:02,  1.28it/s] 21%|        | 165/780 [01:08<06:31,  1.57it/s] 21%|       | 166/780 [01:08<05:27,  1.87it/s] 21%|       | 167/780 [01:09<04:42,  2.17it/s] 22%|       | 168/780 [01:09<04:11,  2.43it/s] 22%|       | 169/780 [01:09<03:55,  2.60it/s] 22%|       | 170/780 [01:10<03:38,  2.80it/s] 22%|       | 171/780 [01:10<03:26,  2.96it/s] 22%|       | 172/780 [01:10<03:17,  3.08it/s] 22%|       | 173/780 [01:11<03:11,  3.17it/s] 22%|       | 174/780 [01:11<03:07,  3.24it/s] 22%|       | 175/780 [01:11<03:03,  3.29it/s] 23%|       | 176/780 [01:11<03:01,  3.33it/s] 23%|       | 177/780 [01:12<02:58,  3.37it/s] 23%|       | 178/780 [01:12<02:57,  3.40it/s] 23%|       | 179/780 [01:12<02:55,  3.42it/s] 23%|       | 180/780 [01:13<02:58,  3.36it/s] 23%|       | 181/780 [01:13<02:56,  3.39it/s] 23%|       | 182/780 [01:13<02:55,  3.41it/s] 23%|       | 183/780 [01:13<02:54,  3.43it/s] 24%|       | 184/780 [01:14<02:53,  3.44it/s] 24%|       | 185/780 [01:14<02:52,  3.45it/s] 24%|       | 186/780 [01:14<02:52,  3.45it/s] 24%|       | 187/780 [01:15<02:51,  3.46it/s] 24%|       | 188/780 [01:15<02:51,  3.46it/s] 24%|       | 189/780 [01:15<02:50,  3.46it/s] 24%|       | 190/780 [01:15<02:50,  3.46it/s] 24%|       | 191/780 [01:16<02:55,  3.36it/s] 25%|       | 192/780 [01:16<02:53,  3.39it/s] 25%|       | 193/780 [01:16<02:52,  3.41it/s] 25%|       | 194/780 [01:17<02:50,  3.43it/s] 25%|       | 195/780 [01:17<02:50,  3.44it/s] 25%|       | 196/780 [01:17<02:49,  3.44it/s] 25%|       | 197/780 [01:17<02:49,  3.45it/s] 25%|       | 198/780 [01:18<02:48,  3.45it/s] 26%|       | 199/780 [01:18<02:48,  3.46it/s] 26%|       | 200/780 [01:18<02:47,  3.46it/s] 26%|       | 201/780 [01:19<02:47,  3.46it/s] 26%|       | 202/780 [01:19<02:50,  3.40it/s] 26%|       | 203/780 [01:19<02:48,  3.42it/s] 26%|       | 204/780 [01:20<02:47,  3.43it/s] 26%|       | 205/780 [01:20<02:47,  3.44it/s] 26%|       | 206/780 [01:20<02:46,  3.45it/s] 27%|       | 207/780 [01:20<02:45,  3.45it/s] 27%|       | 208/780 [01:21<02:45,  3.46it/s] 27%|       | 209/780 [01:21<02:44,  3.46it/s] 27%|       | 210/780 [01:21<02:44,  3.46it/s] 27%|       | 211/780 [01:22<02:44,  3.46it/s] 27%|       | 212/780 [01:22<02:44,  3.46it/s] 27%|       | 213/780 [01:22<02:43,  3.46it/s] 27%|       | 214/780 [01:22<02:43,  3.46it/s] 28%|       | 215/780 [01:23<02:46,  3.39it/s] 28%|       | 216/780 [01:23<02:45,  3.41it/s] 28%|       | 217/780 [01:23<02:44,  3.43it/s] 28%|       | 218/780 [01:24<02:43,  3.44it/s] 28%|       | 219/780 [01:24<02:42,  3.45it/s] 28%|       | 220/780 [01:24<02:42,  3.45it/s] 28%|       | 221/780 [01:24<02:41,  3.45it/s] 28%|       | 222/780 [01:25<02:41,  3.46it/s] 29%|       | 223/780 [01:25<02:41,  3.46it/s] 29%|       | 224/780 [01:25<02:40,  3.46it/s] 29%|       | 225/780 [01:26<02:40,  3.46it/s] 29%|       | 226/780 [01:26<02:44,  3.37it/s] 29%|       | 227/780 [01:26<02:42,  3.40it/s] 29%|       | 228/780 [01:27<02:41,  3.42it/s] 29%|       | 229/780 [01:27<02:40,  3.43it/s] 29%|       | 230/780 [01:27<02:39,  3.44it/s] 30%|       | 231/780 [01:27<02:39,  3.45it/s] 30%|       | 232/780 [01:28<02:38,  3.45it/s] 30%|       | 233/780 [01:28<02:38,  3.45it/s] 30%|       | 234/780 [01:28<02:37,  3.46it/s] 30%|       | 235/780 [01:29<02:37,  3.45it/s] 30%|       | 236/780 [01:29<02:37,  3.45it/s] 30%|       | 237/780 [01:29<02:41,  3.37it/s] 31%|       | 238/780 [01:29<02:39,  3.40it/s] 31%|       | 239/780 [01:30<02:38,  3.42it/s] 31%|       | 240/780 [01:30<02:37,  3.43it/s] 31%|       | 241/780 [01:30<02:36,  3.44it/s] 31%|       | 242/780 [01:31<02:36,  3.45it/s] 31%|       | 243/780 [01:31<02:35,  3.45it/s] 31%|      | 244/780 [01:31<02:35,  3.45it/s] 31%|      | 245/780 [01:31<02:34,  3.46it/s] 32%|      | 246/780 [01:32<02:34,  3.45it/s] 32%|      | 247/780 [01:32<02:34,  3.46it/s] 32%|      | 248/780 [01:32<02:38,  3.36it/s] 32%|      | 249/780 [01:33<02:36,  3.38it/s] 32%|      | 250/780 [01:33<02:35,  3.40it/s] 32%|      | 251/780 [01:33<02:34,  3.42it/s] 32%|      | 252/780 [01:34<02:33,  3.43it/s] 32%|      | 253/780 [01:34<02:33,  3.44it/s] 33%|      | 254/780 [01:34<02:32,  3.45it/s] 33%|      | 255/780 [01:34<02:31,  3.46it/s] 33%|      | 256/780 [01:35<02:31,  3.45it/s] 33%|      | 257/780 [01:35<02:31,  3.46it/s] 33%|      | 258/780 [01:35<02:30,  3.46it/s] 33%|      | 259/780 [01:36<02:39,  3.27it/s] 33%|      | 260/780 [01:36<02:36,  3.33it/s] 33%|      | 261/780 [01:36<02:34,  3.37it/s] 34%|      | 262/780 [01:36<02:32,  3.40it/s] 34%|      | 263/780 [01:37<02:31,  3.41it/s] 34%|      | 264/780 [01:37<02:30,  3.43it/s] 34%|      | 265/780 [01:37<02:29,  3.44it/s] 34%|      | 266/780 [01:38<02:29,  3.45it/s] 34%|      | 267/780 [01:38<02:28,  3.45it/s] 34%|      | 268/780 [01:38<02:28,  3.45it/s] 34%|      | 269/780 [01:38<02:27,  3.45it/s] 35%|      | 270/780 [01:39<02:32,  3.35it/s] 35%|      | 271/780 [01:39<02:30,  3.38it/s] 35%|      | 272/780 [01:39<02:29,  3.41it/s] 35%|      | 273/780 [01:40<02:28,  3.42it/s] 35%|      | 274/780 [01:40<02:27,  3.43it/s] 35%|      | 275/780 [01:40<02:26,  3.44it/s] 35%|      | 276/780 [01:41<02:26,  3.45it/s] 36%|      | 277/780 [01:41<02:25,  3.45it/s] 36%|      | 278/780 [01:41<02:25,  3.45it/s] 36%|      | 279/780 [01:41<02:25,  3.45it/s] 36%|      | 280/780 [01:42<02:24,  3.46it/s] 36%|      | 281/780 [01:42<02:28,  3.35it/s] 36%|      | 282/780 [01:42<02:27,  3.38it/s] 36%|      | 283/780 [01:43<02:25,  3.41it/s] 36%|      | 284/780 [01:43<02:24,  3.42it/s] 37%|      | 285/780 [01:43<02:24,  3.43it/s] 37%|      | 286/780 [01:43<02:23,  3.44it/s] 37%|      | 287/780 [01:44<02:23,  3.44it/s] 37%|      | 288/780 [01:44<02:22,  3.45it/s] 37%|      | 289/780 [01:44<02:22,  3.45it/s] 37%|      | 290/780 [01:45<02:21,  3.46it/s] 37%|      | 291/780 [01:45<02:21,  3.46it/s] 37%|      | 292/780 [01:45<02:23,  3.41it/s] 38%|      | 293/780 [01:45<02:22,  3.42it/s] 38%|      | 294/780 [01:46<02:21,  3.43it/s] 38%|      | 295/780 [01:46<02:20,  3.44it/s] 38%|      | 296/780 [01:46<02:20,  3.44it/s] 38%|      | 297/780 [01:47<02:20,  3.44it/s] 38%|      | 298/780 [01:47<02:19,  3.45it/s] 38%|      | 299/780 [01:47<02:19,  3.45it/s] 38%|      | 300/780 [01:48<02:19,  3.45it/s] 39%|      | 301/780 [01:48<02:18,  3.45it/s] 39%|      | 302/780 [01:48<02:18,  3.45it/s] 39%|      | 303/780 [01:48<02:23,  3.32it/s] 39%|      | 304/780 [01:49<02:21,  3.36it/s] 39%|      | 305/780 [01:49<02:20,  3.39it/s] 39%|      | 306/780 [01:49<02:19,  3.41it/s] 39%|      | 307/780 [01:50<02:18,  3.42it/s] 39%|      | 308/780 [01:50<02:17,  3.43it/s] 40%|      | 309/780 [01:50<02:16,  3.44it/s] 40%|      | 310/780 [01:50<02:16,  3.45it/s] 40%|      | 311/780 [01:51<02:15,  3.45it/s] 40%|      | 312/780 [01:51<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 04:07:21,109 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:07:21,109 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 04:07:21,109 >>   Batch size = 8
{'eval_loss': 1.0694361925125122, 'eval_runtime': 9.8993, 'eval_samples_per_second': 351.743, 'eval_steps_per_second': 44.044, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.14it/s][A
  3%|         | 12/436 [00:00<00:10, 40.31it/s][A
  4%|         | 18/436 [00:00<00:09, 44.17it/s][A
  5%|         | 23/436 [00:00<00:09, 44.49it/s][A
  6%|         | 28/436 [00:00<00:09, 44.62it/s][A
  8%|         | 33/436 [00:00<00:09, 44.71it/s][A
  9%|         | 38/436 [00:00<00:08, 44.75it/s][A
 10%|         | 43/436 [00:00<00:08, 44.73it/s][A
 11%|         | 48/436 [00:01<00:08, 44.80it/s][A
 12%|        | 53/436 [00:01<00:08, 44.79it/s][A
 13%|        | 58/436 [00:01<00:08, 44.42it/s][A
 14%|        | 63/436 [00:01<00:08, 44.53it/s][A
 16%|        | 68/436 [00:01<00:08, 44.65it/s][A
 17%|        | 73/436 [00:01<00:08, 44.79it/s][A
 18%|        | 78/436 [00:01<00:07, 44.85it/s][A
 19%|        | 83/436 [00:01<00:07, 44.76it/s][A
 20%|        | 88/436 [00:01<00:08, 42.39it/s][A
 21%|       | 93/436 [00:02<00:07, 43.25it/s][A
 22%|       | 98/436 [00:02<00:07, 43.62it/s][A
 24%|       | 103/436 [00:02<00:07, 43.89it/s][A
 25%|       | 108/436 [00:02<00:07, 44.09it/s][A
 26%|       | 113/436 [00:02<00:07, 44.35it/s][A
 27%|       | 118/436 [00:02<00:07, 44.51it/s][A
 28%|       | 123/436 [00:02<00:07, 44.64it/s][A
 29%|       | 128/436 [00:02<00:06, 44.49it/s][A
 31%|       | 133/436 [00:02<00:06, 44.57it/s][A
 32%|      | 138/436 [00:03<00:06, 44.66it/s][A
 33%|      | 143/436 [00:03<00:06, 44.51it/s][A
 34%|      | 148/436 [00:03<00:06, 44.65it/s][A
 35%|      | 153/436 [00:03<00:06, 44.67it/s][A
 36%|      | 158/436 [00:03<00:06, 44.76it/s][A
 37%|      | 163/436 [00:03<00:06, 44.84it/s][A
 39%|      | 168/436 [00:03<00:05, 44.78it/s][A
 40%|      | 173/436 [00:03<00:05, 44.64it/s][A
 41%|      | 178/436 [00:04<00:05, 44.59it/s][A
 42%|     | 183/436 [00:04<00:05, 44.66it/s][A
 43%|     | 188/436 [00:04<00:05, 44.79it/s][A
 44%|     | 193/436 [00:04<00:05, 44.81it/s][A
 45%|     | 198/436 [00:04<00:05, 44.86it/s][A
 47%|     | 203/436 [00:04<00:05, 42.48it/s][A
 48%|     | 208/436 [00:04<00:05, 43.40it/s][A
 49%|     | 213/436 [00:04<00:05, 43.86it/s][A
 50%|     | 218/436 [00:04<00:04, 44.07it/s][A
 51%|     | 223/436 [00:05<00:04, 43.11it/s][A
 52%|    | 228/436 [00:05<00:04, 43.73it/s][A
 53%|    | 233/436 [00:05<00:04, 43.99it/s][A
 55%|    | 238/436 [00:05<00:04, 44.18it/s][A
 56%|    | 243/436 [00:05<00:04, 44.16it/s][A
 57%|    | 248/436 [00:05<00:04, 44.21it/s][A
 58%|    | 253/436 [00:05<00:04, 40.55it/s][A
 59%|    | 258/436 [00:06<00:06, 27.12it/s][A
 60%|    | 263/436 [00:06<00:05, 30.85it/s][A
 61%|   | 268/436 [00:06<00:04, 34.13it/s][A
 63%|   | 273/436 [00:06<00:04, 36.82it/s][A
 64%|   | 278/436 [00:06<00:04, 38.92it/s][A
 65%|   | 283/436 [00:06<00:03, 40.71it/s][A
 66%|   | 288/436 [00:06<00:03, 41.94it/s][A
 67%|   | 293/436 [00:06<00:03, 42.91it/s][A
 68%|   | 298/436 [00:06<00:03, 43.01it/s][A
 69%|   | 303/436 [00:07<00:03, 43.24it/s][A
 71%|   | 308/436 [00:07<00:02, 43.36it/s][A
 72%|  | 313/436 [00:07<00:02, 43.85it/s][A
 73%|  | 318/436 [00:07<00:02, 44.31it/s][A
 74%|  | 323/436 [00:07<00:02, 44.57it/s][A
 75%|  | 328/436 [00:07<00:02, 44.85it/s][A
 76%|  | 333/436 [00:07<00:02, 45.03it/s][A
 78%|  | 338/436 [00:07<00:02, 44.91it/s][A
 79%|  | 343/436 [00:07<00:02, 44.64it/s][A
 80%|  | 348/436 [00:08<00:02, 43.68it/s][A
 81%|  | 353/436 [00:08<00:01, 43.79it/s][A
 82%| | 358/436 [00:08<00:01, 44.13it/s][A
 83%| | 363/436 [00:08<00:01, 44.31it/s][A
 84%| | 368/436 [00:08<00:01, 44.68it/s][A
 86%| | 373/436 [00:08<00:01, 44.93it/s][A
 87%| | 378/436 [00:08<00:01, 45.09it/s][A
 88%| | 383/436 [00:08<00:01, 44.89it/s][A
 89%| | 388/436 [00:08<00:01, 44.61it/s][A
 90%| | 393/436 [00:09<00:00, 44.44it/s][A
 91%|| 398/436 [00:09<00:00, 44.42it/s][A
 92%|| 403/436 [00:09<00:00, 44.49it/s][A
 94%|| 408/436 [00:09<00:00, 44.70it/s][A
 95%|| 413/436 [00:09<00:00, 44.84it/s][A
 96%|| 418/436 [00:09<00:00, 44.91it/s][A
 97%|| 423/436 [00:09<00:00, 45.00it/s][A
 98%|| 428/436 [00:09<00:00, 44.92it/s][A
 99%|| 433/436 [00:09<00:00, 44.75it/s][A
                                                 [A                                                 
100%|| 436/436 [00:10<00:00, 44.75it/s][A 40%|      | 312/780 [02:01<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:07:31,333 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 04:07:31,457 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:07:34,581 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:07:34,737 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:07:34,810 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:12<50:22,  6.47s/it] 40%|      | 314/780 [02:12<35:58,  4.63s/it] 40%|      | 315/780 [02:13<25:48,  3.33s/it] 41%|      | 316/780 [02:13<18:42,  2.42s/it] 41%|      | 317/780 [02:13<13:44,  1.78s/it] 41%|      | 318/780 [02:13<10:16,  1.33s/it] 41%|      | 319/780 [02:14<07:50,  1.02s/it] 41%|      | 320/780 [02:14<06:09,  1.25it/s] 41%|      | 321/780 [02:14<04:58,  1.54it/s] 41%|     | 322/780 [02:15<04:08,  1.84it/s] 41%|     | 323/780 [02:15<03:33,  2.14it/s] 42%|     | 324/780 [02:15<03:09,  2.41it/s] 42%|     | 325/780 [02:15<02:54,  2.60it/s] 42%|     | 326/780 [02:16<02:42,  2.80it/s] 42%|     | 327/780 [02:16<02:32,  2.96it/s] 42%|     | 328/780 [02:16<02:26,  3.09it/s] 42%|     | 329/780 [02:17<02:21,  3.18it/s] 42%|     | 330/780 [02:17<02:18,  3.25it/s] 42%|     | 331/780 [02:17<02:16,  3.29it/s] 43%|     | 332/780 [02:18<02:14,  3.33it/s] 43%|     | 333/780 [02:18<02:13,  3.36it/s] 43%|     | 334/780 [02:18<02:12,  3.37it/s] 43%|     | 335/780 [02:18<02:11,  3.38it/s] 43%|     | 336/780 [02:19<02:13,  3.34it/s] 43%|     | 337/780 [02:19<02:11,  3.36it/s] 43%|     | 338/780 [02:19<02:10,  3.38it/s] 43%|     | 339/780 [02:20<02:10,  3.38it/s] 44%|     | 340/780 [02:20<02:09,  3.39it/s] 44%|     | 341/780 [02:20<02:09,  3.40it/s] 44%|     | 342/780 [02:20<02:08,  3.41it/s] 44%|     | 343/780 [02:21<02:08,  3.40it/s] 44%|     | 344/780 [02:21<02:07,  3.41it/s] 44%|     | 345/780 [02:21<02:07,  3.41it/s] 44%|     | 346/780 [02:22<02:07,  3.41it/s] 44%|     | 347/780 [02:22<02:06,  3.41it/s] 45%|     | 348/780 [02:22<02:06,  3.41it/s] 45%|     | 349/780 [02:23<02:06,  3.41it/s] 45%|     | 350/780 [02:23<02:05,  3.41it/s] 45%|     | 351/780 [02:23<02:05,  3.41it/s] 45%|     | 352/780 [02:23<02:05,  3.41it/s] 45%|     | 353/780 [02:24<02:04,  3.42it/s] 45%|     | 354/780 [02:24<02:09,  3.29it/s] 46%|     | 355/780 [02:24<02:07,  3.33it/s] 46%|     | 356/780 [02:25<02:06,  3.35it/s] 46%|     | 357/780 [02:25<02:05,  3.37it/s] 46%|     | 358/780 [02:25<02:04,  3.38it/s] 46%|     | 359/780 [02:26<02:04,  3.39it/s] 46%|     | 360/780 [02:26<02:03,  3.40it/s] 46%|     | 361/780 [02:26<02:03,  3.40it/s] 46%|     | 362/780 [02:26<02:02,  3.40it/s] 47%|     | 363/780 [02:27<02:02,  3.41it/s] 47%|     | 364/780 [02:27<02:02,  3.41it/s] 47%|     | 365/780 [02:27<02:04,  3.32it/s] 47%|     | 366/780 [02:28<02:03,  3.35it/s] 47%|     | 367/780 [02:28<02:02,  3.36it/s] 47%|     | 368/780 [02:28<02:01,  3.38it/s] 47%|     | 369/780 [02:28<02:01,  3.39it/s] 47%|     | 370/780 [02:29<02:00,  3.40it/s] 48%|     | 371/780 [02:29<02:00,  3.40it/s] 48%|     | 372/780 [02:29<01:59,  3.40it/s] 48%|     | 373/780 [02:30<01:59,  3.40it/s] 48%|     | 374/780 [02:30<01:59,  3.41it/s] 48%|     | 375/780 [02:30<01:58,  3.41it/s] 48%|     | 376/780 [02:31<02:00,  3.34it/s] 48%|     | 377/780 [02:31<01:59,  3.36it/s] 48%|     | 378/780 [02:31<01:58,  3.38it/s] 49%|     | 379/780 [02:31<01:58,  3.39it/s] 49%|     | 380/780 [02:32<01:57,  3.40it/s] 49%|     | 381/780 [02:32<01:57,  3.40it/s] 49%|     | 382/780 [02:32<01:56,  3.40it/s] 49%|     | 383/780 [02:33<01:56,  3.41it/s] 49%|     | 384/780 [02:33<01:56,  3.41it/s] 49%|     | 385/780 [02:33<01:55,  3.41it/s] 49%|     | 386/780 [02:33<01:55,  3.41it/s] 50%|     | 387/780 [02:34<02:00,  3.27it/s] 50%|     | 388/780 [02:34<01:58,  3.32it/s] 50%|     | 389/780 [02:34<01:56,  3.35it/s] 50%|     | 390/780 [02:35<01:56,  3.36it/s] 50%|     | 391/780 [02:35<01:55,  3.38it/s] 50%|     | 392/780 [02:35<01:54,  3.38it/s] 50%|     | 393/780 [02:36<01:54,  3.39it/s] 51%|     | 394/780 [02:36<01:53,  3.39it/s] 51%|     | 395/780 [02:36<01:53,  3.40it/s] 51%|     | 396/780 [02:36<01:52,  3.40it/s] 51%|     | 397/780 [02:37<01:52,  3.41it/s] 51%|     | 398/780 [02:37<01:57,  3.25it/s] 51%|     | 399/780 [02:37<01:55,  3.31it/s] 51%|    | 400/780 [02:38<01:53,  3.35it/s] 51%|    | 401/780 [02:38<01:52,  3.38it/s] 52%|    | 402/780 [02:38<01:51,  3.40it/s] 52%|    | 403/780 [02:39<01:50,  3.42it/s] 52%|    | 404/780 [02:39<01:49,  3.43it/s] 52%|    | 405/780 [02:39<01:49,  3.43it/s] 52%|    | 406/780 [02:39<01:48,  3.44it/s] 52%|    | 407/780 [02:40<01:48,  3.45it/s] 52%|    | 408/780 [02:40<01:47,  3.45it/s] 52%|    | 409/780 [02:40<01:51,  3.34it/s] 53%|    | 410/780 [02:41<01:49,  3.38it/s] 53%|    | 411/780 [02:41<01:48,  3.40it/s] 53%|    | 412/780 [02:41<01:47,  3.42it/s] 53%|    | 413/780 [02:41<01:46,  3.43it/s] 53%|    | 414/780 [02:42<01:46,  3.44it/s] 53%|    | 415/780 [02:42<01:45,  3.45it/s] 53%|    | 416/780 [02:42<01:45,  3.46it/s] 53%|    | 417/780 [02:43<01:45,  3.46it/s] 54%|    | 418/780 [02:43<01:44,  3.46it/s] 54%|    | 419/780 [02:43<01:44,  3.46it/s] 54%|    | 420/780 [02:43<01:48,  3.33it/s] 54%|    | 421/780 [02:44<01:46,  3.37it/s] 54%|    | 422/780 [02:44<01:45,  3.40it/s] 54%|    | 423/780 [02:44<01:44,  3.41it/s] 54%|    | 424/780 [02:45<01:43,  3.42it/s] 54%|    | 425/780 [02:45<01:43,  3.44it/s] 55%|    | 426/780 [02:45<01:42,  3.44it/s] 55%|    | 427/780 [02:46<01:42,  3.44it/s] 55%|    | 428/780 [02:46<01:42,  3.45it/s] 55%|    | 429/780 [02:46<01:41,  3.46it/s] 55%|    | 430/780 [02:46<01:41,  3.45it/s] 55%|    | 431/780 [02:47<01:48,  3.22it/s] 55%|    | 432/780 [02:47<01:45,  3.29it/s] 56%|    | 433/780 [02:47<01:43,  3.34it/s] 56%|    | 434/780 [02:48<01:42,  3.38it/s] 56%|    | 435/780 [02:48<01:41,  3.40it/s] 56%|    | 436/780 [02:48<01:40,  3.42it/s] 56%|    | 437/780 [02:48<01:40,  3.43it/s] 56%|    | 438/780 [02:49<01:39,  3.44it/s] 56%|    | 439/780 [02:49<01:39,  3.44it/s] 56%|    | 440/780 [02:49<01:38,  3.45it/s] 57%|    | 441/780 [02:50<01:38,  3.45it/s] 57%|    | 442/780 [02:50<01:43,  3.27it/s] 57%|    | 443/780 [02:50<01:41,  3.33it/s] 57%|    | 444/780 [02:51<01:39,  3.36it/s] 57%|    | 445/780 [02:51<01:38,  3.39it/s] 57%|    | 446/780 [02:51<01:37,  3.41it/s] 57%|    | 447/780 [02:51<01:37,  3.43it/s] 57%|    | 448/780 [02:52<01:36,  3.44it/s] 58%|    | 449/780 [02:52<01:36,  3.44it/s] 58%|    | 450/780 [02:52<01:35,  3.45it/s] 58%|    | 451/780 [02:53<01:35,  3.45it/s] 58%|    | 452/780 [02:53<01:34,  3.46it/s] 58%|    | 453/780 [02:53<01:34,  3.46it/s] 58%|    | 454/780 [02:53<01:34,  3.46it/s] 58%|    | 455/780 [02:54<01:33,  3.46it/s] 58%|    | 456/780 [02:54<01:33,  3.45it/s] 59%|    | 457/780 [02:54<01:33,  3.45it/s] 59%|    | 458/780 [02:55<01:37,  3.31it/s] 59%|    | 459/780 [02:55<01:35,  3.36it/s] 59%|    | 460/780 [02:55<01:34,  3.39it/s] 59%|    | 461/780 [02:56<01:33,  3.40it/s] 59%|    | 462/780 [02:56<01:38,  3.23it/s] 59%|    | 463/780 [02:56<01:36,  3.30it/s] 59%|    | 464/780 [02:56<01:34,  3.35it/s] 60%|    | 465/780 [02:57<01:33,  3.38it/s] 60%|    | 466/780 [02:57<02:07,  2.46it/s] 60%|    | 467/780 [02:58<02:00,  2.60it/s] 60%|    | 468/780 [02:58<01:50,  2.82it/s][INFO|trainer.py:2140] 2023-08-29 04:08:28,109 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:08:28,109 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 04:08:28,109 >>   Batch size = 8
{'eval_loss': 1.0873233079910278, 'eval_runtime': 10.099, 'eval_samples_per_second': 344.785, 'eval_steps_per_second': 43.172, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.99it/s][A
  3%|         | 12/436 [00:00<00:08, 48.96it/s][A
  4%|         | 17/436 [00:00<00:08, 46.57it/s][A
  5%|         | 22/436 [00:00<00:09, 45.43it/s][A
  6%|         | 27/436 [00:00<00:09, 45.13it/s][A
  7%|         | 32/436 [00:00<00:08, 44.97it/s][A
  8%|         | 37/436 [00:00<00:08, 45.05it/s][A
 10%|         | 42/436 [00:00<00:08, 44.82it/s][A
 11%|         | 47/436 [00:01<00:08, 44.88it/s][A
 12%|        | 52/436 [00:01<00:08, 44.92it/s][A
 13%|        | 57/436 [00:01<00:08, 45.10it/s][A
 14%|        | 62/436 [00:01<00:08, 44.93it/s][A
 15%|        | 67/436 [00:01<00:08, 44.76it/s][A
 17%|        | 72/436 [00:01<00:08, 44.67it/s][A
 18%|        | 77/436 [00:01<00:08, 44.68it/s][A
 19%|        | 82/436 [00:01<00:07, 44.69it/s][A
 20%|        | 87/436 [00:01<00:07, 44.70it/s][A
 21%|        | 92/436 [00:02<00:07, 44.77it/s][A
 22%|       | 97/436 [00:02<00:07, 44.95it/s][A
 23%|       | 102/436 [00:02<00:07, 44.96it/s][A
 25%|       | 107/436 [00:02<00:07, 44.88it/s][A
 26%|       | 112/436 [00:02<00:07, 43.37it/s][A
 27%|       | 117/436 [00:02<00:07, 43.82it/s][A
 28%|       | 122/436 [00:02<00:07, 44.08it/s][A
 29%|       | 127/436 [00:02<00:06, 44.30it/s][A
 30%|       | 132/436 [00:02<00:06, 44.47it/s][A
 31%|      | 137/436 [00:03<00:06, 44.72it/s][A
 33%|      | 142/436 [00:03<00:06, 44.78it/s][A
 34%|      | 147/436 [00:03<00:06, 44.83it/s][A
 35%|      | 152/436 [00:03<00:06, 44.61it/s][A
 36%|      | 157/436 [00:03<00:06, 44.55it/s][A
 37%|      | 162/436 [00:03<00:06, 44.65it/s][A
 38%|      | 167/436 [00:03<00:06, 44.61it/s][A
 39%|      | 172/436 [00:03<00:05, 44.63it/s][A
 41%|      | 177/436 [00:03<00:05, 44.78it/s][A
 42%|     | 182/436 [00:04<00:05, 44.85it/s][A
 43%|     | 187/436 [00:04<00:05, 44.88it/s][A
 44%|     | 192/436 [00:04<00:05, 44.69it/s][A
 45%|     | 197/436 [00:04<00:05, 44.59it/s][A
 46%|     | 202/436 [00:04<00:05, 44.58it/s][A
 47%|     | 207/436 [00:04<00:05, 44.60it/s][A
 49%|     | 212/436 [00:04<00:05, 44.64it/s][A
 50%|     | 217/436 [00:04<00:04, 44.74it/s][A
 51%|     | 222/436 [00:04<00:04, 44.92it/s][A
 52%|    | 227/436 [00:05<00:04, 44.87it/s][A
 53%|    | 232/436 [00:05<00:04, 44.86it/s][A
 54%|    | 237/436 [00:05<00:04, 44.75it/s][A
 56%|    | 242/436 [00:05<00:04, 44.70it/s][A
 57%|    | 247/436 [00:05<00:04, 42.48it/s][A
 58%|    | 252/436 [00:05<00:04, 43.15it/s][A
 59%|    | 257/436 [00:05<00:04, 43.67it/s][A
 60%|    | 262/436 [00:05<00:03, 44.08it/s][A
 61%|    | 267/436 [00:05<00:03, 44.36it/s][A
 62%|   | 272/436 [00:06<00:03, 44.55it/s][A
 64%|   | 277/436 [00:06<00:03, 44.50it/s][A
 65%|   | 282/436 [00:06<00:03, 44.54it/s][A
 66%|   | 287/436 [00:06<00:03, 44.26it/s][A
 67%|   | 292/436 [00:06<00:03, 44.39it/s][A
 68%|   | 297/436 [00:06<00:03, 44.36it/s][A
 69%|   | 302/436 [00:06<00:03, 44.60it/s][A
 70%|   | 307/436 [00:06<00:02, 44.76it/s][A
 72%|  | 312/436 [00:06<00:02, 44.70it/s][A
 73%|  | 317/436 [00:07<00:02, 44.81it/s][A
 74%|  | 322/436 [00:07<00:02, 44.81it/s][A
 75%|  | 327/436 [00:07<00:02, 44.62it/s][A
 76%|  | 332/436 [00:07<00:02, 44.57it/s][A
 77%|  | 337/436 [00:07<00:02, 44.55it/s][A
 78%|  | 342/436 [00:07<00:02, 44.62it/s][A
 80%|  | 347/436 [00:07<00:01, 44.75it/s][A
 81%|  | 352/436 [00:07<00:01, 44.79it/s][A
 82%| | 357/436 [00:07<00:01, 44.86it/s][A
 83%| | 362/436 [00:08<00:01, 44.81it/s][A
 84%| | 367/436 [00:08<00:01, 44.77it/s][A
 85%| | 372/436 [00:08<00:01, 44.61it/s][A
 86%| | 377/436 [00:08<00:01, 44.55it/s][A
 88%| | 382/436 [00:08<00:01, 43.68it/s][A
 89%| | 387/436 [00:08<00:01, 44.19it/s][A
 90%| | 392/436 [00:08<00:00, 44.48it/s][A
 91%| | 397/436 [00:08<00:00, 44.60it/s][A
 92%|| 402/436 [00:08<00:00, 44.66it/s][A
 93%|| 407/436 [00:09<00:00, 44.61it/s][A
 94%|| 412/436 [00:09<00:00, 44.55it/s][A
 96%|| 417/436 [00:09<00:00, 44.59it/s][A
 97%|| 422/436 [00:09<00:00, 44.44it/s][A
 98%|| 427/436 [00:09<00:00, 44.41it/s][A
 99%|| 432/436 [00:09<00:00, 44.47it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.47it/s][A 60%|    | 468/780 [03:08<01:50,  2.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:08:38,054 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 04:08:38,187 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:08:41,316 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:08:41,520 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:08:41,626 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:21<36:50,  7.11s/it] 60%|    | 470/780 [03:21<26:11,  5.07s/it] 60%|    | 471/780 [03:21<18:43,  3.64s/it] 61%|    | 472/780 [03:22<13:30,  2.63s/it] 61%|    | 473/780 [03:22<09:52,  1.93s/it] 61%|    | 474/780 [03:22<07:22,  1.44s/it] 61%|    | 475/780 [03:23<05:35,  1.10s/it] 61%|    | 476/780 [03:23<04:20,  1.17it/s] 61%|    | 477/780 [03:23<03:28,  1.45it/s] 61%|   | 478/780 [03:24<02:51,  1.76it/s] 61%|   | 479/780 [03:24<02:26,  2.05it/s] 62%|   | 480/780 [03:24<02:08,  2.33it/s] 62%|   | 481/780 [03:24<01:56,  2.58it/s] 62%|   | 482/780 [03:25<01:46,  2.79it/s] 62%|   | 483/780 [03:25<01:40,  2.97it/s] 62%|   | 484/780 [03:25<01:35,  3.10it/s] 62%|   | 485/780 [03:26<01:34,  3.13it/s] 62%|   | 486/780 [03:26<01:31,  3.21it/s] 62%|   | 487/780 [03:26<01:29,  3.28it/s] 63%|   | 488/780 [03:26<01:27,  3.33it/s] 63%|   | 489/780 [03:27<01:26,  3.38it/s] 63%|   | 490/780 [03:27<01:25,  3.40it/s] 63%|   | 491/780 [03:27<01:24,  3.42it/s] 63%|   | 492/780 [03:28<01:23,  3.44it/s] 63%|   | 493/780 [03:28<01:23,  3.44it/s] 63%|   | 494/780 [03:28<01:22,  3.45it/s] 63%|   | 495/780 [03:28<01:22,  3.45it/s] 64%|   | 496/780 [03:29<01:23,  3.40it/s] 64%|   | 497/780 [03:29<01:22,  3.42it/s] 64%|   | 498/780 [03:29<01:22,  3.43it/s] 64%|   | 499/780 [03:30<01:21,  3.45it/s] 64%|   | 500/780 [03:30<01:21,  3.45it/s]                                                  64%|   | 500/780 [03:30<01:21,  3.45it/s] 64%|   | 501/780 [03:30<01:20,  3.46it/s] 64%|   | 502/780 [03:31<01:20,  3.45it/s] 64%|   | 503/780 [03:31<01:20,  3.46it/s] 65%|   | 504/780 [03:31<01:19,  3.46it/s] 65%|   | 505/780 [03:31<01:19,  3.46it/s] 65%|   | 506/780 [03:32<01:19,  3.46it/s] 65%|   | 507/780 [03:32<01:21,  3.36it/s] 65%|   | 508/780 [03:32<01:20,  3.39it/s] 65%|   | 509/780 [03:33<01:19,  3.41it/s] 65%|   | 510/780 [03:33<01:18,  3.43it/s] 66%|   | 511/780 [03:33<01:18,  3.43it/s] 66%|   | 512/780 [03:33<01:17,  3.44it/s] 66%|   | 513/780 [03:34<01:17,  3.44it/s] 66%|   | 514/780 [03:34<01:17,  3.45it/s] 66%|   | 515/780 [03:34<01:16,  3.45it/s] 66%|   | 516/780 [03:35<01:16,  3.46it/s] 66%|   | 517/780 [03:35<01:16,  3.46it/s] 66%|   | 518/780 [03:35<01:17,  3.38it/s] 67%|   | 519/780 [03:35<01:16,  3.40it/s] 67%|   | 520/780 [03:36<01:16,  3.42it/s] 67%|   | 521/780 [03:36<01:15,  3.43it/s] 67%|   | 522/780 [03:36<01:15,  3.44it/s] 67%|   | 523/780 [03:37<01:14,  3.44it/s] 67%|   | 524/780 [03:37<01:14,  3.45it/s] 67%|   | 525/780 [03:37<01:13,  3.45it/s] 67%|   | 526/780 [03:38<01:13,  3.46it/s] 68%|   | 527/780 [03:38<01:13,  3.45it/s] 68%|   | 528/780 [03:38<01:12,  3.45it/s] 68%|   | 529/780 [03:38<01:14,  3.37it/s] 68%|   | 530/780 [03:39<01:13,  3.40it/s] 68%|   | 531/780 [03:39<01:12,  3.42it/s] 68%|   | 532/780 [03:39<01:12,  3.43it/s] 68%|   | 533/780 [03:40<01:11,  3.44it/s] 68%|   | 534/780 [03:40<01:11,  3.44it/s] 69%|   | 535/780 [03:40<01:11,  3.45it/s] 69%|   | 536/780 [03:40<01:10,  3.45it/s] 69%|   | 537/780 [03:41<01:10,  3.45it/s] 69%|   | 538/780 [03:41<01:10,  3.45it/s] 69%|   | 539/780 [03:41<01:09,  3.46it/s] 69%|   | 540/780 [03:42<01:11,  3.34it/s] 69%|   | 541/780 [03:42<01:10,  3.38it/s] 69%|   | 542/780 [03:42<01:09,  3.40it/s] 70%|   | 543/780 [03:42<01:09,  3.42it/s] 70%|   | 544/780 [03:43<01:08,  3.43it/s] 70%|   | 545/780 [03:43<01:08,  3.44it/s] 70%|   | 546/780 [03:43<01:08,  3.44it/s] 70%|   | 547/780 [03:44<01:07,  3.44it/s] 70%|   | 548/780 [03:44<01:07,  3.45it/s] 70%|   | 549/780 [03:44<01:06,  3.45it/s] 71%|   | 550/780 [03:45<01:06,  3.45it/s] 71%|   | 551/780 [03:45<01:07,  3.38it/s] 71%|   | 552/780 [03:45<01:07,  3.40it/s] 71%|   | 553/780 [03:45<01:06,  3.41it/s] 71%|   | 554/780 [03:46<01:06,  3.42it/s] 71%|   | 555/780 [03:46<01:05,  3.43it/s] 71%|  | 556/780 [03:46<01:05,  3.43it/s] 71%|  | 557/780 [03:47<01:04,  3.44it/s] 72%|  | 558/780 [03:47<01:04,  3.44it/s] 72%|  | 559/780 [03:47<01:04,  3.45it/s] 72%|  | 560/780 [03:47<01:03,  3.45it/s] 72%|  | 561/780 [03:48<01:03,  3.45it/s] 72%|  | 562/780 [03:48<01:03,  3.45it/s] 72%|  | 563/780 [03:48<01:02,  3.46it/s] 72%|  | 564/780 [03:49<01:02,  3.46it/s] 72%|  | 565/780 [03:49<01:02,  3.45it/s] 73%|  | 566/780 [03:49<01:01,  3.46it/s] 73%|  | 567/780 [03:49<01:01,  3.46it/s] 73%|  | 568/780 [03:50<01:01,  3.47it/s] 73%|  | 569/780 [03:50<01:00,  3.46it/s] 73%|  | 570/780 [03:50<01:01,  3.44it/s] 73%|  | 571/780 [03:51<01:00,  3.43it/s] 73%|  | 572/780 [03:51<01:00,  3.43it/s] 73%|  | 573/780 [03:51<01:03,  3.27it/s] 74%|  | 574/780 [03:52<01:02,  3.31it/s] 74%|  | 575/780 [03:52<01:01,  3.34it/s] 74%|  | 576/780 [03:52<01:00,  3.36it/s] 74%|  | 577/780 [03:52<01:00,  3.37it/s] 74%|  | 578/780 [03:53<00:59,  3.39it/s] 74%|  | 579/780 [03:53<00:59,  3.39it/s] 74%|  | 580/780 [03:53<00:58,  3.39it/s] 74%|  | 581/780 [03:54<00:58,  3.40it/s] 75%|  | 582/780 [03:54<00:58,  3.40it/s] 75%|  | 583/780 [03:54<00:57,  3.40it/s] 75%|  | 584/780 [03:54<00:57,  3.40it/s] 75%|  | 585/780 [03:55<00:57,  3.40it/s] 75%|  | 586/780 [03:55<00:57,  3.40it/s] 75%|  | 587/780 [03:55<00:56,  3.40it/s] 75%|  | 588/780 [03:56<00:56,  3.40it/s] 76%|  | 589/780 [03:56<00:58,  3.25it/s] 76%|  | 590/780 [03:56<01:00,  3.16it/s] 76%|  | 591/780 [03:57<00:58,  3.23it/s] 76%|  | 592/780 [03:57<00:57,  3.29it/s] 76%|  | 593/780 [03:58<01:12,  2.56it/s] 76%|  | 594/780 [03:58<01:07,  2.77it/s] 76%|  | 595/780 [03:58<01:03,  2.94it/s] 76%|  | 596/780 [03:58<01:00,  3.06it/s] 77%|  | 597/780 [03:59<00:57,  3.16it/s] 77%|  | 598/780 [03:59<00:56,  3.23it/s] 77%|  | 599/780 [03:59<00:55,  3.29it/s] 77%|  | 600/780 [04:00<00:54,  3.32it/s] 77%|  | 601/780 [04:00<00:53,  3.35it/s] 77%|  | 602/780 [04:00<00:52,  3.36it/s] 77%|  | 603/780 [04:00<00:52,  3.37it/s] 77%|  | 604/780 [04:01<00:52,  3.38it/s] 78%|  | 605/780 [04:01<00:51,  3.39it/s] 78%|  | 606/780 [04:01<00:51,  3.35it/s] 78%|  | 607/780 [04:02<00:51,  3.37it/s] 78%|  | 608/780 [04:02<00:50,  3.38it/s] 78%|  | 609/780 [04:02<00:50,  3.39it/s] 78%|  | 610/780 [04:03<00:50,  3.40it/s] 78%|  | 611/780 [04:03<00:49,  3.40it/s] 78%|  | 612/780 [04:03<00:49,  3.41it/s] 79%|  | 613/780 [04:03<00:49,  3.41it/s] 79%|  | 614/780 [04:04<00:48,  3.41it/s] 79%|  | 615/780 [04:04<00:48,  3.40it/s] 79%|  | 616/780 [04:04<00:48,  3.41it/s] 79%|  | 617/780 [04:05<00:47,  3.41it/s] 79%|  | 618/780 [04:05<00:47,  3.41it/s] 79%|  | 619/780 [04:05<00:47,  3.41it/s] 79%|  | 620/780 [04:05<00:46,  3.41it/s] 80%|  | 621/780 [04:06<00:46,  3.41it/s] 80%|  | 622/780 [04:06<00:46,  3.41it/s] 80%|  | 623/780 [04:06<00:47,  3.27it/s] 80%|  | 624/780 [04:07<00:47,  3.31it/s][INFO|trainer.py:2140] 2023-08-29 04:09:36,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:09:36,764 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 04:09:36,764 >>   Batch size = 8
{'eval_loss': 1.1004410982131958, 'eval_runtime': 9.8114, 'eval_samples_per_second': 354.895, 'eval_steps_per_second': 44.438, 'epoch': 3.0}
{'loss': 0.6437, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.07it/s][A
  3%|         | 12/436 [00:00<00:08, 48.56it/s][A
  4%|         | 17/436 [00:00<00:08, 46.83it/s][A
  5%|         | 22/436 [00:00<00:09, 45.80it/s][A
  6%|         | 27/436 [00:00<00:09, 45.39it/s][A
  7%|         | 32/436 [00:00<00:08, 45.13it/s][A
  8%|         | 37/436 [00:00<00:08, 44.85it/s][A
 10%|         | 42/436 [00:00<00:08, 44.69it/s][A
 11%|         | 47/436 [00:01<00:08, 44.83it/s][A
 12%|        | 52/436 [00:01<00:08, 45.04it/s][A
 13%|        | 57/436 [00:01<00:08, 45.13it/s][A
 14%|        | 62/436 [00:01<00:08, 45.01it/s][A
 15%|        | 67/436 [00:01<00:08, 44.73it/s][A
 17%|        | 72/436 [00:01<00:08, 44.67it/s][A
 18%|        | 77/436 [00:01<00:08, 44.64it/s][A
 19%|        | 82/436 [00:01<00:07, 44.64it/s][A
 20%|        | 87/436 [00:01<00:07, 44.63it/s][A
 21%|        | 92/436 [00:02<00:07, 44.77it/s][A
 22%|       | 97/436 [00:02<00:07, 44.84it/s][A
 23%|       | 102/436 [00:02<00:07, 44.97it/s][A
 25%|       | 107/436 [00:02<00:07, 44.81it/s][A
 26%|       | 112/436 [00:02<00:07, 44.76it/s][A
 27%|       | 117/436 [00:02<00:07, 44.65it/s][A
 28%|       | 122/436 [00:02<00:07, 44.55it/s][A
 29%|       | 127/436 [00:02<00:06, 44.66it/s][A
 30%|       | 132/436 [00:02<00:06, 44.63it/s][A
 31%|      | 137/436 [00:03<00:06, 44.77it/s][A
 33%|      | 142/436 [00:03<00:06, 44.85it/s][A
 34%|      | 147/436 [00:03<00:06, 44.82it/s][A
 35%|      | 152/436 [00:03<00:06, 44.84it/s][A
 36%|      | 157/436 [00:03<00:06, 44.57it/s][A
 37%|      | 162/436 [00:03<00:06, 44.68it/s][A
 38%|      | 167/436 [00:03<00:06, 44.64it/s][A
 39%|      | 172/436 [00:03<00:05, 44.61it/s][A
 41%|      | 177/436 [00:03<00:05, 44.63it/s][A
 42%|     | 182/436 [00:04<00:05, 44.75it/s][A
 43%|     | 187/436 [00:04<00:05, 44.88it/s][A
 44%|     | 192/436 [00:04<00:05, 44.83it/s][A
 45%|     | 197/436 [00:04<00:05, 44.78it/s][A
 46%|     | 202/436 [00:04<00:05, 43.30it/s][A
 47%|     | 207/436 [00:04<00:05, 43.77it/s][A
 49%|     | 212/436 [00:04<00:05, 44.03it/s][A
 50%|     | 217/436 [00:04<00:04, 44.22it/s][A
 51%|     | 222/436 [00:04<00:04, 44.42it/s][A
 52%|    | 227/436 [00:05<00:04, 44.48it/s][A
 53%|    | 232/436 [00:05<00:04, 44.76it/s][A
 54%|    | 237/436 [00:05<00:04, 44.76it/s][A
 56%|    | 242/436 [00:05<00:04, 44.54it/s][A
 57%|    | 247/436 [00:05<00:04, 44.63it/s][A
 58%|    | 252/436 [00:05<00:04, 44.60it/s][A
 59%|    | 257/436 [00:05<00:04, 44.56it/s][A
 60%|    | 262/436 [00:05<00:03, 44.60it/s][A
 61%|    | 267/436 [00:05<00:03, 44.75it/s][A
 62%|   | 272/436 [00:06<00:03, 44.78it/s][A
 64%|   | 277/436 [00:06<00:03, 44.82it/s][A
 65%|   | 282/436 [00:06<00:03, 44.71it/s][A
 66%|   | 287/436 [00:06<00:03, 44.65it/s][A
 67%|   | 292/436 [00:06<00:03, 44.72it/s][A
 68%|   | 297/436 [00:06<00:03, 37.95it/s][A
 69%|   | 302/436 [00:06<00:03, 39.93it/s][A
 70%|   | 307/436 [00:06<00:03, 41.29it/s][A
 72%|  | 312/436 [00:07<00:02, 42.47it/s][A
 73%|  | 317/436 [00:07<00:02, 43.32it/s][A
 74%|  | 322/436 [00:07<00:02, 43.96it/s][A
 75%|  | 327/436 [00:07<00:02, 44.33it/s][A
 76%|  | 332/436 [00:07<00:02, 44.38it/s][A
 77%|  | 337/436 [00:07<00:02, 44.14it/s][A
 78%|  | 342/436 [00:07<00:02, 43.97it/s][A
 80%|  | 347/436 [00:07<00:02, 44.06it/s][A
 81%|  | 352/436 [00:07<00:01, 44.35it/s][A
 82%| | 357/436 [00:08<00:01, 44.67it/s][A
 83%| | 362/436 [00:08<00:01, 44.83it/s][A
 84%| | 367/436 [00:08<00:01, 45.00it/s][A
 85%| | 372/436 [00:08<00:01, 45.11it/s][A
 86%| | 377/436 [00:08<00:01, 44.88it/s][A
 88%| | 382/436 [00:08<00:01, 44.62it/s][A
 89%| | 387/436 [00:08<00:01, 44.19it/s][A
 90%| | 392/436 [00:08<00:00, 44.30it/s][A
 91%| | 397/436 [00:08<00:00, 44.53it/s][A
 92%|| 402/436 [00:09<00:00, 44.76it/s][A
 93%|| 407/436 [00:09<00:00, 44.89it/s][A
 94%|| 412/436 [00:09<00:00, 45.08it/s][A
 96%|| 417/436 [00:09<00:00, 45.07it/s][A
 97%|| 422/436 [00:09<00:00, 44.85it/s][A
 98%|| 427/436 [00:09<00:00, 44.61it/s][A
 99%|| 432/436 [00:09<00:00, 42.10it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 42.10it/s][A 80%|  | 624/780 [04:17<00:47,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:09:46,865 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 04:09:47,166 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:09:50,459 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:09:50,614 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:09:50,707 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:27<16:24,  6.35s/it] 80%|  | 626/780 [04:27<11:39,  4.54s/it] 80%|  | 627/780 [04:28<08:19,  3.27s/it] 81%|  | 628/780 [04:28<06:00,  2.37s/it] 81%|  | 629/780 [04:28<04:24,  1.75s/it] 81%|  | 630/780 [04:29<03:16,  1.31s/it] 81%|  | 631/780 [04:29<02:29,  1.01s/it] 81%|  | 632/780 [04:29<01:57,  1.26it/s] 81%|  | 633/780 [04:29<01:34,  1.56it/s] 81%| | 634/780 [04:30<01:18,  1.86it/s] 81%| | 635/780 [04:30<01:07,  2.16it/s] 82%| | 636/780 [04:30<00:59,  2.43it/s] 82%| | 637/780 [04:31<00:55,  2.57it/s] 82%| | 638/780 [04:31<00:51,  2.77it/s] 82%| | 639/780 [04:31<00:48,  2.94it/s] 82%| | 640/780 [04:32<00:45,  3.07it/s] 82%| | 641/780 [04:32<00:43,  3.16it/s] 82%| | 642/780 [04:32<00:42,  3.23it/s] 82%| | 643/780 [04:32<00:41,  3.29it/s] 83%| | 644/780 [04:33<00:40,  3.32it/s] 83%| | 645/780 [04:33<00:40,  3.35it/s] 83%| | 646/780 [04:33<00:39,  3.37it/s] 83%| | 647/780 [04:34<00:39,  3.38it/s] 83%| | 648/780 [04:34<00:39,  3.35it/s] 83%| | 649/780 [04:34<00:38,  3.36it/s] 83%| | 650/780 [04:35<00:38,  3.38it/s] 83%| | 651/780 [04:35<00:38,  3.39it/s] 84%| | 652/780 [04:35<00:37,  3.40it/s] 84%| | 653/780 [04:35<00:37,  3.40it/s] 84%| | 654/780 [04:36<00:37,  3.40it/s] 84%| | 655/780 [04:36<00:36,  3.40it/s] 84%| | 656/780 [04:36<00:36,  3.41it/s] 84%| | 657/780 [04:37<00:36,  3.41it/s] 84%| | 658/780 [04:37<00:35,  3.41it/s] 84%| | 659/780 [04:37<00:37,  3.24it/s] 85%| | 660/780 [04:38<00:36,  3.29it/s] 85%| | 661/780 [04:38<00:35,  3.32it/s] 85%| | 662/780 [04:38<00:35,  3.35it/s] 85%| | 663/780 [04:38<00:34,  3.37it/s] 85%| | 664/780 [04:39<00:34,  3.38it/s] 85%| | 665/780 [04:39<00:33,  3.39it/s] 85%| | 666/780 [04:39<00:33,  3.39it/s] 86%| | 667/780 [04:40<00:33,  3.40it/s] 86%| | 668/780 [04:40<00:32,  3.40it/s] 86%| | 669/780 [04:40<00:32,  3.40it/s] 86%| | 670/780 [04:40<00:32,  3.33it/s] 86%| | 671/780 [04:41<00:32,  3.35it/s] 86%| | 672/780 [04:41<00:32,  3.37it/s] 86%| | 673/780 [04:41<00:31,  3.39it/s] 86%| | 674/780 [04:42<00:31,  3.39it/s] 87%| | 675/780 [04:42<00:30,  3.40it/s] 87%| | 676/780 [04:42<00:30,  3.40it/s] 87%| | 677/780 [04:43<00:30,  3.40it/s] 87%| | 678/780 [04:43<00:29,  3.40it/s] 87%| | 679/780 [04:43<00:29,  3.41it/s] 87%| | 680/780 [04:43<00:29,  3.41it/s] 87%| | 681/780 [04:44<00:29,  3.36it/s] 87%| | 682/780 [04:44<00:29,  3.37it/s] 88%| | 683/780 [04:44<00:29,  3.32it/s] 88%| | 684/780 [04:45<00:28,  3.35it/s] 88%| | 685/780 [04:45<00:28,  3.36it/s] 88%| | 686/780 [04:45<00:27,  3.38it/s] 88%| | 687/780 [04:45<00:27,  3.39it/s] 88%| | 688/780 [04:46<00:27,  3.39it/s] 88%| | 689/780 [04:46<00:26,  3.40it/s] 88%| | 690/780 [04:46<00:26,  3.41it/s] 89%| | 691/780 [04:47<00:26,  3.41it/s] 89%| | 692/780 [04:47<00:25,  3.41it/s] 89%| | 693/780 [04:47<00:25,  3.41it/s] 89%| | 694/780 [04:48<00:26,  3.28it/s] 89%| | 695/780 [04:48<00:25,  3.32it/s] 89%| | 696/780 [04:48<00:25,  3.35it/s] 89%| | 697/780 [04:48<00:24,  3.39it/s] 89%| | 698/780 [04:49<00:24,  3.41it/s] 90%| | 699/780 [04:49<00:23,  3.43it/s] 90%| | 700/780 [04:49<00:23,  3.43it/s] 90%| | 701/780 [04:50<00:22,  3.44it/s] 90%| | 702/780 [04:50<00:22,  3.45it/s] 90%| | 703/780 [04:50<00:22,  3.45it/s] 90%| | 704/780 [04:50<00:22,  3.45it/s] 90%| | 705/780 [04:51<00:23,  3.26it/s] 91%| | 706/780 [04:51<00:22,  3.32it/s] 91%| | 707/780 [04:51<00:21,  3.36it/s] 91%| | 708/780 [04:52<00:21,  3.39it/s] 91%| | 709/780 [04:52<00:20,  3.41it/s] 91%| | 710/780 [04:52<00:20,  3.43it/s] 91%| | 711/780 [04:53<00:20,  3.44it/s] 91%|| 712/780 [04:53<00:19,  3.45it/s] 91%|| 713/780 [04:53<00:19,  3.45it/s] 92%|| 714/780 [04:53<00:19,  3.45it/s] 92%|| 715/780 [04:54<00:18,  3.45it/s] 92%|| 716/780 [04:54<00:19,  3.30it/s] 92%|| 717/780 [04:54<00:18,  3.34it/s] 92%|| 718/780 [04:55<00:18,  3.38it/s] 92%|| 719/780 [04:55<00:17,  3.40it/s] 92%|| 720/780 [04:55<00:17,  3.42it/s] 92%|| 721/780 [04:55<00:17,  3.43it/s] 93%|| 722/780 [04:56<00:16,  3.44it/s] 93%|| 723/780 [04:56<00:17,  3.30it/s] 93%|| 724/780 [04:56<00:16,  3.35it/s] 93%|| 725/780 [04:57<00:16,  3.38it/s] 93%|| 726/780 [04:57<00:15,  3.41it/s] 93%|| 727/780 [04:58<00:23,  2.29it/s] 93%|| 728/780 [04:58<00:21,  2.46it/s] 93%|| 729/780 [04:58<00:18,  2.69it/s] 94%|| 730/780 [04:59<00:17,  2.89it/s] 94%|| 731/780 [04:59<00:16,  3.04it/s] 94%|| 732/780 [04:59<00:15,  3.15it/s] 94%|| 733/780 [05:00<00:14,  3.24it/s] 94%|| 734/780 [05:00<00:13,  3.30it/s] 94%|| 735/780 [05:00<00:13,  3.35it/s] 94%|| 736/780 [05:00<00:13,  3.38it/s] 94%|| 737/780 [05:01<00:12,  3.41it/s] 95%|| 738/780 [05:01<00:12,  3.42it/s] 95%|| 739/780 [05:01<00:11,  3.43it/s] 95%|| 740/780 [05:02<00:11,  3.44it/s] 95%|| 741/780 [05:02<00:11,  3.45it/s] 95%|| 742/780 [05:02<00:11,  3.45it/s] 95%|| 743/780 [05:02<00:10,  3.45it/s] 95%|| 744/780 [05:03<00:10,  3.39it/s] 96%|| 745/780 [05:03<00:10,  3.41it/s] 96%|| 746/780 [05:03<00:09,  3.43it/s] 96%|| 747/780 [05:04<00:09,  3.44it/s] 96%|| 748/780 [05:04<00:09,  3.44it/s] 96%|| 749/780 [05:04<00:09,  3.44it/s] 96%|| 750/780 [05:04<00:08,  3.45it/s] 96%|| 751/780 [05:05<00:08,  3.45it/s] 96%|| 752/780 [05:05<00:08,  3.45it/s] 97%|| 753/780 [05:05<00:07,  3.46it/s] 97%|| 754/780 [05:06<00:07,  3.46it/s] 97%|| 755/780 [05:06<00:07,  3.33it/s] 97%|| 756/780 [05:06<00:07,  3.37it/s] 97%|| 757/780 [05:07<00:06,  3.40it/s] 97%|| 758/780 [05:07<00:06,  3.42it/s] 97%|| 759/780 [05:07<00:06,  3.43it/s] 97%|| 760/780 [05:07<00:05,  3.44it/s] 98%|| 761/780 [05:08<00:05,  3.45it/s] 98%|| 762/780 [05:08<00:05,  3.46it/s] 98%|| 763/780 [05:08<00:04,  3.46it/s] 98%|| 764/780 [05:09<00:04,  3.46it/s] 98%|| 765/780 [05:09<00:04,  3.46it/s] 98%|| 766/780 [05:09<00:04,  3.40it/s] 98%|| 767/780 [05:09<00:03,  3.42it/s] 98%|| 768/780 [05:10<00:03,  3.43it/s] 99%|| 769/780 [05:10<00:03,  3.44it/s] 99%|| 770/780 [05:10<00:02,  3.45it/s] 99%|| 771/780 [05:11<00:02,  3.46it/s] 99%|| 772/780 [05:11<00:02,  3.46it/s] 99%|| 773/780 [05:11<00:02,  3.46it/s] 99%|| 774/780 [05:11<00:01,  3.46it/s] 99%|| 775/780 [05:12<00:01,  3.46it/s] 99%|| 776/780 [05:12<00:01,  3.45it/s]100%|| 777/780 [05:12<00:00,  3.39it/s]100%|| 778/780 [05:13<00:00,  3.41it/s]100%|| 779/780 [05:13<00:00,  3.43it/s]100%|| 780/780 [05:13<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 04:10:43,246 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:10:43,246 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 04:10:43,246 >>   Batch size = 8
{'eval_loss': 1.1090577840805054, 'eval_runtime': 9.8649, 'eval_samples_per_second': 352.967, 'eval_steps_per_second': 44.197, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 56.18it/s][A
  3%|         | 12/436 [00:00<00:08, 48.47it/s][A
  4%|         | 17/436 [00:00<00:09, 46.55it/s][A
  5%|         | 22/436 [00:00<00:09, 45.86it/s][A
  6%|         | 27/436 [00:00<00:09, 45.43it/s][A
  7%|         | 32/436 [00:00<00:08, 45.12it/s][A
  8%|         | 37/436 [00:00<00:08, 45.10it/s][A
 10%|         | 42/436 [00:00<00:08, 44.92it/s][A
 11%|         | 47/436 [00:01<00:08, 44.92it/s][A
 12%|        | 52/436 [00:01<00:08, 44.90it/s][A
 13%|        | 57/436 [00:01<00:08, 44.96it/s][A
 14%|        | 62/436 [00:01<00:08, 44.88it/s][A
 15%|        | 67/436 [00:01<00:08, 44.80it/s][A
 17%|        | 72/436 [00:01<00:08, 44.77it/s][A
 18%|        | 77/436 [00:01<00:08, 44.78it/s][A
 19%|        | 82/436 [00:01<00:07, 44.79it/s][A
 20%|        | 87/436 [00:01<00:07, 44.76it/s][A
 21%|        | 92/436 [00:02<00:08, 38.93it/s][A
 22%|       | 97/436 [00:02<00:08, 40.66it/s][A
 23%|       | 102/436 [00:02<00:07, 41.87it/s][A
 25%|       | 107/436 [00:02<00:07, 42.75it/s][A
 26%|       | 112/436 [00:02<00:07, 43.49it/s][A
 27%|       | 117/436 [00:02<00:07, 44.04it/s][A
 28%|       | 122/436 [00:02<00:07, 44.44it/s][A
 29%|       | 127/436 [00:02<00:06, 44.65it/s][A
 30%|       | 132/436 [00:02<00:06, 44.28it/s][A
 31%|      | 137/436 [00:03<00:06, 44.05it/s][A
 33%|      | 142/436 [00:03<00:06, 44.25it/s][A
 34%|      | 147/436 [00:03<00:06, 44.41it/s][A
 35%|      | 152/436 [00:03<00:06, 44.66it/s][A
 36%|      | 157/436 [00:03<00:06, 44.85it/s][A
 37%|      | 162/436 [00:03<00:06, 45.02it/s][A
 38%|      | 167/436 [00:03<00:05, 45.19it/s][A
 39%|      | 172/436 [00:03<00:05, 44.97it/s][A
 41%|      | 177/436 [00:03<00:05, 44.68it/s][A
 42%|     | 182/436 [00:04<00:05, 44.42it/s][A
 43%|     | 187/436 [00:04<00:05, 44.34it/s][A
 44%|     | 192/436 [00:04<00:05, 44.44it/s][A
 45%|     | 197/436 [00:04<00:05, 44.60it/s][A
 46%|     | 202/436 [00:04<00:05, 44.82it/s][A
 47%|     | 207/436 [00:04<00:05, 44.94it/s][A
 49%|     | 212/436 [00:04<00:04, 45.04it/s][A
 50%|     | 217/436 [00:04<00:04, 44.96it/s][A
 51%|     | 222/436 [00:05<00:04, 44.70it/s][A
 52%|    | 227/436 [00:05<00:04, 43.23it/s][A
 53%|    | 232/436 [00:05<00:04, 43.57it/s][A
 54%|    | 237/436 [00:05<00:04, 43.87it/s][A
 56%|    | 242/436 [00:05<00:04, 44.05it/s][A
 57%|    | 247/436 [00:05<00:04, 44.28it/s][A
 58%|    | 252/436 [00:05<00:04, 44.43it/s][A
 59%|    | 257/436 [00:05<00:04, 44.54it/s][A
 60%|    | 262/436 [00:05<00:03, 44.40it/s][A
 61%|    | 267/436 [00:06<00:03, 44.15it/s][A
 62%|   | 272/436 [00:06<00:03, 44.29it/s][A
 64%|   | 277/436 [00:06<00:03, 44.29it/s][A
 65%|   | 282/436 [00:06<00:03, 44.40it/s][A
 66%|   | 287/436 [00:06<00:03, 44.46it/s][A
 67%|   | 292/436 [00:06<00:03, 44.65it/s][A
 68%|   | 297/436 [00:06<00:03, 44.91it/s][A
 69%|   | 302/436 [00:06<00:02, 44.97it/s][A
 70%|   | 307/436 [00:06<00:02, 44.80it/s][A
 72%|  | 312/436 [00:07<00:02, 44.53it/s][A
 73%|  | 317/436 [00:07<00:02, 44.30it/s][A
 74%|  | 322/436 [00:07<00:02, 44.34it/s][A
 75%|  | 327/436 [00:07<00:02, 44.46it/s][A
 76%|  | 332/436 [00:07<00:02, 44.57it/s][A
 77%|  | 337/436 [00:07<00:02, 44.82it/s][A
 78%|  | 342/436 [00:07<00:02, 44.90it/s][A
 80%|  | 347/436 [00:07<00:01, 44.97it/s][A
 81%|  | 352/436 [00:07<00:01, 44.88it/s][A
 82%| | 357/436 [00:08<00:01, 44.66it/s][A
 83%| | 362/436 [00:08<00:01, 44.55it/s][A
 84%| | 367/436 [00:08<00:01, 44.46it/s][A
 85%| | 372/436 [00:08<00:01, 44.47it/s][A
 86%| | 377/436 [00:08<00:01, 44.63it/s][A
 88%| | 382/436 [00:08<00:01, 44.80it/s][A
 89%| | 387/436 [00:08<00:01, 44.85it/s][A
 90%| | 392/436 [00:08<00:00, 44.88it/s][A
 91%| | 397/436 [00:08<00:00, 44.75it/s][A
 92%|| 402/436 [00:09<00:00, 44.62it/s][A
 93%|| 407/436 [00:09<00:00, 44.52it/s][A
 94%|| 412/436 [00:09<00:00, 44.45it/s][A
 96%|| 417/436 [00:09<00:00, 44.48it/s][A
 97%|| 422/436 [00:09<00:00, 44.69it/s][A
 98%|| 427/436 [00:09<00:00, 44.76it/s][A
 99%|| 432/436 [00:09<00:00, 44.89it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.89it/s][A100%|| 780/780 [05:23<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:10:53,216 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 04:10:53,336 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:10:56,180 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:10:56,304 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:10:56,373 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:11:02,382 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:11:02,412 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156 (score: 1.0694361925125122).
                                                 100%|| 780/780 [05:41<00:00,  3.44it/s]100%|| 780/780 [05:41<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-29 04:11:10,925 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 04:11:11,132 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:11:14,192 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:11:14,301 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:11:14,351 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:11:14,705 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:14,705 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:14,705 >>   train_loss               =     0.6332
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:14,705 >>   train_runtime            = 0:05:41.30
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:14,705 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:14,705 >>   train_samples_per_second =    146.497
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:14,705 >>   train_steps_per_second   =      2.285
{'eval_loss': 1.1146090030670166, 'eval_runtime': 9.8221, 'eval_samples_per_second': 354.506, 'eval_steps_per_second': 44.39, 'epoch': 5.0}
{'train_runtime': 341.3034, 'train_samples_per_second': 146.497, 'train_steps_per_second': 2.285, 'train_loss': 0.6332488426795373, 'epoch': 5.0}
08/29/2023 04:11:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:11:14,911 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:11:14,911 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 04:11:14,911 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.71it/s]  3%|         | 12/436 [00:00<00:08, 49.11it/s]  4%|         | 17/436 [00:00<00:08, 47.74it/s]  5%|         | 22/436 [00:00<00:08, 46.73it/s]  6%|         | 27/436 [00:00<00:08, 46.32it/s]  7%|         | 32/436 [00:00<00:08, 46.07it/s]  8%|         | 37/436 [00:00<00:08, 45.87it/s] 10%|         | 42/436 [00:00<00:08, 45.50it/s] 11%|         | 47/436 [00:01<00:08, 44.79it/s] 12%|        | 52/436 [00:01<00:08, 44.59it/s] 13%|        | 57/436 [00:01<00:08, 44.75it/s] 14%|        | 62/436 [00:01<00:08, 44.87it/s] 15%|        | 67/436 [00:01<00:08, 45.03it/s] 17%|        | 72/436 [00:01<00:08, 45.05it/s] 18%|        | 77/436 [00:01<00:07, 45.23it/s] 19%|        | 82/436 [00:01<00:07, 45.19it/s] 20%|        | 87/436 [00:01<00:07, 45.02it/s] 21%|        | 92/436 [00:02<00:07, 44.62it/s] 22%|       | 97/436 [00:02<00:07, 44.51it/s] 23%|       | 102/436 [00:02<00:07, 44.64it/s] 25%|       | 107/436 [00:02<00:07, 44.66it/s] 26%|       | 112/436 [00:02<00:07, 43.42it/s] 27%|       | 117/436 [00:02<00:07, 44.03it/s] 28%|       | 122/436 [00:02<00:07, 44.45it/s] 29%|       | 127/436 [00:02<00:06, 44.73it/s] 30%|       | 132/436 [00:02<00:06, 44.58it/s] 31%|      | 137/436 [00:03<00:06, 44.55it/s] 33%|      | 142/436 [00:03<00:06, 44.52it/s] 34%|      | 147/436 [00:03<00:06, 44.58it/s] 35%|      | 152/436 [00:03<00:06, 44.47it/s] 36%|      | 157/436 [00:03<00:06, 44.69it/s] 37%|      | 162/436 [00:03<00:06, 44.83it/s] 38%|      | 167/436 [00:03<00:05, 45.03it/s] 39%|      | 172/436 [00:03<00:05, 44.95it/s] 41%|      | 177/436 [00:03<00:05, 44.94it/s] 42%|     | 182/436 [00:04<00:05, 44.77it/s] 43%|     | 187/436 [00:04<00:05, 44.58it/s] 44%|     | 192/436 [00:04<00:05, 44.62it/s] 45%|     | 197/436 [00:04<00:05, 44.60it/s] 46%|     | 202/436 [00:04<00:05, 44.75it/s] 47%|     | 207/436 [00:04<00:05, 44.88it/s] 49%|     | 212/436 [00:04<00:04, 44.93it/s] 50%|     | 217/436 [00:04<00:04, 45.06it/s] 51%|     | 222/436 [00:04<00:04, 44.98it/s] 52%|    | 227/436 [00:05<00:04, 44.90it/s] 53%|    | 232/436 [00:05<00:04, 44.71it/s] 54%|    | 237/436 [00:05<00:04, 44.73it/s] 56%|    | 242/436 [00:05<00:04, 44.69it/s] 57%|    | 247/436 [00:05<00:04, 44.41it/s] 58%|    | 252/436 [00:05<00:04, 44.75it/s] 59%|    | 257/436 [00:05<00:03, 44.84it/s] 60%|    | 262/436 [00:05<00:03, 44.88it/s] 61%|    | 267/436 [00:05<00:03, 44.81it/s] 62%|   | 272/436 [00:06<00:03, 44.80it/s] 64%|   | 277/436 [00:06<00:03, 44.71it/s] 65%|   | 282/436 [00:06<00:03, 44.66it/s] 66%|   | 287/436 [00:06<00:03, 44.67it/s] 67%|   | 292/436 [00:06<00:03, 44.76it/s] 68%|   | 297/436 [00:06<00:03, 39.07it/s] 69%|   | 303/436 [00:06<00:03, 42.40it/s] 71%|   | 308/436 [00:06<00:02, 43.27it/s] 72%|  | 313/436 [00:06<00:02, 43.87it/s] 73%|  | 318/436 [00:07<00:02, 44.24it/s] 74%|  | 323/436 [00:07<00:02, 44.51it/s] 75%|  | 328/436 [00:07<00:02, 44.55it/s] 76%|  | 333/436 [00:07<00:02, 44.70it/s] 78%|  | 338/436 [00:07<00:02, 44.70it/s] 79%|  | 343/436 [00:07<00:02, 44.32it/s] 80%|  | 348/436 [00:07<00:01, 44.39it/s] 81%|  | 353/436 [00:07<00:01, 44.61it/s] 82%| | 358/436 [00:08<00:01, 44.82it/s] 83%| | 363/436 [00:08<00:01, 44.97it/s] 84%| | 368/436 [00:08<00:01, 45.05it/s] 86%| | 373/436 [00:08<00:01, 44.97it/s] 87%| | 378/436 [00:08<00:01, 44.96it/s] 88%| | 383/436 [00:08<00:01, 44.76it/s] 89%| | 388/436 [00:08<00:01, 44.55it/s] 90%| | 393/436 [00:08<00:01, 39.28it/s] 91%|| 398/436 [00:08<00:00, 40.86it/s] 92%|| 403/436 [00:09<00:00, 42.19it/s] 94%|| 408/436 [00:09<00:00, 43.11it/s] 95%|| 413/436 [00:09<00:00, 43.51it/s] 96%|| 418/436 [00:09<00:00, 44.11it/s] 97%|| 423/436 [00:09<00:00, 44.49it/s] 98%|| 428/436 [00:09<00:00, 44.53it/s] 99%|| 433/436 [00:09<00:00, 44.29it/s]100%|| 436/436 [00:09<00:00, 44.51it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:11:24,724 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   eval_loss               =     1.0694
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   eval_runtime            = 0:00:09.81
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   eval_samples_per_second =     354.83
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   eval_steps_per_second   =      44.43
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:11:24,724 >>   perplexity              =     2.9137
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:38,849 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:38,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:38,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:38,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:38,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:11:39,210 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:11:39,211 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:11:39,494 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:11:40,570 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:11:40,570 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:43,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:43,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:43,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:43,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:11:43,705 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:11:44,365 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:11:44,366 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:11:44,941 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:11:45,114 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:11:45,115 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.54it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:15,  1.62it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:16,  1.57it/s]Extractor Predicting: 28it [00:17,  1.54it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:20,  1.48it/s]Extractor Predicting: 34it [00:21,  1.46it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.54it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:26,  1.57it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.51it/s]Extractor Predicting: 50it [00:31,  1.51it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:33,  1.54it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:37,  1.52it/s]Extractor Predicting: 60it [00:38,  1.52it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:43,  1.58it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:44,  1.58it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:45,  1.62it/s]Extractor Predicting: 72it [00:46,  1.62it/s]Extractor Predicting: 73it [00:46,  1.67it/s]Extractor Predicting: 74it [00:47,  1.67it/s]Extractor Predicting: 75it [00:47,  1.63it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:49,  1.59it/s]Extractor Predicting: 78it [00:49,  1.59it/s]Extractor Predicting: 79it [00:50,  1.57it/s]Extractor Predicting: 80it [00:51,  1.57it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:53,  1.57it/s]Extractor Predicting: 84it [00:53,  1.54it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:54,  1.54it/s]Extractor Predicting: 87it [00:55,  1.58it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:56,  1.53it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:58,  1.50it/s]Extractor Predicting: 92it [00:58,  1.51it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.52it/s]Extractor Predicting: 95it [01:00,  1.56it/s]Extractor Predicting: 96it [01:01,  1.57it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:02,  1.53it/s]Extractor Predicting: 99it [01:03,  1.49it/s]Extractor Predicting: 100it [01:04,  1.48it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.37it/s]Extractor Predicting: 103it [01:06,  1.40it/s]Extractor Predicting: 104it [01:07,  1.45it/s]Extractor Predicting: 105it [01:07,  1.47it/s]Extractor Predicting: 106it [01:08,  1.44it/s]Extractor Predicting: 107it [01:09,  1.47it/s]Extractor Predicting: 108it [01:09,  1.50it/s]Extractor Predicting: 109it [01:10,  1.55it/s]Extractor Predicting: 110it [01:10,  1.56it/s]Extractor Predicting: 111it [01:11,  1.61it/s]Extractor Predicting: 112it [01:12,  1.59it/s]Extractor Predicting: 113it [01:12,  1.57it/s]Extractor Predicting: 114it [01:13,  1.55it/s]Extractor Predicting: 115it [01:14,  1.53it/s]Extractor Predicting: 116it [01:14,  1.53it/s]Extractor Predicting: 117it [01:15,  1.53it/s]Extractor Predicting: 118it [01:16,  1.52it/s]Extractor Predicting: 119it [01:16,  1.53it/s]Extractor Predicting: 120it [01:17,  1.57it/s]Extractor Predicting: 121it [01:18,  1.53it/s]Extractor Predicting: 122it [01:18,  1.55it/s]Extractor Predicting: 123it [01:19,  1.52it/s]Extractor Predicting: 124it [01:19,  1.54it/s]Extractor Predicting: 125it [01:20,  1.54it/s]Extractor Predicting: 126it [01:21,  1.54it/s]Extractor Predicting: 127it [01:21,  1.54it/s]Extractor Predicting: 128it [01:22,  1.51it/s]Extractor Predicting: 129it [01:23,  1.52it/s]Extractor Predicting: 130it [01:23,  1.50it/s]Extractor Predicting: 131it [01:24,  1.53it/s]Extractor Predicting: 132it [01:25,  1.55it/s]Extractor Predicting: 133it [01:25,  1.51it/s]Extractor Predicting: 134it [01:26,  1.47it/s]Extractor Predicting: 135it [01:27,  1.49it/s]Extractor Predicting: 136it [01:27,  1.50it/s]Extractor Predicting: 137it [01:28,  1.51it/s]Extractor Predicting: 138it [01:29,  1.52it/s]Extractor Predicting: 139it [01:29,  1.50it/s]Extractor Predicting: 140it [01:30,  1.49it/s]Extractor Predicting: 141it [01:31,  1.49it/s]Extractor Predicting: 142it [01:31,  1.52it/s]Extractor Predicting: 143it [01:32,  1.56it/s]Extractor Predicting: 143it [01:32,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:34,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:34,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:34,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:34,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:34,059 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:13:35,048 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:13:35,049 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:13:35,352 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:13:36,469 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:13:36,469 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:39,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:39,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:39,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:39,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:13:39,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:13:39,679 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:13:39,680 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:13:40,409 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:13:40,624 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:13:40,625 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.29838709677419356,
  "recall": 0.053130384836300976,
  "score": 0.0901999024865919,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.79it/s]Extractor Predicting: 3it [00:01,  1.72it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.73it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:18,  1.60it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:20,  1.54it/s]Extractor Predicting: 35it [00:21,  1.54it/s]Extractor Predicting: 36it [00:21,  1.54it/s]Extractor Predicting: 37it [00:22,  1.55it/s]Extractor Predicting: 38it [00:23,  1.53it/s]Extractor Predicting: 39it [00:23,  1.54it/s]Extractor Predicting: 40it [00:24,  1.51it/s]Extractor Predicting: 41it [00:25,  1.54it/s]Extractor Predicting: 42it [00:25,  1.55it/s]Extractor Predicting: 43it [00:26,  1.52it/s]Extractor Predicting: 44it [00:27,  1.54it/s]Extractor Predicting: 45it [00:27,  1.54it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.53it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:31,  1.55it/s]Extractor Predicting: 52it [00:32,  1.60it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:33,  1.55it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:38,  1.55it/s]Extractor Predicting: 63it [00:39,  1.60it/s]Extractor Predicting: 64it [00:39,  1.61it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:41,  1.57it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:43,  1.59it/s]Extractor Predicting: 70it [00:43,  1.58it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:45,  1.52it/s]Extractor Predicting: 74it [00:46,  1.55it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:48,  1.62it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:50,  1.58it/s]Extractor Predicting: 82it [00:51,  1.61it/s]Extractor Predicting: 83it [00:51,  1.59it/s]Extractor Predicting: 84it [00:52,  1.58it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:53,  1.56it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:55,  1.58it/s]Extractor Predicting: 89it [00:55,  1.63it/s]Extractor Predicting: 90it [00:56,  1.63it/s]Extractor Predicting: 91it [00:56,  1.62it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:58,  1.58it/s]Extractor Predicting: 94it [00:59,  1.40it/s]Extractor Predicting: 95it [00:59,  1.43it/s]Extractor Predicting: 96it [01:00,  1.45it/s]Extractor Predicting: 97it [01:01,  1.49it/s]Extractor Predicting: 98it [01:01,  1.49it/s]Extractor Predicting: 99it [01:02,  1.52it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:03,  1.52it/s]Extractor Predicting: 102it [01:04,  1.54it/s]Extractor Predicting: 103it [01:05,  1.55it/s]Extractor Predicting: 104it [01:05,  1.55it/s]Extractor Predicting: 105it [01:06,  1.54it/s]Extractor Predicting: 106it [01:06,  1.55it/s]Extractor Predicting: 107it [01:07,  1.54it/s]Extractor Predicting: 108it [01:08,  1.55it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:09,  1.52it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:10,  1.52it/s]Extractor Predicting: 113it [01:11,  1.54it/s]Extractor Predicting: 114it [01:12,  1.55it/s]Extractor Predicting: 115it [01:12,  1.54it/s]Extractor Predicting: 116it [01:13,  1.61it/s]Extractor Predicting: 117it [01:13,  1.69it/s]Extractor Predicting: 118it [01:14,  1.72it/s]Extractor Predicting: 119it [01:15,  1.71it/s]Extractor Predicting: 120it [01:15,  1.70it/s]Extractor Predicting: 121it [01:16,  1.68it/s]Extractor Predicting: 122it [01:16,  1.66it/s]Extractor Predicting: 123it [01:17,  1.64it/s]Extractor Predicting: 124it [01:18,  1.68it/s]Extractor Predicting: 125it [01:18,  1.74it/s]Extractor Predicting: 126it [01:19,  1.72it/s]Extractor Predicting: 127it [01:19,  1.72it/s]Extractor Predicting: 128it [01:20,  1.74it/s]Extractor Predicting: 129it [01:20,  1.71it/s]Extractor Predicting: 130it [01:21,  1.70it/s]Extractor Predicting: 131it [01:22,  1.72it/s]Extractor Predicting: 132it [01:22,  1.72it/s]Extractor Predicting: 133it [01:23,  1.71it/s]Extractor Predicting: 134it [01:23,  1.72it/s]Extractor Predicting: 135it [01:24,  1.74it/s]Extractor Predicting: 136it [01:24,  1.75it/s]Extractor Predicting: 137it [01:25,  1.77it/s]Extractor Predicting: 138it [01:26,  1.72it/s]Extractor Predicting: 139it [01:26,  1.69it/s]Extractor Predicting: 140it [01:27,  1.71it/s]Extractor Predicting: 141it [01:27,  1.68it/s]Extractor Predicting: 142it [01:28,  1.67it/s]Extractor Predicting: 143it [01:29,  1.70it/s]Extractor Predicting: 144it [01:29,  1.66it/s]Extractor Predicting: 145it [01:30,  1.67it/s]Extractor Predicting: 146it [01:30,  1.69it/s]Extractor Predicting: 147it [01:31,  1.69it/s]Extractor Predicting: 148it [01:32,  1.67it/s]Extractor Predicting: 149it [01:32,  1.73it/s]Extractor Predicting: 150it [01:33,  1.71it/s]Extractor Predicting: 151it [01:33,  1.68it/s]Extractor Predicting: 152it [01:34,  1.71it/s]Extractor Predicting: 153it [01:35,  1.73it/s]Extractor Predicting: 154it [01:35,  1.74it/s]Extractor Predicting: 155it [01:36,  1.74it/s]Extractor Predicting: 156it [01:36,  1.72it/s]Extractor Predicting: 157it [01:37,  1.71it/s]Extractor Predicting: 158it [01:37,  1.73it/s]Extractor Predicting: 159it [01:38,  1.68it/s]Extractor Predicting: 160it [01:39,  1.69it/s]Extractor Predicting: 161it [01:39,  1.69it/s]Extractor Predicting: 162it [01:40,  1.66it/s]Extractor Predicting: 163it [01:40,  1.68it/s]Extractor Predicting: 164it [01:41,  1.67it/s]Extractor Predicting: 165it [01:42,  1.70it/s]Extractor Predicting: 166it [01:42,  1.70it/s]Extractor Predicting: 167it [01:43,  1.71it/s]Extractor Predicting: 168it [01:43,  1.66it/s]Extractor Predicting: 169it [01:44,  1.67it/s]Extractor Predicting: 170it [01:45,  1.60it/s]Extractor Predicting: 171it [01:45,  1.59it/s]Extractor Predicting: 172it [01:46,  1.66it/s]Extractor Predicting: 173it [01:47,  1.61it/s]Extractor Predicting: 174it [01:47,  1.60it/s]Extractor Predicting: 175it [01:48,  1.58it/s]Extractor Predicting: 176it [01:48,  1.59it/s]Extractor Predicting: 177it [01:49,  1.57it/s]Extractor Predicting: 178it [01:50,  1.53it/s]Extractor Predicting: 179it [01:50,  1.55it/s]Extractor Predicting: 180it [01:51,  1.59it/s]Extractor Predicting: 181it [01:52,  1.59it/s]Extractor Predicting: 182it [01:52,  1.57it/s]Extractor Predicting: 183it [01:53,  1.58it/s]Extractor Predicting: 184it [01:54,  1.56it/s]Extractor Predicting: 185it [01:54,  1.55it/s]Extractor Predicting: 186it [01:55,  1.53it/s]Extractor Predicting: 187it [01:56,  1.52it/s]Extractor Predicting: 188it [01:56,  1.57it/s]Extractor Predicting: 189it [01:57,  1.57it/s]Extractor Predicting: 190it [01:57,  1.58it/s]Extractor Predicting: 191it [01:58,  1.55it/s]Extractor Predicting: 192it [01:59,  1.55it/s]Extractor Predicting: 193it [01:59,  1.53it/s]Extractor Predicting: 194it [02:00,  1.52it/s]Extractor Predicting: 195it [02:01,  1.52it/s]Extractor Predicting: 196it [02:01,  1.54it/s]Extractor Predicting: 197it [02:02,  1.53it/s]Extractor Predicting: 198it [02:03,  1.54it/s]Extractor Predicting: 199it [02:03,  1.54it/s]Extractor Predicting: 200it [02:04,  1.53it/s]Extractor Predicting: 201it [02:05,  1.37it/s]Extractor Predicting: 202it [02:06,  1.41it/s]Extractor Predicting: 203it [02:06,  1.46it/s]Extractor Predicting: 204it [02:07,  1.50it/s]Extractor Predicting: 205it [02:07,  1.57it/s]Extractor Predicting: 206it [02:08,  1.57it/s]Extractor Predicting: 207it [02:09,  1.56it/s]Extractor Predicting: 208it [02:09,  1.57it/s]Extractor Predicting: 209it [02:10,  1.56it/s]Extractor Predicting: 210it [02:11,  1.57it/s]Extractor Predicting: 211it [02:11,  1.61it/s]Extractor Predicting: 212it [02:12,  1.62it/s]Extractor Predicting: 213it [02:12,  1.65it/s]Extractor Predicting: 214it [02:13,  1.63it/s]Extractor Predicting: 215it [02:14,  1.58it/s]Extractor Predicting: 216it [02:14,  1.60it/s]Extractor Predicting: 217it [02:15,  1.61it/s]Extractor Predicting: 218it [02:15,  1.62it/s]Extractor Predicting: 219it [02:16,  1.65it/s]Extractor Predicting: 220it [02:17,  1.63it/s]Extractor Predicting: 221it [02:17,  1.66it/s]Extractor Predicting: 222it [02:18,  1.68it/s]Extractor Predicting: 223it [02:18,  1.65it/s]Extractor Predicting: 224it [02:19,  1.65it/s]Extractor Predicting: 225it [02:20,  1.65it/s]Extractor Predicting: 226it [02:20,  1.60it/s]Extractor Predicting: 227it [02:21,  1.57it/s]Extractor Predicting: 228it [02:22,  1.59it/s]Extractor Predicting: 229it [02:22,  1.59it/s]Extractor Predicting: 230it [02:23,  1.58it/s]Extractor Predicting: 231it [02:23,  1.59it/s]Extractor Predicting: 232it [02:24,  1.62it/s]Extractor Predicting: 233it [02:25,  1.68it/s]Extractor Predicting: 234it [02:25,  1.69it/s]Extractor Predicting: 235it [02:26,  1.68it/s]Extractor Predicting: 236it [02:26,  1.73it/s]Extractor Predicting: 237it [02:27,  1.74it/s]Extractor Predicting: 238it [02:27,  1.75it/s]Extractor Predicting: 239it [02:28,  1.74it/s]Extractor Predicting: 240it [02:29,  1.76it/s]Extractor Predicting: 241it [02:29,  1.75it/s]Extractor Predicting: 242it [02:30,  1.75it/s]Extractor Predicting: 243it [02:30,  1.80it/s]Extractor Predicting: 244it [02:31,  1.76it/s]Extractor Predicting: 245it [02:31,  1.83it/s]Extractor Predicting: 246it [02:32,  1.87it/s]Extractor Predicting: 247it [02:32,  1.81it/s]Extractor Predicting: 248it [02:33,  1.75it/s]Extractor Predicting: 249it [02:34,  1.77it/s]Extractor Predicting: 250it [02:34,  1.77it/s]Extractor Predicting: 251it [02:35,  1.79it/s]Extractor Predicting: 252it [02:35,  1.81it/s]Extractor Predicting: 253it [02:36,  1.77it/s]Extractor Predicting: 254it [02:36,  1.75it/s]Extractor Predicting: 255it [02:37,  1.78it/s]Extractor Predicting: 256it [02:38,  1.78it/s]Extractor Predicting: 257it [02:38,  1.81it/s]Extractor Predicting: 258it [02:39,  1.81it/s]Extractor Predicting: 259it [02:39,  1.83it/s]Extractor Predicting: 260it [02:40,  1.80it/s]Extractor Predicting: 261it [02:40,  1.70it/s]Extractor Predicting: 262it [02:41,  1.69it/s]Extractor Predicting: 263it [02:42,  1.64it/s]Extractor Predicting: 264it [02:42,  1.60it/s]Extractor Predicting: 265it [02:43,  1.57it/s]Extractor Predicting: 266it [02:44,  1.58it/s]Extractor Predicting: 267it [02:44,  1.62it/s]Extractor Predicting: 268it [02:45,  1.62it/s]Extractor Predicting: 269it [02:46,  1.58it/s]Extractor Predicting: 270it [02:46,  1.53it/s]Extractor Predicting: 271it [02:47,  1.53it/s]Extractor Predicting: 272it [02:47,  1.56it/s]Extractor Predicting: 273it [02:48,  1.52it/s]Extractor Predicting: 274it [02:49,  1.53it/s]Extractor Predicting: 275it [02:49,  1.54it/s]Extractor Predicting: 276it [02:50,  1.55it/s]Extractor Predicting: 277it [02:51,  1.54it/s]Extractor Predicting: 278it [02:51,  1.50it/s]Extractor Predicting: 279it [02:52,  1.55it/s]Extractor Predicting: 280it [02:53,  1.54it/s]Extractor Predicting: 281it [02:53,  1.53it/s]Extractor Predicting: 282it [02:54,  1.53it/s]Extractor Predicting: 283it [02:55,  1.54it/s]Extractor Predicting: 284it [02:55,  1.54it/s]Extractor Predicting: 285it [02:56,  1.55it/s]Extractor Predicting: 286it [02:57,  1.54it/s]Extractor Predicting: 287it [02:57,  1.53it/s]Extractor Predicting: 288it [02:58,  1.59it/s]Extractor Predicting: 289it [02:58,  1.59it/s]Extractor Predicting: 290it [02:59,  1.61it/s]Extractor Predicting: 291it [03:00,  1.62it/s]Extractor Predicting: 292it [03:00,  1.62it/s]Extractor Predicting: 293it [03:01,  1.62it/s]Extractor Predicting: 294it [03:02,  1.63it/s]Extractor Predicting: 295it [03:02,  1.63it/s]Extractor Predicting: 296it [03:03,  1.62it/s]Extractor Predicting: 297it [03:03,  1.64it/s]Extractor Predicting: 298it [03:04,  1.69it/s]Extractor Predicting: 299it [03:05,  1.65it/s]Extractor Predicting: 300it [03:05,  1.65it/s]Extractor Predicting: 301it [03:06,  1.64it/s]Extractor Predicting: 302it [03:06,  1.62it/s]Extractor Predicting: 303it [03:07,  1.61it/s]Extractor Predicting: 304it [03:08,  1.60it/s]Extractor Predicting: 305it [03:08,  1.59it/s]Extractor Predicting: 306it [03:09,  1.61it/s]Extractor Predicting: 307it [03:10,  1.61it/s]Extractor Predicting: 308it [03:10,  1.60it/s]Extractor Predicting: 309it [03:11,  1.57it/s]Extractor Predicting: 310it [03:11,  1.59it/s]Extractor Predicting: 311it [03:12,  1.54it/s]Extractor Predicting: 312it [03:13,  1.56it/s]Extractor Predicting: 313it [03:13,  1.58it/s]Extractor Predicting: 314it [03:14,  1.58it/s]Extractor Predicting: 315it [03:15,  1.60it/s]Extractor Predicting: 316it [03:15,  1.60it/s]Extractor Predicting: 317it [03:16,  1.63it/s]Extractor Predicting: 318it [03:16,  1.66it/s]Extractor Predicting: 319it [03:17,  1.44it/s]Extractor Predicting: 320it [03:18,  1.48it/s]Extractor Predicting: 321it [03:19,  1.51it/s]Extractor Predicting: 322it [03:19,  1.53it/s]Extractor Predicting: 323it [03:20,  1.58it/s]Extractor Predicting: 324it [03:20,  1.60it/s]Extractor Predicting: 325it [03:21,  1.60it/s]Extractor Predicting: 326it [03:22,  1.62it/s]Extractor Predicting: 327it [03:22,  1.61it/s]Extractor Predicting: 328it [03:23,  1.60it/s]Extractor Predicting: 329it [03:23,  1.61it/s]Extractor Predicting: 330it [03:24,  1.59it/s]Extractor Predicting: 331it [03:25,  1.59it/s]Extractor Predicting: 332it [03:25,  1.62it/s]Extractor Predicting: 333it [03:26,  1.59it/s]Extractor Predicting: 334it [03:27,  1.63it/s]Extractor Predicting: 335it [03:27,  1.60it/s]Extractor Predicting: 336it [03:28,  1.63it/s]Extractor Predicting: 337it [03:28,  1.61it/s]Extractor Predicting: 338it [03:29,  1.62it/s]Extractor Predicting: 339it [03:30,  1.62it/s]Extractor Predicting: 340it [03:30,  1.60it/s]Extractor Predicting: 341it [03:31,  1.63it/s]Extractor Predicting: 342it [03:32,  1.65it/s]Extractor Predicting: 343it [03:32,  1.61it/s]Extractor Predicting: 344it [03:33,  1.66it/s]Extractor Predicting: 345it [03:33,  1.62it/s]Extractor Predicting: 346it [03:34,  1.62it/s]Extractor Predicting: 347it [03:35,  1.63it/s]Extractor Predicting: 348it [03:35,  1.59it/s]Extractor Predicting: 349it [03:36,  1.61it/s]Extractor Predicting: 350it [03:37,  1.59it/s]Extractor Predicting: 351it [03:37,  1.61it/s]Extractor Predicting: 352it [03:38,  1.58it/s]Extractor Predicting: 353it [03:38,  1.59it/s]Extractor Predicting: 354it [03:39,  1.59it/s]Extractor Predicting: 355it [03:40,  1.58it/s]Extractor Predicting: 356it [03:40,  1.59it/s]Extractor Predicting: 357it [03:41,  1.57it/s]Extractor Predicting: 358it [03:42,  1.59it/s]Extractor Predicting: 359it [03:42,  1.60it/s]Extractor Predicting: 360it [03:43,  1.59it/s]Extractor Predicting: 361it [03:43,  1.59it/s]Extractor Predicting: 362it [03:44,  1.62it/s]Extractor Predicting: 363it [03:45,  1.62it/s]Extractor Predicting: 364it [03:45,  1.64it/s]Extractor Predicting: 365it [03:46,  1.62it/s]Extractor Predicting: 366it [03:46,  1.63it/s]Extractor Predicting: 367it [03:47,  1.62it/s]Extractor Predicting: 368it [03:48,  1.60it/s]Extractor Predicting: 369it [03:48,  1.57it/s]Extractor Predicting: 370it [03:49,  1.57it/s]Extractor Predicting: 371it [03:50,  1.58it/s]Extractor Predicting: 372it [03:50,  1.59it/s]Extractor Predicting: 373it [03:51,  1.59it/s]Extractor Predicting: 374it [03:52,  1.62it/s]Extractor Predicting: 375it [03:52,  1.61it/s]Extractor Predicting: 376it [03:53,  1.56it/s]Extractor Predicting: 377it [03:53,  1.63it/s]Extractor Predicting: 378it [03:54,  1.63it/s]Extractor Predicting: 379it [03:55,  1.66it/s]Extractor Predicting: 380it [03:55,  1.63it/s]Extractor Predicting: 381it [03:56,  1.63it/s]Extractor Predicting: 382it [03:56,  1.63it/s]Extractor Predicting: 383it [03:57,  1.60it/s]Extractor Predicting: 384it [03:58,  1.58it/s]Extractor Predicting: 385it [03:58,  1.60it/s]Extractor Predicting: 386it [03:59,  1.59it/s]Extractor Predicting: 387it [04:00,  1.58it/s]Extractor Predicting: 388it [04:00,  1.53it/s]Extractor Predicting: 389it [04:01,  1.55it/s]Extractor Predicting: 390it [04:02,  1.56it/s]Extractor Predicting: 391it [04:02,  1.59it/s]Extractor Predicting: 392it [04:03,  1.60it/s]Extractor Predicting: 393it [04:03,  1.59it/s]Extractor Predicting: 394it [04:04,  1.60it/s]Extractor Predicting: 395it [04:05,  1.61it/s]Extractor Predicting: 396it [04:05,  1.59it/s]Extractor Predicting: 397it [04:06,  1.60it/s]Extractor Predicting: 398it [04:07,  1.59it/s]Extractor Predicting: 399it [04:07,  1.63it/s]Extractor Predicting: 400it [04:08,  1.60it/s]Extractor Predicting: 401it [04:08,  1.59it/s]Extractor Predicting: 402it [04:09,  1.59it/s]Extractor Predicting: 403it [04:10,  1.57it/s]Extractor Predicting: 404it [04:10,  1.59it/s]Extractor Predicting: 405it [04:11,  1.60it/s]Extractor Predicting: 406it [04:12,  1.62it/s]Extractor Predicting: 407it [04:12,  1.62it/s]Extractor Predicting: 408it [04:13,  1.60it/s]Extractor Predicting: 409it [04:13,  1.58it/s]Extractor Predicting: 410it [04:14,  1.60it/s]Extractor Predicting: 411it [04:15,  1.60it/s]Extractor Predicting: 412it [04:15,  1.62it/s]Extractor Predicting: 413it [04:16,  1.62it/s]Extractor Predicting: 414it [04:17,  1.61it/s]Extractor Predicting: 415it [04:17,  1.59it/s]Extractor Predicting: 416it [04:18,  1.43it/s]Extractor Predicting: 417it [04:19,  1.47it/s]Extractor Predicting: 418it [04:19,  1.53it/s]Extractor Predicting: 419it [04:20,  1.57it/s]Extractor Predicting: 420it [04:20,  1.59it/s]Extractor Predicting: 421it [04:21,  1.58it/s]Extractor Predicting: 422it [04:22,  1.60it/s]Extractor Predicting: 423it [04:22,  1.61it/s]Extractor Predicting: 424it [04:23,  1.61it/s]Extractor Predicting: 425it [04:24,  1.58it/s]Extractor Predicting: 426it [04:24,  1.62it/s]Extractor Predicting: 427it [04:25,  1.61it/s]Extractor Predicting: 428it [04:25,  1.57it/s]Extractor Predicting: 429it [04:26,  1.81it/s]Extractor Predicting: 429it [04:26,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:19,562 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:19,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:19,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:19,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:19,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:18:20,392 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:18:20,394 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:18:20,999 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:18:22,116 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:18:22,116 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:25,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:25,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:25,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:25,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:18:25,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:18:25,945 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:18:25,946 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:18:26,538 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:18:26,773 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:18:26,773 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3004557291666667,
  "recall": 0.08976852752382805,
  "score": 0.1382357346113524,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.73it/s]Extractor Predicting: 5it [00:03,  1.63it/s]
[INFO|configuration_utils.py:515] 2023-08-29 04:18:31,796 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:18:31,797 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:18:31,881 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:18:31,882 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 04:18:31,924 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:18:44,813 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 04:18:44,839 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 04:18:44,909 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:18:44,910 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:18:44,955 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:18:44,989 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:18:44,989 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:18:44,989 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:18:44,989 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:18:44,989 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:18:44,989 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2777777777777778,
  "recall": 0.022935779816513763,
  "score": 0.04237288135593221,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 04:18:45,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:45,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:46,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:46,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:47,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:48,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:48,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:49,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:49,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:50,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:50,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:51,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:52,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:52,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:53,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:53,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:54,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:55,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:55,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:56,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:56,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:11<03:45, 11.86s/it][WARNING|generation_utils.py:914] 2023-08-29 04:18:57,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:57,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:58,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:58,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:59,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:18:59,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:00,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:01,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:01,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:02,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:02,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:03,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:03,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:04,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:04,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:05,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:06,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:06,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:07,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:07,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:23<03:26, 11.48s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:08,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:08,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:09,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:10,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:10,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:11,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:12,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:12,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:13,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:14,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:14,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:15,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:16,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:16,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:17,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:18,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:18,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:19,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:19,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:20,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:21,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:21,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:37<03:38, 12.87s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:22,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:23,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:24,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:24,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:25,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:25,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:26,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:27,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:27,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:28,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:28,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:29,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:30,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:30,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:31,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:31,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:32,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:33,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:33,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:34,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:49<03:19, 12.45s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:34,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:35,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:35,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:36,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:37,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:37,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:38,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:38,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:39,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:40,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:40,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:41,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:42,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:42,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:43,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:44,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:44,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:45,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:46,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:46,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:47,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:02<03:11, 12.80s/it][WARNING|generation_utils.py:914] 2023-08-29 04:19:48,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:48,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:49,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:49,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:50,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:51,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:51,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:52,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:52,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:53,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:54,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:54,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:55,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:56,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:56,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:57,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:57,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:58,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:58,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:19:59,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:14<02:55, 12.57s/it][WARNING|generation_utils.py:914] 2023-08-29 04:20:00,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:00,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:01,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:01,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:02,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:03,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:03,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:04,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:05,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:05,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:06,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:06,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:07,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:07,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:08,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:09,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:09,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:10,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:11,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:11,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:12,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:12,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:28<02:46, 12.81s/it][WARNING|generation_utils.py:914] 2023-08-29 04:20:13,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:14,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:14,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:15,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:15,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:16,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:16,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:17,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:18,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:19,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:19,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:20,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:20,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:21,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:21,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:22,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:22,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:23,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:24,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:24,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:25,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:25,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:26,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:26,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:42<02:38, 13.22s/it][WARNING|generation_utils.py:914] 2023-08-29 04:20:27,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:28,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:28,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:29,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:30,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:30,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:31,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:31,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:32,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:33,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:33,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:34,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:34,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:35,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:36,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:36,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:37,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:38,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:38,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:39,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:40,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [01:55<02:25, 13.21s/it][WARNING|generation_utils.py:914] 2023-08-29 04:20:40,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:41,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:41,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:42,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:43,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:43,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:44,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:44,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:45,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:46,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:46,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:47,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:47,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:48,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:48,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:49,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:50,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:50,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:51,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:51,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:07<02:07, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-29 04:20:52,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:53,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:53,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:54,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:55,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:55,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:56,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:56,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:57,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:57,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:58,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:58,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:20:59,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:00,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:00,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:01,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:01,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:02,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:03,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:03,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:04,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:04,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:20<01:54, 12.77s/it][WARNING|generation_utils.py:914] 2023-08-29 04:21:05,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:05,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:06,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:07,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:07,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:08,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:08,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:09,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:10,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:10,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:11,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:12,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:12,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:13,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:13,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:14,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:15,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:15,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:16,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:16,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:17,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:32<01:42, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-29 04:21:18,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:18,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:19,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:19,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:20,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:21,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:21,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:22,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:23,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:24,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:24,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:25,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:25,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:26,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:27,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:27,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:28,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:29,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:29,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:30,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [02:45<01:29, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-29 04:21:30,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:31,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:32,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:32,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:33,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:34,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:34,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:35,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:36,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:36,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:37,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:37,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:38,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:39,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:39,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:40,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:40,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:41,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:42,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:42,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:43,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [02:58<01:17, 12.90s/it][WARNING|generation_utils.py:914] 2023-08-29 04:21:44,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:44,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:45,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:45,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:46,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:47,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:47,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:48,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:49,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:49,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:50,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:51,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:52,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:52,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:53,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:53,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:54,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:55,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:55,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:56,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:57,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:12<01:05, 13.10s/it][WARNING|generation_utils.py:914] 2023-08-29 04:21:57,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:58,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:58,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:59,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:21:59,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:00,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:01,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:01,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:02,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:02,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:03,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:04,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:04,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:05,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:06,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:06,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:07,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:07,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:08,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:08,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:09,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:10,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:11,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:26<00:53, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-29 04:22:11,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:12,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:12,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:13,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:14,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:14,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:15,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:16,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:16,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:17,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:18,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:18,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:19,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:19,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:20,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:21,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:22,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:22,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:23,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:24,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:24,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:40<00:40, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-29 04:22:25,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:25,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:26,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:27,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:27,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:28,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:28,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:29,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:29,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:30,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:31,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:32,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:32,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:33,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:33,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:34,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:35,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:35,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:36,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [03:51<00:25, 12.88s/it][WARNING|generation_utils.py:914] 2023-08-29 04:22:36,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:37,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:38,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:38,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:39,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:39,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:40,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:40,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:41,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:41,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:42,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:43,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:43,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:44,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:44,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:45,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:45,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:46,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:47,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:47,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:48,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:03<00:12, 12.57s/it][WARNING|generation_utils.py:914] 2023-08-29 04:22:48,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:49,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:50,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:50,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:51,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:51,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:52,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:52,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:53,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:54,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:54,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:55,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:55,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:56,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:57,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:57,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:58,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:59,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:22:59,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:23:00,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:15<00:00, 12.43s/it]Generating: 100%|| 20/20 [04:15<00:00, 12.78s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:10,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:10,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:10,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:10,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:10,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:23:11,073 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:23:11,074 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:23:11,790 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:23:12,928 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:23:12,928 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:16,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:16,119 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:16,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:16,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:23:16,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:23:16,995 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:23:16,996 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:23:17,598 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:23:17,805 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:23:17,805 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9017857142857143, 'errors': {''}}
['Relation : licensed to broadcast to . Context : Following his initial film debut , " The New Man " ( 1982 ) , he starred in the series \' third season and also made his first TV appearance in " The Last Man on Earth " . Head Entity : The New Man , Tail Entity : CBS .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 502, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 596, 'raw': 608}
{'target': 600, 'success': 628, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.98125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.959375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8072916666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9211309523809523, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.9578125, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8892045454545454, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location of formation . Context : Following his victories in the 1972 Pan American Games , he represented Chile at the 2002 Copa Centurio , in Las Palmas . Head Entity : 2002 Pan American Games , Tail Entity : Las Palmas .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9196428571428571, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9107142857142857, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.95, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.959375, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 11660
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11760, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.65it/s]Extractor Estimating: 4it [00:02,  1.78it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.71it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.77it/s]Extractor Estimating: 9it [00:05,  1.86it/s]Extractor Estimating: 10it [00:05,  1.77it/s]Extractor Estimating: 11it [00:06,  1.73it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.74it/s]Extractor Estimating: 15it [00:08,  1.69it/s]Extractor Estimating: 16it [00:09,  1.67it/s]Extractor Estimating: 17it [00:09,  1.68it/s]Extractor Estimating: 18it [00:10,  1.69it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:11,  1.66it/s]Extractor Estimating: 21it [00:12,  1.70it/s]Extractor Estimating: 22it [00:12,  1.73it/s]Extractor Estimating: 23it [00:13,  1.72it/s]Extractor Estimating: 24it [00:14,  1.78it/s]Extractor Estimating: 25it [00:14,  1.88it/s]Extractor Estimating: 26it [00:15,  1.80it/s]Extractor Estimating: 27it [00:15,  1.73it/s]Extractor Estimating: 28it [00:16,  1.64it/s]Extractor Estimating: 29it [00:16,  1.68it/s]Extractor Estimating: 30it [00:17,  1.71it/s]Extractor Estimating: 31it [00:18,  1.69it/s]Extractor Estimating: 32it [00:18,  1.70it/s]Extractor Estimating: 33it [00:19,  1.72it/s]Extractor Estimating: 34it [00:19,  1.67it/s]Extractor Estimating: 35it [00:20,  1.66it/s]Extractor Estimating: 36it [00:21,  1.65it/s]Extractor Estimating: 37it [00:21,  1.61it/s]Extractor Estimating: 38it [00:22,  1.60it/s]Extractor Estimating: 39it [00:23,  1.62it/s]Extractor Estimating: 40it [00:23,  1.62it/s]Extractor Estimating: 41it [00:24,  1.62it/s]Extractor Estimating: 42it [00:24,  1.67it/s]Extractor Estimating: 43it [00:25,  1.67it/s]Extractor Estimating: 44it [00:26,  1.67it/s]Extractor Estimating: 45it [00:26,  1.68it/s]Extractor Estimating: 46it [00:27,  1.68it/s]Extractor Estimating: 47it [00:27,  1.67it/s]Extractor Estimating: 48it [00:28,  1.62it/s]Extractor Estimating: 49it [00:29,  1.64it/s]Extractor Estimating: 50it [00:29,  1.61it/s]Extractor Estimating: 51it [00:30,  1.62it/s]Extractor Estimating: 52it [00:30,  1.60it/s]Extractor Estimating: 53it [00:31,  1.61it/s]Extractor Estimating: 54it [00:32,  1.67it/s]Extractor Estimating: 55it [00:32,  1.70it/s]Extractor Estimating: 56it [00:33,  1.70it/s]Extractor Estimating: 57it [00:33,  1.71it/s]Extractor Estimating: 58it [00:34,  1.73it/s]Extractor Estimating: 59it [00:34,  1.73it/s]Extractor Estimating: 60it [00:35,  1.61it/s]Extractor Estimating: 61it [00:36,  1.60it/s]Extractor Estimating: 62it [00:37,  1.52it/s]Extractor Estimating: 63it [00:37,  1.62it/s]Extractor Estimating: 64it [00:38,  1.68it/s]Extractor Estimating: 65it [00:38,  1.65it/s]Extractor Estimating: 66it [00:39,  1.67it/s]Extractor Estimating: 67it [00:39,  1.77it/s]Extractor Estimating: 68it [00:40,  1.69it/s]Extractor Estimating: 69it [00:41,  1.73it/s]Extractor Estimating: 70it [00:41,  1.68it/s]Extractor Estimating: 71it [00:42,  1.64it/s]Extractor Estimating: 72it [00:42,  1.67it/s]Extractor Estimating: 73it [00:43,  1.66it/s]Extractor Estimating: 74it [00:44,  1.68it/s]Extractor Estimating: 75it [00:44,  1.68it/s]Extractor Estimating: 76it [00:45,  1.70it/s]Extractor Estimating: 77it [00:45,  1.58it/s]Extractor Estimating: 78it [00:46,  1.59it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:47,  1.55it/s]Extractor Estimating: 81it [00:48,  1.56it/s]Extractor Estimating: 82it [00:49,  1.52it/s]Extractor Estimating: 83it [00:49,  1.57it/s]Extractor Estimating: 84it [00:50,  1.61it/s]Extractor Estimating: 85it [00:51,  1.56it/s]Extractor Estimating: 86it [00:51,  1.56it/s]Extractor Estimating: 87it [00:52,  1.60it/s]Extractor Estimating: 88it [00:52,  1.63it/s]Extractor Estimating: 89it [00:53,  1.62it/s]Extractor Estimating: 90it [00:54,  1.63it/s]Extractor Estimating: 91it [00:54,  1.67it/s]Extractor Estimating: 92it [00:55,  1.56it/s]Extractor Estimating: 93it [00:56,  1.57it/s]Extractor Estimating: 94it [00:56,  1.59it/s]Extractor Estimating: 95it [00:57,  1.58it/s]Extractor Estimating: 96it [00:57,  1.61it/s]Extractor Estimating: 97it [00:58,  1.57it/s]Extractor Estimating: 98it [00:59,  1.62it/s]Extractor Estimating: 99it [00:59,  1.65it/s]Extractor Estimating: 100it [01:00,  1.61it/s]Extractor Estimating: 101it [01:00,  1.63it/s]Extractor Estimating: 102it [01:01,  1.70it/s]Extractor Estimating: 103it [01:02,  1.62it/s]Extractor Estimating: 104it [01:02,  1.66it/s]Extractor Estimating: 105it [01:03,  1.73it/s]Extractor Estimating: 106it [01:03,  1.75it/s]Extractor Estimating: 107it [01:04,  1.74it/s]Extractor Estimating: 108it [01:04,  1.81it/s]Extractor Estimating: 109it [01:05,  1.69it/s]Extractor Estimating: 110it [01:06,  1.71it/s]Extractor Estimating: 111it [01:06,  1.78it/s]Extractor Estimating: 112it [01:07,  1.73it/s]Extractor Estimating: 113it [01:07,  1.74it/s]Extractor Estimating: 114it [01:08,  1.70it/s]Extractor Estimating: 115it [01:09,  1.76it/s]Extractor Estimating: 116it [01:09,  1.77it/s]Extractor Estimating: 117it [01:10,  1.68it/s]Extractor Estimating: 118it [01:11,  1.54it/s]Extractor Estimating: 119it [01:11,  1.59it/s]Extractor Estimating: 120it [01:12,  1.60it/s]Extractor Estimating: 121it [01:12,  1.52it/s]Extractor Estimating: 122it [01:13,  1.65it/s]Extractor Estimating: 123it [01:13,  1.69it/s]Extractor Estimating: 124it [01:14,  1.66it/s]Extractor Estimating: 125it [01:15,  1.70it/s]Extractor Estimating: 126it [01:15,  1.72it/s]Extractor Estimating: 127it [01:16,  1.69it/s]Extractor Estimating: 128it [01:16,  1.65it/s]Extractor Estimating: 129it [01:17,  1.54it/s]Extractor Estimating: 130it [01:18,  1.54it/s]Extractor Estimating: 131it [01:18,  1.58it/s]Extractor Estimating: 132it [01:19,  1.64it/s]Extractor Estimating: 133it [01:20,  1.61it/s]Extractor Estimating: 134it [01:20,  1.70it/s]Extractor Estimating: 135it [01:21,  1.62it/s]Extractor Estimating: 136it [01:22,  1.58it/s]Extractor Estimating: 137it [01:22,  1.58it/s]Extractor Estimating: 138it [01:23,  1.54it/s]Extractor Estimating: 139it [01:23,  1.57it/s]Extractor Estimating: 140it [01:24,  1.50it/s]Extractor Estimating: 141it [01:25,  1.53it/s]Extractor Estimating: 142it [01:25,  1.53it/s]Extractor Estimating: 143it [01:26,  1.56it/s]Extractor Estimating: 144it [01:27,  1.61it/s]Extractor Estimating: 145it [01:27,  1.49it/s]Extractor Estimating: 146it [01:28,  1.53it/s]Extractor Estimating: 147it [01:29,  1.54it/s]Extractor Estimating: 148it [01:29,  1.54it/s]Extractor Estimating: 149it [01:30,  1.51it/s]Extractor Estimating: 150it [01:31,  1.55it/s]Extractor Estimating: 151it [01:31,  1.62it/s]Extractor Estimating: 152it [01:32,  1.68it/s]Extractor Estimating: 153it [01:32,  1.71it/s]Extractor Estimating: 154it [01:33,  1.71it/s]Extractor Estimating: 155it [01:34,  1.69it/s]Extractor Estimating: 156it [01:34,  1.73it/s]Extractor Estimating: 157it [01:35,  1.75it/s]Extractor Estimating: 158it [01:35,  1.79it/s]Extractor Estimating: 159it [01:36,  1.76it/s]Extractor Estimating: 160it [01:36,  1.77it/s]Extractor Estimating: 161it [01:37,  1.74it/s]Extractor Estimating: 162it [01:37,  1.74it/s]Extractor Estimating: 163it [01:38,  1.74it/s]Extractor Estimating: 164it [01:39,  1.70it/s]Extractor Estimating: 165it [01:39,  1.68it/s]Extractor Estimating: 166it [01:40,  1.65it/s]Extractor Estimating: 167it [01:40,  1.70it/s]Extractor Estimating: 168it [01:41,  1.70it/s]Extractor Estimating: 169it [01:42,  1.69it/s]Extractor Estimating: 170it [01:42,  1.71it/s]Extractor Estimating: 171it [01:43,  1.73it/s]Extractor Estimating: 172it [01:43,  1.63it/s]Extractor Estimating: 173it [01:44,  1.66it/s]Extractor Estimating: 174it [01:45,  1.70it/s]Extractor Estimating: 175it [01:45,  1.69it/s]Extractor Estimating: 176it [01:46,  1.65it/s]Extractor Estimating: 177it [01:47,  1.59it/s]Extractor Estimating: 178it [01:47,  1.67it/s]Extractor Estimating: 179it [01:48,  1.72it/s]Extractor Estimating: 180it [01:48,  1.79it/s]Extractor Estimating: 181it [01:49,  1.70it/s]Extractor Estimating: 182it [01:49,  1.74it/s]Extractor Estimating: 183it [01:50,  1.79it/s]Extractor Estimating: 184it [01:50,  1.74it/s]Extractor Estimating: 185it [01:51,  1.81it/s]Extractor Estimating: 186it [01:51,  1.84it/s]Extractor Estimating: 187it [01:52,  1.81it/s]Extractor Estimating: 188it [01:53,  1.82it/s]Extractor Estimating: 189it [01:53,  1.82it/s]Extractor Estimating: 190it [01:54,  1.75it/s]Extractor Estimating: 191it [01:54,  1.77it/s]Extractor Estimating: 192it [01:55,  1.78it/s]Extractor Estimating: 193it [01:55,  1.80it/s]Extractor Estimating: 194it [01:56,  1.73it/s]Extractor Estimating: 195it [01:57,  1.77it/s]Extractor Estimating: 196it [01:57,  1.63it/s]Extractor Estimating: 197it [01:58,  1.69it/s]Extractor Estimating: 198it [01:58,  1.66it/s]Extractor Estimating: 199it [01:59,  1.71it/s]Extractor Estimating: 200it [02:00,  1.68it/s]Extractor Estimating: 201it [02:00,  1.69it/s]Extractor Estimating: 202it [02:01,  1.67it/s]Extractor Estimating: 203it [02:01,  1.64it/s]Extractor Estimating: 204it [02:02,  1.59it/s]Extractor Estimating: 205it [02:03,  1.60it/s]Extractor Estimating: 206it [02:03,  1.54it/s]Extractor Estimating: 207it [02:04,  1.56it/s]Extractor Estimating: 208it [02:05,  1.57it/s]Extractor Estimating: 209it [02:05,  1.55it/s]Extractor Estimating: 210it [02:06,  1.48it/s]Extractor Estimating: 211it [02:07,  1.56it/s]Extractor Estimating: 212it [02:07,  1.57it/s]Extractor Estimating: 213it [02:08,  1.55it/s]Extractor Estimating: 214it [02:09,  1.56it/s]Extractor Estimating: 215it [02:09,  1.59it/s]Extractor Estimating: 216it [02:10,  1.58it/s]Extractor Estimating: 217it [02:10,  1.65it/s]Extractor Estimating: 218it [02:11,  1.56it/s]Extractor Estimating: 219it [02:12,  1.59it/s]Extractor Estimating: 220it [02:12,  1.53it/s]Extractor Estimating: 221it [02:13,  1.56it/s]Extractor Estimating: 222it [02:14,  1.53it/s]Extractor Estimating: 223it [02:14,  1.51it/s]Extractor Estimating: 224it [02:15,  1.56it/s]Extractor Estimating: 225it [02:16,  1.57it/s]Extractor Estimating: 226it [02:16,  1.63it/s]Extractor Estimating: 227it [02:17,  1.62it/s]Extractor Estimating: 228it [02:17,  1.58it/s]Extractor Estimating: 229it [02:18,  1.57it/s]Extractor Estimating: 230it [02:19,  1.59it/s]Extractor Estimating: 231it [02:19,  1.64it/s]Extractor Estimating: 232it [02:20,  1.45it/s]Extractor Estimating: 233it [02:21,  1.47it/s]Extractor Estimating: 234it [02:21,  1.48it/s]Extractor Estimating: 235it [02:22,  1.43it/s]Extractor Estimating: 236it [02:23,  1.49it/s]Extractor Estimating: 237it [02:23,  1.56it/s]Extractor Estimating: 238it [02:24,  1.58it/s]Extractor Estimating: 239it [02:25,  1.59it/s]Extractor Estimating: 240it [02:25,  1.56it/s]Extractor Estimating: 241it [02:26,  1.62it/s]Extractor Estimating: 242it [02:26,  1.66it/s]Extractor Estimating: 243it [02:27,  1.57it/s]Extractor Estimating: 244it [02:28,  1.61it/s]Extractor Estimating: 245it [02:28,  1.60it/s]Extractor Estimating: 246it [02:29,  1.57it/s]Extractor Estimating: 247it [02:30,  1.62it/s]Extractor Estimating: 248it [02:30,  1.51it/s]Extractor Estimating: 249it [02:31,  1.56it/s]Extractor Estimating: 250it [02:32,  1.55it/s]Extractor Estimating: 251it [02:32,  1.59it/s]Extractor Estimating: 252it [02:33,  1.58it/s]Extractor Estimating: 253it [02:34,  1.57it/s]Extractor Estimating: 254it [02:34,  1.61it/s]Extractor Estimating: 255it [02:35,  1.61it/s]Extractor Estimating: 256it [02:35,  1.55it/s]Extractor Estimating: 257it [02:36,  1.58it/s]Extractor Estimating: 258it [02:37,  1.68it/s]Extractor Estimating: 259it [02:37,  1.71it/s]Extractor Estimating: 260it [02:38,  1.71it/s]Extractor Estimating: 261it [02:38,  1.62it/s]Extractor Estimating: 262it [02:39,  1.63it/s]Extractor Estimating: 263it [02:40,  1.61it/s]Extractor Estimating: 264it [02:40,  1.64it/s]Extractor Estimating: 265it [02:41,  1.63it/s]Extractor Estimating: 266it [02:41,  1.66it/s]Extractor Estimating: 267it [02:42,  1.62it/s]Extractor Estimating: 268it [02:43,  1.59it/s]Extractor Estimating: 269it [02:43,  1.60it/s]Extractor Estimating: 270it [02:44,  1.59it/s]Extractor Estimating: 271it [02:45,  1.63it/s]Extractor Estimating: 272it [02:45,  1.66it/s]Extractor Estimating: 273it [02:46,  1.70it/s]Extractor Estimating: 274it [02:46,  1.65it/s]Extractor Estimating: 275it [02:47,  1.64it/s]Extractor Estimating: 276it [02:48,  1.64it/s]Extractor Estimating: 277it [02:48,  1.61it/s]Extractor Estimating: 278it [02:49,  1.64it/s]Extractor Estimating: 279it [02:49,  1.60it/s]Extractor Estimating: 280it [02:50,  1.63it/s]Extractor Estimating: 281it [02:51,  1.65it/s]Extractor Estimating: 282it [02:51,  1.69it/s]Extractor Estimating: 283it [02:52,  1.69it/s]Extractor Estimating: 284it [02:52,  1.68it/s]Extractor Estimating: 285it [02:53,  1.66it/s]Extractor Estimating: 286it [02:54,  1.58it/s]Extractor Estimating: 287it [02:54,  1.60it/s]Extractor Estimating: 288it [02:55,  1.63it/s]Extractor Estimating: 289it [02:55,  1.64it/s]Extractor Estimating: 290it [02:56,  1.67it/s]Extractor Estimating: 291it [02:57,  1.67it/s]Extractor Estimating: 292it [02:57,  1.64it/s]Extractor Estimating: 293it [02:58,  1.64it/s]Extractor Estimating: 294it [02:58,  1.70it/s]Extractor Estimating: 295it [02:59,  1.63it/s]Extractor Estimating: 296it [03:00,  1.63it/s]Extractor Estimating: 297it [03:00,  1.65it/s]Extractor Estimating: 298it [03:01,  1.64it/s]Extractor Estimating: 299it [03:02,  1.62it/s]Extractor Estimating: 300it [03:02,  1.64it/s]Extractor Estimating: 301it [03:03,  1.70it/s]Extractor Estimating: 302it [03:03,  1.71it/s]Extractor Estimating: 303it [03:04,  1.80it/s]Extractor Estimating: 304it [03:04,  1.77it/s]Extractor Estimating: 305it [03:05,  1.76it/s]Extractor Estimating: 306it [03:06,  1.52it/s]Extractor Estimating: 307it [03:06,  1.57it/s]Extractor Estimating: 308it [03:07,  1.60it/s]Extractor Estimating: 309it [03:08,  1.62it/s]Extractor Estimating: 310it [03:08,  1.66it/s]Extractor Estimating: 311it [03:09,  1.62it/s]Extractor Estimating: 312it [03:09,  1.69it/s]Extractor Estimating: 313it [03:10,  1.72it/s]Extractor Estimating: 314it [03:10,  1.76it/s]Extractor Estimating: 315it [03:11,  1.79it/s]Extractor Estimating: 316it [03:12,  1.74it/s]Extractor Estimating: 317it [03:12,  1.75it/s]Extractor Estimating: 318it [03:13,  1.82it/s]Extractor Estimating: 319it [03:13,  1.85it/s]Extractor Estimating: 320it [03:14,  1.79it/s]Extractor Estimating: 321it [03:14,  1.77it/s]Extractor Estimating: 322it [03:15,  1.75it/s]Extractor Estimating: 323it [03:15,  1.74it/s]Extractor Estimating: 324it [03:16,  1.81it/s]Extractor Estimating: 325it [03:17,  1.74it/s]Extractor Estimating: 326it [03:17,  1.73it/s]Extractor Estimating: 327it [03:18,  1.77it/s]Extractor Estimating: 328it [03:18,  1.80it/s]Extractor Estimating: 329it [03:19,  1.79it/s]Extractor Estimating: 330it [03:19,  1.78it/s]Extractor Estimating: 331it [03:20,  1.76it/s]Extractor Estimating: 332it [03:21,  1.71it/s]Extractor Estimating: 333it [03:21,  1.75it/s]Extractor Estimating: 334it [03:22,  1.82it/s]Extractor Estimating: 335it [03:22,  1.75it/s]Extractor Estimating: 336it [03:23,  1.80it/s]Extractor Estimating: 337it [03:23,  1.76it/s]Extractor Estimating: 338it [03:24,  1.78it/s]Extractor Estimating: 339it [03:25,  1.79it/s]Extractor Estimating: 340it [03:25,  1.79it/s]Extractor Estimating: 341it [03:26,  1.78it/s]Extractor Estimating: 342it [03:26,  1.77it/s]Extractor Estimating: 343it [03:27,  1.73it/s]Extractor Estimating: 344it [03:27,  1.77it/s]Extractor Estimating: 345it [03:28,  1.77it/s]Extractor Estimating: 346it [03:28,  1.75it/s]Extractor Estimating: 347it [03:29,  1.75it/s]Extractor Estimating: 348it [03:30,  1.78it/s]Extractor Estimating: 349it [03:30,  1.73it/s]Extractor Estimating: 350it [03:31,  1.68it/s]Extractor Estimating: 351it [03:31,  1.65it/s]Extractor Estimating: 352it [03:32,  1.60it/s]Extractor Estimating: 353it [03:33,  1.60it/s]Extractor Estimating: 354it [03:33,  1.59it/s]Extractor Estimating: 355it [03:34,  1.62it/s]Extractor Estimating: 356it [03:35,  1.63it/s]Extractor Estimating: 357it [03:35,  1.61it/s]Extractor Estimating: 358it [03:36,  1.56it/s]Extractor Estimating: 359it [03:37,  1.60it/s]Extractor Estimating: 360it [03:37,  1.56it/s]Extractor Estimating: 361it [03:38,  1.62it/s]Extractor Estimating: 362it [03:38,  1.63it/s]Extractor Estimating: 363it [03:39,  1.66it/s]Extractor Estimating: 364it [03:40,  1.54it/s]Extractor Estimating: 365it [03:40,  1.52it/s]Extractor Estimating: 366it [03:41,  1.61it/s]Extractor Estimating: 367it [03:42,  1.61it/s]Extractor Estimating: 368it [03:42,  1.67it/s]Extractor Estimating: 369it [03:43,  1.65it/s]Extractor Estimating: 370it [03:43,  1.62it/s]Extractor Estimating: 371it [03:44,  1.68it/s]Extractor Estimating: 372it [03:44,  1.68it/s]Extractor Estimating: 373it [03:45,  1.65it/s]Extractor Estimating: 374it [03:46,  1.64it/s]Extractor Estimating: 375it [03:46,  1.61it/s]Extractor Estimating: 376it [03:47,  1.61it/s]Extractor Estimating: 377it [03:48,  1.64it/s]Extractor Estimating: 378it [03:48,  1.65it/s]Extractor Estimating: 379it [03:49,  1.69it/s]Extractor Estimating: 380it [03:49,  1.71it/s]Extractor Estimating: 381it [03:50,  1.69it/s]Extractor Estimating: 382it [03:51,  1.68it/s]Extractor Estimating: 383it [03:51,  1.50it/s]Extractor Estimating: 384it [03:52,  1.54it/s]Extractor Estimating: 385it [03:53,  1.51it/s]Extractor Estimating: 386it [03:53,  1.52it/s]Extractor Estimating: 387it [03:54,  1.55it/s]Extractor Estimating: 388it [03:55,  1.56it/s]Extractor Estimating: 389it [03:55,  1.57it/s]Extractor Estimating: 390it [03:56,  1.56it/s]Extractor Estimating: 391it [03:56,  1.56it/s]Extractor Estimating: 392it [03:57,  1.63it/s]Extractor Estimating: 393it [03:58,  1.63it/s]Extractor Estimating: 394it [03:58,  1.72it/s]Extractor Estimating: 395it [03:59,  1.69it/s]Extractor Estimating: 396it [03:59,  1.67it/s]Extractor Estimating: 397it [04:00,  1.62it/s]Extractor Estimating: 398it [04:01,  1.68it/s]Extractor Estimating: 399it [04:01,  1.56it/s]Extractor Estimating: 400it [04:02,  1.61it/s]Extractor Estimating: 401it [04:03,  1.61it/s]Extractor Estimating: 402it [04:03,  1.67it/s]Extractor Estimating: 403it [04:04,  1.70it/s]Extractor Estimating: 404it [04:04,  1.72it/s]Extractor Estimating: 405it [04:05,  1.76it/s]Extractor Estimating: 406it [04:05,  1.76it/s]Extractor Estimating: 407it [04:06,  1.72it/s]Extractor Estimating: 408it [04:06,  1.77it/s]Extractor Estimating: 409it [04:07,  1.76it/s]Extractor Estimating: 410it [04:08,  1.71it/s]Extractor Estimating: 411it [04:08,  1.71it/s]Extractor Estimating: 412it [04:09,  1.67it/s]Extractor Estimating: 413it [04:09,  1.73it/s]Extractor Estimating: 414it [04:10,  1.71it/s]Extractor Estimating: 415it [04:11,  1.73it/s]Extractor Estimating: 416it [04:11,  1.71it/s]Extractor Estimating: 417it [04:12,  1.68it/s]Extractor Estimating: 418it [04:12,  1.71it/s]Extractor Estimating: 419it [04:13,  1.74it/s]Extractor Estimating: 420it [04:14,  1.69it/s]Extractor Estimating: 421it [04:14,  1.69it/s]Extractor Estimating: 422it [04:15,  1.69it/s]Extractor Estimating: 423it [04:15,  1.71it/s]Extractor Estimating: 424it [04:16,  1.67it/s]Extractor Estimating: 425it [04:17,  1.57it/s]Extractor Estimating: 426it [04:17,  1.57it/s]Extractor Estimating: 427it [04:18,  1.60it/s]Extractor Estimating: 428it [04:19,  1.59it/s]Extractor Estimating: 429it [04:19,  1.62it/s]Extractor Estimating: 430it [04:20,  1.61it/s]Extractor Estimating: 431it [04:20,  1.68it/s]Extractor Estimating: 432it [04:21,  1.62it/s]Extractor Estimating: 433it [04:22,  1.61it/s]Extractor Estimating: 434it [04:22,  1.65it/s]Extractor Estimating: 435it [04:23,  1.62it/s]Extractor Estimating: 436it [04:23,  1.61it/s]Extractor Estimating: 437it [04:24,  1.60it/s]Extractor Estimating: 438it [04:25,  1.64it/s]Extractor Estimating: 439it [04:25,  1.68it/s]Extractor Estimating: 440it [04:26,  1.66it/s]Extractor Estimating: 441it [04:26,  1.63it/s]Extractor Estimating: 442it [04:27,  1.63it/s]Extractor Estimating: 443it [04:28,  1.60it/s]Extractor Estimating: 444it [04:28,  1.62it/s]Extractor Estimating: 445it [04:29,  1.58it/s]Extractor Estimating: 446it [04:30,  1.52it/s]Extractor Estimating: 447it [04:30,  1.56it/s]Extractor Estimating: 448it [04:31,  1.57it/s]Extractor Estimating: 449it [04:32,  1.52it/s]Extractor Estimating: 450it [04:32,  1.57it/s]Extractor Estimating: 451it [04:33,  1.60it/s]Extractor Estimating: 452it [04:33,  1.61it/s]Extractor Estimating: 453it [04:34,  1.62it/s]Extractor Estimating: 454it [04:35,  1.64it/s]Extractor Estimating: 455it [04:35,  1.68it/s]Extractor Estimating: 456it [04:36,  1.72it/s]Extractor Estimating: 457it [04:36,  1.74it/s]Extractor Estimating: 458it [04:37,  1.71it/s]Extractor Estimating: 459it [04:37,  1.73it/s]Extractor Estimating: 460it [04:38,  1.77it/s]Extractor Estimating: 461it [04:39,  1.80it/s]Extractor Estimating: 462it [04:39,  1.82it/s]Extractor Estimating: 463it [04:40,  1.83it/s]Extractor Estimating: 464it [04:40,  1.77it/s]Extractor Estimating: 465it [04:41,  1.77it/s]Extractor Estimating: 466it [04:42,  1.59it/s]Extractor Estimating: 467it [04:42,  1.57it/s]Extractor Estimating: 468it [04:43,  1.62it/s]Extractor Estimating: 469it [04:43,  1.67it/s]Extractor Estimating: 470it [04:44,  1.73it/s]Extractor Estimating: 471it [04:44,  1.71it/s]Extractor Estimating: 472it [04:45,  1.68it/s]Extractor Estimating: 473it [04:46,  1.72it/s]Extractor Estimating: 474it [04:46,  1.72it/s]Extractor Estimating: 475it [04:47,  1.68it/s]Extractor Estimating: 476it [04:48,  1.60it/s]Extractor Estimating: 477it [04:48,  1.63it/s]Extractor Estimating: 478it [04:49,  1.61it/s]Extractor Estimating: 479it [04:49,  1.65it/s]Extractor Estimating: 480it [04:50,  1.66it/s]Extractor Estimating: 481it [04:51,  1.61it/s]Extractor Estimating: 482it [04:51,  1.62it/s]Extractor Estimating: 483it [04:52,  1.61it/s]Extractor Estimating: 484it [04:52,  1.57it/s]Extractor Estimating: 485it [04:53,  1.55it/s]Extractor Estimating: 486it [04:54,  1.57it/s]Extractor Estimating: 487it [04:54,  1.58it/s]Extractor Estimating: 488it [04:55,  1.57it/s]Extractor Estimating: 489it [04:56,  1.60it/s]Extractor Estimating: 490it [04:56,  1.63it/s]Extractor Estimating: 491it [04:57,  1.61it/s]Extractor Estimating: 492it [04:57,  1.60it/s]Extractor Estimating: 493it [04:58,  1.60it/s]Extractor Estimating: 494it [04:59,  1.55it/s]Extractor Estimating: 495it [04:59,  1.59it/s]Extractor Estimating: 496it [05:00,  1.62it/s]Extractor Estimating: 497it [05:01,  1.59it/s]Extractor Estimating: 498it [05:01,  1.57it/s]Extractor Estimating: 499it [05:02,  1.55it/s]Extractor Estimating: 500it [05:02,  1.72it/s]Extractor Estimating: 500it [05:02,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:37,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:37,245 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:37,245 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:37,245 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:37,245 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:28:38,031 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:28:38,032 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:28:38,674 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:28:39,801 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:28:39,801 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:42,919 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:42,943 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:42,943 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:42,943 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:28:42,943 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:28:43,789 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:28:43,791 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:28:44,399 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:28:44,609 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:28:44,609 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 07:19:48,892 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 07:19:48,939 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9985 mean pseudo reward: 0.9296912395045072
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 21991
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22091, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22091, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.009, loss:716.8739
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.996, loss:655.3173
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.984, loss:666.5665
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.030, loss:647.1808
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.992, loss:623.7259
>> valid entity prec:0.5566, rec:0.5756, f1:0.5659
>> valid relation prec:0.0275, rec:0.0080, f1:0.0124
>> valid relation with NER prec:0.0275, rec:0.0080, f1:0.0124
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.255, loss:626.9762
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.980, loss:632.7002
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.984, loss:635.4395
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.016, loss:608.2297
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.982, loss:618.0812
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5909, rec:0.5240, f1:0.5555
>> valid relation prec:0.0633, rec:0.0161, f1:0.0257
>> valid relation with NER prec:0.0633, rec:0.0161, f1:0.0257
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.226, loss:627.7230
g_step 1200, step 366, avg_time 0.986, loss:667.7584
g_step 1300, step 49, avg_time 0.986, loss:610.9763
g_step 1400, step 149, avg_time 0.994, loss:626.7296
g_step 1500, step 249, avg_time 0.988, loss:612.9245
>> valid entity prec:0.5441, rec:0.5748, f1:0.5591
>> valid relation prec:0.0331, rec:0.0106, f1:0.0161
>> valid relation with NER prec:0.0331, rec:0.0106, f1:0.0161
g_step 1600, step 349, avg_time 2.228, loss:615.0256
g_step 1700, step 32, avg_time 0.981, loss:581.7944
g_step 1800, step 132, avg_time 0.974, loss:572.8196
g_step 1900, step 232, avg_time 0.984, loss:573.3145
g_step 2000, step 332, avg_time 0.998, loss:588.7288
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5286, rec:0.5618, f1:0.5447
>> valid relation prec:0.0544, rec:0.0138, f1:0.0220
>> valid relation with NER prec:0.0544, rec:0.0138, f1:0.0220
g_step 2100, step 15, avg_time 2.226, loss:583.8989
g_step 2200, step 115, avg_time 1.001, loss:536.6545
g_step 2300, step 215, avg_time 0.978, loss:544.8716
g_step 2400, step 315, avg_time 1.003, loss:590.0235
g_step 2500, step 415, avg_time 0.979, loss:591.2832
>> valid entity prec:0.5348, rec:0.5481, f1:0.5414
>> valid relation prec:0.0133, rec:0.0040, f1:0.0062
>> valid relation with NER prec:0.0133, rec:0.0040, f1:0.0062
g_step 2600, step 98, avg_time 2.223, loss:528.1758
g_step 2700, step 198, avg_time 1.008, loss:543.4807
g_step 2800, step 298, avg_time 0.970, loss:527.9082
g_step 2900, step 398, avg_time 1.004, loss:563.6437
g_step 3000, step 81, avg_time 0.988, loss:507.1516
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5637, rec:0.5262, f1:0.5443
>> valid relation prec:0.0339, rec:0.0109, f1:0.0165
>> valid relation with NER prec:0.0339, rec:0.0109, f1:0.0165
g_step 3100, step 181, avg_time 2.215, loss:506.9860
g_step 3200, step 281, avg_time 0.992, loss:529.4766
g_step 3300, step 381, avg_time 0.996, loss:532.1682
g_step 3400, step 64, avg_time 0.990, loss:493.5228
g_step 3500, step 164, avg_time 0.982, loss:490.0478
>> valid entity prec:0.5415, rec:0.5246, f1:0.5329
>> valid relation prec:0.0116, rec:0.0034, f1:0.0053
>> valid relation with NER prec:0.0116, rec:0.0034, f1:0.0053
g_step 3600, step 264, avg_time 2.217, loss:498.7950
g_step 3700, step 364, avg_time 1.001, loss:526.7891
g_step 3800, step 47, avg_time 1.002, loss:483.4319
g_step 3900, step 147, avg_time 0.996, loss:477.7147
g_step 4000, step 247, avg_time 0.994, loss:490.3914
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5345, rec:0.5143, f1:0.5242
>> valid relation prec:0.0229, rec:0.0066, f1:0.0103
>> valid relation with NER prec:0.0229, rec:0.0066, f1:0.0103
g_step 4100, step 347, avg_time 2.242, loss:491.7357
g_step 4200, step 30, avg_time 0.981, loss:462.7482
g_step 4300, step 130, avg_time 1.000, loss:454.8180
g_step 4400, step 230, avg_time 0.987, loss:480.5317
g_step 4500, step 330, avg_time 0.995, loss:483.8560
>> valid entity prec:0.5606, rec:0.5465, f1:0.5534
>> valid relation prec:0.0175, rec:0.0055, f1:0.0083
>> valid relation with NER prec:0.0175, rec:0.0055, f1:0.0083
g_step 4600, step 13, avg_time 2.230, loss:466.4654
g_step 4700, step 113, avg_time 0.999, loss:433.5293
g_step 4800, step 213, avg_time 0.981, loss:458.4580
g_step 4900, step 313, avg_time 1.012, loss:485.2326
g_step 5000, step 413, avg_time 0.992, loss:471.5407
learning rate was adjusted to 0.0008
>> valid entity prec:0.5526, rec:0.5213, f1:0.5365
>> valid relation prec:0.0331, rec:0.0106, f1:0.0161
>> valid relation with NER prec:0.0331, rec:0.0106, f1:0.0161
g_step 5100, step 96, avg_time 2.222, loss:439.3106
g_step 5200, step 196, avg_time 0.994, loss:432.5203
g_step 5300, step 296, avg_time 0.996, loss:448.2389
g_step 5400, step 396, avg_time 0.995, loss:450.3493
g_step 5500, step 79, avg_time 0.988, loss:432.2162
>> valid entity prec:0.5351, rec:0.5145, f1:0.5246
>> valid relation prec:0.0398, rec:0.0123, f1:0.0188
>> valid relation with NER prec:0.0398, rec:0.0123, f1:0.0188
g_step 5600, step 179, avg_time 2.241, loss:433.8798
g_step 5700, step 279, avg_time 1.002, loss:432.3208
g_step 5800, step 379, avg_time 0.981, loss:436.9084
g_step 5900, step 62, avg_time 0.993, loss:440.9413
g_step 6000, step 162, avg_time 0.992, loss:404.7908
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5591, rec:0.5130, f1:0.5351
>> valid relation prec:0.0470, rec:0.0146, f1:0.0223
>> valid relation with NER prec:0.0470, rec:0.0146, f1:0.0223
g_step 6100, step 262, avg_time 2.239, loss:417.7256
g_step 6200, step 362, avg_time 0.994, loss:423.0246
g_step 6300, step 45, avg_time 0.981, loss:406.0527
g_step 6400, step 145, avg_time 1.006, loss:391.5595
g_step 6500, step 245, avg_time 0.999, loss:407.4957
>> valid entity prec:0.5675, rec:0.5093, f1:0.5368
>> valid relation prec:0.0336, rec:0.0103, f1:0.0158
>> valid relation with NER prec:0.0336, rec:0.0103, f1:0.0158
g_step 6600, step 345, avg_time 2.227, loss:418.0414
g_step 6700, step 28, avg_time 0.991, loss:426.1774
g_step 6800, step 128, avg_time 0.989, loss:398.9934
g_step 6900, step 228, avg_time 1.000, loss:401.9646
g_step 7000, step 328, avg_time 1.006, loss:411.1833
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5312, rec:0.5316, f1:0.5314
>> valid relation prec:0.0287, rec:0.0089, f1:0.0136
>> valid relation with NER prec:0.0287, rec:0.0089, f1:0.0136
g_step 7100, step 11, avg_time 2.221, loss:396.4275
g_step 7200, step 111, avg_time 0.993, loss:368.2891
g_step 7300, step 211, avg_time 0.998, loss:385.7082
g_step 7400, step 311, avg_time 0.982, loss:388.3384
g_step 7500, step 411, avg_time 0.996, loss:402.0338
>> valid entity prec:0.5290, rec:0.5000, f1:0.5141
>> valid relation prec:0.0334, rec:0.0103, f1:0.0158
>> valid relation with NER prec:0.0334, rec:0.0103, f1:0.0158
g_step 7600, step 94, avg_time 2.229, loss:366.1007
g_step 7700, step 194, avg_time 0.996, loss:378.7745
g_step 7800, step 294, avg_time 0.985, loss:381.2642
g_step 7900, step 394, avg_time 0.997, loss:374.9555
g_step 8000, step 77, avg_time 0.974, loss:358.2766
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5536, rec:0.5266, f1:0.5398
>> valid relation prec:0.0311, rec:0.0098, f1:0.0149
>> valid relation with NER prec:0.0311, rec:0.0098, f1:0.0149
g_step 8100, step 177, avg_time 2.241, loss:340.0718
g_step 8200, step 277, avg_time 0.965, loss:384.2394
g_step 8300, step 377, avg_time 1.008, loss:367.6930
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 07:19:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 07:19:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_07-19-48_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 07:19:50 - WARNING - datasets.builder -   Using custom data configuration default-7c05c7da3dfd42cb
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7c05c7da3dfd42cb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 07:19:53,166 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:19:53,168 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:19:53,168 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:19:53,169 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:19:53,323 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:53,407 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:53,407 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:53,407 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:53,407 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:53,407 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:53,407 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 07:19:53,926 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:19:57,214 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 07:19:57,254 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7c05c7da3dfd42cb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.75ba/s] 20%|        | 2/10 [00:00<00:02,  3.75ba/s] 30%|       | 3/10 [00:00<00:01,  4.23ba/s] 40%|      | 4/10 [00:00<00:01,  4.51ba/s] 50%|     | 5/10 [00:01<00:01,  4.65ba/s] 60%|    | 6/10 [00:01<00:00,  4.72ba/s] 70%|   | 7/10 [00:01<00:00,  4.79ba/s] 80%|  | 8/10 [00:01<00:00,  4.83ba/s] 90%| | 9/10 [00:01<00:00,  4.83ba/s]100%|| 10/10 [00:02<00:00,  4.87ba/s]100%|| 10/10 [00:02<00:00,  4.58ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.69ba/s] 50%|     | 2/4 [00:00<00:00,  4.13ba/s] 75%|  | 3/4 [00:00<00:00,  4.29ba/s]100%|| 4/4 [00:00<00:00,  5.14ba/s]100%|| 4/4 [00:00<00:00,  4.70ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  6.28ba/s] 30%|       | 3/10 [00:00<00:00,  9.52ba/s] 50%|     | 5/10 [00:00<00:00, 10.46ba/s] 70%|   | 7/10 [00:00<00:00, 10.92ba/s] 90%| | 9/10 [00:00<00:00, 11.17ba/s]100%|| 10/10 [00:00<00:00, 10.69ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.19ba/s] 75%|  | 3/4 [00:00<00:00,  8.78ba/s]100%|| 4/4 [00:00<00:00,  9.81ba/s]
[INFO|trainer.py:414] 2023-08-29 07:20:02,773 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 07:20:02,781 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 07:20:02,781 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 07:20:02,781 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 07:20:02,781 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 07:20:02,782 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 07:20:02,782 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 07:20:02,782 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:51,  3.37it/s]  0%|          | 2/780 [00:00<03:46,  3.43it/s]  0%|          | 3/780 [00:00<03:53,  3.33it/s]  1%|          | 4/780 [00:01<03:50,  3.37it/s]  1%|          | 5/780 [00:01<03:48,  3.39it/s]  1%|          | 6/780 [00:01<03:47,  3.40it/s]  1%|          | 7/780 [00:02<03:46,  3.41it/s]  1%|          | 8/780 [00:02<03:46,  3.41it/s]  1%|          | 9/780 [00:02<03:45,  3.42it/s]  1%|         | 10/780 [00:02<03:45,  3.42it/s]  1%|         | 11/780 [00:03<03:44,  3.42it/s]  2%|         | 12/780 [00:03<03:44,  3.42it/s]  2%|         | 13/780 [00:03<03:44,  3.42it/s]  2%|         | 14/780 [00:04<03:48,  3.35it/s]  2%|         | 15/780 [00:04<03:46,  3.37it/s]  2%|         | 16/780 [00:04<03:45,  3.39it/s]  2%|         | 17/780 [00:05<03:44,  3.40it/s]  2%|         | 18/780 [00:05<03:43,  3.40it/s]  2%|         | 19/780 [00:05<03:43,  3.41it/s]  3%|         | 20/780 [00:05<03:42,  3.41it/s]  3%|         | 21/780 [00:06<03:42,  3.41it/s]  3%|         | 22/780 [00:06<03:41,  3.42it/s]  3%|         | 23/780 [00:06<03:41,  3.42it/s]  3%|         | 24/780 [00:07<03:41,  3.42it/s]  3%|         | 25/780 [00:07<03:47,  3.32it/s]  3%|         | 26/780 [00:07<03:45,  3.35it/s]  3%|         | 27/780 [00:07<03:43,  3.37it/s]  4%|         | 28/780 [00:08<03:42,  3.39it/s]  4%|         | 29/780 [00:08<03:40,  3.40it/s]  4%|         | 30/780 [00:08<03:40,  3.41it/s]  4%|         | 31/780 [00:09<03:39,  3.41it/s]  4%|         | 32/780 [00:09<03:38,  3.42it/s]  4%|         | 33/780 [00:09<03:38,  3.42it/s]  4%|         | 34/780 [00:09<03:37,  3.42it/s]  4%|         | 35/780 [00:10<03:37,  3.42it/s]  5%|         | 36/780 [00:10<03:44,  3.31it/s]  5%|         | 37/780 [00:10<03:42,  3.34it/s]  5%|         | 38/780 [00:11<03:40,  3.37it/s]  5%|         | 39/780 [00:11<03:39,  3.38it/s]  5%|         | 40/780 [00:11<03:38,  3.39it/s]  5%|         | 41/780 [00:12<03:37,  3.40it/s]  5%|         | 42/780 [00:12<03:36,  3.41it/s]  6%|         | 43/780 [00:12<03:36,  3.41it/s]  6%|         | 44/780 [00:12<03:35,  3.41it/s]  6%|         | 45/780 [00:13<03:35,  3.42it/s]  6%|         | 46/780 [00:13<03:34,  3.42it/s]  6%|         | 47/780 [00:13<03:39,  3.34it/s]  6%|         | 48/780 [00:14<03:37,  3.36it/s]  6%|         | 49/780 [00:14<03:36,  3.38it/s]  6%|         | 50/780 [00:14<03:35,  3.39it/s]  7%|         | 51/780 [00:15<03:34,  3.40it/s]  7%|         | 52/780 [00:15<03:33,  3.41it/s]  7%|         | 53/780 [00:15<03:33,  3.41it/s]  7%|         | 54/780 [00:15<03:32,  3.42it/s]  7%|         | 55/780 [00:16<03:32,  3.42it/s]  7%|         | 56/780 [00:16<03:31,  3.42it/s]  7%|         | 57/780 [00:16<03:31,  3.42it/s]  7%|         | 58/780 [00:17<03:36,  3.33it/s]  8%|         | 59/780 [00:17<03:34,  3.36it/s]  8%|         | 60/780 [00:17<03:33,  3.38it/s]  8%|         | 61/780 [00:17<03:32,  3.39it/s]  8%|         | 62/780 [00:18<03:31,  3.40it/s]  8%|         | 63/780 [00:18<03:30,  3.41it/s]  8%|         | 64/780 [00:18<03:29,  3.41it/s]  8%|         | 65/780 [00:19<03:29,  3.42it/s]  8%|         | 66/780 [00:19<03:28,  3.42it/s]  9%|         | 67/780 [00:19<03:28,  3.42it/s]  9%|         | 68/780 [00:20<03:28,  3.42it/s]  9%|         | 69/780 [00:20<03:30,  3.38it/s]  9%|         | 70/780 [00:20<03:29,  3.39it/s]  9%|         | 71/780 [00:20<03:28,  3.40it/s]  9%|         | 72/780 [00:21<03:28,  3.40it/s]  9%|         | 73/780 [00:21<03:29,  3.38it/s]  9%|         | 74/780 [00:21<03:28,  3.39it/s] 10%|         | 75/780 [00:22<03:27,  3.40it/s] 10%|         | 76/780 [00:22<03:25,  3.42it/s] 10%|         | 77/780 [00:22<03:24,  3.44it/s] 10%|         | 78/780 [00:22<03:23,  3.45it/s] 10%|         | 79/780 [00:23<03:22,  3.46it/s] 10%|         | 80/780 [00:23<03:22,  3.46it/s] 10%|         | 81/780 [00:23<03:21,  3.46it/s] 11%|         | 82/780 [00:24<03:21,  3.46it/s] 11%|         | 83/780 [00:24<03:21,  3.45it/s] 11%|         | 84/780 [00:24<03:26,  3.36it/s] 11%|         | 85/780 [00:24<03:25,  3.39it/s] 11%|         | 86/780 [00:25<03:23,  3.42it/s] 11%|         | 87/780 [00:25<03:21,  3.44it/s] 11%|        | 88/780 [00:25<03:20,  3.45it/s] 11%|        | 89/780 [00:26<03:19,  3.46it/s] 12%|        | 90/780 [00:26<03:19,  3.47it/s] 12%|        | 91/780 [00:26<03:18,  3.47it/s] 12%|        | 92/780 [00:27<03:18,  3.47it/s] 12%|        | 93/780 [00:27<03:17,  3.48it/s] 12%|        | 94/780 [00:27<03:17,  3.48it/s] 12%|        | 95/780 [00:27<03:24,  3.35it/s] 12%|        | 96/780 [00:28<03:21,  3.39it/s] 12%|        | 97/780 [00:28<03:19,  3.42it/s] 13%|        | 98/780 [00:28<03:18,  3.44it/s] 13%|        | 99/780 [00:29<03:17,  3.45it/s] 13%|        | 100/780 [00:29<03:16,  3.46it/s] 13%|        | 101/780 [00:29<03:15,  3.46it/s] 13%|        | 102/780 [00:29<03:15,  3.47it/s] 13%|        | 103/780 [00:30<03:15,  3.46it/s] 13%|        | 104/780 [00:30<03:15,  3.46it/s] 13%|        | 105/780 [00:30<03:15,  3.46it/s] 14%|        | 106/780 [00:31<03:22,  3.33it/s] 14%|        | 107/780 [00:31<03:19,  3.37it/s] 14%|        | 108/780 [00:31<03:17,  3.40it/s] 14%|        | 109/780 [00:31<03:15,  3.42it/s] 14%|        | 110/780 [00:32<03:14,  3.44it/s] 14%|        | 111/780 [00:32<03:13,  3.45it/s] 14%|        | 112/780 [00:32<03:13,  3.46it/s] 14%|        | 113/780 [00:33<03:12,  3.46it/s] 15%|        | 114/780 [00:33<03:12,  3.46it/s] 15%|        | 115/780 [00:33<03:12,  3.46it/s] 15%|        | 116/780 [00:33<03:11,  3.47it/s] 15%|        | 117/780 [00:34<03:14,  3.41it/s] 15%|        | 118/780 [00:34<03:13,  3.43it/s] 15%|        | 119/780 [00:34<03:12,  3.44it/s] 15%|        | 120/780 [00:35<03:11,  3.45it/s] 16%|        | 121/780 [00:35<03:10,  3.45it/s] 16%|        | 122/780 [00:35<03:10,  3.46it/s] 16%|        | 123/780 [00:36<03:09,  3.46it/s] 16%|        | 124/780 [00:36<03:09,  3.46it/s] 16%|        | 125/780 [00:36<03:09,  3.46it/s] 16%|        | 126/780 [00:36<03:08,  3.46it/s] 16%|        | 127/780 [00:37<03:08,  3.46it/s] 16%|        | 128/780 [00:37<03:13,  3.37it/s] 17%|        | 129/780 [00:37<03:11,  3.40it/s] 17%|        | 130/780 [00:38<03:10,  3.42it/s] 17%|        | 131/780 [00:38<03:09,  3.43it/s] 17%|        | 132/780 [00:38<03:08,  3.44it/s] 17%|        | 133/780 [00:38<03:07,  3.45it/s] 17%|        | 134/780 [00:39<03:07,  3.45it/s] 17%|        | 135/780 [00:39<03:06,  3.46it/s] 17%|        | 136/780 [00:39<03:06,  3.46it/s] 18%|        | 137/780 [00:40<03:05,  3.46it/s] 18%|        | 138/780 [00:40<03:05,  3.46it/s] 18%|        | 139/780 [00:40<03:10,  3.36it/s] 18%|        | 140/780 [00:40<03:08,  3.39it/s] 18%|        | 141/780 [00:41<03:07,  3.41it/s] 18%|        | 142/780 [00:41<03:05,  3.43it/s] 18%|        | 143/780 [00:41<03:05,  3.44it/s] 18%|        | 144/780 [00:42<03:04,  3.45it/s] 19%|        | 145/780 [00:42<03:03,  3.45it/s] 19%|        | 146/780 [00:42<03:03,  3.46it/s] 19%|        | 147/780 [00:43<03:02,  3.46it/s] 19%|        | 148/780 [00:43<03:02,  3.46it/s] 19%|        | 149/780 [00:43<03:02,  3.46it/s] 19%|        | 150/780 [00:43<03:06,  3.39it/s] 19%|        | 151/780 [00:44<03:04,  3.41it/s] 19%|        | 152/780 [00:44<03:03,  3.42it/s] 20%|        | 153/780 [00:44<03:02,  3.44it/s] 20%|        | 154/780 [00:45<03:01,  3.45it/s] 20%|        | 155/780 [00:45<03:01,  3.45it/s] 20%|        | 156/780 [00:45<03:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 07:20:48,450 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:20:48,450 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 07:20:48,450 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.78it/s][A
  3%|         | 12/436 [00:00<00:08, 48.49it/s][A
  4%|         | 17/436 [00:00<00:08, 46.67it/s][A
  5%|         | 22/436 [00:00<00:08, 46.00it/s][A
  6%|         | 27/436 [00:00<00:08, 45.53it/s][A
  7%|         | 32/436 [00:00<00:08, 45.22it/s][A
  8%|         | 37/436 [00:00<00:08, 45.14it/s][A
 10%|         | 42/436 [00:00<00:08, 44.94it/s][A
 11%|         | 47/436 [00:01<00:09, 40.80it/s][A
 12%|        | 52/436 [00:01<00:09, 39.45it/s][A
 13%|        | 57/436 [00:01<00:09, 41.28it/s][A
 14%|        | 62/436 [00:01<00:08, 42.43it/s][A
 15%|        | 67/436 [00:01<00:08, 43.34it/s][A
 17%|        | 72/436 [00:01<00:08, 43.99it/s][A
 18%|        | 77/436 [00:01<00:08, 44.37it/s][A
 19%|        | 82/436 [00:01<00:07, 44.43it/s][A
 20%|        | 87/436 [00:01<00:07, 44.18it/s][A
 21%|        | 92/436 [00:02<00:07, 44.20it/s][A
 22%|       | 97/436 [00:02<00:07, 44.15it/s][A
 23%|       | 102/436 [00:02<00:09, 36.94it/s][A
 24%|       | 106/436 [00:02<00:13, 24.54it/s][A
 25%|       | 110/436 [00:02<00:11, 27.18it/s][A
 26%|       | 115/436 [00:02<00:10, 31.16it/s][A
 28%|       | 120/436 [00:03<00:09, 34.64it/s][A
 29%|       | 125/436 [00:03<00:08, 37.41it/s][A
 30%|       | 130/436 [00:03<00:07, 39.57it/s][A
 31%|       | 135/436 [00:03<00:07, 41.20it/s][A
 32%|      | 140/436 [00:03<00:06, 42.47it/s][A
 33%|      | 145/436 [00:03<00:06, 43.08it/s][A
 34%|      | 150/436 [00:03<00:06, 43.13it/s][A
 36%|      | 155/436 [00:03<00:06, 43.40it/s][A
 37%|      | 160/436 [00:03<00:06, 43.81it/s][A
 38%|      | 165/436 [00:04<00:06, 42.44it/s][A
 39%|      | 170/436 [00:04<00:06, 43.36it/s][A
 40%|      | 175/436 [00:04<00:05, 43.91it/s][A
 41%|     | 180/436 [00:04<00:05, 44.42it/s][A
 42%|     | 185/436 [00:04<00:05, 44.68it/s][A
 44%|     | 190/436 [00:04<00:05, 44.61it/s][A
 45%|     | 195/436 [00:04<00:05, 44.34it/s][A
 46%|     | 200/436 [00:04<00:05, 44.25it/s][A
 47%|     | 205/436 [00:04<00:05, 44.20it/s][A
 48%|     | 210/436 [00:05<00:05, 44.43it/s][A
 49%|     | 215/436 [00:05<00:04, 44.70it/s][A
 50%|     | 220/436 [00:05<00:04, 44.90it/s][A
 52%|    | 225/436 [00:05<00:04, 45.07it/s][A
 53%|    | 230/436 [00:05<00:04, 45.19it/s][A
 54%|    | 235/436 [00:05<00:04, 45.06it/s][A
 55%|    | 240/436 [00:05<00:04, 44.65it/s][A
 56%|    | 245/436 [00:05<00:04, 44.56it/s][A
 57%|    | 250/436 [00:05<00:04, 44.50it/s][A
 58%|    | 255/436 [00:06<00:04, 44.60it/s][A
 60%|    | 260/436 [00:06<00:03, 44.89it/s][A
 61%|    | 265/436 [00:06<00:04, 36.75it/s][A
 62%|   | 270/436 [00:06<00:04, 39.06it/s][A
 63%|   | 275/436 [00:06<00:03, 40.74it/s][A
 64%|   | 280/436 [00:06<00:03, 42.08it/s][A
 65%|   | 285/436 [00:06<00:03, 43.00it/s][A
 67%|   | 290/436 [00:06<00:03, 43.71it/s][A
 68%|   | 295/436 [00:07<00:03, 44.19it/s][A
 69%|   | 300/436 [00:07<00:03, 44.37it/s][A
 70%|   | 305/436 [00:07<00:02, 44.15it/s][A
 71%|   | 310/436 [00:07<00:02, 43.95it/s][A
 72%|  | 315/436 [00:07<00:02, 44.05it/s][A
 73%|  | 320/436 [00:07<00:02, 44.34it/s][A
 75%|  | 325/436 [00:07<00:02, 44.60it/s][A
 76%|  | 330/436 [00:07<00:02, 44.88it/s][A
 77%|  | 335/436 [00:07<00:02, 45.04it/s][A
 78%|  | 340/436 [00:08<00:02, 45.15it/s][A
 79%|  | 345/436 [00:08<00:02, 45.08it/s][A
 80%|  | 350/436 [00:08<00:01, 44.64it/s][A
 81%| | 355/436 [00:08<00:01, 44.38it/s][A
 83%| | 360/436 [00:08<00:01, 44.34it/s][A
 84%| | 365/436 [00:08<00:01, 44.46it/s][A
 85%| | 370/436 [00:08<00:01, 44.70it/s][A
 86%| | 375/436 [00:08<00:01, 44.85it/s][A
 87%| | 380/436 [00:08<00:01, 45.00it/s][A
 88%| | 385/436 [00:09<00:01, 45.10it/s][A
 89%| | 390/436 [00:09<00:01, 45.14it/s][A
 91%| | 395/436 [00:09<00:00, 44.88it/s][A
 92%|| 400/436 [00:09<00:00, 42.69it/s][A
 93%|| 405/436 [00:09<00:00, 43.31it/s][A
 94%|| 410/436 [00:09<00:00, 43.76it/s][A
 95%|| 415/436 [00:09<00:00, 44.12it/s][A
 96%|| 420/436 [00:09<00:00, 44.39it/s][A
 97%|| 425/436 [00:09<00:00, 44.67it/s][A
 99%|| 430/436 [00:10<00:00, 44.81it/s][A
100%|| 435/436 [00:10<00:00, 44.84it/s][A
                                                 [A                                                 
100%|| 436/436 [00:10<00:00, 44.84it/s][A 20%|        | 156/780 [00:55<03:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:20:58,860 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 07:20:58,994 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:21:01,427 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:21:01,533 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:21:01,595 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:05<1:04:04,  6.17s/it] 20%|        | 158/780 [01:05<45:47,  4.42s/it]   20%|        | 159/780 [01:06<32:54,  3.18s/it] 21%|        | 160/780 [01:06<23:54,  2.31s/it] 21%|        | 161/780 [01:06<17:36,  1.71s/it] 21%|        | 162/780 [01:07<13:12,  1.28s/it] 21%|        | 163/780 [01:07<10:08,  1.01it/s] 21%|        | 164/780 [01:07<07:59,  1.29it/s] 21%|        | 165/780 [01:07<06:28,  1.58it/s] 21%|       | 166/780 [01:08<05:25,  1.89it/s] 21%|       | 167/780 [01:08<04:41,  2.18it/s] 22%|       | 168/780 [01:08<04:10,  2.45it/s] 22%|       | 169/780 [01:09<03:52,  2.63it/s] 22%|       | 170/780 [01:09<03:35,  2.82it/s] 22%|       | 171/780 [01:09<03:24,  2.98it/s] 22%|       | 172/780 [01:09<03:16,  3.10it/s] 22%|       | 173/780 [01:10<03:10,  3.18it/s] 22%|       | 174/780 [01:10<03:06,  3.25it/s] 22%|       | 175/780 [01:10<03:03,  3.30it/s] 23%|       | 176/780 [01:11<03:01,  3.34it/s] 23%|       | 177/780 [01:11<02:59,  3.36it/s] 23%|       | 178/780 [01:11<02:58,  3.38it/s] 23%|       | 179/780 [01:12<02:57,  3.39it/s] 23%|       | 180/780 [01:12<03:04,  3.25it/s] 23%|       | 181/780 [01:12<03:01,  3.30it/s] 23%|       | 182/780 [01:12<02:59,  3.34it/s] 23%|       | 183/780 [01:13<02:57,  3.36it/s] 24%|       | 184/780 [01:13<02:56,  3.38it/s] 24%|       | 185/780 [01:13<02:55,  3.39it/s] 24%|       | 186/780 [01:14<02:54,  3.40it/s] 24%|       | 187/780 [01:14<02:53,  3.41it/s] 24%|       | 188/780 [01:14<02:53,  3.41it/s] 24%|       | 189/780 [01:14<02:52,  3.42it/s] 24%|       | 190/780 [01:15<02:52,  3.42it/s] 24%|       | 191/780 [01:15<02:54,  3.37it/s] 25%|       | 192/780 [01:15<02:53,  3.39it/s] 25%|       | 193/780 [01:16<02:52,  3.40it/s] 25%|       | 194/780 [01:16<02:52,  3.41it/s] 25%|       | 195/780 [01:16<02:51,  3.41it/s] 25%|       | 196/780 [01:17<02:50,  3.42it/s] 25%|       | 197/780 [01:17<02:50,  3.42it/s] 25%|       | 198/780 [01:17<02:50,  3.42it/s] 26%|       | 199/780 [01:17<02:49,  3.42it/s] 26%|       | 200/780 [01:18<02:49,  3.42it/s] 26%|       | 201/780 [01:18<02:49,  3.42it/s] 26%|       | 202/780 [01:18<02:53,  3.34it/s] 26%|       | 203/780 [01:19<02:51,  3.36it/s] 26%|       | 204/780 [01:19<02:50,  3.38it/s] 26%|       | 205/780 [01:19<02:49,  3.39it/s] 26%|       | 206/780 [01:19<02:48,  3.40it/s] 27%|       | 207/780 [01:20<02:48,  3.41it/s] 27%|       | 208/780 [01:20<02:47,  3.41it/s] 27%|       | 209/780 [01:20<02:47,  3.41it/s] 27%|       | 210/780 [01:21<02:46,  3.42it/s] 27%|       | 211/780 [01:21<02:46,  3.42it/s] 27%|       | 212/780 [01:21<02:46,  3.42it/s] 27%|       | 213/780 [01:22<02:50,  3.33it/s] 27%|       | 214/780 [01:22<02:49,  3.34it/s] 28%|       | 215/780 [01:22<02:48,  3.36it/s] 28%|       | 216/780 [01:22<02:49,  3.34it/s] 28%|       | 217/780 [01:23<02:47,  3.36it/s] 28%|       | 218/780 [01:23<02:46,  3.38it/s] 28%|       | 219/780 [01:23<02:45,  3.39it/s] 28%|       | 220/780 [01:24<02:44,  3.41it/s] 28%|       | 221/780 [01:24<02:43,  3.41it/s] 28%|       | 222/780 [01:24<02:43,  3.42it/s] 29%|       | 223/780 [01:24<02:43,  3.41it/s] 29%|       | 224/780 [01:25<02:43,  3.41it/s] 29%|       | 225/780 [01:25<02:43,  3.40it/s] 29%|       | 226/780 [01:25<02:42,  3.40it/s] 29%|       | 227/780 [01:26<02:55,  3.15it/s] 29%|       | 228/780 [01:26<02:51,  3.22it/s] 29%|       | 229/780 [01:26<02:48,  3.28it/s] 29%|       | 230/780 [01:27<02:45,  3.31it/s] 30%|       | 231/780 [01:27<02:44,  3.34it/s] 30%|       | 232/780 [01:27<02:43,  3.36it/s] 30%|       | 233/780 [01:28<02:42,  3.37it/s] 30%|       | 234/780 [01:28<02:41,  3.38it/s] 30%|       | 235/780 [01:28<02:41,  3.38it/s] 30%|       | 236/780 [01:28<02:40,  3.39it/s] 30%|       | 237/780 [01:29<02:46,  3.26it/s] 31%|       | 238/780 [01:29<02:44,  3.29it/s] 31%|       | 239/780 [01:29<02:42,  3.32it/s] 31%|       | 240/780 [01:30<02:41,  3.34it/s] 31%|       | 241/780 [01:30<02:40,  3.36it/s] 31%|       | 242/780 [01:30<02:39,  3.37it/s] 31%|       | 243/780 [01:30<02:38,  3.39it/s] 31%|      | 244/780 [01:31<02:37,  3.40it/s] 31%|      | 245/780 [01:31<02:37,  3.40it/s] 32%|      | 246/780 [01:31<02:36,  3.41it/s] 32%|      | 247/780 [01:32<02:36,  3.41it/s] 32%|      | 248/780 [01:32<02:46,  3.20it/s] 32%|      | 249/780 [01:32<02:42,  3.26it/s] 32%|      | 250/780 [01:33<02:40,  3.31it/s] 32%|      | 251/780 [01:33<02:38,  3.34it/s] 32%|      | 252/780 [01:33<02:37,  3.36it/s] 32%|      | 253/780 [01:33<02:36,  3.38it/s] 33%|      | 254/780 [01:34<02:35,  3.39it/s] 33%|      | 255/780 [01:34<02:34,  3.40it/s] 33%|      | 256/780 [01:34<02:33,  3.41it/s] 33%|      | 257/780 [01:35<02:32,  3.43it/s] 33%|      | 258/780 [01:35<02:31,  3.45it/s] 33%|      | 259/780 [01:35<02:34,  3.36it/s] 33%|      | 260/780 [01:36<02:33,  3.39it/s] 33%|      | 261/780 [01:36<02:31,  3.41it/s] 34%|      | 262/780 [01:36<02:31,  3.43it/s] 34%|      | 263/780 [01:36<02:30,  3.43it/s] 34%|      | 264/780 [01:37<02:30,  3.43it/s] 34%|      | 265/780 [01:37<02:29,  3.44it/s] 34%|      | 266/780 [01:37<02:29,  3.44it/s] 34%|      | 267/780 [01:38<02:28,  3.44it/s] 34%|      | 268/780 [01:38<02:28,  3.45it/s] 34%|      | 269/780 [01:38<02:27,  3.46it/s] 35%|      | 270/780 [01:38<02:34,  3.31it/s] 35%|      | 271/780 [01:39<02:31,  3.36it/s] 35%|      | 272/780 [01:39<02:29,  3.40it/s] 35%|      | 273/780 [01:39<02:28,  3.42it/s] 35%|      | 274/780 [01:40<02:27,  3.44it/s] 35%|      | 275/780 [01:40<02:26,  3.44it/s] 35%|      | 276/780 [01:40<02:26,  3.44it/s] 36%|      | 277/780 [01:40<02:25,  3.45it/s] 36%|      | 278/780 [01:41<02:25,  3.46it/s] 36%|      | 279/780 [01:41<02:24,  3.46it/s] 36%|      | 280/780 [01:41<02:24,  3.46it/s] 36%|      | 281/780 [01:42<02:25,  3.42it/s] 36%|      | 282/780 [01:42<02:25,  3.43it/s] 36%|      | 283/780 [01:42<02:24,  3.44it/s] 36%|      | 284/780 [01:43<02:23,  3.45it/s] 37%|      | 285/780 [01:43<02:23,  3.45it/s] 37%|      | 286/780 [01:43<02:23,  3.44it/s] 37%|      | 287/780 [01:43<02:23,  3.44it/s] 37%|      | 288/780 [01:44<02:23,  3.44it/s] 37%|      | 289/780 [01:44<02:22,  3.45it/s] 37%|      | 290/780 [01:44<02:21,  3.45it/s] 37%|      | 291/780 [01:45<02:21,  3.46it/s] 37%|      | 292/780 [01:45<02:23,  3.41it/s] 38%|      | 293/780 [01:45<02:22,  3.43it/s] 38%|      | 294/780 [01:45<02:21,  3.44it/s] 38%|      | 295/780 [01:46<02:20,  3.45it/s] 38%|      | 296/780 [01:46<02:20,  3.45it/s] 38%|      | 297/780 [01:46<02:19,  3.46it/s] 38%|      | 298/780 [01:47<02:19,  3.45it/s] 38%|      | 299/780 [01:47<02:18,  3.46it/s] 38%|      | 300/780 [01:47<02:18,  3.46it/s] 39%|      | 301/780 [01:47<02:18,  3.46it/s] 39%|      | 302/780 [01:48<02:45,  2.89it/s] 39%|      | 303/780 [01:48<02:43,  2.92it/s] 39%|      | 304/780 [01:49<02:35,  3.06it/s] 39%|      | 305/780 [01:49<02:29,  3.18it/s] 39%|      | 306/780 [01:49<02:25,  3.26it/s] 39%|      | 307/780 [01:49<02:22,  3.32it/s] 39%|      | 308/780 [01:50<02:20,  3.36it/s] 40%|      | 309/780 [01:50<02:18,  3.39it/s] 40%|      | 310/780 [01:50<02:17,  3.41it/s] 40%|      | 311/780 [01:51<02:16,  3.43it/s] 40%|      | 312/780 [01:51<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 07:21:54,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:21:54,200 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 07:21:54,200 >>   Batch size = 8
{'eval_loss': 1.099610447883606, 'eval_runtime': 10.2357, 'eval_samples_per_second': 340.183, 'eval_steps_per_second': 42.596, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  2%|         | 7/436 [00:00<00:07, 58.72it/s][A
  3%|         | 13/436 [00:00<00:08, 49.94it/s][A
  4%|         | 19/436 [00:00<00:08, 47.64it/s][A
  6%|         | 24/436 [00:00<00:08, 46.46it/s][A
  7%|         | 29/436 [00:00<00:08, 45.93it/s][A
  8%|         | 34/436 [00:00<00:08, 45.55it/s][A
  9%|         | 39/436 [00:00<00:08, 45.13it/s][A
 10%|         | 44/436 [00:00<00:08, 44.83it/s][A
 11%|         | 49/436 [00:01<00:08, 44.83it/s][A
 12%|        | 54/436 [00:01<00:08, 45.04it/s][A
 14%|        | 59/436 [00:01<00:08, 45.05it/s][A
 15%|        | 64/436 [00:01<00:08, 44.94it/s][A
 16%|        | 69/436 [00:01<00:08, 44.90it/s][A
 17%|        | 74/436 [00:01<00:08, 44.86it/s][A
 18%|        | 79/436 [00:01<00:07, 44.78it/s][A
 19%|        | 84/436 [00:01<00:07, 44.61it/s][A
 20%|        | 89/436 [00:01<00:08, 40.12it/s][A
 22%|       | 94/436 [00:02<00:08, 41.62it/s][A
 23%|       | 99/436 [00:02<00:07, 42.71it/s][A
 24%|       | 104/436 [00:02<00:07, 43.41it/s][A
 25%|       | 109/436 [00:02<00:07, 44.05it/s][A
 26%|       | 114/436 [00:02<00:07, 44.47it/s][A
 27%|       | 119/436 [00:02<00:07, 44.58it/s][A
 28%|       | 124/436 [00:02<00:07, 44.53it/s][A
 30%|       | 129/436 [00:02<00:06, 44.11it/s][A
 31%|       | 134/436 [00:02<00:06, 44.06it/s][A
 32%|      | 139/436 [00:03<00:06, 44.35it/s][A
 33%|      | 144/436 [00:03<00:06, 44.61it/s][A
 34%|      | 149/436 [00:03<00:06, 44.90it/s][A
 35%|      | 154/436 [00:03<00:06, 45.06it/s][A
 36%|      | 159/436 [00:03<00:06, 45.10it/s][A
 38%|      | 164/436 [00:03<00:06, 45.06it/s][A
 39%|      | 169/436 [00:03<00:05, 44.78it/s][A
 40%|      | 174/436 [00:03<00:05, 44.50it/s][A
 41%|      | 179/436 [00:04<00:05, 44.38it/s][A
 42%|     | 184/436 [00:04<00:05, 44.40it/s][A
 43%|     | 189/436 [00:04<00:05, 44.66it/s][A
 44%|     | 194/436 [00:04<00:05, 44.86it/s][A
 46%|     | 199/436 [00:04<00:05, 44.94it/s][A
 47%|     | 204/436 [00:04<00:05, 45.03it/s][A
 48%|     | 209/436 [00:04<00:05, 44.98it/s][A
 49%|     | 214/436 [00:04<00:04, 44.86it/s][A
 50%|     | 219/436 [00:04<00:04, 44.67it/s][A
 51%|    | 224/436 [00:05<00:04, 44.16it/s][A
 53%|    | 229/436 [00:05<00:04, 44.27it/s][A
 54%|    | 234/436 [00:05<00:04, 44.50it/s][A
 55%|    | 239/436 [00:05<00:04, 44.56it/s][A
 56%|    | 244/436 [00:05<00:04, 44.82it/s][A
 57%|    | 249/436 [00:05<00:04, 44.95it/s][A
 58%|    | 254/436 [00:05<00:04, 44.91it/s][A
 59%|    | 259/436 [00:05<00:03, 44.86it/s][A
 61%|    | 264/436 [00:05<00:03, 44.66it/s][A
 62%|   | 269/436 [00:06<00:03, 44.49it/s][A
 63%|   | 274/436 [00:06<00:03, 44.55it/s][A
 64%|   | 279/436 [00:06<00:03, 44.71it/s][A
 65%|   | 284/436 [00:06<00:03, 44.87it/s][A
 66%|   | 289/436 [00:06<00:03, 44.93it/s][A
 67%|   | 294/436 [00:06<00:03, 44.91it/s][A
 69%|   | 299/436 [00:06<00:03, 44.89it/s][A
 70%|   | 304/436 [00:06<00:02, 44.71it/s][A
 71%|   | 309/436 [00:06<00:02, 44.68it/s][A
 72%|  | 314/436 [00:07<00:02, 44.60it/s][A
 73%|  | 319/436 [00:07<00:02, 44.60it/s][A
 74%|  | 324/436 [00:07<00:02, 44.62it/s][A
 75%|  | 329/436 [00:07<00:02, 44.85it/s][A
 77%|  | 334/436 [00:07<00:02, 44.94it/s][A
 78%|  | 339/436 [00:07<00:02, 44.94it/s][A
 79%|  | 344/436 [00:07<00:02, 44.89it/s][A
 80%|  | 349/436 [00:07<00:01, 44.79it/s][A
 81%|  | 354/436 [00:07<00:01, 44.63it/s][A
 82%| | 359/436 [00:08<00:01, 42.60it/s][A
 83%| | 364/436 [00:08<00:01, 43.29it/s][A
 85%| | 369/436 [00:08<00:01, 43.84it/s][A
 86%| | 374/436 [00:08<00:01, 44.24it/s][A
 87%| | 379/436 [00:08<00:01, 44.52it/s][A
 88%| | 384/436 [00:08<00:01, 44.53it/s][A
 89%| | 389/436 [00:08<00:01, 44.63it/s][A
 90%| | 394/436 [00:08<00:00, 44.54it/s][A
 92%|| 399/436 [00:08<00:00, 44.28it/s][A
 93%|| 404/436 [00:09<00:00, 44.45it/s][A
 94%|| 409/436 [00:09<00:00, 44.58it/s][A
 95%|| 414/436 [00:09<00:00, 44.85it/s][A
 96%|| 419/436 [00:09<00:00, 44.90it/s][A
 97%|| 424/436 [00:09<00:00, 44.95it/s][A
 98%|| 429/436 [00:09<00:00, 44.96it/s][A
100%|| 434/436 [00:09<00:00, 44.75it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.75it/s][A 40%|      | 312/780 [02:01<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:22:04,197 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 07:22:04,360 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:22:07,830 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:22:07,945 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:22:08,004 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:11<49:07,  6.31s/it] 40%|      | 314/780 [02:12<35:02,  4.51s/it] 40%|      | 315/780 [02:12<25:09,  3.25s/it] 41%|      | 316/780 [02:12<18:15,  2.36s/it] 41%|      | 317/780 [02:12<13:25,  1.74s/it] 41%|      | 318/780 [02:13<10:03,  1.31s/it] 41%|      | 319/780 [02:13<07:41,  1.00s/it] 41%|      | 320/780 [02:13<06:01,  1.27it/s] 41%|      | 321/780 [02:14<04:52,  1.57it/s] 41%|     | 322/780 [02:14<04:03,  1.88it/s] 41%|     | 323/780 [02:14<03:30,  2.18it/s] 42%|     | 324/780 [02:14<03:06,  2.45it/s] 42%|     | 325/780 [02:15<02:57,  2.56it/s] 42%|     | 326/780 [02:15<02:43,  2.78it/s] 42%|     | 327/780 [02:15<02:33,  2.96it/s] 42%|     | 328/780 [02:16<02:25,  3.10it/s] 42%|     | 329/780 [02:16<02:21,  3.19it/s] 42%|     | 330/780 [02:16<02:18,  3.26it/s] 42%|     | 331/780 [02:17<02:15,  3.31it/s] 43%|     | 332/780 [02:17<02:13,  3.36it/s] 43%|     | 333/780 [02:17<02:12,  3.38it/s] 43%|     | 334/780 [02:17<02:10,  3.41it/s] 43%|     | 335/780 [02:18<02:10,  3.42it/s] 43%|     | 336/780 [02:18<02:12,  3.36it/s] 43%|     | 337/780 [02:18<02:10,  3.39it/s] 43%|     | 338/780 [02:19<02:09,  3.41it/s] 43%|     | 339/780 [02:19<02:09,  3.42it/s] 44%|     | 340/780 [02:19<02:08,  3.43it/s] 44%|     | 341/780 [02:19<02:07,  3.43it/s] 44%|     | 342/780 [02:20<02:07,  3.44it/s] 44%|     | 343/780 [02:20<02:06,  3.44it/s] 44%|     | 344/780 [02:20<02:06,  3.45it/s] 44%|     | 345/780 [02:21<02:06,  3.45it/s] 44%|     | 346/780 [02:21<02:05,  3.45it/s] 44%|     | 347/780 [02:21<02:08,  3.38it/s] 45%|     | 348/780 [02:21<02:06,  3.40it/s] 45%|     | 349/780 [02:22<02:06,  3.42it/s] 45%|     | 350/780 [02:22<02:05,  3.43it/s] 45%|     | 351/780 [02:22<02:04,  3.44it/s] 45%|     | 352/780 [02:23<02:04,  3.44it/s] 45%|     | 353/780 [02:23<02:03,  3.45it/s] 45%|     | 354/780 [02:23<02:03,  3.45it/s] 46%|     | 355/780 [02:24<02:02,  3.46it/s] 46%|     | 356/780 [02:24<02:02,  3.45it/s] 46%|     | 357/780 [02:24<02:02,  3.46it/s] 46%|     | 358/780 [02:24<02:02,  3.46it/s] 46%|     | 359/780 [02:25<02:01,  3.46it/s] 46%|     | 360/780 [02:25<02:01,  3.46it/s] 46%|     | 361/780 [02:25<02:01,  3.46it/s] 46%|     | 362/780 [02:26<02:01,  3.45it/s] 47%|     | 363/780 [02:26<02:00,  3.46it/s] 47%|     | 364/780 [02:26<02:00,  3.45it/s] 47%|     | 365/780 [02:26<01:59,  3.46it/s] 47%|     | 366/780 [02:27<01:59,  3.45it/s] 47%|     | 367/780 [02:27<02:00,  3.42it/s] 47%|     | 368/780 [02:27<02:00,  3.43it/s] 47%|     | 369/780 [02:28<01:59,  3.44it/s] 47%|     | 370/780 [02:28<01:59,  3.44it/s] 48%|     | 371/780 [02:28<01:58,  3.44it/s] 48%|     | 372/780 [02:28<01:58,  3.45it/s] 48%|     | 373/780 [02:29<01:58,  3.45it/s] 48%|     | 374/780 [02:29<01:57,  3.45it/s] 48%|     | 375/780 [02:29<01:57,  3.45it/s] 48%|     | 376/780 [02:30<01:57,  3.45it/s] 48%|     | 377/780 [02:30<01:56,  3.45it/s] 48%|     | 378/780 [02:30<01:59,  3.36it/s] 49%|     | 379/780 [02:31<01:58,  3.38it/s] 49%|     | 380/780 [02:31<01:57,  3.41it/s] 49%|     | 381/780 [02:31<01:56,  3.42it/s] 49%|     | 382/780 [02:31<01:55,  3.43it/s] 49%|     | 383/780 [02:32<01:55,  3.44it/s] 49%|     | 384/780 [02:32<01:55,  3.44it/s] 49%|     | 385/780 [02:32<01:54,  3.44it/s] 49%|     | 386/780 [02:33<01:54,  3.45it/s] 50%|     | 387/780 [02:33<01:54,  3.45it/s] 50%|     | 388/780 [02:33<01:53,  3.45it/s] 50%|     | 389/780 [02:33<01:57,  3.32it/s] 50%|     | 390/780 [02:34<01:55,  3.36it/s] 50%|     | 391/780 [02:34<01:54,  3.39it/s] 50%|     | 392/780 [02:34<01:53,  3.41it/s] 50%|     | 393/780 [02:35<01:53,  3.42it/s] 51%|     | 394/780 [02:35<01:52,  3.43it/s] 51%|     | 395/780 [02:35<01:52,  3.43it/s] 51%|     | 396/780 [02:35<01:51,  3.43it/s] 51%|     | 397/780 [02:36<01:51,  3.43it/s] 51%|     | 398/780 [02:36<01:51,  3.44it/s] 51%|     | 399/780 [02:36<01:50,  3.44it/s] 51%|    | 400/780 [02:37<01:54,  3.33it/s] 51%|    | 401/780 [02:37<01:52,  3.36it/s] 52%|    | 402/780 [02:37<01:51,  3.39it/s] 52%|    | 403/780 [02:38<01:50,  3.41it/s] 52%|    | 404/780 [02:38<01:49,  3.42it/s] 52%|    | 405/780 [02:38<01:49,  3.44it/s] 52%|    | 406/780 [02:38<01:48,  3.44it/s] 52%|    | 407/780 [02:39<01:48,  3.45it/s] 52%|    | 408/780 [02:39<01:47,  3.45it/s] 52%|    | 409/780 [02:39<01:47,  3.45it/s] 53%|    | 410/780 [02:40<01:47,  3.45it/s] 53%|    | 411/780 [02:40<01:48,  3.40it/s] 53%|    | 412/780 [02:40<01:47,  3.41it/s] 53%|    | 413/780 [02:40<01:47,  3.43it/s] 53%|    | 414/780 [02:41<01:46,  3.43it/s] 53%|    | 415/780 [02:41<01:46,  3.44it/s] 53%|    | 416/780 [02:41<01:45,  3.44it/s] 53%|    | 417/780 [02:42<01:45,  3.45it/s] 54%|    | 418/780 [02:42<01:44,  3.45it/s] 54%|    | 419/780 [02:42<01:44,  3.45it/s] 54%|    | 420/780 [02:42<01:44,  3.45it/s] 54%|    | 421/780 [02:43<01:43,  3.45it/s] 54%|    | 422/780 [02:43<01:46,  3.37it/s] 54%|    | 423/780 [02:43<01:45,  3.40it/s] 54%|    | 424/780 [02:44<01:44,  3.41it/s] 54%|    | 425/780 [02:44<01:43,  3.43it/s] 55%|    | 426/780 [02:44<01:43,  3.44it/s] 55%|    | 427/780 [02:45<01:42,  3.44it/s] 55%|    | 428/780 [02:45<01:42,  3.44it/s] 55%|    | 429/780 [02:45<01:41,  3.45it/s] 55%|    | 430/780 [02:45<01:41,  3.45it/s] 55%|    | 431/780 [02:46<01:41,  3.45it/s] 55%|    | 432/780 [02:46<01:40,  3.45it/s] 56%|    | 433/780 [02:46<01:43,  3.34it/s] 56%|    | 434/780 [02:47<01:43,  3.35it/s] 56%|    | 435/780 [02:47<01:42,  3.38it/s] 56%|    | 436/780 [02:47<01:41,  3.40it/s] 56%|    | 437/780 [02:47<01:40,  3.42it/s] 56%|    | 438/780 [02:48<02:22,  2.39it/s] 56%|    | 439/780 [02:48<02:09,  2.64it/s] 56%|    | 440/780 [02:49<01:59,  2.84it/s] 57%|    | 441/780 [02:49<01:52,  3.00it/s] 57%|    | 442/780 [02:49<01:51,  3.02it/s] 57%|    | 443/780 [02:50<01:47,  3.15it/s] 57%|    | 444/780 [02:50<01:43,  3.23it/s] 57%|    | 445/780 [02:50<01:41,  3.30it/s] 57%|    | 446/780 [02:51<01:39,  3.34it/s] 57%|    | 447/780 [02:51<01:38,  3.38it/s] 57%|    | 448/780 [02:51<01:37,  3.40it/s] 58%|    | 449/780 [02:51<01:36,  3.41it/s] 58%|    | 450/780 [02:52<01:36,  3.43it/s] 58%|    | 451/780 [02:52<01:35,  3.43it/s] 58%|    | 452/780 [02:52<01:35,  3.44it/s] 58%|    | 453/780 [02:53<01:39,  3.29it/s] 58%|    | 454/780 [02:53<01:37,  3.34it/s] 58%|    | 455/780 [02:53<01:36,  3.37it/s] 58%|    | 456/780 [02:53<01:35,  3.40it/s] 59%|    | 457/780 [02:54<01:34,  3.41it/s] 59%|    | 458/780 [02:54<01:33,  3.43it/s] 59%|    | 459/780 [02:54<01:33,  3.43it/s] 59%|    | 460/780 [02:55<01:34,  3.39it/s] 59%|    | 461/780 [02:55<01:33,  3.41it/s] 59%|    | 462/780 [02:55<01:32,  3.42it/s] 59%|    | 463/780 [02:55<01:32,  3.43it/s] 59%|    | 464/780 [02:56<01:31,  3.44it/s] 60%|    | 465/780 [02:56<01:31,  3.44it/s] 60%|    | 466/780 [02:56<01:31,  3.44it/s] 60%|    | 467/780 [02:57<01:30,  3.45it/s] 60%|    | 468/780 [02:57<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 07:23:00,263 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:23:00,264 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 07:23:00,264 >>   Batch size = 8
{'eval_loss': 1.1197882890701294, 'eval_runtime': 9.8345, 'eval_samples_per_second': 354.061, 'eval_steps_per_second': 44.334, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.46it/s][A
  3%|         | 12/436 [00:00<00:08, 48.50it/s][A
  4%|         | 17/436 [00:00<00:08, 46.81it/s][A
  5%|         | 22/436 [00:00<00:09, 45.92it/s][A
  6%|         | 27/436 [00:00<00:09, 45.34it/s][A
  7%|         | 32/436 [00:00<00:08, 45.18it/s][A
  8%|         | 37/436 [00:00<00:08, 45.02it/s][A
 10%|         | 42/436 [00:00<00:08, 44.79it/s][A
 11%|         | 47/436 [00:01<00:08, 44.87it/s][A
 12%|        | 52/436 [00:01<00:08, 44.91it/s][A
 13%|        | 57/436 [00:01<00:08, 45.02it/s][A
 14%|        | 62/436 [00:01<00:08, 44.93it/s][A
 15%|        | 67/436 [00:01<00:08, 44.84it/s][A
 17%|        | 72/436 [00:01<00:08, 44.80it/s][A
 18%|        | 77/436 [00:01<00:08, 44.75it/s][A
 19%|        | 82/436 [00:01<00:07, 44.65it/s][A
 20%|        | 87/436 [00:01<00:07, 44.69it/s][A
 21%|        | 92/436 [00:02<00:07, 44.82it/s][A
 22%|       | 97/436 [00:02<00:07, 44.80it/s][A
 23%|       | 102/436 [00:02<00:07, 44.90it/s][A
 25%|       | 107/436 [00:02<00:07, 44.72it/s][A
 26%|       | 112/436 [00:02<00:07, 44.86it/s][A
 27%|       | 117/436 [00:02<00:07, 44.74it/s][A
 28%|       | 122/436 [00:02<00:07, 44.67it/s][A
 29%|       | 127/436 [00:02<00:06, 44.78it/s][A
 30%|       | 132/436 [00:02<00:06, 44.62it/s][A
 31%|      | 137/436 [00:03<00:06, 44.75it/s][A
 33%|      | 142/436 [00:03<00:06, 44.86it/s][A
 34%|      | 147/436 [00:03<00:06, 44.76it/s][A
 35%|      | 152/436 [00:03<00:06, 44.36it/s][A
 36%|      | 157/436 [00:03<00:06, 44.48it/s][A
 37%|      | 162/436 [00:03<00:06, 44.44it/s][A
 38%|      | 167/436 [00:03<00:06, 44.58it/s][A
 39%|      | 172/436 [00:03<00:05, 44.62it/s][A
 41%|      | 177/436 [00:03<00:05, 44.68it/s][A
 42%|     | 182/436 [00:04<00:05, 44.76it/s][A
 43%|     | 187/436 [00:04<00:05, 44.76it/s][A
 44%|     | 192/436 [00:04<00:05, 44.75it/s][A
 45%|     | 197/436 [00:04<00:05, 44.71it/s][A
 46%|     | 202/436 [00:04<00:05, 44.73it/s][A
 47%|     | 207/436 [00:04<00:05, 44.70it/s][A
 49%|     | 212/436 [00:04<00:05, 44.71it/s][A
 50%|     | 217/436 [00:04<00:04, 44.61it/s][A
 51%|     | 222/436 [00:04<00:04, 44.74it/s][A
 52%|    | 227/436 [00:05<00:04, 44.76it/s][A
 53%|    | 232/436 [00:05<00:04, 44.80it/s][A
 54%|    | 237/436 [00:05<00:04, 44.80it/s][A
 56%|    | 242/436 [00:05<00:04, 44.71it/s][A
 57%|    | 247/436 [00:05<00:04, 44.79it/s][A
 58%|    | 252/436 [00:05<00:04, 44.72it/s][A
 59%|    | 257/436 [00:05<00:04, 44.73it/s][A
 60%|    | 262/436 [00:05<00:03, 44.71it/s][A
 61%|    | 267/436 [00:05<00:03, 44.69it/s][A
 62%|   | 272/436 [00:06<00:03, 44.82it/s][A
 64%|   | 277/436 [00:06<00:03, 44.74it/s][A
 65%|   | 282/436 [00:06<00:03, 44.82it/s][A
 66%|   | 287/436 [00:06<00:03, 43.01it/s][A
 67%|   | 292/436 [00:06<00:03, 43.47it/s][A
 68%|   | 297/436 [00:06<00:03, 43.99it/s][A
 69%|   | 302/436 [00:06<00:03, 44.10it/s][A
 70%|   | 307/436 [00:06<00:02, 44.28it/s][A
 72%|  | 312/436 [00:06<00:02, 44.50it/s][A
 73%|  | 317/436 [00:07<00:02, 44.70it/s][A
 74%|  | 322/436 [00:07<00:02, 44.74it/s][A
 75%|  | 327/436 [00:07<00:02, 44.43it/s][A
 76%|  | 332/436 [00:07<00:02, 44.59it/s][A
 77%|  | 337/436 [00:07<00:02, 44.66it/s][A
 78%|  | 342/436 [00:07<00:02, 44.74it/s][A
 80%|  | 347/436 [00:07<00:01, 44.68it/s][A
 81%|  | 352/436 [00:07<00:01, 44.56it/s][A
 82%| | 357/436 [00:07<00:01, 44.75it/s][A
 83%| | 362/436 [00:08<00:01, 44.91it/s][A
 84%| | 367/436 [00:08<00:01, 44.90it/s][A
 85%| | 372/436 [00:08<00:01, 44.74it/s][A
 86%| | 377/436 [00:08<00:01, 44.69it/s][A
 88%| | 382/436 [00:08<00:01, 44.77it/s][A
 89%| | 387/436 [00:08<00:01, 44.77it/s][A
 90%| | 392/436 [00:08<00:00, 44.79it/s][A
 91%| | 397/436 [00:08<00:00, 44.49it/s][A
 92%|| 402/436 [00:08<00:00, 44.81it/s][A
 93%|| 407/436 [00:09<00:00, 44.86it/s][A
 94%|| 412/436 [00:09<00:00, 44.72it/s][A
 96%|| 417/436 [00:09<00:00, 44.93it/s][A
 97%|| 422/436 [00:09<00:00, 44.77it/s][A
 98%|| 427/436 [00:09<00:00, 44.62it/s][A
 99%|| 432/436 [00:09<00:00, 44.67it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.67it/s][A 60%|    | 468/780 [03:07<01:30,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:23:10,215 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 07:23:10,340 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:23:13,326 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:23:13,505 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:23:13,597 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:18<33:44,  6.51s/it] 60%|    | 470/780 [03:18<24:02,  4.65s/it] 60%|    | 471/780 [03:19<17:13,  3.35s/it] 61%|    | 472/780 [03:19<12:28,  2.43s/it] 61%|    | 473/780 [03:19<09:09,  1.79s/it] 61%|    | 474/780 [03:19<06:50,  1.34s/it] 61%|    | 475/780 [03:20<05:12,  1.03s/it] 61%|    | 476/780 [03:20<04:05,  1.24it/s] 61%|    | 477/780 [03:20<03:17,  1.53it/s] 61%|   | 478/780 [03:21<02:44,  1.84it/s] 61%|   | 479/780 [03:21<02:21,  2.13it/s] 62%|   | 480/780 [03:21<02:05,  2.40it/s] 62%|   | 481/780 [03:22<01:55,  2.59it/s] 62%|   | 482/780 [03:22<01:46,  2.79it/s] 62%|   | 483/780 [03:22<01:40,  2.95it/s] 62%|   | 484/780 [03:22<01:36,  3.07it/s] 62%|   | 485/780 [03:23<01:33,  3.17it/s] 62%|   | 486/780 [03:23<01:30,  3.24it/s] 62%|   | 487/780 [03:23<01:29,  3.29it/s] 63%|   | 488/780 [03:24<01:27,  3.32it/s] 63%|   | 489/780 [03:24<01:26,  3.35it/s] 63%|   | 490/780 [03:24<01:26,  3.37it/s] 63%|   | 491/780 [03:24<01:25,  3.38it/s] 63%|   | 492/780 [03:25<01:28,  3.26it/s] 63%|   | 493/780 [03:25<01:26,  3.30it/s] 63%|   | 494/780 [03:25<01:25,  3.33it/s] 63%|   | 495/780 [03:26<01:25,  3.35it/s] 64%|   | 496/780 [03:26<01:24,  3.37it/s] 64%|   | 497/780 [03:26<01:23,  3.38it/s] 64%|   | 498/780 [03:27<01:23,  3.38it/s] 64%|   | 499/780 [03:27<01:22,  3.39it/s] 64%|   | 500/780 [03:27<01:22,  3.39it/s]                                                  64%|   | 500/780 [03:27<01:22,  3.39it/s] 64%|   | 501/780 [03:27<01:22,  3.40it/s] 64%|   | 502/780 [03:28<01:21,  3.40it/s] 64%|   | 503/780 [03:28<01:21,  3.41it/s] 65%|   | 504/780 [03:28<01:22,  3.35it/s] 65%|   | 505/780 [03:29<01:21,  3.37it/s] 65%|   | 506/780 [03:29<01:21,  3.38it/s] 65%|   | 507/780 [03:29<01:20,  3.39it/s] 65%|   | 508/780 [03:30<01:20,  3.40it/s] 65%|   | 509/780 [03:30<01:19,  3.40it/s] 65%|   | 510/780 [03:30<01:19,  3.40it/s] 66%|   | 511/780 [03:30<01:19,  3.40it/s] 66%|   | 512/780 [03:31<01:18,  3.40it/s] 66%|   | 513/780 [03:31<01:18,  3.40it/s] 66%|   | 514/780 [03:31<01:18,  3.41it/s] 66%|   | 515/780 [03:32<01:20,  3.31it/s] 66%|   | 516/780 [03:32<01:19,  3.34it/s] 66%|   | 517/780 [03:32<01:18,  3.36it/s] 66%|   | 518/780 [03:32<01:17,  3.38it/s] 67%|   | 519/780 [03:33<01:17,  3.38it/s] 67%|   | 520/780 [03:33<01:16,  3.40it/s] 67%|   | 521/780 [03:33<01:16,  3.39it/s] 67%|   | 522/780 [03:34<01:15,  3.41it/s] 67%|   | 523/780 [03:34<01:15,  3.40it/s] 67%|   | 524/780 [03:34<01:15,  3.41it/s] 67%|   | 525/780 [03:35<01:14,  3.41it/s] 67%|   | 526/780 [03:35<01:15,  3.38it/s] 68%|   | 527/780 [03:35<01:14,  3.39it/s] 68%|   | 528/780 [03:35<01:14,  3.40it/s] 68%|   | 529/780 [03:36<01:13,  3.40it/s] 68%|   | 530/780 [03:36<01:13,  3.40it/s] 68%|   | 531/780 [03:36<01:13,  3.39it/s] 68%|   | 532/780 [03:37<01:13,  3.39it/s] 68%|   | 533/780 [03:37<01:12,  3.40it/s] 68%|   | 534/780 [03:37<01:12,  3.40it/s] 69%|   | 535/780 [03:37<01:12,  3.39it/s] 69%|   | 536/780 [03:38<01:11,  3.40it/s] 69%|   | 537/780 [03:38<01:13,  3.29it/s] 69%|   | 538/780 [03:38<01:12,  3.32it/s] 69%|   | 539/780 [03:39<01:11,  3.35it/s] 69%|   | 540/780 [03:39<01:11,  3.36it/s] 69%|   | 541/780 [03:39<01:10,  3.37it/s] 69%|   | 542/780 [03:40<01:10,  3.38it/s] 70%|   | 543/780 [03:40<01:10,  3.38it/s] 70%|   | 544/780 [03:40<01:09,  3.38it/s] 70%|   | 545/780 [03:40<01:09,  3.39it/s] 70%|   | 546/780 [03:41<01:08,  3.40it/s] 70%|   | 547/780 [03:41<01:08,  3.39it/s] 70%|   | 548/780 [03:41<01:10,  3.30it/s] 70%|   | 549/780 [03:42<01:09,  3.33it/s] 71%|   | 550/780 [03:42<01:08,  3.35it/s] 71%|   | 551/780 [03:42<01:07,  3.37it/s] 71%|   | 552/780 [03:43<01:07,  3.38it/s] 71%|   | 553/780 [03:43<01:07,  3.38it/s] 71%|   | 554/780 [03:43<01:06,  3.39it/s] 71%|   | 555/780 [03:43<01:06,  3.40it/s] 71%|  | 556/780 [03:44<01:05,  3.40it/s] 71%|  | 557/780 [03:44<01:05,  3.40it/s] 72%|  | 558/780 [03:44<01:05,  3.40it/s] 72%|  | 559/780 [03:45<01:06,  3.34it/s] 72%|  | 560/780 [03:45<01:05,  3.36it/s] 72%|  | 561/780 [03:45<01:04,  3.38it/s] 72%|  | 562/780 [03:45<01:04,  3.38it/s] 72%|  | 563/780 [03:46<01:03,  3.39it/s] 72%|  | 564/780 [03:46<01:03,  3.39it/s] 72%|  | 565/780 [03:46<01:03,  3.40it/s] 73%|  | 566/780 [03:47<01:04,  3.33it/s] 73%|  | 567/780 [03:47<01:03,  3.36it/s] 73%|  | 568/780 [03:47<01:02,  3.37it/s] 73%|  | 569/780 [03:48<01:02,  3.39it/s] 73%|  | 570/780 [03:48<01:09,  3.02it/s] 73%|  | 571/780 [03:48<01:14,  2.79it/s] 73%|  | 572/780 [03:49<01:10,  2.95it/s] 73%|  | 573/780 [03:49<01:07,  3.07it/s] 74%|  | 574/780 [03:49<01:04,  3.17it/s] 74%|  | 575/780 [03:50<01:03,  3.23it/s] 74%|  | 576/780 [03:50<01:02,  3.28it/s] 74%|  | 577/780 [03:50<01:01,  3.32it/s] 74%|  | 578/780 [03:50<01:00,  3.35it/s] 74%|  | 579/780 [03:51<00:59,  3.36it/s] 74%|  | 580/780 [03:51<01:03,  3.16it/s] 74%|  | 581/780 [03:51<01:01,  3.23it/s] 75%|  | 582/780 [03:52<01:00,  3.28it/s] 75%|  | 583/780 [03:52<00:59,  3.31it/s] 75%|  | 584/780 [03:52<00:58,  3.34it/s] 75%|  | 585/780 [03:53<00:58,  3.36it/s] 75%|  | 586/780 [03:53<00:57,  3.38it/s] 75%|  | 587/780 [03:53<00:57,  3.38it/s] 75%|  | 588/780 [03:53<00:56,  3.39it/s] 76%|  | 589/780 [03:54<00:56,  3.39it/s] 76%|  | 590/780 [03:54<00:58,  3.24it/s] 76%|  | 591/780 [03:54<00:57,  3.28it/s] 76%|  | 592/780 [03:55<00:56,  3.32it/s] 76%|  | 593/780 [03:55<00:55,  3.35it/s] 76%|  | 594/780 [03:55<00:55,  3.36it/s] 76%|  | 595/780 [03:56<00:54,  3.37it/s] 76%|  | 596/780 [03:56<00:54,  3.39it/s] 77%|  | 597/780 [03:56<00:54,  3.39it/s] 77%|  | 598/780 [03:56<00:53,  3.39it/s] 77%|  | 599/780 [03:57<00:53,  3.39it/s] 77%|  | 600/780 [03:57<00:53,  3.39it/s] 77%|  | 601/780 [03:57<00:52,  3.39it/s] 77%|  | 602/780 [03:58<00:52,  3.39it/s] 77%|  | 603/780 [03:58<00:52,  3.39it/s] 77%|  | 604/780 [03:58<00:51,  3.40it/s] 78%|  | 605/780 [03:59<00:51,  3.40it/s] 78%|  | 606/780 [03:59<00:51,  3.40it/s] 78%|  | 607/780 [03:59<00:54,  3.19it/s] 78%|  | 608/780 [03:59<00:52,  3.25it/s] 78%|  | 609/780 [04:00<00:51,  3.30it/s] 78%|  | 610/780 [04:00<00:51,  3.33it/s] 78%|  | 611/780 [04:00<00:50,  3.35it/s] 78%|  | 612/780 [04:01<00:49,  3.37it/s] 79%|  | 613/780 [04:01<00:49,  3.37it/s] 79%|  | 614/780 [04:01<00:49,  3.38it/s] 79%|  | 615/780 [04:02<00:48,  3.39it/s] 79%|  | 616/780 [04:02<00:48,  3.39it/s] 79%|  | 617/780 [04:02<00:49,  3.31it/s] 79%|  | 618/780 [04:02<00:48,  3.34it/s] 79%|  | 619/780 [04:03<00:47,  3.36it/s] 79%|  | 620/780 [04:03<00:47,  3.37it/s] 80%|  | 621/780 [04:03<00:47,  3.38it/s] 80%|  | 622/780 [04:04<00:46,  3.39it/s] 80%|  | 623/780 [04:04<00:46,  3.39it/s] 80%|  | 624/780 [04:04<00:45,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 07:24:07,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:24:07,519 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 07:24:07,519 >>   Batch size = 8
{'eval_loss': 1.1286590099334717, 'eval_runtime': 9.7808, 'eval_samples_per_second': 356.003, 'eval_steps_per_second': 44.577, 'epoch': 3.0}
{'loss': 0.544, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.60it/s][A
  3%|         | 12/436 [00:00<00:08, 48.64it/s][A
  4%|         | 17/436 [00:00<00:08, 46.96it/s][A
  5%|         | 22/436 [00:00<00:08, 46.08it/s][A
  6%|         | 27/436 [00:00<00:08, 45.50it/s][A
  7%|         | 32/436 [00:00<00:09, 44.69it/s][A
  8%|         | 37/436 [00:00<00:08, 44.67it/s][A
 10%|         | 42/436 [00:00<00:08, 44.39it/s][A
 11%|         | 47/436 [00:01<00:08, 44.73it/s][A
 12%|        | 52/436 [00:01<00:08, 44.96it/s][A
 13%|        | 57/436 [00:01<00:08, 45.06it/s][A
 14%|        | 62/436 [00:01<00:08, 44.95it/s][A
 15%|        | 67/436 [00:01<00:08, 44.79it/s][A
 17%|        | 72/436 [00:01<00:08, 44.69it/s][A
 18%|        | 77/436 [00:01<00:08, 44.66it/s][A
 19%|        | 82/436 [00:01<00:07, 44.56it/s][A
 20%|        | 87/436 [00:01<00:07, 44.54it/s][A
 21%|        | 92/436 [00:02<00:07, 44.60it/s][A
 22%|       | 97/436 [00:02<00:07, 44.74it/s][A
 23%|       | 102/436 [00:02<00:07, 45.08it/s][A
 25%|       | 107/436 [00:02<00:07, 45.04it/s][A
 26%|       | 112/436 [00:02<00:07, 44.91it/s][A
 27%|       | 117/436 [00:02<00:07, 44.68it/s][A
 28%|       | 122/436 [00:02<00:07, 44.59it/s][A
 29%|       | 127/436 [00:02<00:06, 44.65it/s][A
 30%|       | 132/436 [00:02<00:06, 44.57it/s][A
 31%|      | 137/436 [00:03<00:06, 44.74it/s][A
 33%|      | 142/436 [00:03<00:06, 44.76it/s][A
 34%|      | 147/436 [00:03<00:06, 44.98it/s][A
 35%|      | 152/436 [00:03<00:06, 44.89it/s][A
 36%|      | 157/436 [00:03<00:06, 44.83it/s][A
 37%|      | 162/436 [00:03<00:06, 44.77it/s][A
 38%|      | 167/436 [00:03<00:06, 43.39it/s][A
 39%|      | 172/436 [00:03<00:06, 43.85it/s][A
 41%|      | 177/436 [00:03<00:05, 44.00it/s][A
 42%|     | 182/436 [00:04<00:05, 44.40it/s][A
 43%|     | 187/436 [00:04<00:05, 44.64it/s][A
 44%|     | 192/436 [00:04<00:05, 44.72it/s][A
 45%|     | 197/436 [00:04<00:05, 44.80it/s][A
 46%|     | 202/436 [00:04<00:05, 44.66it/s][A
 47%|     | 207/436 [00:04<00:05, 44.57it/s][A
 49%|     | 212/436 [00:04<00:05, 44.51it/s][A
 50%|     | 217/436 [00:04<00:04, 44.65it/s][A
 51%|     | 222/436 [00:04<00:04, 44.52it/s][A
 52%|    | 227/436 [00:05<00:04, 44.62it/s][A
 53%|    | 232/436 [00:05<00:04, 44.76it/s][A
 54%|    | 237/436 [00:05<00:04, 44.91it/s][A
 56%|    | 242/436 [00:05<00:04, 44.92it/s][A
 57%|    | 247/436 [00:05<00:04, 44.84it/s][A
 58%|    | 252/436 [00:05<00:04, 44.74it/s][A
 59%|    | 257/436 [00:05<00:04, 44.54it/s][A
 60%|    | 262/436 [00:05<00:03, 44.61it/s][A
 61%|    | 267/436 [00:05<00:03, 44.65it/s][A
 62%|   | 272/436 [00:06<00:03, 44.76it/s][A
 64%|   | 277/436 [00:06<00:03, 44.87it/s][A
 65%|   | 282/436 [00:06<00:03, 44.91it/s][A
 66%|   | 287/436 [00:06<00:03, 44.92it/s][A
 67%|   | 292/436 [00:06<00:03, 44.84it/s][A
 68%|   | 297/436 [00:06<00:03, 44.77it/s][A
 69%|   | 302/436 [00:06<00:03, 43.64it/s][A
 70%|   | 307/436 [00:06<00:02, 44.04it/s][A
 72%|  | 312/436 [00:06<00:02, 44.24it/s][A
 73%|  | 317/436 [00:07<00:02, 44.59it/s][A
 74%|  | 322/436 [00:07<00:02, 44.68it/s][A
 75%|  | 327/436 [00:07<00:02, 44.75it/s][A
 76%|  | 332/436 [00:07<00:02, 44.75it/s][A
 77%|  | 337/436 [00:07<00:02, 44.57it/s][A
 78%|  | 342/436 [00:07<00:02, 44.50it/s][A
 80%|  | 347/436 [00:07<00:02, 44.48it/s][A
 81%|  | 352/436 [00:07<00:01, 44.58it/s][A
 82%| | 357/436 [00:07<00:01, 44.74it/s][A
 83%| | 362/436 [00:08<00:01, 44.84it/s][A
 84%| | 367/436 [00:08<00:01, 44.85it/s][A
 85%| | 372/436 [00:08<00:01, 44.87it/s][A
 86%| | 377/436 [00:08<00:01, 44.92it/s][A
 88%| | 382/436 [00:08<00:01, 44.82it/s][A
 89%| | 387/436 [00:08<00:01, 44.64it/s][A
 90%| | 392/436 [00:08<00:00, 44.56it/s][A
 91%| | 397/436 [00:08<00:00, 44.61it/s][A
 92%|| 402/436 [00:08<00:00, 44.74it/s][A
 93%|| 407/436 [00:09<00:00, 44.84it/s][A
 94%|| 412/436 [00:09<00:00, 44.88it/s][A
 96%|| 417/436 [00:09<00:00, 44.94it/s][A
 97%|| 422/436 [00:09<00:00, 44.92it/s][A
 98%|| 427/436 [00:09<00:00, 44.84it/s][A
 99%|| 432/436 [00:09<00:00, 44.74it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.74it/s][A 80%|  | 624/780 [04:14<00:45,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:24:17,481 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 07:24:17,714 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:24:21,385 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:24:21,658 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:24:21,757 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:25<16:36,  6.43s/it] 80%|  | 626/780 [04:25<11:47,  4.60s/it] 80%|  | 627/780 [04:26<08:25,  3.30s/it] 81%|  | 628/780 [04:26<06:04,  2.40s/it] 81%|  | 629/780 [04:26<04:26,  1.77s/it] 81%|  | 630/780 [04:26<03:18,  1.32s/it] 81%|  | 631/780 [04:27<02:30,  1.01s/it] 81%|  | 632/780 [04:27<01:57,  1.26it/s] 81%|  | 633/780 [04:27<01:34,  1.55it/s] 81%| | 634/780 [04:28<01:18,  1.86it/s] 81%| | 635/780 [04:28<01:07,  2.16it/s] 82%| | 636/780 [04:28<00:59,  2.43it/s] 82%| | 637/780 [04:28<00:54,  2.63it/s] 82%| | 638/780 [04:29<00:50,  2.83it/s] 82%| | 639/780 [04:29<00:47,  3.00it/s] 82%| | 640/780 [04:29<00:44,  3.12it/s] 82%| | 641/780 [04:30<00:43,  3.22it/s] 82%| | 642/780 [04:30<00:41,  3.29it/s] 82%| | 643/780 [04:30<00:41,  3.34it/s] 83%| | 644/780 [04:30<00:40,  3.37it/s] 83%| | 645/780 [04:31<00:39,  3.39it/s] 83%| | 646/780 [04:31<00:39,  3.41it/s] 83%| | 647/780 [04:31<00:38,  3.43it/s] 83%| | 648/780 [04:32<00:39,  3.36it/s] 83%| | 649/780 [04:32<00:38,  3.39it/s] 83%| | 650/780 [04:32<00:38,  3.41it/s] 83%| | 651/780 [04:33<00:37,  3.42it/s] 84%| | 652/780 [04:33<00:37,  3.43it/s] 84%| | 653/780 [04:33<00:36,  3.44it/s] 84%| | 654/780 [04:33<00:36,  3.45it/s] 84%| | 655/780 [04:34<00:36,  3.45it/s] 84%| | 656/780 [04:34<00:35,  3.45it/s] 84%| | 657/780 [04:34<00:35,  3.45it/s] 84%| | 658/780 [04:35<00:35,  3.45it/s] 84%| | 659/780 [04:35<00:36,  3.35it/s] 85%| | 660/780 [04:35<00:35,  3.38it/s] 85%| | 661/780 [04:35<00:34,  3.40it/s] 85%| | 662/780 [04:36<00:34,  3.42it/s] 85%| | 663/780 [04:36<00:34,  3.43it/s] 85%| | 664/780 [04:36<00:33,  3.43it/s] 85%| | 665/780 [04:37<00:33,  3.44it/s] 85%| | 666/780 [04:37<00:33,  3.44it/s] 86%| | 667/780 [04:37<00:32,  3.45it/s] 86%| | 668/780 [04:37<00:32,  3.45it/s] 86%| | 669/780 [04:38<00:32,  3.46it/s] 86%| | 670/780 [04:38<00:32,  3.37it/s] 86%| | 671/780 [04:38<00:32,  3.40it/s] 86%| | 672/780 [04:39<00:31,  3.41it/s] 86%| | 673/780 [04:39<00:31,  3.43it/s] 86%| | 674/780 [04:39<00:30,  3.44it/s] 87%| | 675/780 [04:40<00:30,  3.45it/s] 87%| | 676/780 [04:40<00:30,  3.45it/s] 87%| | 677/780 [04:40<00:29,  3.46it/s] 87%| | 678/780 [04:40<00:29,  3.46it/s] 87%| | 679/780 [04:41<00:29,  3.45it/s] 87%| | 680/780 [04:41<00:29,  3.45it/s] 87%| | 681/780 [04:41<00:29,  3.35it/s] 87%| | 682/780 [04:42<00:29,  3.38it/s] 88%| | 683/780 [04:42<00:28,  3.41it/s] 88%| | 684/780 [04:42<00:28,  3.42it/s] 88%| | 685/780 [04:42<00:27,  3.44it/s] 88%| | 686/780 [04:43<00:27,  3.44it/s] 88%| | 687/780 [04:43<00:27,  3.44it/s] 88%| | 688/780 [04:43<00:26,  3.44it/s] 88%| | 689/780 [04:44<00:26,  3.45it/s] 88%| | 690/780 [04:44<00:26,  3.44it/s] 89%| | 691/780 [04:44<00:25,  3.44it/s] 89%| | 692/780 [04:44<00:25,  3.44it/s] 89%| | 693/780 [04:45<00:25,  3.45it/s] 89%| | 694/780 [04:45<00:25,  3.32it/s] 89%| | 695/780 [04:45<00:25,  3.36it/s] 89%| | 696/780 [04:46<00:24,  3.39it/s] 89%| | 697/780 [04:46<00:24,  3.41it/s] 89%| | 698/780 [04:46<00:23,  3.42it/s] 90%| | 699/780 [04:47<00:23,  3.43it/s] 90%| | 700/780 [04:47<00:23,  3.35it/s] 90%| | 701/780 [04:47<00:23,  3.38it/s] 90%| | 702/780 [04:47<00:22,  3.40it/s] 90%| | 703/780 [04:48<00:22,  3.42it/s] 90%| | 704/780 [04:48<00:22,  3.43it/s] 90%| | 705/780 [04:48<00:22,  3.27it/s] 91%| | 706/780 [04:49<00:22,  3.24it/s] 91%| | 707/780 [04:49<00:22,  3.30it/s] 91%| | 708/780 [04:49<00:21,  3.34it/s] 91%| | 709/780 [04:50<00:21,  3.37it/s] 91%| | 710/780 [04:50<00:20,  3.39it/s] 91%| | 711/780 [04:50<00:20,  3.41it/s] 91%|| 712/780 [04:50<00:19,  3.42it/s] 91%|| 713/780 [04:51<00:19,  3.43it/s] 92%|| 714/780 [04:51<00:19,  3.44it/s] 92%|| 715/780 [04:51<00:18,  3.44it/s] 92%|| 716/780 [04:52<00:19,  3.28it/s] 92%|| 717/780 [04:52<00:18,  3.33it/s] 92%|| 718/780 [04:52<00:18,  3.37it/s] 92%|| 719/780 [04:52<00:17,  3.39it/s] 92%|| 720/780 [04:53<00:17,  3.41it/s] 92%|| 721/780 [04:53<00:17,  3.42it/s] 93%|| 722/780 [04:53<00:16,  3.43it/s] 93%|| 723/780 [04:54<00:16,  3.44it/s] 93%|| 724/780 [04:54<00:16,  3.44it/s] 93%|| 725/780 [04:54<00:15,  3.45it/s] 93%|| 726/780 [04:55<00:15,  3.45it/s] 93%|| 727/780 [04:55<00:16,  3.29it/s] 93%|| 728/780 [04:55<00:15,  3.33it/s] 93%|| 729/780 [04:55<00:15,  3.37it/s] 94%|| 730/780 [04:56<00:14,  3.39it/s] 94%|| 731/780 [04:56<00:14,  3.42it/s] 94%|| 732/780 [04:56<00:14,  3.43it/s] 94%|| 733/780 [04:57<00:13,  3.44it/s] 94%|| 734/780 [04:57<00:13,  3.44it/s] 94%|| 735/780 [04:57<00:13,  3.45it/s] 94%|| 736/780 [04:57<00:12,  3.44it/s] 94%|| 737/780 [04:58<00:12,  3.45it/s] 95%|| 738/780 [04:58<00:12,  3.30it/s] 95%|| 739/780 [04:58<00:12,  3.34it/s] 95%|| 740/780 [04:59<00:11,  3.38it/s] 95%|| 741/780 [04:59<00:11,  3.40it/s] 95%|| 742/780 [04:59<00:11,  3.42it/s] 95%|| 743/780 [05:00<00:10,  3.42it/s] 95%|| 744/780 [05:00<00:10,  3.44it/s] 96%|| 745/780 [05:00<00:10,  3.44it/s] 96%|| 746/780 [05:00<00:09,  3.44it/s] 96%|| 747/780 [05:01<00:09,  3.44it/s] 96%|| 748/780 [05:01<00:09,  3.45it/s] 96%|| 749/780 [05:01<00:09,  3.30it/s] 96%|| 750/780 [05:02<00:08,  3.34it/s] 96%|| 751/780 [05:02<00:08,  3.38it/s] 96%|| 752/780 [05:02<00:08,  3.40it/s] 97%|| 753/780 [05:02<00:07,  3.42it/s] 97%|| 754/780 [05:03<00:07,  3.43it/s] 97%|| 755/780 [05:03<00:07,  3.44it/s] 97%|| 756/780 [05:03<00:06,  3.44it/s] 97%|| 757/780 [05:04<00:06,  3.44it/s] 97%|| 758/780 [05:04<00:06,  3.44it/s] 97%|| 759/780 [05:04<00:06,  3.44it/s] 97%|| 760/780 [05:05<00:05,  3.37it/s] 98%|| 761/780 [05:05<00:05,  3.39it/s] 98%|| 762/780 [05:05<00:05,  3.41it/s] 98%|| 763/780 [05:05<00:04,  3.42it/s] 98%|| 764/780 [05:06<00:04,  3.43it/s] 98%|| 765/780 [05:06<00:04,  3.44it/s] 98%|| 766/780 [05:06<00:04,  3.45it/s] 98%|| 767/780 [05:07<00:03,  3.44it/s] 98%|| 768/780 [05:07<00:03,  3.45it/s] 99%|| 769/780 [05:07<00:03,  3.45it/s] 99%|| 770/780 [05:07<00:02,  3.45it/s] 99%|| 771/780 [05:08<00:02,  3.38it/s] 99%|| 772/780 [05:08<00:02,  3.40it/s] 99%|| 773/780 [05:08<00:02,  3.42it/s] 99%|| 774/780 [05:09<00:01,  3.43it/s] 99%|| 775/780 [05:09<00:01,  3.44it/s] 99%|| 776/780 [05:09<00:01,  3.44it/s]100%|| 777/780 [05:09<00:00,  3.45it/s]100%|| 778/780 [05:10<00:00,  3.45it/s]100%|| 779/780 [05:10<00:00,  3.45it/s]100%|| 780/780 [05:10<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 07:25:13,621 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:25:13,621 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 07:25:13,621 >>   Batch size = 8
{'eval_loss': 1.1364030838012695, 'eval_runtime': 9.7792, 'eval_samples_per_second': 356.063, 'eval_steps_per_second': 44.585, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.89it/s][A
  3%|         | 12/436 [00:00<00:09, 45.43it/s][A
  4%|         | 17/436 [00:00<00:09, 45.15it/s][A
  5%|         | 22/436 [00:00<00:09, 45.07it/s][A
  6%|         | 27/436 [00:00<00:09, 44.73it/s][A
  7%|         | 32/436 [00:00<00:09, 44.74it/s][A
  8%|         | 37/436 [00:00<00:08, 44.75it/s][A
 10%|         | 42/436 [00:00<00:08, 44.70it/s][A
 11%|         | 47/436 [00:01<00:08, 44.82it/s][A
 12%|        | 52/436 [00:01<00:08, 44.80it/s][A
 13%|        | 57/436 [00:01<00:08, 44.85it/s][A
 14%|        | 62/436 [00:01<00:08, 44.78it/s][A
 15%|        | 67/436 [00:01<00:08, 44.81it/s][A
 17%|        | 72/436 [00:01<00:08, 44.77it/s][A
 18%|        | 77/436 [00:01<00:08, 44.75it/s][A
 19%|        | 82/436 [00:01<00:07, 44.72it/s][A
 20%|        | 87/436 [00:01<00:07, 44.74it/s][A
 21%|        | 92/436 [00:02<00:07, 44.82it/s][A
 22%|       | 97/436 [00:02<00:07, 44.82it/s][A
 23%|       | 102/436 [00:02<00:07, 42.79it/s][A
 25%|       | 107/436 [00:02<00:07, 43.54it/s][A
 26%|       | 112/436 [00:02<00:07, 43.94it/s][A
 27%|       | 117/436 [00:02<00:07, 44.17it/s][A
 28%|       | 122/436 [00:02<00:07, 44.34it/s][A
 29%|       | 127/436 [00:02<00:06, 44.45it/s][A
 30%|       | 132/436 [00:02<00:06, 44.59it/s][A
 31%|      | 137/436 [00:03<00:06, 44.74it/s][A
 33%|      | 142/436 [00:03<00:06, 44.54it/s][A
 34%|      | 147/436 [00:03<00:06, 44.50it/s][A
 35%|      | 152/436 [00:03<00:06, 44.63it/s][A
 36%|      | 157/436 [00:03<00:06, 44.83it/s][A
 37%|      | 162/436 [00:03<00:06, 44.87it/s][A
 38%|      | 167/436 [00:03<00:06, 44.81it/s][A
 39%|      | 172/436 [00:03<00:05, 44.73it/s][A
 41%|      | 177/436 [00:03<00:05, 44.73it/s][A
 42%|     | 182/436 [00:04<00:05, 44.70it/s][A
 43%|     | 187/436 [00:04<00:05, 44.65it/s][A
 44%|     | 192/436 [00:04<00:05, 44.62it/s][A
 45%|     | 197/436 [00:04<00:05, 44.66it/s][A
 46%|     | 202/436 [00:04<00:05, 44.80it/s][A
 47%|     | 207/436 [00:04<00:05, 44.89it/s][A
 49%|     | 212/436 [00:04<00:04, 44.94it/s][A
 50%|     | 217/436 [00:04<00:04, 44.87it/s][A
 51%|     | 222/436 [00:04<00:04, 44.59it/s][A
 52%|    | 227/436 [00:05<00:04, 44.68it/s][A
 53%|    | 232/436 [00:05<00:04, 44.69it/s][A
 54%|    | 237/436 [00:05<00:04, 40.40it/s][A
 56%|    | 242/436 [00:05<00:04, 41.65it/s][A
 57%|    | 247/436 [00:05<00:04, 42.75it/s][A
 58%|    | 252/436 [00:05<00:04, 43.50it/s][A
 59%|    | 257/436 [00:05<00:04, 44.06it/s][A
 60%|    | 262/436 [00:05<00:03, 44.24it/s][A
 61%|    | 267/436 [00:06<00:03, 44.33it/s][A
 62%|   | 272/436 [00:06<00:03, 44.40it/s][A
 64%|   | 277/436 [00:06<00:03, 44.09it/s][A
 65%|   | 282/436 [00:06<00:03, 44.16it/s][A
 66%|   | 287/436 [00:06<00:03, 44.48it/s][A
 67%|   | 292/436 [00:06<00:03, 44.62it/s][A
 68%|   | 297/436 [00:06<00:03, 44.83it/s][A
 69%|   | 302/436 [00:06<00:02, 44.87it/s][A
 70%|   | 307/436 [00:06<00:02, 45.03it/s][A
 72%|  | 312/436 [00:07<00:02, 44.84it/s][A
 73%|  | 317/436 [00:07<00:02, 44.71it/s][A
 74%|  | 322/436 [00:07<00:02, 44.50it/s][A
 75%|  | 327/436 [00:07<00:02, 44.37it/s][A
 76%|  | 332/436 [00:07<00:02, 44.55it/s][A
 77%|  | 337/436 [00:07<00:02, 44.69it/s][A
 78%|  | 342/436 [00:07<00:02, 44.92it/s][A
 80%|  | 347/436 [00:07<00:01, 44.98it/s][A
 81%|  | 352/436 [00:07<00:01, 45.02it/s][A
 82%| | 357/436 [00:08<00:01, 44.73it/s][A
 83%| | 362/436 [00:08<00:01, 44.61it/s][A
 84%| | 367/436 [00:08<00:01, 44.55it/s][A
 85%| | 372/436 [00:08<00:01, 44.11it/s][A
 86%| | 377/436 [00:08<00:01, 44.29it/s][A
 88%| | 382/436 [00:08<00:01, 44.58it/s][A
 89%| | 387/436 [00:08<00:01, 44.75it/s][A
 90%| | 392/436 [00:08<00:00, 44.92it/s][A
 91%| | 397/436 [00:08<00:00, 44.97it/s][A
 92%|| 402/436 [00:09<00:00, 44.68it/s][A
 93%|| 407/436 [00:09<00:00, 44.67it/s][A
 94%|| 412/436 [00:09<00:00, 44.53it/s][A
 96%|| 417/436 [00:09<00:00, 44.51it/s][A
 97%|| 422/436 [00:09<00:00, 44.63it/s][A
 98%|| 427/436 [00:09<00:00, 44.56it/s][A
 99%|| 432/436 [00:09<00:00, 44.81it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.81it/s][A100%|| 780/780 [05:20<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:25:23,541 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 07:25:23,699 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:25:26,658 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:25:26,725 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:25:26,763 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 07:25:33,182 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 07:25:33,205 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156 (score: 1.099610447883606).
                                                 100%|| 780/780 [05:38<00:00,  3.45it/s]100%|| 780/780 [05:38<00:00,  2.30it/s]
[INFO|trainer.py:1894] 2023-08-29 07:25:41,301 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 07:25:41,384 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:25:43,718 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:25:43,810 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:25:43,858 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:25:44,321 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:44,340 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:44,340 >>   train_loss               =     0.5343
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:44,340 >>   train_runtime            = 0:05:38.48
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:44,341 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:44,341 >>   train_samples_per_second =    147.702
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:44,341 >>   train_steps_per_second   =      2.304
{'eval_loss': 1.1431071758270264, 'eval_runtime': 9.8021, 'eval_samples_per_second': 355.23, 'eval_steps_per_second': 44.48, 'epoch': 5.0}
{'train_runtime': 338.4864, 'train_samples_per_second': 147.702, 'train_steps_per_second': 2.304, 'train_loss': 0.534273920303736, 'epoch': 5.0}
08/29/2023 07:25:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 07:25:44,525 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:25:44,525 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 07:25:44,525 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.91it/s]  3%|         | 12/436 [00:00<00:08, 49.11it/s]  4%|         | 17/436 [00:00<00:08, 47.62it/s]  5%|         | 22/436 [00:00<00:08, 46.78it/s]  6%|         | 27/436 [00:00<00:08, 46.37it/s]  7%|         | 32/436 [00:00<00:08, 46.07it/s]  8%|         | 37/436 [00:00<00:08, 45.83it/s] 10%|         | 42/436 [00:00<00:08, 45.46it/s] 11%|         | 47/436 [00:01<00:08, 44.82it/s] 12%|        | 52/436 [00:01<00:08, 44.67it/s] 13%|        | 57/436 [00:01<00:08, 44.75it/s] 14%|        | 62/436 [00:01<00:08, 44.77it/s] 15%|        | 67/436 [00:01<00:08, 44.93it/s] 17%|        | 72/436 [00:01<00:08, 45.01it/s] 18%|        | 77/436 [00:01<00:07, 45.22it/s] 19%|        | 82/436 [00:01<00:07, 45.29it/s] 20%|        | 87/436 [00:01<00:07, 45.01it/s] 21%|        | 92/436 [00:02<00:07, 44.70it/s] 22%|       | 97/436 [00:02<00:07, 44.58it/s] 23%|       | 102/436 [00:02<00:07, 44.54it/s] 25%|       | 107/436 [00:02<00:07, 44.62it/s] 26%|       | 112/436 [00:02<00:07, 44.86it/s] 27%|       | 117/436 [00:02<00:07, 44.93it/s] 28%|       | 122/436 [00:02<00:06, 45.11it/s] 29%|       | 127/436 [00:02<00:06, 45.18it/s] 30%|       | 132/436 [00:02<00:06, 44.83it/s] 31%|      | 137/436 [00:03<00:06, 44.61it/s] 33%|      | 142/436 [00:03<00:06, 44.50it/s] 34%|      | 147/436 [00:03<00:06, 44.45it/s] 35%|      | 152/436 [00:03<00:06, 44.66it/s] 36%|      | 157/436 [00:03<00:06, 44.78it/s] 37%|      | 162/436 [00:03<00:06, 44.89it/s] 38%|      | 167/436 [00:03<00:05, 45.02it/s] 39%|      | 172/436 [00:03<00:05, 45.11it/s] 41%|      | 177/436 [00:03<00:05, 44.97it/s] 42%|     | 182/436 [00:04<00:05, 44.78it/s] 43%|     | 187/436 [00:04<00:05, 44.62it/s] 44%|     | 192/436 [00:04<00:05, 44.59it/s] 45%|     | 197/436 [00:04<00:05, 44.53it/s] 46%|     | 202/436 [00:04<00:05, 44.75it/s] 47%|     | 207/436 [00:04<00:05, 44.87it/s] 49%|     | 212/436 [00:04<00:04, 44.97it/s] 50%|     | 217/436 [00:04<00:04, 44.99it/s] 51%|     | 222/436 [00:04<00:04, 45.02it/s] 52%|    | 227/436 [00:05<00:04, 44.67it/s] 53%|    | 232/436 [00:05<00:04, 43.88it/s] 54%|    | 237/436 [00:05<00:04, 44.20it/s] 56%|    | 242/436 [00:05<00:04, 44.29it/s] 57%|    | 247/436 [00:05<00:04, 44.52it/s] 58%|    | 252/436 [00:05<00:04, 43.07it/s] 59%|    | 257/436 [00:05<00:04, 44.09it/s] 60%|    | 262/436 [00:05<00:03, 44.43it/s] 61%|    | 267/436 [00:05<00:03, 44.70it/s] 62%|   | 272/436 [00:06<00:03, 44.57it/s] 64%|   | 277/436 [00:06<00:03, 44.65it/s] 65%|   | 282/436 [00:06<00:03, 44.74it/s] 66%|   | 287/436 [00:06<00:03, 44.73it/s] 67%|   | 292/436 [00:06<00:03, 44.77it/s] 68%|   | 297/436 [00:06<00:03, 44.57it/s] 69%|   | 302/436 [00:06<00:04, 30.90it/s] 70%|   | 307/436 [00:06<00:03, 34.51it/s] 71%|  | 311/436 [00:07<00:03, 35.37it/s] 72%|  | 316/436 [00:07<00:03, 38.54it/s] 74%|  | 321/436 [00:07<00:02, 40.36it/s] 75%|  | 326/436 [00:07<00:02, 39.61it/s] 76%|  | 331/436 [00:07<00:02, 41.38it/s] 77%|  | 336/436 [00:07<00:02, 42.48it/s] 78%|  | 341/436 [00:07<00:02, 43.10it/s] 79%|  | 346/436 [00:07<00:02, 43.30it/s] 81%|  | 351/436 [00:08<00:01, 43.73it/s] 82%| | 356/436 [00:08<00:01, 44.06it/s] 83%| | 361/436 [00:08<00:01, 43.63it/s] 84%| | 366/436 [00:08<00:01, 44.11it/s] 85%| | 371/436 [00:08<00:01, 44.18it/s] 86%| | 376/436 [00:08<00:01, 44.57it/s] 87%| | 381/436 [00:08<00:01, 44.79it/s] 89%| | 386/436 [00:08<00:01, 44.73it/s] 90%| | 391/436 [00:08<00:01, 44.62it/s] 91%| | 396/436 [00:09<00:00, 44.57it/s] 92%|| 401/436 [00:09<00:00, 44.55it/s] 93%|| 406/436 [00:09<00:00, 44.72it/s] 94%|| 411/436 [00:09<00:00, 44.75it/s] 95%|| 416/436 [00:09<00:00, 44.83it/s] 97%|| 421/436 [00:09<00:00, 44.86it/s] 98%|| 426/436 [00:09<00:00, 44.84it/s] 99%|| 431/436 [00:09<00:00, 44.78it/s]100%|| 436/436 [00:09<00:00, 44.78it/s]100%|| 436/436 [00:09<00:00, 44.01it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:25:54,450 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   eval_loss               =     1.0996
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   eval_runtime            = 0:00:09.92
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   eval_samples_per_second =    350.834
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   eval_steps_per_second   =      43.93
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:25:54,450 >>   perplexity              =      3.003
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:07,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:07,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:07,849 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:07,849 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:07,849 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:26:08,555 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:26:08,556 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:26:09,118 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:26:10,174 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:26:10,174 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:13,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:13,105 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:13,105 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:13,105 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:13,105 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:26:13,754 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:26:13,755 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:26:14,346 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:26:14,521 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:26:14,521 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:15,  1.62it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.55it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.44it/s]Extractor Predicting: 39it [00:24,  1.47it/s]Extractor Predicting: 40it [00:25,  1.50it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.53it/s]Extractor Predicting: 54it [00:34,  1.49it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.53it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:38,  1.50it/s]Extractor Predicting: 61it [00:39,  1.53it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.51it/s]Extractor Predicting: 64it [00:41,  1.47it/s]Extractor Predicting: 65it [00:41,  1.53it/s]Extractor Predicting: 66it [00:42,  1.51it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:43,  1.55it/s]Extractor Predicting: 69it [00:44,  1.54it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:45,  1.59it/s]Extractor Predicting: 72it [00:46,  1.60it/s]Extractor Predicting: 73it [00:46,  1.61it/s]Extractor Predicting: 74it [00:47,  1.63it/s]Extractor Predicting: 75it [00:48,  1.60it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.56it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:50,  1.54it/s]Extractor Predicting: 80it [00:51,  1.55it/s]Extractor Predicting: 81it [00:52,  1.54it/s]Extractor Predicting: 82it [00:52,  1.54it/s]Extractor Predicting: 83it [00:53,  1.56it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:54,  1.58it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:56,  1.54it/s]Extractor Predicting: 89it [00:57,  1.53it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:58,  1.49it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [01:00,  1.54it/s]Extractor Predicting: 94it [01:00,  1.52it/s]Extractor Predicting: 95it [01:01,  1.56it/s]Extractor Predicting: 96it [01:01,  1.57it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:03,  1.50it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:05,  1.50it/s]Extractor Predicting: 102it [01:05,  1.49it/s]Extractor Predicting: 103it [01:06,  1.49it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:08,  1.48it/s]Extractor Predicting: 107it [01:09,  1.51it/s]Extractor Predicting: 108it [01:09,  1.53it/s]Extractor Predicting: 109it [01:10,  1.57it/s]Extractor Predicting: 110it [01:11,  1.58it/s]Extractor Predicting: 111it [01:11,  1.61it/s]Extractor Predicting: 112it [01:12,  1.60it/s]Extractor Predicting: 113it [01:12,  1.59it/s]Extractor Predicting: 114it [01:13,  1.57it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:15,  1.53it/s]Extractor Predicting: 118it [01:16,  1.42it/s]Extractor Predicting: 119it [01:17,  1.46it/s]Extractor Predicting: 120it [01:17,  1.51it/s]Extractor Predicting: 121it [01:18,  1.50it/s]Extractor Predicting: 122it [01:19,  1.53it/s]Extractor Predicting: 123it [01:19,  1.50it/s]Extractor Predicting: 124it [01:20,  1.53it/s]Extractor Predicting: 125it [01:20,  1.52it/s]Extractor Predicting: 126it [01:21,  1.54it/s]Extractor Predicting: 127it [01:22,  1.53it/s]Extractor Predicting: 128it [01:22,  1.51it/s]Extractor Predicting: 129it [01:23,  1.52it/s]Extractor Predicting: 130it [01:24,  1.49it/s]Extractor Predicting: 131it [01:24,  1.52it/s]Extractor Predicting: 132it [01:25,  1.53it/s]Extractor Predicting: 133it [01:26,  1.49it/s]Extractor Predicting: 134it [01:27,  1.46it/s]Extractor Predicting: 135it [01:27,  1.48it/s]Extractor Predicting: 136it [01:28,  1.50it/s]Extractor Predicting: 137it [01:28,  1.51it/s]Extractor Predicting: 138it [01:29,  1.52it/s]Extractor Predicting: 139it [01:30,  1.49it/s]Extractor Predicting: 140it [01:31,  1.46it/s]Extractor Predicting: 141it [01:31,  1.48it/s]Extractor Predicting: 142it [01:32,  1.52it/s]Extractor Predicting: 143it [01:32,  1.55it/s]Extractor Predicting: 143it [01:32,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:56,801 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:56,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:56,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:56,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:56,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:27:57,142 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:27:57,143 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:27:57,430 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:27:58,481 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:27:58,481 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:27:59,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:00,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:00,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:00,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:00,037 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:28:00,395 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:28:00,396 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:28:00,696 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:28:00,842 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:28:00,842 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3373205741626794,
  "recall": 0.040493968983342905,
  "score": 0.07230769230769231,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.60it/s]Extractor Predicting: 32it [00:19,  1.57it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:20,  1.55it/s]Extractor Predicting: 35it [00:21,  1.54it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:22,  1.54it/s]Extractor Predicting: 38it [00:23,  1.53it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:24,  1.55it/s]Extractor Predicting: 41it [00:25,  1.57it/s]Extractor Predicting: 42it [00:25,  1.58it/s]Extractor Predicting: 43it [00:26,  1.54it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:27,  1.55it/s]Extractor Predicting: 46it [00:28,  1.55it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:29,  1.56it/s]Extractor Predicting: 49it [00:30,  1.55it/s]Extractor Predicting: 50it [00:31,  1.57it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.54it/s]Extractor Predicting: 54it [00:33,  1.56it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:34,  1.61it/s]Extractor Predicting: 57it [00:35,  1.59it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:36,  1.58it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.62it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:41,  1.57it/s]Extractor Predicting: 67it [00:41,  1.59it/s]Extractor Predicting: 68it [00:42,  1.58it/s]Extractor Predicting: 69it [00:43,  1.59it/s]Extractor Predicting: 70it [00:43,  1.60it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:45,  1.56it/s]Extractor Predicting: 74it [00:46,  1.59it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:48,  1.62it/s]Extractor Predicting: 78it [00:48,  1.67it/s]Extractor Predicting: 79it [00:49,  1.65it/s]Extractor Predicting: 80it [00:49,  1.61it/s]Extractor Predicting: 81it [00:50,  1.60it/s]Extractor Predicting: 82it [00:51,  1.63it/s]Extractor Predicting: 83it [00:51,  1.61it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:53,  1.58it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:54,  1.61it/s]Extractor Predicting: 89it [00:55,  1.65it/s]Extractor Predicting: 90it [00:56,  1.66it/s]Extractor Predicting: 91it [00:56,  1.65it/s]Extractor Predicting: 92it [00:57,  1.60it/s]Extractor Predicting: 93it [00:57,  1.60it/s]Extractor Predicting: 94it [00:58,  1.56it/s]Extractor Predicting: 95it [00:59,  1.56it/s]Extractor Predicting: 96it [00:59,  1.56it/s]Extractor Predicting: 97it [01:00,  1.57it/s]Extractor Predicting: 98it [01:01,  1.56it/s]Extractor Predicting: 99it [01:01,  1.58it/s]Extractor Predicting: 100it [01:02,  1.56it/s]Extractor Predicting: 101it [01:03,  1.55it/s]Extractor Predicting: 102it [01:03,  1.57it/s]Extractor Predicting: 103it [01:04,  1.58it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:05,  1.57it/s]Extractor Predicting: 106it [01:06,  1.57it/s]Extractor Predicting: 107it [01:06,  1.56it/s]Extractor Predicting: 108it [01:07,  1.58it/s]Extractor Predicting: 109it [01:08,  1.57it/s]Extractor Predicting: 110it [01:08,  1.56it/s]Extractor Predicting: 111it [01:09,  1.55it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:10,  1.56it/s]Extractor Predicting: 114it [01:11,  1.57it/s]Extractor Predicting: 115it [01:12,  1.55it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:13,  1.70it/s]Extractor Predicting: 118it [01:13,  1.73it/s]Extractor Predicting: 119it [01:14,  1.72it/s]Extractor Predicting: 120it [01:14,  1.71it/s]Extractor Predicting: 121it [01:15,  1.69it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:16,  1.65it/s]Extractor Predicting: 124it [01:17,  1.69it/s]Extractor Predicting: 125it [01:17,  1.75it/s]Extractor Predicting: 126it [01:18,  1.73it/s]Extractor Predicting: 127it [01:19,  1.71it/s]Extractor Predicting: 128it [01:19,  1.74it/s]Extractor Predicting: 129it [01:20,  1.73it/s]Extractor Predicting: 130it [01:20,  1.72it/s]Extractor Predicting: 131it [01:21,  1.73it/s]Extractor Predicting: 132it [01:22,  1.53it/s]Extractor Predicting: 133it [01:22,  1.57it/s]Extractor Predicting: 134it [01:23,  1.62it/s]Extractor Predicting: 135it [01:23,  1.66it/s]Extractor Predicting: 136it [01:24,  1.71it/s]Extractor Predicting: 137it [01:25,  1.73it/s]Extractor Predicting: 138it [01:25,  1.71it/s]Extractor Predicting: 139it [01:26,  1.68it/s]Extractor Predicting: 140it [01:26,  1.71it/s]Extractor Predicting: 141it [01:27,  1.67it/s]Extractor Predicting: 142it [01:28,  1.67it/s]Extractor Predicting: 143it [01:28,  1.69it/s]Extractor Predicting: 144it [01:29,  1.65it/s]Extractor Predicting: 145it [01:29,  1.65it/s]Extractor Predicting: 146it [01:30,  1.67it/s]Extractor Predicting: 147it [01:31,  1.67it/s]Extractor Predicting: 148it [01:31,  1.65it/s]Extractor Predicting: 149it [01:32,  1.71it/s]Extractor Predicting: 150it [01:32,  1.68it/s]Extractor Predicting: 151it [01:33,  1.66it/s]Extractor Predicting: 152it [01:34,  1.68it/s]Extractor Predicting: 153it [01:34,  1.73it/s]Extractor Predicting: 154it [01:35,  1.73it/s]Extractor Predicting: 155it [01:35,  1.73it/s]Extractor Predicting: 156it [01:36,  1.72it/s]Extractor Predicting: 157it [01:36,  1.70it/s]Extractor Predicting: 158it [01:37,  1.71it/s]Extractor Predicting: 159it [01:38,  1.67it/s]Extractor Predicting: 160it [01:38,  1.67it/s]Extractor Predicting: 161it [01:39,  1.66it/s]Extractor Predicting: 162it [01:39,  1.66it/s]Extractor Predicting: 163it [01:40,  1.66it/s]Extractor Predicting: 164it [01:41,  1.65it/s]Extractor Predicting: 165it [01:41,  1.67it/s]Extractor Predicting: 166it [01:42,  1.68it/s]Extractor Predicting: 167it [01:42,  1.69it/s]Extractor Predicting: 168it [01:43,  1.65it/s]Extractor Predicting: 169it [01:44,  1.64it/s]Extractor Predicting: 170it [01:44,  1.62it/s]Extractor Predicting: 171it [01:45,  1.60it/s]Extractor Predicting: 172it [01:45,  1.67it/s]Extractor Predicting: 173it [01:46,  1.62it/s]Extractor Predicting: 174it [01:47,  1.60it/s]Extractor Predicting: 175it [01:47,  1.57it/s]Extractor Predicting: 176it [01:48,  1.59it/s]Extractor Predicting: 177it [01:49,  1.57it/s]Extractor Predicting: 178it [01:49,  1.54it/s]Extractor Predicting: 179it [01:50,  1.55it/s]Extractor Predicting: 180it [01:51,  1.59it/s]Extractor Predicting: 181it [01:51,  1.58it/s]Extractor Predicting: 182it [01:52,  1.56it/s]Extractor Predicting: 183it [01:53,  1.56it/s]Extractor Predicting: 184it [01:53,  1.54it/s]Extractor Predicting: 185it [01:54,  1.54it/s]Extractor Predicting: 186it [01:55,  1.51it/s]Extractor Predicting: 187it [01:55,  1.51it/s]Extractor Predicting: 188it [01:56,  1.56it/s]Extractor Predicting: 189it [01:56,  1.56it/s]Extractor Predicting: 190it [01:57,  1.57it/s]Extractor Predicting: 191it [01:58,  1.54it/s]Extractor Predicting: 192it [01:58,  1.54it/s]Extractor Predicting: 193it [01:59,  1.49it/s]Extractor Predicting: 194it [02:00,  1.50it/s]Extractor Predicting: 195it [02:00,  1.49it/s]Extractor Predicting: 196it [02:01,  1.51it/s]Extractor Predicting: 197it [02:02,  1.51it/s]Extractor Predicting: 198it [02:02,  1.51it/s]Extractor Predicting: 199it [02:03,  1.52it/s]Extractor Predicting: 200it [02:04,  1.51it/s]Extractor Predicting: 201it [02:04,  1.51it/s]Extractor Predicting: 202it [02:05,  1.52it/s]Extractor Predicting: 203it [02:06,  1.55it/s]Extractor Predicting: 204it [02:06,  1.57it/s]Extractor Predicting: 205it [02:07,  1.63it/s]Extractor Predicting: 206it [02:07,  1.61it/s]Extractor Predicting: 207it [02:08,  1.60it/s]Extractor Predicting: 208it [02:09,  1.60it/s]Extractor Predicting: 209it [02:09,  1.58it/s]Extractor Predicting: 210it [02:10,  1.60it/s]Extractor Predicting: 211it [02:11,  1.65it/s]Extractor Predicting: 212it [02:11,  1.65it/s]Extractor Predicting: 213it [02:12,  1.67it/s]Extractor Predicting: 214it [02:12,  1.64it/s]Extractor Predicting: 215it [02:13,  1.60it/s]Extractor Predicting: 216it [02:14,  1.61it/s]Extractor Predicting: 217it [02:14,  1.62it/s]Extractor Predicting: 218it [02:15,  1.64it/s]Extractor Predicting: 219it [02:15,  1.67it/s]Extractor Predicting: 220it [02:16,  1.64it/s]Extractor Predicting: 221it [02:17,  1.67it/s]Extractor Predicting: 222it [02:17,  1.69it/s]Extractor Predicting: 223it [02:18,  1.65it/s]Extractor Predicting: 224it [02:18,  1.66it/s]Extractor Predicting: 225it [02:19,  1.65it/s]Extractor Predicting: 226it [02:20,  1.59it/s]Extractor Predicting: 227it [02:20,  1.59it/s]Extractor Predicting: 228it [02:21,  1.60it/s]Extractor Predicting: 229it [02:22,  1.59it/s]Extractor Predicting: 230it [02:22,  1.58it/s]Extractor Predicting: 231it [02:23,  1.60it/s]Extractor Predicting: 232it [02:23,  1.64it/s]Extractor Predicting: 233it [02:24,  1.70it/s]Extractor Predicting: 234it [02:25,  1.69it/s]Extractor Predicting: 235it [02:25,  1.69it/s]Extractor Predicting: 236it [02:26,  1.75it/s]Extractor Predicting: 237it [02:26,  1.75it/s]Extractor Predicting: 238it [02:27,  1.76it/s]Extractor Predicting: 239it [02:27,  1.75it/s]Extractor Predicting: 240it [02:28,  1.77it/s]Extractor Predicting: 241it [02:29,  1.77it/s]Extractor Predicting: 242it [02:29,  1.77it/s]Extractor Predicting: 243it [02:30,  1.80it/s]Extractor Predicting: 244it [02:30,  1.78it/s]Extractor Predicting: 245it [02:31,  1.85it/s]Extractor Predicting: 246it [02:31,  1.89it/s]Extractor Predicting: 247it [02:32,  1.80it/s]Extractor Predicting: 248it [02:32,  1.75it/s]Extractor Predicting: 249it [02:33,  1.77it/s]Extractor Predicting: 250it [02:34,  1.78it/s]Extractor Predicting: 251it [02:34,  1.81it/s]Extractor Predicting: 252it [02:35,  1.82it/s]Extractor Predicting: 253it [02:35,  1.79it/s]Extractor Predicting: 254it [02:36,  1.77it/s]Extractor Predicting: 255it [02:36,  1.80it/s]Extractor Predicting: 256it [02:37,  1.80it/s]Extractor Predicting: 257it [02:37,  1.83it/s]Extractor Predicting: 258it [02:38,  1.83it/s]Extractor Predicting: 259it [02:38,  1.84it/s]Extractor Predicting: 260it [02:39,  1.82it/s]Extractor Predicting: 261it [02:40,  1.72it/s]Extractor Predicting: 262it [02:40,  1.71it/s]Extractor Predicting: 263it [02:41,  1.46it/s]Extractor Predicting: 264it [02:42,  1.47it/s]Extractor Predicting: 265it [02:42,  1.50it/s]Extractor Predicting: 266it [02:43,  1.53it/s]Extractor Predicting: 267it [02:44,  1.58it/s]Extractor Predicting: 268it [02:44,  1.59it/s]Extractor Predicting: 269it [02:45,  1.56it/s]Extractor Predicting: 270it [02:46,  1.52it/s]Extractor Predicting: 271it [02:46,  1.52it/s]Extractor Predicting: 272it [02:47,  1.56it/s]Extractor Predicting: 273it [02:48,  1.55it/s]Extractor Predicting: 274it [02:48,  1.55it/s]Extractor Predicting: 275it [02:49,  1.55it/s]Extractor Predicting: 276it [02:50,  1.56it/s]Extractor Predicting: 277it [02:50,  1.54it/s]Extractor Predicting: 278it [02:51,  1.53it/s]Extractor Predicting: 279it [02:51,  1.56it/s]Extractor Predicting: 280it [02:52,  1.55it/s]Extractor Predicting: 281it [02:53,  1.53it/s]Extractor Predicting: 282it [02:53,  1.53it/s]Extractor Predicting: 283it [02:54,  1.54it/s]Extractor Predicting: 284it [02:55,  1.53it/s]Extractor Predicting: 285it [02:55,  1.52it/s]Extractor Predicting: 286it [02:56,  1.53it/s]Extractor Predicting: 287it [02:57,  1.52it/s]Extractor Predicting: 288it [02:57,  1.57it/s]Extractor Predicting: 289it [02:58,  1.58it/s]Extractor Predicting: 290it [02:59,  1.60it/s]Extractor Predicting: 291it [02:59,  1.62it/s]Extractor Predicting: 292it [03:00,  1.62it/s]Extractor Predicting: 293it [03:00,  1.62it/s]Extractor Predicting: 294it [03:01,  1.61it/s]Extractor Predicting: 295it [03:02,  1.62it/s]Extractor Predicting: 296it [03:02,  1.61it/s]Extractor Predicting: 297it [03:03,  1.61it/s]Extractor Predicting: 298it [03:03,  1.66it/s]Extractor Predicting: 299it [03:04,  1.63it/s]Extractor Predicting: 300it [03:05,  1.64it/s]Extractor Predicting: 301it [03:05,  1.63it/s]Extractor Predicting: 302it [03:06,  1.58it/s]Extractor Predicting: 303it [03:07,  1.59it/s]Extractor Predicting: 304it [03:07,  1.59it/s]Extractor Predicting: 305it [03:08,  1.59it/s]Extractor Predicting: 306it [03:08,  1.61it/s]Extractor Predicting: 307it [03:09,  1.61it/s]Extractor Predicting: 308it [03:10,  1.61it/s]Extractor Predicting: 309it [03:10,  1.57it/s]Extractor Predicting: 310it [03:11,  1.58it/s]Extractor Predicting: 311it [03:12,  1.59it/s]Extractor Predicting: 312it [03:12,  1.60it/s]Extractor Predicting: 313it [03:13,  1.60it/s]Extractor Predicting: 314it [03:13,  1.60it/s]Extractor Predicting: 315it [03:14,  1.62it/s]Extractor Predicting: 316it [03:15,  1.61it/s]Extractor Predicting: 317it [03:15,  1.64it/s]Extractor Predicting: 318it [03:16,  1.65it/s]Extractor Predicting: 319it [03:17,  1.65it/s]Extractor Predicting: 320it [03:17,  1.63it/s]Extractor Predicting: 321it [03:18,  1.61it/s]Extractor Predicting: 322it [03:18,  1.61it/s]Extractor Predicting: 323it [03:19,  1.64it/s]Extractor Predicting: 324it [03:20,  1.64it/s]Extractor Predicting: 325it [03:20,  1.63it/s]Extractor Predicting: 326it [03:21,  1.64it/s]Extractor Predicting: 327it [03:21,  1.63it/s]Extractor Predicting: 328it [03:22,  1.62it/s]Extractor Predicting: 329it [03:23,  1.63it/s]Extractor Predicting: 330it [03:23,  1.62it/s]Extractor Predicting: 331it [03:24,  1.61it/s]Extractor Predicting: 332it [03:25,  1.64it/s]Extractor Predicting: 333it [03:25,  1.61it/s]Extractor Predicting: 334it [03:26,  1.64it/s]Extractor Predicting: 335it [03:26,  1.61it/s]Extractor Predicting: 336it [03:27,  1.63it/s]Extractor Predicting: 337it [03:28,  1.62it/s]Extractor Predicting: 338it [03:28,  1.63it/s]Extractor Predicting: 339it [03:29,  1.64it/s]Extractor Predicting: 340it [03:29,  1.62it/s]Extractor Predicting: 341it [03:30,  1.64it/s]Extractor Predicting: 342it [03:31,  1.66it/s]Extractor Predicting: 343it [03:31,  1.65it/s]Extractor Predicting: 344it [03:32,  1.68it/s]Extractor Predicting: 345it [03:32,  1.64it/s]Extractor Predicting: 346it [03:33,  1.64it/s]Extractor Predicting: 347it [03:34,  1.65it/s]Extractor Predicting: 348it [03:34,  1.61it/s]Extractor Predicting: 349it [03:35,  1.63it/s]Extractor Predicting: 350it [03:36,  1.61it/s]Extractor Predicting: 351it [03:36,  1.62it/s]Extractor Predicting: 352it [03:37,  1.60it/s]Extractor Predicting: 353it [03:37,  1.60it/s]Extractor Predicting: 354it [03:38,  1.62it/s]Extractor Predicting: 355it [03:39,  1.60it/s]Extractor Predicting: 356it [03:39,  1.62it/s]Extractor Predicting: 357it [03:40,  1.59it/s]Extractor Predicting: 358it [03:41,  1.61it/s]Extractor Predicting: 359it [03:41,  1.61it/s]Extractor Predicting: 360it [03:42,  1.61it/s]Extractor Predicting: 361it [03:42,  1.60it/s]Extractor Predicting: 362it [03:43,  1.64it/s]Extractor Predicting: 363it [03:44,  1.63it/s]Extractor Predicting: 364it [03:44,  1.66it/s]Extractor Predicting: 365it [03:45,  1.64it/s]Extractor Predicting: 366it [03:45,  1.65it/s]Extractor Predicting: 367it [03:46,  1.64it/s]Extractor Predicting: 368it [03:47,  1.62it/s]Extractor Predicting: 369it [03:47,  1.58it/s]Extractor Predicting: 370it [03:48,  1.59it/s]Extractor Predicting: 371it [03:49,  1.40it/s]Extractor Predicting: 372it [03:49,  1.47it/s]Extractor Predicting: 373it [03:50,  1.50it/s]Extractor Predicting: 374it [03:51,  1.55it/s]Extractor Predicting: 375it [03:51,  1.56it/s]Extractor Predicting: 376it [03:52,  1.54it/s]Extractor Predicting: 377it [03:53,  1.61it/s]Extractor Predicting: 378it [03:53,  1.62it/s]Extractor Predicting: 379it [03:54,  1.64it/s]Extractor Predicting: 380it [03:54,  1.61it/s]Extractor Predicting: 381it [03:55,  1.62it/s]Extractor Predicting: 382it [03:56,  1.61it/s]Extractor Predicting: 383it [03:56,  1.59it/s]Extractor Predicting: 384it [03:57,  1.54it/s]Extractor Predicting: 385it [03:58,  1.57it/s]Extractor Predicting: 386it [03:58,  1.56it/s]Extractor Predicting: 387it [03:59,  1.50it/s]Extractor Predicting: 388it [04:00,  1.47it/s]Extractor Predicting: 389it [04:00,  1.51it/s]Extractor Predicting: 390it [04:01,  1.53it/s]Extractor Predicting: 391it [04:02,  1.56it/s]Extractor Predicting: 392it [04:02,  1.56it/s]Extractor Predicting: 393it [04:03,  1.56it/s]Extractor Predicting: 394it [04:03,  1.57it/s]Extractor Predicting: 395it [04:04,  1.58it/s]Extractor Predicting: 396it [04:05,  1.56it/s]Extractor Predicting: 397it [04:05,  1.58it/s]Extractor Predicting: 398it [04:06,  1.57it/s]Extractor Predicting: 399it [04:07,  1.61it/s]Extractor Predicting: 400it [04:07,  1.54it/s]Extractor Predicting: 401it [04:08,  1.57it/s]Extractor Predicting: 402it [04:09,  1.57it/s]Extractor Predicting: 403it [04:09,  1.56it/s]Extractor Predicting: 404it [04:10,  1.59it/s]Extractor Predicting: 405it [04:10,  1.60it/s]Extractor Predicting: 406it [04:11,  1.62it/s]Extractor Predicting: 407it [04:12,  1.61it/s]Extractor Predicting: 408it [04:12,  1.59it/s]Extractor Predicting: 409it [04:13,  1.61it/s]Extractor Predicting: 410it [04:14,  1.62it/s]Extractor Predicting: 411it [04:14,  1.61it/s]Extractor Predicting: 412it [04:15,  1.64it/s]Extractor Predicting: 413it [04:15,  1.64it/s]Extractor Predicting: 414it [04:16,  1.63it/s]Extractor Predicting: 415it [04:17,  1.59it/s]Extractor Predicting: 416it [04:17,  1.62it/s]Extractor Predicting: 417it [04:18,  1.63it/s]Extractor Predicting: 418it [04:18,  1.65it/s]Extractor Predicting: 419it [04:19,  1.66it/s]Extractor Predicting: 420it [04:20,  1.67it/s]Extractor Predicting: 421it [04:20,  1.64it/s]Extractor Predicting: 422it [04:21,  1.65it/s]Extractor Predicting: 423it [04:21,  1.66it/s]Extractor Predicting: 424it [04:22,  1.65it/s]Extractor Predicting: 425it [04:23,  1.61it/s]Extractor Predicting: 426it [04:23,  1.66it/s]Extractor Predicting: 427it [04:24,  1.64it/s]Extractor Predicting: 428it [04:25,  1.58it/s]Extractor Predicting: 429it [04:25,  1.82it/s]Extractor Predicting: 429it [04:25,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:38,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:38,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:38,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:38,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:38,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:32:38,543 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:32:38,544 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:32:38,817 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:32:39,885 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:32:39,904 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:41,259 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:41,283 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:41,283 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:41,283 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:32:41,283 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:32:41,612 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:32:41,613 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:32:41,884 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:32:42,028 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:32:42,028 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.37827393731215114,
  "recall": 0.08568371912079362,
  "score": 0.13971929268099278,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.76it/s]Extractor Predicting: 5it [00:03,  1.66it/s]
[INFO|configuration_utils.py:515] 2023-08-29 07:32:46,243 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:32:46,243 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:32:46,281 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:32:46,282 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 07:32:46,298 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:32:53,991 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 07:32:54,019 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 07:32:54,259 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:32:54,260 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:32:54,330 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:54,372 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:54,372 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:54,372 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:54,372 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:54,372 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:32:54,373 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3333333333333333,
  "recall": 0.013761467889908258,
  "score": 0.026431718061674013,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 07:32:54,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:55,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:55,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:56,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:56,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:57,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:57,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:58,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:59,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:32:59,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:00,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:00,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:01,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:01,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:02,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:03,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:03,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:04,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:04,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:05,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:05,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:11<03:38, 11.52s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:06,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:06,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:07,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:07,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:08,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:08,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:09,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:09,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:10,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:10,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:11,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:11,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:12,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:13,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:13,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:14,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:14,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:15,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:15,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:16,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:22<03:17, 10.97s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:16,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:17,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:17,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:18,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:19,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:19,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:20,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:21,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:21,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:22,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:23,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:23,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:24,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:25,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:25,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:26,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:26,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:27,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:28,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:28,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:29,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:35<03:25, 12.11s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:30,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:30,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:31,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:32,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:32,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:33,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:33,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:34,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:34,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:35,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:36,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:36,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:37,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:37,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:38,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:38,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:39,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:40,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:40,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:41,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:47<03:11, 11.95s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:41,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:42,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:43,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:43,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:44,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:45,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:45,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:46,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:47,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:47,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:48,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:49,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:49,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:50,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:51,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:51,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:52,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:53,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:53,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:54,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:55,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:00<03:08, 12.58s/it][WARNING|generation_utils.py:914] 2023-08-29 07:33:55,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:56,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:56,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:57,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:57,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:58,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:59,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:33:59,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:00,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:00,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:01,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:01,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:02,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:03,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:03,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:04,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:04,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:05,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:05,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:06,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:12<02:50, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:07,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:07,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:08,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:08,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:09,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:10,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:10,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:11,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:11,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:12,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:12,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:13,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:14,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:14,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:15,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:15,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:16,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:17,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:17,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:18,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:18,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:24<02:38, 12.22s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:19,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:19,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:20,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:20,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:21,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:22,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:22,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:23,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:23,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:24,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:24,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:25,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:26,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:26,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:27,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:27,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:28,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:28,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:29,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:29,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:30,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:30,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:31,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:37<02:27, 12.31s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:31,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:32,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:33,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:33,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:34,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:34,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:35,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:35,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:36,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:37,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:37,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:39,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:40,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:41,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:42,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:42,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:43,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:44,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [01:49<02:16, 12.45s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:44,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:45,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:45,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:46,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:46,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:47,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:48,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:48,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:49,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:49,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:50,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:50,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:51,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:51,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:52,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:53,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:53,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:54,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:54,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:55,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:01<02:01, 12.10s/it][WARNING|generation_utils.py:914] 2023-08-29 07:34:55,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:56,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:57,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:57,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:58,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:58,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:34:59,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:00,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:00,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:01,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:01,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:02,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:03,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:03,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:04,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:04,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:05,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:05,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:06,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:06,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:07,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:08,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:14<01:50, 12.31s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:08,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:09,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:09,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:10,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:10,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:11,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:12,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:12,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:13,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:13,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:14,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:14,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:15,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:16,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:16,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:17,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:17,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:18,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:18,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:19,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:19,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:25<01:37, 12.16s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:20,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:21,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:21,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:22,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:23,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:23,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:24,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:24,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:25,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:26,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:26,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:27,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:27,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:28,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:28,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:29,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:30,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:30,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:31,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:32,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [02:37<01:24, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:32,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:33,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:33,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:34,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:35,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:35,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:36,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:36,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:37,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:37,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:38,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:38,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:39,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:40,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:40,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:41,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:41,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:42,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:43,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:43,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [02:49<01:12, 12.01s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:44,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:44,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:45,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:46,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:46,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:47,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:48,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:48,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:49,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:49,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:50,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:50,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:51,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:52,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:52,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:53,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:53,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:54,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:55,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:55,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:56,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:02<01:00, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-29 07:35:56,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:57,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:58,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:58,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:59,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:35:59,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:00,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:01,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:01,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:02,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:02,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:03,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:04,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:04,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:05,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:05,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:06,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:06,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:07,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:07,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:08,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:09,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:14<00:49, 12.35s/it][WARNING|generation_utils.py:914] 2023-08-29 07:36:09,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:10,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:10,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:11,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:12,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:13,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:13,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:14,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:15,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:15,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:16,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:16,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:17,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:18,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:18,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:19,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:19,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:20,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:21,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:22,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:28<00:37, 12.64s/it][WARNING|generation_utils.py:914] 2023-08-29 07:36:22,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:23,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:24,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:24,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:25,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:25,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:26,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:26,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:27,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:28,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:28,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:29,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:30,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:30,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:31,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:31,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:32,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:32,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:33,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:33,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [03:39<00:24, 12.30s/it][WARNING|generation_utils.py:914] 2023-08-29 07:36:34,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:35,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:35,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:36,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:36,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:37,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:37,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:38,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:38,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:39,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:39,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:40,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:40,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:41,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:42,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:42,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:43,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:43,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:44,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:44,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:45,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [03:51<00:12, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-29 07:36:46,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:47,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:47,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:48,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:48,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:49,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:49,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:50,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:51,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:51,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:52,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:52,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:53,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:53,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:54,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:55,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:55,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:56,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:56,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:36:57,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:03<00:00, 11.92s/it]Generating: 100%|| 20/20 [04:03<00:00, 12.15s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:04,837 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:04,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:04,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:04,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:04,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:37:05,456 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:37:05,457 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:37:06,041 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:37:07,103 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:37:07,104 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:10,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:10,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:10,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:10,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:10,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:37:10,927 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:37:10,928 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:37:11,549 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:37:11,722 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:37:11,722 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9796875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8958333333333334, 'errors': {'', '(\'Lionel Messi\', \'competition class\', \'\', "He also competed in the 2009 World Cup , beating Brazil \'s Lionel Messi in the men \'s 400 freestyle , finishing 9th .")'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8288043478260869, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.965625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8707386363636364, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9484375, 'errors': {'', "('Giuseppe', 'mountain range', '', 'He is the grandson of the late Italian writer Giuseppe and is one of the most famous naturalists and sculptor of the period .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9255952380952381, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9047619047619048, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9515625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 9537
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9637, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.46it/s]Extractor Estimating: 2it [00:01,  1.67it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.81it/s]Extractor Estimating: 5it [00:02,  1.84it/s]Extractor Estimating: 6it [00:03,  1.83it/s]Extractor Estimating: 7it [00:03,  1.78it/s]Extractor Estimating: 8it [00:04,  1.80it/s]Extractor Estimating: 9it [00:05,  1.76it/s]Extractor Estimating: 10it [00:05,  1.76it/s]Extractor Estimating: 11it [00:06,  1.85it/s]Extractor Estimating: 12it [00:06,  1.82it/s]Extractor Estimating: 13it [00:07,  1.72it/s]Extractor Estimating: 14it [00:07,  1.70it/s]Extractor Estimating: 15it [00:08,  1.74it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:09,  1.74it/s]Extractor Estimating: 18it [00:10,  1.69it/s]Extractor Estimating: 19it [00:10,  1.73it/s]Extractor Estimating: 20it [00:11,  1.76it/s]Extractor Estimating: 21it [00:11,  1.86it/s]Extractor Estimating: 22it [00:12,  1.88it/s]Extractor Estimating: 23it [00:12,  1.96it/s]Extractor Estimating: 24it [00:13,  1.94it/s]Extractor Estimating: 25it [00:13,  1.86it/s]Extractor Estimating: 26it [00:14,  1.87it/s]Extractor Estimating: 27it [00:15,  1.85it/s]Extractor Estimating: 28it [00:15,  1.85it/s]Extractor Estimating: 29it [00:16,  1.84it/s]Extractor Estimating: 30it [00:16,  1.76it/s]Extractor Estimating: 31it [00:17,  1.79it/s]Extractor Estimating: 32it [00:17,  1.84it/s]Extractor Estimating: 33it [00:18,  1.77it/s]Extractor Estimating: 34it [00:18,  1.86it/s]Extractor Estimating: 35it [00:19,  1.82it/s]Extractor Estimating: 36it [00:20,  1.80it/s]Extractor Estimating: 37it [00:20,  1.78it/s]Extractor Estimating: 38it [00:21,  1.80it/s]Extractor Estimating: 39it [00:21,  1.77it/s]Extractor Estimating: 40it [00:22,  1.80it/s]Extractor Estimating: 41it [00:22,  1.75it/s]Extractor Estimating: 42it [00:23,  1.70it/s]Extractor Estimating: 43it [00:24,  1.75it/s]Extractor Estimating: 44it [00:24,  1.77it/s]Extractor Estimating: 45it [00:25,  1.79it/s]Extractor Estimating: 46it [00:25,  1.74it/s]Extractor Estimating: 47it [00:26,  1.73it/s]Extractor Estimating: 48it [00:26,  1.67it/s]Extractor Estimating: 49it [00:27,  1.63it/s]Extractor Estimating: 50it [00:28,  1.65it/s]Extractor Estimating: 51it [00:28,  1.67it/s]Extractor Estimating: 52it [00:29,  1.75it/s]Extractor Estimating: 53it [00:29,  1.73it/s]Extractor Estimating: 54it [00:30,  1.69it/s]Extractor Estimating: 55it [00:31,  1.71it/s]Extractor Estimating: 56it [00:31,  1.72it/s]Extractor Estimating: 57it [00:32,  1.70it/s]Extractor Estimating: 58it [00:32,  1.74it/s]Extractor Estimating: 59it [00:33,  1.75it/s]Extractor Estimating: 60it [00:34,  1.63it/s]Extractor Estimating: 61it [00:34,  1.64it/s]Extractor Estimating: 62it [00:35,  1.70it/s]Extractor Estimating: 63it [00:35,  1.73it/s]Extractor Estimating: 64it [00:36,  1.74it/s]Extractor Estimating: 65it [00:36,  1.72it/s]Extractor Estimating: 66it [00:37,  1.70it/s]Extractor Estimating: 67it [00:38,  1.67it/s]Extractor Estimating: 68it [00:38,  1.69it/s]Extractor Estimating: 69it [00:39,  1.75it/s]Extractor Estimating: 70it [00:39,  1.77it/s]Extractor Estimating: 71it [00:40,  1.75it/s]Extractor Estimating: 72it [00:41,  1.69it/s]Extractor Estimating: 73it [00:41,  1.70it/s]Extractor Estimating: 74it [00:42,  1.64it/s]Extractor Estimating: 75it [00:42,  1.62it/s]Extractor Estimating: 76it [00:43,  1.66it/s]Extractor Estimating: 77it [00:44,  1.65it/s]Extractor Estimating: 78it [00:44,  1.73it/s]Extractor Estimating: 79it [00:45,  1.71it/s]Extractor Estimating: 80it [00:45,  1.70it/s]Extractor Estimating: 81it [00:46,  1.69it/s]Extractor Estimating: 82it [00:47,  1.69it/s]Extractor Estimating: 83it [00:47,  1.67it/s]Extractor Estimating: 84it [00:48,  1.66it/s]Extractor Estimating: 85it [00:48,  1.65it/s]Extractor Estimating: 86it [00:49,  1.68it/s]Extractor Estimating: 87it [00:50,  1.63it/s]Extractor Estimating: 88it [00:50,  1.51it/s]Extractor Estimating: 89it [00:51,  1.56it/s]Extractor Estimating: 90it [00:52,  1.58it/s]Extractor Estimating: 91it [00:52,  1.58it/s]Extractor Estimating: 92it [00:53,  1.64it/s]Extractor Estimating: 93it [00:53,  1.63it/s]Extractor Estimating: 94it [00:54,  1.60it/s]Extractor Estimating: 95it [00:55,  1.63it/s]Extractor Estimating: 96it [00:55,  1.64it/s]Extractor Estimating: 97it [00:56,  1.65it/s]Extractor Estimating: 98it [00:56,  1.63it/s]Extractor Estimating: 99it [00:57,  1.66it/s]Extractor Estimating: 100it [00:58,  1.63it/s]Extractor Estimating: 101it [00:58,  1.59it/s]Extractor Estimating: 102it [00:59,  1.61it/s]Extractor Estimating: 103it [01:00,  1.65it/s]Extractor Estimating: 104it [01:00,  1.68it/s]Extractor Estimating: 105it [01:01,  1.67it/s]Extractor Estimating: 106it [01:01,  1.61it/s]Extractor Estimating: 107it [01:02,  1.62it/s]Extractor Estimating: 108it [01:03,  1.47it/s]Extractor Estimating: 109it [01:03,  1.57it/s]Extractor Estimating: 110it [01:04,  1.55it/s]Extractor Estimating: 111it [01:05,  1.59it/s]Extractor Estimating: 112it [01:05,  1.54it/s]Extractor Estimating: 113it [01:06,  1.55it/s]Extractor Estimating: 114it [01:06,  1.63it/s]Extractor Estimating: 115it [01:07,  1.63it/s]Extractor Estimating: 116it [01:08,  1.62it/s]Extractor Estimating: 117it [01:08,  1.70it/s]Extractor Estimating: 118it [01:09,  1.64it/s]Extractor Estimating: 119it [01:10,  1.61it/s]Extractor Estimating: 120it [01:10,  1.67it/s]Extractor Estimating: 121it [01:11,  1.61it/s]Extractor Estimating: 122it [01:11,  1.60it/s]Extractor Estimating: 123it [01:12,  1.64it/s]Extractor Estimating: 124it [01:13,  1.66it/s]Extractor Estimating: 125it [01:13,  1.57it/s]Extractor Estimating: 126it [01:14,  1.60it/s]Extractor Estimating: 127it [01:14,  1.62it/s]Extractor Estimating: 128it [01:15,  1.62it/s]Extractor Estimating: 129it [01:16,  1.69it/s]Extractor Estimating: 130it [01:16,  1.73it/s]Extractor Estimating: 131it [01:17,  1.75it/s]Extractor Estimating: 132it [01:17,  1.78it/s]Extractor Estimating: 133it [01:18,  1.72it/s]Extractor Estimating: 134it [01:18,  1.76it/s]Extractor Estimating: 135it [01:19,  1.78it/s]Extractor Estimating: 136it [01:20,  1.79it/s]Extractor Estimating: 137it [01:20,  1.78it/s]Extractor Estimating: 138it [01:21,  1.70it/s]Extractor Estimating: 139it [01:21,  1.69it/s]Extractor Estimating: 140it [01:22,  1.68it/s]Extractor Estimating: 141it [01:23,  1.68it/s]Extractor Estimating: 142it [01:23,  1.60it/s]Extractor Estimating: 143it [01:24,  1.61it/s]Extractor Estimating: 144it [01:24,  1.63it/s]Extractor Estimating: 145it [01:25,  1.64it/s]Extractor Estimating: 146it [01:26,  1.74it/s]Extractor Estimating: 147it [01:26,  1.73it/s]Extractor Estimating: 148it [01:27,  1.73it/s]Extractor Estimating: 149it [01:27,  1.73it/s]Extractor Estimating: 150it [01:28,  1.75it/s]Extractor Estimating: 151it [01:28,  1.78it/s]Extractor Estimating: 152it [01:29,  1.78it/s]Extractor Estimating: 153it [01:29,  1.78it/s]Extractor Estimating: 154it [01:30,  1.80it/s]Extractor Estimating: 155it [01:31,  1.83it/s]Extractor Estimating: 156it [01:31,  1.77it/s]Extractor Estimating: 157it [01:32,  1.80it/s]Extractor Estimating: 158it [01:32,  1.81it/s]Extractor Estimating: 159it [01:33,  1.83it/s]Extractor Estimating: 160it [01:33,  1.77it/s]Extractor Estimating: 161it [01:34,  1.80it/s]Extractor Estimating: 162it [01:34,  1.82it/s]Extractor Estimating: 163it [01:35,  1.83it/s]Extractor Estimating: 164it [01:36,  1.79it/s]Extractor Estimating: 165it [01:36,  1.80it/s]Extractor Estimating: 166it [01:37,  1.81it/s]Extractor Estimating: 167it [01:37,  1.82it/s]Extractor Estimating: 168it [01:38,  1.82it/s]Extractor Estimating: 169it [01:38,  1.73it/s]Extractor Estimating: 170it [01:39,  1.72it/s]Extractor Estimating: 171it [01:40,  1.71it/s]Extractor Estimating: 172it [01:40,  1.70it/s]Extractor Estimating: 173it [01:41,  1.73it/s]Extractor Estimating: 174it [01:41,  1.71it/s]Extractor Estimating: 175it [01:42,  1.73it/s]Extractor Estimating: 176it [01:42,  1.77it/s]Extractor Estimating: 177it [01:43,  1.75it/s]Extractor Estimating: 178it [01:44,  1.74it/s]Extractor Estimating: 179it [01:44,  1.70it/s]Extractor Estimating: 180it [01:45,  1.78it/s]Extractor Estimating: 181it [01:45,  1.83it/s]Extractor Estimating: 182it [01:46,  1.86it/s]Extractor Estimating: 183it [01:46,  1.85it/s]Extractor Estimating: 184it [01:47,  1.81it/s]Extractor Estimating: 185it [01:47,  1.87it/s]Extractor Estimating: 186it [01:48,  1.86it/s]Extractor Estimating: 187it [01:49,  1.79it/s]Extractor Estimating: 188it [01:49,  1.85it/s]Extractor Estimating: 189it [01:50,  1.88it/s]Extractor Estimating: 190it [01:50,  1.90it/s]Extractor Estimating: 191it [01:51,  1.89it/s]Extractor Estimating: 192it [01:51,  1.92it/s]Extractor Estimating: 193it [01:52,  1.94it/s]Extractor Estimating: 194it [01:52,  1.94it/s]Extractor Estimating: 195it [01:53,  1.90it/s]Extractor Estimating: 196it [01:53,  1.87it/s]Extractor Estimating: 197it [01:54,  1.68it/s]Extractor Estimating: 198it [01:55,  1.67it/s]Extractor Estimating: 199it [01:55,  1.79it/s]Extractor Estimating: 200it [01:56,  1.79it/s]Extractor Estimating: 201it [01:56,  1.73it/s]Extractor Estimating: 202it [01:57,  1.67it/s]Extractor Estimating: 203it [01:57,  1.69it/s]Extractor Estimating: 204it [01:58,  1.69it/s]Extractor Estimating: 205it [01:59,  1.61it/s]Extractor Estimating: 206it [01:59,  1.62it/s]Extractor Estimating: 207it [02:00,  1.53it/s]Extractor Estimating: 208it [02:01,  1.51it/s]Extractor Estimating: 209it [02:01,  1.54it/s]Extractor Estimating: 210it [02:02,  1.56it/s]Extractor Estimating: 211it [02:03,  1.62it/s]Extractor Estimating: 212it [02:03,  1.62it/s]Extractor Estimating: 213it [02:04,  1.60it/s]Extractor Estimating: 214it [02:04,  1.61it/s]Extractor Estimating: 215it [02:05,  1.59it/s]Extractor Estimating: 216it [02:06,  1.63it/s]Extractor Estimating: 217it [02:06,  1.60it/s]Extractor Estimating: 218it [02:07,  1.58it/s]Extractor Estimating: 219it [02:08,  1.58it/s]Extractor Estimating: 220it [02:08,  1.57it/s]Extractor Estimating: 221it [02:09,  1.52it/s]Extractor Estimating: 222it [02:10,  1.48it/s]Extractor Estimating: 223it [02:10,  1.55it/s]Extractor Estimating: 224it [02:11,  1.58it/s]Extractor Estimating: 225it [02:11,  1.56it/s]Extractor Estimating: 226it [02:12,  1.55it/s]Extractor Estimating: 227it [02:13,  1.57it/s]Extractor Estimating: 228it [02:13,  1.51it/s]Extractor Estimating: 229it [02:14,  1.50it/s]Extractor Estimating: 230it [02:15,  1.52it/s]Extractor Estimating: 231it [02:15,  1.56it/s]Extractor Estimating: 232it [02:16,  1.61it/s]Extractor Estimating: 233it [02:17,  1.65it/s]Extractor Estimating: 234it [02:17,  1.68it/s]Extractor Estimating: 235it [02:18,  1.65it/s]Extractor Estimating: 236it [02:18,  1.64it/s]Extractor Estimating: 237it [02:19,  1.61it/s]Extractor Estimating: 238it [02:20,  1.59it/s]Extractor Estimating: 239it [02:20,  1.62it/s]Extractor Estimating: 240it [02:21,  1.63it/s]Extractor Estimating: 241it [02:21,  1.64it/s]Extractor Estimating: 242it [02:22,  1.58it/s]Extractor Estimating: 243it [02:23,  1.60it/s]Extractor Estimating: 244it [02:23,  1.63it/s]Extractor Estimating: 245it [02:24,  1.57it/s]Extractor Estimating: 246it [02:25,  1.59it/s]Extractor Estimating: 247it [02:25,  1.62it/s]Extractor Estimating: 248it [02:26,  1.69it/s]Extractor Estimating: 249it [02:26,  1.65it/s]Extractor Estimating: 250it [02:27,  1.71it/s]Extractor Estimating: 251it [02:28,  1.71it/s]Extractor Estimating: 252it [02:28,  1.68it/s]Extractor Estimating: 253it [02:29,  1.69it/s]Extractor Estimating: 254it [02:29,  1.71it/s]Extractor Estimating: 255it [02:30,  1.71it/s]Extractor Estimating: 256it [02:31,  1.65it/s]Extractor Estimating: 257it [02:31,  1.66it/s]Extractor Estimating: 258it [02:32,  1.67it/s]Extractor Estimating: 259it [02:32,  1.66it/s]Extractor Estimating: 260it [02:33,  1.64it/s]Extractor Estimating: 261it [02:33,  1.70it/s]Extractor Estimating: 262it [02:34,  1.66it/s]Extractor Estimating: 263it [02:35,  1.62it/s]Extractor Estimating: 264it [02:35,  1.68it/s]Extractor Estimating: 265it [02:36,  1.73it/s]Extractor Estimating: 266it [02:36,  1.73it/s]Extractor Estimating: 267it [02:37,  1.73it/s]Extractor Estimating: 268it [02:38,  1.73it/s]Extractor Estimating: 269it [02:38,  1.77it/s]Extractor Estimating: 270it [02:39,  1.71it/s]Extractor Estimating: 271it [02:39,  1.73it/s]Extractor Estimating: 272it [02:40,  1.77it/s]Extractor Estimating: 273it [02:40,  1.78it/s]Extractor Estimating: 274it [02:41,  1.74it/s]Extractor Estimating: 275it [02:42,  1.76it/s]Extractor Estimating: 276it [02:42,  1.76it/s]Extractor Estimating: 277it [02:43,  1.74it/s]Extractor Estimating: 278it [02:43,  1.73it/s]Extractor Estimating: 279it [02:44,  1.73it/s]Extractor Estimating: 280it [02:44,  1.76it/s]Extractor Estimating: 281it [02:45,  1.53it/s]Extractor Estimating: 282it [02:46,  1.61it/s]Extractor Estimating: 283it [02:46,  1.65it/s]Extractor Estimating: 284it [02:47,  1.73it/s]Extractor Estimating: 285it [02:47,  1.75it/s]Extractor Estimating: 286it [02:48,  1.73it/s]Extractor Estimating: 287it [02:49,  1.73it/s]Extractor Estimating: 288it [02:49,  1.73it/s]Extractor Estimating: 289it [02:50,  1.68it/s]Extractor Estimating: 290it [02:50,  1.73it/s]Extractor Estimating: 291it [02:51,  1.70it/s]Extractor Estimating: 292it [02:52,  1.67it/s]Extractor Estimating: 293it [02:52,  1.76it/s]Extractor Estimating: 294it [02:53,  1.76it/s]Extractor Estimating: 295it [02:53,  1.77it/s]Extractor Estimating: 296it [02:54,  1.74it/s]Extractor Estimating: 297it [02:54,  1.73it/s]Extractor Estimating: 298it [02:55,  1.70it/s]Extractor Estimating: 299it [02:56,  1.74it/s]Extractor Estimating: 300it [02:56,  1.76it/s]Extractor Estimating: 301it [02:57,  1.79it/s]Extractor Estimating: 302it [02:57,  1.81it/s]Extractor Estimating: 303it [02:58,  1.81it/s]Extractor Estimating: 304it [02:58,  1.80it/s]Extractor Estimating: 305it [02:59,  1.79it/s]Extractor Estimating: 306it [02:59,  1.83it/s]Extractor Estimating: 307it [03:00,  1.83it/s]Extractor Estimating: 308it [03:01,  1.81it/s]Extractor Estimating: 309it [03:01,  1.82it/s]Extractor Estimating: 310it [03:02,  1.82it/s]Extractor Estimating: 311it [03:02,  1.78it/s]Extractor Estimating: 312it [03:03,  1.79it/s]Extractor Estimating: 313it [03:03,  1.85it/s]Extractor Estimating: 314it [03:04,  1.82it/s]Extractor Estimating: 315it [03:04,  1.82it/s]Extractor Estimating: 316it [03:05,  1.85it/s]Extractor Estimating: 317it [03:05,  1.84it/s]Extractor Estimating: 318it [03:06,  1.82it/s]Extractor Estimating: 319it [03:07,  1.77it/s]Extractor Estimating: 320it [03:07,  1.86it/s]Extractor Estimating: 321it [03:08,  1.82it/s]Extractor Estimating: 322it [03:08,  1.84it/s]Extractor Estimating: 323it [03:09,  1.85it/s]Extractor Estimating: 324it [03:09,  1.90it/s]Extractor Estimating: 325it [03:10,  1.86it/s]Extractor Estimating: 326it [03:10,  1.80it/s]Extractor Estimating: 327it [03:11,  1.83it/s]Extractor Estimating: 328it [03:11,  1.85it/s]Extractor Estimating: 329it [03:12,  1.87it/s]Extractor Estimating: 330it [03:12,  1.88it/s]Extractor Estimating: 331it [03:13,  1.85it/s]Extractor Estimating: 332it [03:14,  1.90it/s]Extractor Estimating: 333it [03:14,  1.89it/s]Extractor Estimating: 334it [03:15,  1.90it/s]Extractor Estimating: 335it [03:15,  1.84it/s]Extractor Estimating: 336it [03:16,  1.80it/s]Extractor Estimating: 337it [03:16,  1.84it/s]Extractor Estimating: 338it [03:17,  1.81it/s]Extractor Estimating: 339it [03:17,  1.85it/s]Extractor Estimating: 340it [03:18,  1.83it/s]Extractor Estimating: 341it [03:18,  1.85it/s]Extractor Estimating: 342it [03:19,  1.86it/s]Extractor Estimating: 343it [03:20,  1.86it/s]Extractor Estimating: 344it [03:20,  1.85it/s]Extractor Estimating: 345it [03:21,  1.84it/s]Extractor Estimating: 346it [03:21,  1.89it/s]Extractor Estimating: 347it [03:22,  1.90it/s]Extractor Estimating: 348it [03:22,  1.88it/s]Extractor Estimating: 349it [03:23,  1.89it/s]Extractor Estimating: 350it [03:23,  1.87it/s]Extractor Estimating: 351it [03:24,  1.81it/s]Extractor Estimating: 352it [03:24,  1.80it/s]Extractor Estimating: 353it [03:25,  1.80it/s]Extractor Estimating: 354it [03:26,  1.64it/s]Extractor Estimating: 355it [03:26,  1.69it/s]Extractor Estimating: 356it [03:27,  1.73it/s]Extractor Estimating: 357it [03:27,  1.72it/s]Extractor Estimating: 358it [03:28,  1.75it/s]Extractor Estimating: 359it [03:29,  1.73it/s]Extractor Estimating: 360it [03:29,  1.76it/s]Extractor Estimating: 361it [03:30,  1.72it/s]Extractor Estimating: 362it [03:30,  1.71it/s]Extractor Estimating: 363it [03:31,  1.71it/s]Extractor Estimating: 364it [03:31,  1.76it/s]Extractor Estimating: 365it [03:32,  1.76it/s]Extractor Estimating: 366it [03:33,  1.75it/s]Extractor Estimating: 367it [03:33,  1.71it/s]Extractor Estimating: 368it [03:34,  1.74it/s]Extractor Estimating: 369it [03:34,  1.78it/s]Extractor Estimating: 370it [03:35,  1.71it/s]Extractor Estimating: 371it [03:35,  1.73it/s]Extractor Estimating: 372it [03:36,  1.72it/s]Extractor Estimating: 373it [03:37,  1.69it/s]Extractor Estimating: 374it [03:37,  1.71it/s]Extractor Estimating: 375it [03:38,  1.73it/s]Extractor Estimating: 376it [03:38,  1.68it/s]Extractor Estimating: 377it [03:39,  1.66it/s]Extractor Estimating: 378it [03:40,  1.71it/s]Extractor Estimating: 379it [03:40,  1.75it/s]Extractor Estimating: 380it [03:41,  1.71it/s]Extractor Estimating: 381it [03:41,  1.66it/s]Extractor Estimating: 382it [03:42,  1.65it/s]Extractor Estimating: 383it [03:43,  1.68it/s]Extractor Estimating: 384it [03:43,  1.72it/s]Extractor Estimating: 385it [03:44,  1.74it/s]Extractor Estimating: 386it [03:44,  1.71it/s]Extractor Estimating: 387it [03:45,  1.75it/s]Extractor Estimating: 388it [03:45,  1.77it/s]Extractor Estimating: 389it [03:46,  1.75it/s]Extractor Estimating: 390it [03:46,  1.78it/s]Extractor Estimating: 391it [03:47,  1.56it/s]Extractor Estimating: 392it [03:48,  1.65it/s]Extractor Estimating: 393it [03:48,  1.71it/s]Extractor Estimating: 394it [03:49,  1.69it/s]Extractor Estimating: 395it [03:50,  1.69it/s]Extractor Estimating: 396it [03:50,  1.71it/s]Extractor Estimating: 397it [03:51,  1.71it/s]Extractor Estimating: 398it [03:51,  1.68it/s]Extractor Estimating: 399it [03:52,  1.66it/s]Extractor Estimating: 400it [03:53,  1.69it/s]Extractor Estimating: 401it [03:53,  1.68it/s]Extractor Estimating: 402it [03:54,  1.72it/s]Extractor Estimating: 403it [03:54,  1.74it/s]Extractor Estimating: 404it [03:55,  1.75it/s]Extractor Estimating: 405it [03:55,  1.71it/s]Extractor Estimating: 406it [03:56,  1.67it/s]Extractor Estimating: 407it [03:57,  1.68it/s]Extractor Estimating: 408it [03:57,  1.73it/s]Extractor Estimating: 409it [03:58,  1.72it/s]Extractor Estimating: 410it [03:58,  1.69it/s]Extractor Estimating: 411it [03:59,  1.67it/s]Extractor Estimating: 412it [04:00,  1.69it/s]Extractor Estimating: 413it [04:00,  1.67it/s]Extractor Estimating: 414it [04:01,  1.72it/s]Extractor Estimating: 415it [04:01,  1.70it/s]Extractor Estimating: 416it [04:02,  1.73it/s]Extractor Estimating: 417it [04:02,  1.74it/s]Extractor Estimating: 418it [04:03,  1.73it/s]Extractor Estimating: 419it [04:04,  1.72it/s]Extractor Estimating: 420it [04:04,  1.75it/s]Extractor Estimating: 421it [04:05,  1.77it/s]Extractor Estimating: 422it [04:05,  1.84it/s]Extractor Estimating: 423it [04:06,  1.87it/s]Extractor Estimating: 424it [04:06,  1.87it/s]Extractor Estimating: 425it [04:07,  1.73it/s]Extractor Estimating: 426it [04:08,  1.74it/s]Extractor Estimating: 427it [04:08,  1.76it/s]Extractor Estimating: 428it [04:09,  1.78it/s]Extractor Estimating: 429it [04:09,  1.68it/s]Extractor Estimating: 430it [04:10,  1.70it/s]Extractor Estimating: 431it [04:10,  1.69it/s]Extractor Estimating: 432it [04:11,  1.71it/s]Extractor Estimating: 433it [04:12,  1.76it/s]Extractor Estimating: 434it [04:12,  1.69it/s]Extractor Estimating: 435it [04:13,  1.72it/s]Extractor Estimating: 436it [04:13,  1.74it/s]Extractor Estimating: 437it [04:14,  1.72it/s]Extractor Estimating: 438it [04:14,  1.72it/s]Extractor Estimating: 439it [04:15,  1.64it/s]Extractor Estimating: 440it [04:16,  1.60it/s]Extractor Estimating: 441it [04:16,  1.61it/s]Extractor Estimating: 442it [04:17,  1.68it/s]Extractor Estimating: 443it [04:18,  1.66it/s]Extractor Estimating: 444it [04:18,  1.70it/s]Extractor Estimating: 445it [04:19,  1.71it/s]Extractor Estimating: 446it [04:19,  1.76it/s]Extractor Estimating: 447it [04:20,  1.68it/s]Extractor Estimating: 448it [04:21,  1.68it/s]Extractor Estimating: 449it [04:21,  1.60it/s]Extractor Estimating: 450it [04:22,  1.65it/s]Extractor Estimating: 451it [04:22,  1.64it/s]Extractor Estimating: 452it [04:23,  1.66it/s]Extractor Estimating: 453it [04:24,  1.69it/s]Extractor Estimating: 454it [04:24,  1.77it/s]Extractor Estimating: 455it [04:25,  1.80it/s]Extractor Estimating: 456it [04:25,  1.83it/s]Extractor Estimating: 457it [04:26,  1.84it/s]Extractor Estimating: 458it [04:26,  1.83it/s]Extractor Estimating: 459it [04:27,  1.80it/s]Extractor Estimating: 460it [04:27,  1.81it/s]Extractor Estimating: 461it [04:28,  1.83it/s]Extractor Estimating: 462it [04:28,  1.86it/s]Extractor Estimating: 463it [04:29,  1.83it/s]Extractor Estimating: 464it [04:30,  1.77it/s]Extractor Estimating: 465it [04:30,  1.77it/s]Extractor Estimating: 466it [04:31,  1.78it/s]Extractor Estimating: 467it [04:31,  1.77it/s]Extractor Estimating: 468it [04:32,  1.79it/s]Extractor Estimating: 469it [04:32,  1.76it/s]Extractor Estimating: 470it [04:33,  1.70it/s]Extractor Estimating: 471it [04:34,  1.55it/s]Extractor Estimating: 472it [04:34,  1.61it/s]Extractor Estimating: 473it [04:35,  1.64it/s]Extractor Estimating: 474it [04:35,  1.68it/s]Extractor Estimating: 475it [04:36,  1.65it/s]Extractor Estimating: 476it [04:37,  1.61it/s]Extractor Estimating: 477it [04:37,  1.62it/s]Extractor Estimating: 478it [04:38,  1.62it/s]Extractor Estimating: 479it [04:39,  1.66it/s]Extractor Estimating: 480it [04:39,  1.63it/s]Extractor Estimating: 481it [04:40,  1.69it/s]Extractor Estimating: 482it [04:40,  1.68it/s]Extractor Estimating: 483it [04:41,  1.69it/s]Extractor Estimating: 484it [04:41,  1.74it/s]Extractor Estimating: 485it [04:42,  1.70it/s]Extractor Estimating: 486it [04:43,  1.62it/s]Extractor Estimating: 487it [04:43,  1.66it/s]Extractor Estimating: 488it [04:44,  1.73it/s]Extractor Estimating: 489it [04:44,  1.73it/s]Extractor Estimating: 490it [04:45,  1.67it/s]Extractor Estimating: 491it [04:46,  1.67it/s]Extractor Estimating: 492it [04:46,  1.69it/s]Extractor Estimating: 493it [04:47,  1.76it/s]Extractor Estimating: 494it [04:47,  1.74it/s]Extractor Estimating: 495it [04:48,  1.74it/s]Extractor Estimating: 496it [04:48,  1.77it/s]Extractor Estimating: 497it [04:49,  1.77it/s]Extractor Estimating: 498it [04:50,  1.80it/s]Extractor Estimating: 499it [04:50,  1.81it/s]Extractor Estimating: 499it [04:50,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:18,506 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:18,542 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:18,542 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:18,542 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:18,542 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:42:19,279 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:42:19,280 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:42:19,894 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:42:20,950 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:42:20,951 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:23,967 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:24,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:24,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:24,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:42:24,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:42:24,721 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:42:24,723 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:42:25,319 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:42:25,486 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:42:25,486 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:35:22,757 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:35:22,883 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9976 mean pseudo reward: 0.9421151166048818
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 17237
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17337, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17337, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.953, loss:744.2311
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.996, loss:740.7511
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.955, loss:715.3435
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.959, loss:702.5400
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.965, loss:664.3646
>> valid entity prec:0.5678, rec:0.6246, f1:0.5948
>> valid relation prec:0.0478, rec:0.0109, f1:0.0178
>> valid relation with NER prec:0.0478, rec:0.0109, f1:0.0178
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.254, loss:661.9941
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.001, loss:675.8344
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.027, loss:664.1161
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.988, loss:655.3480
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.008, loss:669.4619
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5508, rec:0.6000, f1:0.5744
>> valid relation prec:0.0202, rec:0.0057, f1:0.0089
>> valid relation with NER prec:0.0202, rec:0.0057, f1:0.0089
g_step 1100, step 268, avg_time 2.289, loss:672.6042
g_step 1200, step 368, avg_time 0.990, loss:662.3182
g_step 1300, step 52, avg_time 0.995, loss:630.3955
g_step 1400, step 152, avg_time 0.994, loss:631.9833
g_step 1500, step 252, avg_time 0.988, loss:650.1850
>> valid entity prec:0.5485, rec:0.5489, f1:0.5487
>> valid relation prec:0.0988, rec:0.0261, f1:0.0413
>> valid relation with NER prec:0.0988, rec:0.0261, f1:0.0413
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.290, loss:628.6831
g_step 1700, step 36, avg_time 1.003, loss:622.2549
g_step 1800, step 136, avg_time 1.024, loss:582.8911
g_step 1900, step 236, avg_time 0.999, loss:612.4547
g_step 2000, step 336, avg_time 1.000, loss:639.5411
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5641, rec:0.6025, f1:0.5827
>> valid relation prec:0.0652, rec:0.0184, f1:0.0287
>> valid relation with NER prec:0.0652, rec:0.0184, f1:0.0287
g_step 2100, step 20, avg_time 2.274, loss:608.6795
g_step 2200, step 120, avg_time 1.003, loss:584.6212
g_step 2300, step 220, avg_time 0.986, loss:594.1133
g_step 2400, step 320, avg_time 1.025, loss:597.4243
g_step 2500, step 4, avg_time 1.004, loss:570.1692
>> valid entity prec:0.5710, rec:0.5465, f1:0.5585
>> valid relation prec:0.0487, rec:0.0138, f1:0.0215
>> valid relation with NER prec:0.0487, rec:0.0138, f1:0.0215
g_step 2600, step 104, avg_time 2.274, loss:570.4250
g_step 2700, step 204, avg_time 1.004, loss:574.7899
g_step 2800, step 304, avg_time 1.003, loss:575.7946
g_step 2900, step 404, avg_time 1.011, loss:577.0956
g_step 3000, step 88, avg_time 1.004, loss:535.3109
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5539, rec:0.5963, f1:0.5743
>> valid relation prec:0.1415, rec:0.0482, f1:0.0720
>> valid relation with NER prec:0.1415, rec:0.0482, f1:0.0720
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.279, loss:561.9427
g_step 3200, step 288, avg_time 1.001, loss:548.3355
g_step 3300, step 388, avg_time 1.004, loss:540.2838
g_step 3400, step 72, avg_time 0.994, loss:531.9481
g_step 3500, step 172, avg_time 1.005, loss:539.9043
>> valid entity prec:0.5409, rec:0.5744, f1:0.5571
>> valid relation prec:0.0678, rec:0.0238, f1:0.0353
>> valid relation with NER prec:0.0678, rec:0.0238, f1:0.0353
g_step 3600, step 272, avg_time 2.287, loss:541.7631
g_step 3700, step 372, avg_time 1.010, loss:527.6614
g_step 3800, step 56, avg_time 0.996, loss:542.2418
g_step 3900, step 156, avg_time 1.012, loss:501.0984
g_step 4000, step 256, avg_time 0.996, loss:521.4805
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5480, rec:0.5696, f1:0.5586
>> valid relation prec:0.0413, rec:0.0169, f1:0.0240
>> valid relation with NER prec:0.0413, rec:0.0169, f1:0.0240
g_step 4100, step 356, avg_time 2.289, loss:528.0345
g_step 4200, step 40, avg_time 1.005, loss:529.7765
g_step 4300, step 140, avg_time 0.988, loss:499.0010
g_step 4400, step 240, avg_time 1.005, loss:493.6453
g_step 4500, step 340, avg_time 0.994, loss:522.2478
>> valid entity prec:0.5885, rec:0.5337, f1:0.5598
>> valid relation prec:0.0282, rec:0.0092, f1:0.0139
>> valid relation with NER prec:0.0282, rec:0.0092, f1:0.0139
g_step 4600, step 24, avg_time 2.289, loss:486.4815
g_step 4700, step 124, avg_time 0.991, loss:477.6137
g_step 4800, step 224, avg_time 1.016, loss:506.8997
g_step 4900, step 324, avg_time 1.014, loss:502.9558
g_step 5000, step 8, avg_time 1.001, loss:513.2087
learning rate was adjusted to 0.0008
>> valid entity prec:0.5684, rec:0.5657, f1:0.5670
>> valid relation prec:0.0813, rec:0.0281, f1:0.0418
>> valid relation with NER prec:0.0813, rec:0.0281, f1:0.0418
g_step 5100, step 108, avg_time 2.303, loss:455.3562
g_step 5200, step 208, avg_time 0.982, loss:473.8327
g_step 5300, step 308, avg_time 1.012, loss:496.6766
g_step 5400, step 408, avg_time 1.004, loss:483.1993
g_step 5500, step 92, avg_time 1.006, loss:461.4483
>> valid entity prec:0.5491, rec:0.5554, f1:0.5522
>> valid relation prec:0.0607, rec:0.0224, f1:0.0327
>> valid relation with NER prec:0.0607, rec:0.0224, f1:0.0327
g_step 5600, step 192, avg_time 2.294, loss:452.0641
g_step 5700, step 292, avg_time 0.991, loss:487.6471
g_step 5800, step 392, avg_time 1.005, loss:478.2297
g_step 5900, step 76, avg_time 0.987, loss:452.8133
g_step 6000, step 176, avg_time 1.009, loss:457.5598
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5549, rec:0.5100, f1:0.5315
>> valid relation prec:0.0816, rec:0.0296, f1:0.0434
>> valid relation with NER prec:0.0816, rec:0.0296, f1:0.0434
g_step 6100, step 276, avg_time 2.283, loss:477.8877
g_step 6200, step 376, avg_time 1.002, loss:458.5723
g_step 6300, step 60, avg_time 1.016, loss:454.2114
g_step 6400, step 160, avg_time 0.990, loss:438.9032
g_step 6500, step 260, avg_time 1.018, loss:441.0903
>> valid entity prec:0.5526, rec:0.5596, f1:0.5561
>> valid relation prec:0.0774, rec:0.0304, f1:0.0437
>> valid relation with NER prec:0.0774, rec:0.0304, f1:0.0437
g_step 6600, step 360, avg_time 2.293, loss:463.2194
g_step 6700, step 44, avg_time 0.980, loss:434.0679
g_step 6800, step 144, avg_time 0.994, loss:444.2619
g_step 6900, step 244, avg_time 1.013, loss:406.6785
g_step 7000, step 344, avg_time 1.004, loss:447.4519
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5857, rec:0.4948, f1:0.5364
>> valid relation prec:0.0692, rec:0.0244, f1:0.0361
>> valid relation with NER prec:0.0692, rec:0.0244, f1:0.0361
g_step 7100, step 28, avg_time 2.277, loss:449.9630
g_step 7200, step 128, avg_time 0.987, loss:403.0346
g_step 7300, step 228, avg_time 1.008, loss:434.0855
g_step 7400, step 328, avg_time 1.024, loss:433.8349
g_step 7500, step 12, avg_time 1.000, loss:429.1267
>> valid entity prec:0.6007, rec:0.5223, f1:0.5588
>> valid relation prec:0.0622, rec:0.0238, f1:0.0345
>> valid relation with NER prec:0.0622, rec:0.0238, f1:0.0345
g_step 7600, step 112, avg_time 2.271, loss:400.8293
g_step 7700, step 212, avg_time 0.993, loss:403.3714
g_step 7800, step 312, avg_time 0.986, loss:413.0330
g_step 7900, step 412, avg_time 1.017, loss:434.1805
g_step 8000, step 96, avg_time 1.001, loss:393.5273
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5559, rec:0.5667, f1:0.5613
>> valid relation prec:0.0716, rec:0.0296, f1:0.0419
>> valid relation with NER prec:0.0716, rec:0.0296, f1:0.0419
g_step 8100, step 196, avg_time 2.277, loss:410.1411
g_step 8200, step 296, avg_time 0.994, loss:406.7865
g_step 8300, step 396, avg_time 1.033, loss:414.2376
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:35:22 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:35:22 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-35-22_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:35:25 - WARNING - datasets.builder -   Using custom data configuration default-a9930be8d4d2dae5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a9930be8d4d2dae5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:02,  2.27s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:35:34,121 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:35:34,122 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:35:34,122 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:35:34,123 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:35:34,231 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:35:34,278 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:35:34,278 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:35:34,279 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:35:34,279 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:35:34,279 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:35:34,279 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:35:34,922 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:35:38,223 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:35:38,224 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a9930be8d4d2dae5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:04,  2.03ba/s] 20%|        | 2/10 [00:00<00:02,  3.12ba/s] 30%|       | 3/10 [00:00<00:01,  3.78ba/s] 40%|      | 4/10 [00:01<00:01,  4.21ba/s] 50%|     | 5/10 [00:01<00:01,  4.46ba/s] 60%|    | 6/10 [00:01<00:00,  4.64ba/s] 70%|   | 7/10 [00:01<00:00,  4.77ba/s] 80%|  | 8/10 [00:02<00:00,  4.00ba/s] 90%| | 9/10 [00:02<00:00,  4.28ba/s]100%|| 10/10 [00:02<00:00,  4.33ba/s]100%|| 10/10 [00:02<00:00,  4.09ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.85ba/s] 50%|     | 2/4 [00:00<00:00,  3.58ba/s] 75%|  | 3/4 [00:00<00:00,  3.94ba/s]100%|| 4/4 [00:00<00:00,  5.04ba/s]100%|| 4/4 [00:00<00:00,  4.36ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.15ba/s] 30%|       | 3/10 [00:00<00:00,  8.44ba/s] 40%|      | 4/10 [00:00<00:00,  8.87ba/s] 60%|    | 6/10 [00:00<00:00,  9.91ba/s] 80%|  | 8/10 [00:00<00:00, 10.37ba/s]100%|| 10/10 [00:01<00:00, 10.50ba/s]100%|| 10/10 [00:01<00:00,  9.76ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.47ba/s] 75%|  | 3/4 [00:00<00:00,  9.42ba/s]100%|| 4/4 [00:00<00:00, 10.59ba/s]
[INFO|trainer.py:414] 2023-08-29 10:35:44,457 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:35:44,542 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:35:44,542 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 10:35:44,542 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:35:44,542 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:35:44,542 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:35:44,542 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:35:44,542 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<10:00,  1.30it/s]  0%|          | 2/780 [00:01<06:22,  2.03it/s]  0%|          | 3/780 [00:01<05:27,  2.38it/s]  1%|          | 4/780 [00:01<04:47,  2.69it/s]  1%|          | 5/780 [00:01<04:26,  2.91it/s]  1%|          | 6/780 [00:02<04:13,  3.06it/s]  1%|          | 7/780 [00:02<04:04,  3.16it/s]  1%|          | 8/780 [00:02<03:58,  3.24it/s]  1%|          | 9/780 [00:03<03:54,  3.28it/s]  1%|         | 10/780 [00:03<03:58,  3.23it/s]  1%|         | 11/780 [00:03<03:54,  3.28it/s]  2%|         | 12/780 [00:04<03:51,  3.31it/s]  2%|         | 13/780 [00:04<03:49,  3.34it/s]  2%|         | 14/780 [00:04<03:48,  3.35it/s]  2%|         | 15/780 [00:04<03:47,  3.37it/s]  2%|         | 16/780 [00:05<03:46,  3.37it/s]  2%|         | 17/780 [00:05<03:45,  3.38it/s]  2%|         | 18/780 [00:05<03:44,  3.39it/s]  2%|         | 19/780 [00:06<03:44,  3.39it/s]  3%|         | 20/780 [00:06<03:43,  3.40it/s]  3%|         | 21/780 [00:06<03:48,  3.33it/s]  3%|         | 22/780 [00:07<03:46,  3.35it/s]  3%|         | 23/780 [00:07<03:44,  3.37it/s]  3%|         | 24/780 [00:07<03:43,  3.38it/s]  3%|         | 25/780 [00:07<03:42,  3.39it/s]  3%|         | 26/780 [00:08<03:41,  3.40it/s]  3%|         | 27/780 [00:08<03:42,  3.39it/s]  4%|         | 28/780 [00:08<03:41,  3.39it/s]  4%|         | 29/780 [00:09<03:40,  3.40it/s]  4%|         | 30/780 [00:09<03:40,  3.40it/s]  4%|         | 31/780 [00:09<03:40,  3.40it/s]  4%|         | 32/780 [00:09<03:39,  3.41it/s]  4%|         | 33/780 [00:10<03:39,  3.41it/s]  4%|         | 34/780 [00:10<03:39,  3.40it/s]  4%|         | 35/780 [00:10<03:38,  3.40it/s]  5%|         | 36/780 [00:11<03:38,  3.41it/s]  5%|         | 37/780 [00:11<03:38,  3.41it/s]  5%|         | 38/780 [00:11<03:45,  3.28it/s]  5%|         | 39/780 [00:12<03:43,  3.31it/s]  5%|         | 40/780 [00:12<03:41,  3.34it/s]  5%|         | 41/780 [00:12<03:40,  3.35it/s]  5%|         | 42/780 [00:12<03:39,  3.36it/s]  6%|         | 43/780 [00:13<03:38,  3.37it/s]  6%|         | 44/780 [00:13<03:37,  3.38it/s]  6%|         | 45/780 [00:13<03:37,  3.38it/s]  6%|         | 46/780 [00:14<03:36,  3.39it/s]  6%|         | 47/780 [00:14<03:36,  3.39it/s]  6%|         | 48/780 [00:14<03:35,  3.40it/s]  6%|         | 49/780 [00:15<03:35,  3.40it/s]  6%|         | 50/780 [00:15<03:34,  3.40it/s]  7%|         | 51/780 [00:15<03:34,  3.40it/s]  7%|         | 52/780 [00:15<03:34,  3.40it/s]  7%|         | 53/780 [00:16<03:33,  3.40it/s]  7%|         | 54/780 [00:16<03:33,  3.40it/s]  7%|         | 55/780 [00:16<03:39,  3.30it/s]  7%|         | 56/780 [00:17<03:37,  3.33it/s]  7%|         | 57/780 [00:17<03:36,  3.34it/s]  7%|         | 58/780 [00:17<03:35,  3.36it/s]  8%|         | 59/780 [00:17<03:34,  3.37it/s]  8%|         | 60/780 [00:18<03:33,  3.38it/s]  8%|         | 61/780 [00:18<03:32,  3.38it/s]  8%|         | 62/780 [00:18<03:41,  3.24it/s]  8%|         | 63/780 [00:19<03:38,  3.28it/s]  8%|         | 64/780 [00:19<03:36,  3.31it/s]  8%|         | 65/780 [00:19<03:34,  3.34it/s]  8%|         | 66/780 [00:20<03:33,  3.35it/s]  9%|         | 67/780 [00:20<03:32,  3.36it/s]  9%|         | 68/780 [00:20<03:31,  3.37it/s]  9%|         | 69/780 [00:20<03:30,  3.38it/s]  9%|         | 70/780 [00:21<03:30,  3.38it/s]  9%|         | 71/780 [00:21<03:29,  3.38it/s]  9%|         | 72/780 [00:21<03:36,  3.28it/s]  9%|         | 73/780 [00:22<03:33,  3.31it/s]  9%|         | 74/780 [00:22<03:31,  3.33it/s] 10%|         | 75/780 [00:22<03:30,  3.35it/s] 10%|         | 76/780 [00:23<03:29,  3.36it/s] 10%|         | 77/780 [00:23<03:28,  3.37it/s] 10%|         | 78/780 [00:23<03:28,  3.37it/s] 10%|         | 79/780 [00:24<03:43,  3.14it/s] 10%|         | 80/780 [00:24<03:38,  3.21it/s] 10%|         | 81/780 [00:24<03:34,  3.26it/s] 11%|         | 82/780 [00:24<03:31,  3.30it/s] 11%|         | 83/780 [00:25<03:29,  3.33it/s] 11%|         | 84/780 [00:25<03:28,  3.34it/s] 11%|         | 85/780 [00:25<03:27,  3.36it/s] 11%|         | 86/780 [00:26<03:26,  3.36it/s] 11%|         | 87/780 [00:26<03:25,  3.37it/s] 11%|        | 88/780 [00:26<03:24,  3.38it/s] 11%|        | 89/780 [00:27<03:34,  3.23it/s] 12%|        | 90/780 [00:27<03:30,  3.27it/s] 12%|        | 91/780 [00:27<03:28,  3.30it/s] 12%|        | 92/780 [00:27<03:26,  3.33it/s] 12%|        | 93/780 [00:28<03:25,  3.35it/s] 12%|        | 94/780 [00:28<03:24,  3.36it/s] 12%|        | 95/780 [00:28<03:23,  3.37it/s] 12%|        | 96/780 [00:29<03:34,  3.18it/s] 12%|        | 97/780 [00:29<03:30,  3.24it/s] 13%|        | 98/780 [00:29<03:27,  3.29it/s] 13%|        | 99/780 [00:30<03:25,  3.31it/s] 13%|        | 100/780 [00:30<03:23,  3.34it/s] 13%|        | 101/780 [00:30<03:22,  3.35it/s] 13%|        | 102/780 [00:30<03:21,  3.36it/s] 13%|        | 103/780 [00:31<03:21,  3.37it/s] 13%|        | 104/780 [00:31<03:20,  3.38it/s] 13%|        | 105/780 [00:31<03:19,  3.38it/s] 14%|        | 106/780 [00:32<03:31,  3.19it/s] 14%|        | 107/780 [00:32<03:27,  3.24it/s] 14%|        | 108/780 [00:32<03:24,  3.29it/s] 14%|        | 109/780 [00:33<03:22,  3.32it/s] 14%|        | 110/780 [00:33<03:20,  3.34it/s] 14%|        | 111/780 [00:33<03:19,  3.35it/s] 14%|        | 112/780 [00:33<03:18,  3.36it/s] 14%|        | 113/780 [00:34<03:18,  3.37it/s] 15%|        | 114/780 [00:34<03:17,  3.37it/s] 15%|        | 115/780 [00:34<03:16,  3.38it/s] 15%|        | 116/780 [00:35<03:16,  3.38it/s] 15%|        | 117/780 [00:35<03:15,  3.40it/s] 15%|        | 118/780 [00:35<03:14,  3.41it/s] 15%|        | 119/780 [00:36<03:13,  3.42it/s] 15%|        | 120/780 [00:36<03:12,  3.42it/s] 16%|        | 121/780 [00:36<03:12,  3.43it/s] 16%|        | 122/780 [00:36<03:11,  3.43it/s] 16%|        | 123/780 [00:37<03:11,  3.43it/s] 16%|        | 124/780 [00:37<03:15,  3.35it/s] 16%|        | 125/780 [00:37<03:13,  3.38it/s] 16%|        | 126/780 [00:38<03:12,  3.39it/s] 16%|        | 127/780 [00:38<03:21,  3.24it/s] 16%|        | 128/780 [00:38<03:17,  3.30it/s] 17%|        | 129/780 [00:38<03:15,  3.33it/s] 17%|        | 130/780 [00:39<03:13,  3.36it/s] 17%|        | 131/780 [00:39<03:11,  3.38it/s] 17%|        | 132/780 [00:39<03:10,  3.40it/s] 17%|        | 133/780 [00:40<03:09,  3.41it/s] 17%|        | 134/780 [00:40<03:09,  3.42it/s] 17%|        | 135/780 [00:40<03:08,  3.42it/s] 17%|        | 136/780 [00:41<04:38,  2.32it/s] 18%|        | 137/780 [00:41<04:10,  2.57it/s] 18%|        | 138/780 [00:42<03:51,  2.78it/s] 18%|        | 139/780 [00:42<03:54,  2.74it/s] 18%|        | 140/780 [00:42<03:39,  2.92it/s] 18%|        | 141/780 [00:43<03:29,  3.06it/s] 18%|        | 142/780 [00:43<03:21,  3.16it/s] 18%|        | 143/780 [00:43<03:16,  3.24it/s] 18%|        | 144/780 [00:43<03:12,  3.30it/s] 19%|        | 145/780 [00:44<03:10,  3.34it/s] 19%|        | 146/780 [00:44<03:15,  3.24it/s] 19%|        | 147/780 [00:44<03:11,  3.30it/s] 19%|        | 148/780 [00:45<03:09,  3.34it/s] 19%|        | 149/780 [00:45<03:07,  3.37it/s] 19%|        | 150/780 [00:45<03:05,  3.39it/s] 19%|        | 151/780 [00:45<03:04,  3.41it/s] 19%|        | 152/780 [00:46<03:03,  3.42it/s] 20%|        | 153/780 [00:46<03:03,  3.42it/s] 20%|        | 154/780 [00:46<03:02,  3.43it/s] 20%|        | 155/780 [00:47<03:02,  3.43it/s] 20%|        | 156/780 [00:47<03:18,  3.14it/s][INFO|trainer.py:2140] 2023-08-29 10:36:32,308 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:36:32,308 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 10:36:32,308 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.98it/s][A
  3%|         | 12/436 [00:00<00:08, 49.04it/s][A
  4%|         | 17/436 [00:00<00:08, 47.22it/s][A
  5%|         | 22/436 [00:00<00:08, 46.41it/s][A
  6%|         | 27/436 [00:00<00:08, 45.93it/s][A
  7%|         | 32/436 [00:00<00:08, 45.72it/s][A
  8%|         | 37/436 [00:00<00:08, 45.47it/s][A
 10%|         | 42/436 [00:00<00:08, 44.81it/s][A
 11%|         | 47/436 [00:01<00:08, 44.49it/s][A
 12%|        | 52/436 [00:01<00:10, 37.87it/s][A
 13%|        | 57/436 [00:01<00:09, 39.95it/s][A
 14%|        | 62/436 [00:01<00:09, 41.31it/s][A
 15%|        | 67/436 [00:01<00:08, 42.18it/s][A
 17%|        | 72/436 [00:01<00:09, 37.47it/s][A
 18%|        | 77/436 [00:01<00:09, 39.42it/s][A
 19%|        | 82/436 [00:01<00:08, 41.01it/s][A
 20%|        | 87/436 [00:02<00:08, 42.06it/s][A
 21%|        | 92/436 [00:02<00:08, 42.76it/s][A
 22%|       | 97/436 [00:02<00:07, 43.45it/s][A
 23%|       | 102/436 [00:02<00:07, 43.99it/s][A
 25%|       | 107/436 [00:02<00:07, 44.14it/s][A
 26%|       | 112/436 [00:02<00:07, 43.94it/s][A
 27%|       | 117/436 [00:02<00:07, 44.07it/s][A
 28%|       | 122/436 [00:02<00:07, 44.17it/s][A
 29%|       | 127/436 [00:02<00:06, 44.43it/s][A
 30%|       | 132/436 [00:03<00:06, 44.45it/s][A
 31%|      | 137/436 [00:03<00:06, 44.44it/s][A
 33%|      | 142/436 [00:03<00:06, 44.65it/s][A
 34%|      | 147/436 [00:03<00:06, 44.68it/s][A
 35%|      | 152/436 [00:03<00:06, 44.68it/s][A
 36%|      | 157/436 [00:03<00:06, 44.42it/s][A
 37%|      | 162/436 [00:03<00:06, 44.34it/s][A
 38%|      | 167/436 [00:03<00:06, 44.46it/s][A
 39%|      | 172/436 [00:03<00:05, 44.56it/s][A
 41%|      | 177/436 [00:04<00:06, 42.43it/s][A
 42%|     | 182/436 [00:04<00:05, 43.23it/s][A
 43%|     | 187/436 [00:04<00:05, 43.71it/s][A
 44%|     | 192/436 [00:04<00:05, 44.06it/s][A
 45%|     | 197/436 [00:04<00:05, 44.07it/s][A
 46%|     | 202/436 [00:04<00:05, 41.05it/s][A
 47%|     | 207/436 [00:04<00:05, 42.09it/s][A
 49%|     | 212/436 [00:04<00:05, 42.95it/s][A
 50%|     | 217/436 [00:04<00:05, 43.28it/s][A
 51%|     | 222/436 [00:05<00:04, 43.74it/s][A
 52%|    | 227/436 [00:05<00:04, 44.16it/s][A
 53%|    | 232/436 [00:05<00:04, 44.39it/s][A
 54%|    | 237/436 [00:05<00:04, 44.48it/s][A
 56%|    | 242/436 [00:05<00:04, 44.16it/s][A
 57%|    | 247/436 [00:05<00:04, 44.20it/s][A
 58%|    | 252/436 [00:05<00:04, 44.39it/s][A
 59%|    | 257/436 [00:05<00:04, 44.48it/s][A
 60%|    | 262/436 [00:06<00:03, 44.48it/s][A
 61%|    | 267/436 [00:06<00:03, 44.62it/s][A
 62%|   | 272/436 [00:06<00:03, 44.70it/s][A
 64%|   | 277/436 [00:06<00:03, 44.66it/s][A
 65%|   | 282/436 [00:06<00:03, 44.57it/s][A
 66%|   | 287/436 [00:06<00:03, 44.29it/s][A
 67%|   | 292/436 [00:06<00:03, 44.36it/s][A
 68%|   | 297/436 [00:06<00:03, 44.50it/s][A
 69%|   | 302/436 [00:06<00:03, 43.32it/s][A
 70%|   | 307/436 [00:07<00:02, 43.91it/s][A
 72%|  | 312/436 [00:07<00:02, 44.15it/s][A
 73%|  | 317/436 [00:07<00:02, 44.25it/s][A
 74%|  | 322/436 [00:07<00:02, 44.34it/s][A
 75%|  | 327/436 [00:07<00:02, 44.15it/s][A
 76%|  | 332/436 [00:07<00:02, 44.11it/s][A
 77%|  | 337/436 [00:07<00:02, 44.18it/s][A
 78%|  | 342/436 [00:07<00:02, 44.25it/s][A
 80%|  | 347/436 [00:07<00:02, 44.45it/s][A
 81%|  | 352/436 [00:08<00:01, 44.54it/s][A
 82%| | 357/436 [00:08<00:01, 44.74it/s][A
 83%| | 362/436 [00:08<00:01, 44.84it/s][A
 84%| | 367/436 [00:08<00:01, 44.80it/s][A
 85%| | 372/436 [00:08<00:01, 44.65it/s][A
 86%| | 377/436 [00:08<00:01, 44.43it/s][A
 88%| | 382/436 [00:08<00:01, 44.37it/s][A
 89%| | 387/436 [00:08<00:01, 44.32it/s][A
 90%| | 392/436 [00:08<00:00, 44.40it/s][A
 91%| | 397/436 [00:09<00:00, 44.58it/s][A
 92%|| 402/436 [00:09<00:00, 44.61it/s][A
 93%|| 407/436 [00:09<00:00, 44.80it/s][A
 94%|| 412/436 [00:09<00:00, 44.72it/s][A
 96%|| 417/436 [00:09<00:00, 44.62it/s][A
 97%|| 422/436 [00:09<00:00, 44.54it/s][A
 98%|| 427/436 [00:09<00:00, 40.35it/s][A
 99%|| 432/436 [00:09<00:00, 41.69it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 41.69it/s][A 20%|        | 156/780 [00:57<03:18,  3.14it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:36:42,680 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 10:36:42,959 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:36:47,114 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:36:47,555 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:36:47,695 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:12<1:21:36,  7.86s/it] 20%|        | 158/780 [01:13<58:05,  5.60s/it]   20%|        | 159/780 [01:13<41:31,  4.01s/it] 21%|        | 160/780 [01:13<29:55,  2.90s/it] 21%|        | 161/780 [01:14<21:49,  2.12s/it] 21%|        | 162/780 [01:14<16:09,  1.57s/it] 21%|        | 163/780 [01:14<12:12,  1.19s/it] 21%|        | 164/780 [01:15<09:26,  1.09it/s] 21%|        | 165/780 [01:15<07:30,  1.37it/s] 21%|       | 166/780 [01:15<06:08,  1.67it/s] 21%|       | 167/780 [01:15<05:11,  1.97it/s] 22%|       | 168/780 [01:16<04:31,  2.25it/s] 22%|       | 169/780 [01:16<04:09,  2.45it/s] 22%|       | 170/780 [01:16<03:47,  2.68it/s] 22%|       | 171/780 [01:17<03:32,  2.87it/s] 22%|       | 172/780 [01:17<03:21,  3.02it/s] 22%|       | 173/780 [01:17<03:13,  3.13it/s] 22%|       | 174/780 [01:18<03:08,  3.22it/s] 22%|       | 175/780 [01:18<03:04,  3.28it/s] 23%|       | 176/780 [01:18<03:01,  3.33it/s] 23%|       | 177/780 [01:18<02:59,  3.36it/s] 23%|       | 178/780 [01:19<02:58,  3.38it/s] 23%|       | 179/780 [01:19<02:56,  3.40it/s] 23%|       | 180/780 [01:19<03:04,  3.25it/s] 23%|       | 181/780 [01:20<03:01,  3.31it/s] 23%|       | 182/780 [01:20<02:58,  3.34it/s] 23%|       | 183/780 [01:20<02:57,  3.37it/s] 24%|       | 184/780 [01:21<02:55,  3.39it/s] 24%|       | 185/780 [01:21<02:54,  3.40it/s] 24%|       | 186/780 [01:21<02:53,  3.42it/s] 24%|       | 187/780 [01:21<02:53,  3.42it/s] 24%|       | 188/780 [01:22<02:52,  3.43it/s] 24%|       | 189/780 [01:22<02:52,  3.43it/s] 24%|       | 190/780 [01:22<02:51,  3.43it/s] 24%|       | 191/780 [01:23<02:58,  3.29it/s] 25%|       | 192/780 [01:23<02:56,  3.32it/s] 25%|       | 193/780 [01:23<02:54,  3.36it/s] 25%|       | 194/780 [01:23<02:53,  3.38it/s] 25%|       | 195/780 [01:24<02:52,  3.40it/s] 25%|       | 196/780 [01:24<02:51,  3.41it/s] 25%|       | 197/780 [01:24<02:50,  3.41it/s] 25%|       | 198/780 [01:25<02:50,  3.42it/s] 26%|       | 199/780 [01:25<02:56,  3.29it/s] 26%|       | 200/780 [01:25<02:53,  3.34it/s] 26%|       | 201/780 [01:26<02:52,  3.36it/s] 26%|       | 202/780 [01:26<02:50,  3.38it/s] 26%|       | 203/780 [01:26<02:49,  3.40it/s] 26%|       | 204/780 [01:26<02:48,  3.41it/s] 26%|       | 205/780 [01:27<02:48,  3.42it/s] 26%|       | 206/780 [01:27<02:47,  3.42it/s] 27%|       | 207/780 [01:27<02:47,  3.43it/s] 27%|       | 208/780 [01:28<02:46,  3.43it/s] 27%|       | 209/780 [01:28<02:46,  3.44it/s] 27%|       | 210/780 [01:28<02:53,  3.28it/s] 27%|       | 211/780 [01:28<02:51,  3.32it/s] 27%|       | 212/780 [01:29<02:49,  3.36it/s] 27%|       | 213/780 [01:29<02:47,  3.38it/s] 27%|       | 214/780 [01:29<02:46,  3.40it/s] 28%|       | 215/780 [01:30<02:45,  3.41it/s] 28%|       | 216/780 [01:30<02:45,  3.41it/s] 28%|       | 217/780 [01:30<02:44,  3.42it/s] 28%|       | 218/780 [01:31<02:44,  3.42it/s] 28%|       | 219/780 [01:31<02:43,  3.42it/s] 28%|       | 220/780 [01:31<02:43,  3.43it/s] 28%|       | 221/780 [01:31<02:49,  3.30it/s] 28%|       | 222/780 [01:32<02:47,  3.34it/s] 29%|       | 223/780 [01:32<02:45,  3.37it/s] 29%|       | 224/780 [01:32<02:44,  3.39it/s] 29%|       | 225/780 [01:33<02:43,  3.40it/s] 29%|       | 226/780 [01:33<02:42,  3.41it/s] 29%|       | 227/780 [01:33<02:41,  3.42it/s] 29%|       | 228/780 [01:33<02:41,  3.43it/s] 29%|       | 229/780 [01:34<02:40,  3.43it/s] 29%|       | 230/780 [01:34<02:40,  3.44it/s] 30%|       | 231/780 [01:34<02:39,  3.44it/s] 30%|       | 232/780 [01:35<02:48,  3.26it/s] 30%|       | 233/780 [01:35<02:45,  3.31it/s] 30%|       | 234/780 [01:35<02:43,  3.34it/s] 30%|       | 235/780 [01:36<02:41,  3.37it/s] 30%|       | 236/780 [01:36<02:40,  3.39it/s] 30%|       | 237/780 [01:36<02:39,  3.40it/s] 31%|       | 238/780 [01:36<02:39,  3.41it/s] 31%|       | 239/780 [01:37<02:38,  3.41it/s] 31%|       | 240/780 [01:37<02:38,  3.41it/s] 31%|       | 241/780 [01:37<02:37,  3.42it/s] 31%|       | 242/780 [01:38<02:37,  3.42it/s] 31%|       | 243/780 [01:38<02:44,  3.26it/s] 31%|      | 244/780 [01:38<02:41,  3.31it/s] 31%|      | 245/780 [01:39<02:39,  3.35it/s] 32%|      | 246/780 [01:39<02:38,  3.38it/s] 32%|      | 247/780 [01:39<02:37,  3.39it/s] 32%|      | 248/780 [01:39<02:36,  3.41it/s] 32%|      | 249/780 [01:40<02:35,  3.42it/s] 32%|      | 250/780 [01:40<02:34,  3.42it/s] 32%|      | 251/780 [01:41<03:11,  2.76it/s] 32%|      | 252/780 [01:41<04:09,  2.12it/s] 32%|      | 253/780 [01:42<03:41,  2.38it/s] 33%|      | 254/780 [01:42<03:20,  2.62it/s] 33%|      | 255/780 [01:42<03:07,  2.81it/s] 33%|      | 256/780 [01:42<02:57,  2.96it/s] 33%|      | 257/780 [01:43<02:49,  3.08it/s] 33%|      | 258/780 [01:43<02:44,  3.16it/s] 33%|      | 259/780 [01:43<02:55,  2.96it/s] 33%|      | 260/780 [01:44<02:48,  3.08it/s] 33%|      | 261/780 [01:44<02:44,  3.16it/s] 34%|      | 262/780 [01:44<02:40,  3.22it/s] 34%|      | 263/780 [01:45<02:38,  3.27it/s] 34%|      | 264/780 [01:45<02:36,  3.30it/s] 34%|      | 265/780 [01:45<02:34,  3.33it/s] 34%|      | 266/780 [01:45<02:33,  3.34it/s] 34%|      | 267/780 [01:46<02:32,  3.36it/s] 34%|      | 268/780 [01:46<02:32,  3.37it/s] 34%|      | 269/780 [01:46<02:31,  3.37it/s] 35%|      | 270/780 [01:47<02:31,  3.37it/s] 35%|      | 271/780 [01:47<02:30,  3.37it/s] 35%|      | 272/780 [01:47<02:30,  3.38it/s] 35%|      | 273/780 [01:48<02:30,  3.38it/s] 35%|      | 274/780 [01:48<02:29,  3.38it/s] 35%|      | 275/780 [01:48<02:29,  3.38it/s] 35%|      | 276/780 [01:48<02:29,  3.38it/s] 36%|      | 277/780 [01:49<02:36,  3.22it/s] 36%|      | 278/780 [01:49<02:33,  3.26it/s] 36%|      | 279/780 [01:49<02:31,  3.30it/s] 36%|      | 280/780 [01:50<02:30,  3.32it/s] 36%|      | 281/780 [01:50<02:29,  3.34it/s] 36%|      | 282/780 [01:50<02:28,  3.36it/s] 36%|      | 283/780 [01:51<02:27,  3.36it/s] 36%|      | 284/780 [01:51<02:47,  2.96it/s] 37%|      | 285/780 [01:51<02:40,  3.08it/s] 37%|      | 286/780 [01:52<02:36,  3.16it/s] 37%|      | 287/780 [01:52<02:33,  3.22it/s] 37%|      | 288/780 [01:52<02:30,  3.27it/s] 37%|      | 289/780 [01:52<02:28,  3.30it/s] 37%|      | 290/780 [01:53<02:27,  3.33it/s] 37%|      | 291/780 [01:53<02:26,  3.34it/s] 37%|      | 292/780 [01:53<02:25,  3.35it/s] 38%|      | 293/780 [01:54<02:25,  3.36it/s] 38%|      | 294/780 [01:54<02:32,  3.20it/s] 38%|      | 295/780 [01:54<02:29,  3.25it/s] 38%|      | 296/780 [01:55<02:27,  3.28it/s] 38%|      | 297/780 [01:55<02:25,  3.31it/s] 38%|      | 298/780 [01:55<02:24,  3.34it/s] 38%|      | 299/780 [01:55<02:23,  3.35it/s] 38%|      | 300/780 [01:56<02:22,  3.36it/s] 39%|      | 301/780 [01:56<02:22,  3.36it/s] 39%|      | 302/780 [01:56<02:21,  3.37it/s] 39%|      | 303/780 [01:57<02:21,  3.37it/s] 39%|      | 304/780 [01:57<02:21,  3.38it/s] 39%|      | 305/780 [01:57<02:20,  3.37it/s] 39%|      | 306/780 [01:58<02:20,  3.37it/s] 39%|      | 307/780 [01:58<02:20,  3.38it/s] 39%|      | 308/780 [01:58<02:19,  3.38it/s] 40%|      | 309/780 [01:58<02:19,  3.38it/s] 40%|      | 310/780 [01:59<02:19,  3.37it/s] 40%|      | 311/780 [01:59<02:18,  3.37it/s] 40%|      | 312/780 [01:59<02:24,  3.25it/s][INFO|trainer.py:2140] 2023-08-29 10:37:44,452 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:37:44,452 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 10:37:44,452 >>   Batch size = 8
{'eval_loss': 1.156178593635559, 'eval_runtime': 10.0153, 'eval_samples_per_second': 347.667, 'eval_steps_per_second': 43.533, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.43it/s][A
  3%|         | 12/436 [00:00<00:08, 48.65it/s][A
  4%|         | 17/436 [00:00<00:08, 47.07it/s][A
  5%|         | 22/436 [00:00<00:08, 46.37it/s][A
  6%|         | 27/436 [00:00<00:09, 45.43it/s][A
  7%|         | 32/436 [00:00<00:09, 44.84it/s][A
  8%|         | 37/436 [00:00<00:08, 44.55it/s][A
 10%|         | 42/436 [00:00<00:08, 44.32it/s][A
 11%|         | 47/436 [00:01<00:08, 44.52it/s][A
 12%|        | 52/436 [00:01<00:08, 44.76it/s][A
 13%|        | 57/436 [00:01<00:08, 44.79it/s][A
 14%|        | 62/436 [00:01<00:08, 44.95it/s][A
 15%|        | 67/436 [00:01<00:08, 44.84it/s][A
 17%|        | 72/436 [00:01<00:08, 44.50it/s][A
 18%|        | 77/436 [00:01<00:08, 44.30it/s][A
 19%|        | 82/436 [00:01<00:08, 44.19it/s][A
 20%|        | 87/436 [00:01<00:07, 44.15it/s][A
 21%|        | 92/436 [00:02<00:07, 44.39it/s][A
 22%|       | 97/436 [00:02<00:07, 44.60it/s][A
 23%|       | 102/436 [00:02<00:07, 44.73it/s][A
 25%|       | 107/436 [00:02<00:07, 44.88it/s][A
 26%|       | 112/436 [00:02<00:07, 44.66it/s][A
 27%|       | 117/436 [00:02<00:07, 44.58it/s][A
 28%|       | 122/436 [00:02<00:07, 41.04it/s][A
 29%|       | 127/436 [00:02<00:07, 42.27it/s][A
 30%|       | 132/436 [00:02<00:07, 43.08it/s][A
 31%|      | 137/436 [00:03<00:06, 43.65it/s][A
 33%|      | 142/436 [00:03<00:06, 43.73it/s][A
 34%|      | 147/436 [00:03<00:06, 44.52it/s][A
 35%|      | 152/436 [00:03<00:06, 44.59it/s][A
 36%|      | 157/436 [00:03<00:06, 44.66it/s][A
 37%|      | 162/436 [00:03<00:06, 44.33it/s][A
 38%|      | 167/436 [00:03<00:06, 44.07it/s][A
 39%|      | 172/436 [00:03<00:05, 44.12it/s][A
 41%|      | 177/436 [00:03<00:05, 44.31it/s][A
 42%|     | 182/436 [00:04<00:05, 44.44it/s][A
 43%|     | 187/436 [00:04<00:06, 39.09it/s][A
 44%|     | 193/436 [00:04<00:05, 42.03it/s][A
 45%|     | 198/436 [00:04<00:05, 42.92it/s][A
 47%|     | 203/436 [00:04<00:05, 43.44it/s][A
 48%|     | 208/436 [00:04<00:05, 42.19it/s][A
 49%|     | 213/436 [00:04<00:05, 42.86it/s][A
 50%|     | 218/436 [00:04<00:05, 43.37it/s][A
 51%|     | 223/436 [00:05<00:04, 43.84it/s][A
 52%|    | 228/436 [00:05<00:04, 43.73it/s][A
 53%|    | 233/436 [00:05<00:04, 43.75it/s][A
 55%|    | 238/436 [00:05<00:04, 44.07it/s][A
 56%|    | 243/436 [00:05<00:04, 44.37it/s][A
 57%|    | 248/436 [00:05<00:04, 44.35it/s][A
 58%|    | 253/436 [00:05<00:04, 44.51it/s][A
 59%|    | 258/436 [00:05<00:03, 44.51it/s][A
 60%|    | 263/436 [00:05<00:03, 44.65it/s][A
 61%|   | 268/436 [00:06<00:03, 44.12it/s][A
 63%|   | 273/436 [00:06<00:03, 44.22it/s][A
 64%|   | 278/436 [00:06<00:03, 44.24it/s][A
 65%|   | 283/436 [00:06<00:03, 44.41it/s][A
 66%|   | 288/436 [00:06<00:03, 44.55it/s][A
 67%|   | 293/436 [00:06<00:03, 44.54it/s][A
 68%|   | 298/436 [00:06<00:03, 40.57it/s][A
 69%|   | 303/436 [00:06<00:03, 41.77it/s][A
 71%|   | 308/436 [00:07<00:02, 42.73it/s][A
 72%|  | 313/436 [00:07<00:02, 43.44it/s][A
 73%|  | 318/436 [00:07<00:02, 43.72it/s][A
 74%|  | 323/436 [00:07<00:02, 44.04it/s][A
 75%|  | 328/436 [00:07<00:02, 44.25it/s][A
 76%|  | 333/436 [00:07<00:02, 44.32it/s][A
 78%|  | 338/436 [00:07<00:02, 44.08it/s][A
 79%|  | 343/436 [00:07<00:02, 44.14it/s][A
 80%|  | 348/436 [00:07<00:01, 44.23it/s][A
 81%|  | 353/436 [00:08<00:01, 44.46it/s][A
 82%| | 358/436 [00:08<00:01, 44.62it/s][A
 83%| | 363/436 [00:08<00:01, 44.62it/s][A
 84%| | 368/436 [00:08<00:01, 44.67it/s][A
 86%| | 373/436 [00:08<00:01, 44.64it/s][A
 87%| | 378/436 [00:08<00:01, 44.59it/s][A
 88%| | 383/436 [00:08<00:01, 44.44it/s][A
 89%| | 388/436 [00:08<00:01, 44.31it/s][A
 90%| | 393/436 [00:08<00:00, 44.01it/s][A
 91%|| 398/436 [00:09<00:00, 44.39it/s][A
 92%|| 403/436 [00:09<00:00, 44.58it/s][A
 94%|| 408/436 [00:09<00:00, 44.72it/s][A
 95%|| 413/436 [00:09<00:00, 44.73it/s][A
 96%|| 418/436 [00:09<00:00, 44.63it/s][A
 97%|| 423/436 [00:09<00:00, 44.48it/s][A
 98%|| 428/436 [00:09<00:00, 44.33it/s][A
 99%|| 433/436 [00:09<00:00, 43.08it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 43.08it/s][A 40%|      | 312/780 [02:09<02:24,  3.25it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:37:54,628 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 10:37:54,836 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:37:58,864 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:37:59,121 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:37:59,233 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:23<56:11,  7.22s/it] 40%|      | 314/780 [02:23<40:02,  5.16s/it] 40%|      | 315/780 [02:23<28:39,  3.70s/it] 41%|      | 316/780 [02:24<20:41,  2.68s/it] 41%|      | 317/780 [02:24<15:08,  1.96s/it] 41%|      | 318/780 [02:24<11:15,  1.46s/it] 41%|      | 319/780 [02:25<08:32,  1.11s/it] 41%|      | 320/780 [02:25<06:38,  1.15it/s] 41%|      | 321/780 [02:25<05:19,  1.44it/s] 41%|     | 322/780 [02:25<04:23,  1.74it/s] 41%|     | 323/780 [02:26<03:44,  2.04it/s] 42%|     | 324/780 [02:26<03:17,  2.31it/s] 42%|     | 325/780 [02:26<03:02,  2.50it/s] 42%|     | 326/780 [02:27<02:47,  2.71it/s] 42%|     | 327/780 [02:27<02:36,  2.89it/s] 42%|     | 328/780 [02:27<02:29,  3.02it/s] 42%|     | 329/780 [02:27<02:24,  3.13it/s] 42%|     | 330/780 [02:28<02:19,  3.22it/s] 42%|     | 331/780 [02:28<02:16,  3.28it/s] 43%|     | 332/780 [02:28<02:14,  3.33it/s] 43%|     | 333/780 [02:29<02:12,  3.36it/s] 43%|     | 334/780 [02:29<02:11,  3.38it/s] 43%|     | 335/780 [02:29<02:10,  3.40it/s] 43%|     | 336/780 [02:30<02:16,  3.25it/s] 43%|     | 337/780 [02:30<02:14,  3.30it/s] 43%|     | 338/780 [02:30<02:12,  3.34it/s] 43%|     | 339/780 [02:30<02:10,  3.37it/s] 44%|     | 340/780 [02:31<02:09,  3.39it/s] 44%|     | 341/780 [02:31<02:08,  3.40it/s] 44%|     | 342/780 [02:31<02:08,  3.41it/s] 44%|     | 343/780 [02:32<02:07,  3.42it/s] 44%|     | 344/780 [02:32<02:07,  3.43it/s] 44%|     | 345/780 [02:32<02:06,  3.43it/s] 44%|     | 346/780 [02:32<02:06,  3.43it/s] 44%|     | 347/780 [02:33<02:11,  3.30it/s] 45%|     | 348/780 [02:33<02:09,  3.34it/s] 45%|     | 349/780 [02:33<02:07,  3.37it/s] 45%|     | 350/780 [02:34<02:06,  3.39it/s] 45%|     | 351/780 [02:34<02:05,  3.41it/s] 45%|     | 352/780 [02:34<02:05,  3.42it/s] 45%|     | 353/780 [02:35<02:04,  3.43it/s] 45%|     | 354/780 [02:35<02:04,  3.43it/s] 46%|     | 355/780 [02:35<02:03,  3.44it/s] 46%|     | 356/780 [02:35<02:03,  3.44it/s] 46%|     | 357/780 [02:36<02:03,  3.44it/s] 46%|     | 358/780 [02:36<02:07,  3.30it/s] 46%|     | 359/780 [02:36<02:05,  3.34it/s] 46%|     | 360/780 [02:37<02:04,  3.37it/s] 46%|     | 361/780 [02:37<02:03,  3.39it/s] 46%|     | 362/780 [02:37<02:02,  3.41it/s] 47%|     | 363/780 [02:38<02:02,  3.42it/s] 47%|     | 364/780 [02:38<02:01,  3.43it/s] 47%|     | 365/780 [02:38<02:06,  3.27it/s] 47%|     | 366/780 [02:38<02:04,  3.32it/s] 47%|     | 367/780 [02:39<02:03,  3.36it/s] 47%|     | 368/780 [02:39<02:01,  3.38it/s] 47%|     | 369/780 [02:39<02:01,  3.39it/s] 47%|     | 370/780 [02:40<02:00,  3.40it/s] 48%|     | 371/780 [02:40<01:59,  3.41it/s] 48%|     | 372/780 [02:40<02:04,  3.28it/s] 48%|     | 373/780 [02:41<02:02,  3.32it/s] 48%|     | 374/780 [02:41<02:00,  3.36it/s] 48%|     | 375/780 [02:42<03:03,  2.20it/s] 48%|     | 376/780 [02:42<02:43,  2.47it/s] 48%|     | 377/780 [02:42<02:29,  2.70it/s] 48%|     | 378/780 [02:42<02:19,  2.89it/s] 49%|     | 379/780 [02:43<02:12,  3.03it/s] 49%|     | 380/780 [02:43<02:07,  3.15it/s] 49%|     | 381/780 [02:43<02:11,  3.04it/s] 49%|     | 382/780 [02:44<02:06,  3.15it/s] 49%|     | 383/780 [02:44<02:02,  3.23it/s] 49%|     | 384/780 [02:44<02:00,  3.29it/s] 49%|     | 385/780 [02:45<01:58,  3.33it/s] 49%|     | 386/780 [02:45<01:57,  3.36it/s] 50%|     | 387/780 [02:45<01:56,  3.39it/s] 50%|     | 388/780 [02:46<02:02,  3.20it/s] 50%|     | 389/780 [02:46<01:59,  3.27it/s] 50%|     | 390/780 [02:46<01:57,  3.32it/s] 50%|     | 391/780 [02:46<02:03,  3.15it/s] 50%|     | 392/780 [02:47<02:00,  3.22it/s] 50%|     | 393/780 [02:47<01:57,  3.28it/s] 51%|     | 394/780 [02:47<01:56,  3.33it/s] 51%|     | 395/780 [02:48<01:54,  3.36it/s] 51%|     | 396/780 [02:48<01:53,  3.38it/s] 51%|     | 397/780 [02:48<01:52,  3.40it/s] 51%|     | 398/780 [02:49<01:52,  3.41it/s] 51%|     | 399/780 [02:49<01:51,  3.41it/s] 51%|    | 400/780 [02:49<01:51,  3.42it/s] 51%|    | 401/780 [02:49<01:50,  3.43it/s] 52%|    | 402/780 [02:50<01:57,  3.21it/s] 52%|    | 403/780 [02:50<01:55,  3.27it/s] 52%|    | 404/780 [02:50<01:53,  3.32it/s] 52%|    | 405/780 [02:51<01:51,  3.35it/s] 52%|    | 406/780 [02:51<01:50,  3.38it/s] 52%|    | 407/780 [02:51<01:49,  3.40it/s] 52%|    | 408/780 [02:51<01:49,  3.41it/s] 52%|    | 409/780 [02:52<01:48,  3.42it/s] 53%|    | 410/780 [02:52<01:47,  3.43it/s] 53%|    | 411/780 [02:52<01:47,  3.43it/s] 53%|    | 412/780 [02:53<01:47,  3.43it/s] 53%|    | 413/780 [02:53<01:49,  3.34it/s] 53%|    | 414/780 [02:53<01:48,  3.36it/s] 53%|    | 415/780 [02:54<01:47,  3.39it/s] 53%|    | 416/780 [02:54<01:47,  3.40it/s] 53%|    | 417/780 [02:54<01:46,  3.41it/s] 54%|    | 418/780 [02:54<01:45,  3.42it/s] 54%|    | 419/780 [02:55<01:45,  3.42it/s] 54%|    | 420/780 [02:55<01:45,  3.42it/s] 54%|    | 421/780 [02:55<01:44,  3.43it/s] 54%|    | 422/780 [02:56<01:44,  3.43it/s] 54%|    | 423/780 [02:56<01:44,  3.43it/s] 54%|    | 424/780 [02:56<01:46,  3.35it/s] 54%|    | 425/780 [02:56<01:45,  3.38it/s] 55%|    | 426/780 [02:57<01:44,  3.40it/s] 55%|    | 427/780 [02:57<01:43,  3.41it/s] 55%|    | 428/780 [02:57<01:42,  3.42it/s] 55%|    | 429/780 [02:58<01:42,  3.43it/s] 55%|    | 430/780 [02:58<01:42,  3.43it/s] 55%|    | 431/780 [02:58<01:41,  3.43it/s] 55%|    | 432/780 [02:59<01:41,  3.44it/s] 56%|    | 433/780 [02:59<01:40,  3.44it/s] 56%|    | 434/780 [02:59<01:40,  3.44it/s] 56%|    | 435/780 [02:59<01:40,  3.44it/s] 56%|    | 436/780 [03:00<01:39,  3.44it/s] 56%|    | 437/780 [03:00<01:39,  3.44it/s] 56%|    | 438/780 [03:00<01:39,  3.44it/s] 56%|    | 439/780 [03:01<01:41,  3.34it/s] 56%|    | 440/780 [03:01<01:40,  3.37it/s] 57%|    | 441/780 [03:01<01:39,  3.39it/s] 57%|    | 442/780 [03:01<01:39,  3.40it/s] 57%|    | 443/780 [03:02<01:38,  3.41it/s] 57%|    | 444/780 [03:02<01:38,  3.42it/s] 57%|    | 445/780 [03:02<01:41,  3.29it/s] 57%|    | 446/780 [03:03<01:40,  3.33it/s] 57%|    | 447/780 [03:03<01:38,  3.37it/s] 57%|    | 448/780 [03:03<01:37,  3.39it/s] 58%|    | 449/780 [03:04<01:37,  3.40it/s] 58%|    | 450/780 [03:04<01:49,  3.01it/s] 58%|    | 451/780 [03:04<01:45,  3.12it/s] 58%|    | 452/780 [03:05<01:42,  3.21it/s] 58%|    | 453/780 [03:05<01:39,  3.27it/s] 58%|    | 454/780 [03:05<01:38,  3.32it/s] 58%|    | 455/780 [03:05<01:36,  3.36it/s] 58%|    | 456/780 [03:06<01:40,  3.23it/s] 59%|    | 457/780 [03:06<01:38,  3.29it/s] 59%|    | 458/780 [03:06<01:36,  3.33it/s] 59%|    | 459/780 [03:07<01:35,  3.36it/s] 59%|    | 460/780 [03:07<01:34,  3.38it/s] 59%|    | 461/780 [03:07<01:33,  3.40it/s] 59%|    | 462/780 [03:07<01:33,  3.41it/s] 59%|    | 463/780 [03:08<01:32,  3.42it/s] 59%|    | 464/780 [03:08<01:32,  3.42it/s] 60%|    | 465/780 [03:08<01:31,  3.43it/s] 60%|    | 466/780 [03:09<01:31,  3.43it/s] 60%|    | 467/780 [03:09<01:31,  3.43it/s] 60%|    | 468/780 [03:09<01:30,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 10:38:54,319 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:38:54,319 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 10:38:54,319 >>   Batch size = 8
{'eval_loss': 1.1737349033355713, 'eval_runtime': 9.9345, 'eval_samples_per_second': 350.498, 'eval_steps_per_second': 43.888, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.10it/s][A
  3%|         | 12/436 [00:00<00:08, 47.77it/s][A
  4%|         | 17/436 [00:00<00:09, 46.24it/s][A
  5%|         | 22/436 [00:00<00:09, 45.48it/s][A
  6%|         | 27/436 [00:00<00:09, 45.20it/s][A
  7%|         | 32/436 [00:00<00:08, 44.90it/s][A
  8%|         | 37/436 [00:00<00:08, 44.77it/s][A
 10%|         | 42/436 [00:00<00:08, 44.74it/s][A
 11%|         | 47/436 [00:01<00:08, 44.80it/s][A
 12%|        | 52/436 [00:01<00:08, 44.81it/s][A
 13%|        | 57/436 [00:01<00:08, 44.56it/s][A
 14%|        | 62/436 [00:01<00:09, 40.59it/s][A
 15%|        | 67/436 [00:01<00:08, 41.86it/s][A
 17%|        | 72/436 [00:01<00:08, 42.82it/s][A
 18%|        | 77/436 [00:01<00:08, 43.25it/s][A
 19%|        | 82/436 [00:01<00:08, 43.71it/s][A
 20%|        | 87/436 [00:01<00:07, 43.97it/s][A
 21%|        | 92/436 [00:02<00:07, 44.26it/s][A
 22%|       | 97/436 [00:02<00:07, 44.37it/s][A
 23%|       | 102/436 [00:02<00:07, 44.02it/s][A
 25%|       | 107/436 [00:02<00:07, 44.17it/s][A
 26%|       | 112/436 [00:02<00:07, 44.35it/s][A
 27%|       | 117/436 [00:02<00:07, 44.49it/s][A
 28%|       | 122/436 [00:02<00:07, 44.65it/s][A
 29%|       | 127/436 [00:02<00:06, 44.50it/s][A
 30%|       | 132/436 [00:02<00:06, 44.58it/s][A
 31%|      | 137/436 [00:03<00:06, 44.67it/s][A
 33%|      | 142/436 [00:03<00:06, 44.48it/s][A
 34%|      | 147/436 [00:03<00:06, 44.31it/s][A
 35%|      | 152/436 [00:03<00:07, 38.38it/s][A
 36%|      | 157/436 [00:03<00:06, 40.23it/s][A
 37%|      | 162/436 [00:03<00:06, 41.61it/s][A
 38%|      | 167/436 [00:03<00:06, 42.63it/s][A
 39%|      | 172/436 [00:03<00:06, 43.29it/s][A
 41%|      | 177/436 [00:04<00:05, 43.76it/s][A
 42%|     | 182/436 [00:04<00:05, 44.23it/s][A
 43%|     | 187/436 [00:04<00:05, 44.38it/s][A
 44%|     | 192/436 [00:04<00:05, 43.98it/s][A
 45%|     | 197/436 [00:04<00:05, 43.59it/s][A
 46%|     | 202/436 [00:04<00:05, 43.89it/s][A
 47%|     | 207/436 [00:04<00:05, 44.22it/s][A
 49%|     | 212/436 [00:04<00:05, 44.44it/s][A
 50%|     | 217/436 [00:04<00:04, 44.68it/s][A
 51%|     | 222/436 [00:05<00:04, 44.74it/s][A
 52%|    | 227/436 [00:05<00:04, 44.87it/s][A
 53%|    | 232/436 [00:05<00:04, 44.66it/s][A
 54%|    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|    | 242/436 [00:05<00:04, 44.20it/s][A
 57%|    | 247/436 [00:05<00:04, 44.17it/s][A
 58%|    | 252/436 [00:05<00:04, 44.48it/s][A
 59%|    | 257/436 [00:05<00:04, 44.61it/s][A
 60%|    | 262/436 [00:05<00:03, 44.68it/s][A
 61%|    | 267/436 [00:06<00:03, 44.81it/s][A
 62%|   | 272/436 [00:06<00:03, 44.71it/s][A
 64%|   | 277/436 [00:06<00:03, 44.52it/s][A
 65%|   | 282/436 [00:06<00:03, 44.21it/s][A
 66%|   | 287/436 [00:06<00:03, 41.25it/s][A
 67%|   | 292/436 [00:06<00:03, 42.40it/s][A
 68%|   | 297/436 [00:06<00:03, 43.12it/s][A
 69%|   | 302/436 [00:06<00:03, 43.80it/s][A
 70%|   | 307/436 [00:06<00:02, 44.08it/s][A
 72%|  | 312/436 [00:07<00:02, 44.37it/s][A
 73%|  | 317/436 [00:07<00:02, 44.53it/s][A
 74%|  | 322/436 [00:07<00:02, 44.29it/s][A
 75%|  | 327/436 [00:07<00:02, 43.90it/s][A
 76%|  | 332/436 [00:07<00:02, 43.89it/s][A
 77%|  | 337/436 [00:07<00:02, 44.21it/s][A
 78%|  | 342/436 [00:07<00:02, 44.44it/s][A
 80%|  | 347/436 [00:07<00:01, 44.67it/s][A
 81%|  | 352/436 [00:07<00:01, 44.74it/s][A
 82%| | 357/436 [00:08<00:01, 44.86it/s][A
 83%| | 362/436 [00:08<00:01, 44.83it/s][A
 84%| | 367/436 [00:08<00:01, 44.54it/s][A
 85%| | 372/436 [00:08<00:01, 44.20it/s][A
 86%| | 377/436 [00:08<00:01, 40.11it/s][A
 88%| | 382/436 [00:08<00:01, 41.51it/s][A
 89%| | 387/436 [00:08<00:01, 42.58it/s][A
 90%| | 392/436 [00:08<00:01, 43.31it/s][A
 91%| | 397/436 [00:09<00:00, 43.78it/s][A
 92%|| 402/436 [00:09<00:00, 44.17it/s][A
 93%|| 407/436 [00:09<00:00, 44.46it/s][A
 94%|| 412/436 [00:09<00:00, 44.29it/s][A
 96%|| 417/436 [00:09<00:00, 43.91it/s][A
 97%|| 422/436 [00:09<00:00, 43.95it/s][A
 98%|| 427/436 [00:09<00:00, 44.15it/s][A
 99%|| 432/436 [00:09<00:00, 44.39it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.39it/s][A 60%|    | 468/780 [03:19<01:30,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:39:04,642 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 10:39:04,896 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:39:09,064 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:39:09,262 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:39:09,375 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:33<38:26,  7.42s/it] 60%|    | 470/780 [03:34<27:19,  5.29s/it] 60%|    | 471/780 [03:34<19:31,  3.79s/it] 61%|    | 472/780 [03:34<14:04,  2.74s/it] 61%|    | 473/780 [03:34<10:15,  2.01s/it] 61%|    | 474/780 [03:35<07:36,  1.49s/it] 61%|    | 475/780 [03:35<05:45,  1.13s/it] 61%|    | 476/780 [03:35<04:27,  1.14it/s] 61%|    | 477/780 [03:36<03:32,  1.42it/s] 61%|   | 478/780 [03:36<02:54,  1.73it/s] 61%|   | 479/780 [03:36<02:28,  2.03it/s] 62%|   | 480/780 [03:37<02:09,  2.32it/s] 62%|   | 481/780 [03:37<01:59,  2.50it/s] 62%|   | 482/780 [03:37<01:49,  2.72it/s] 62%|   | 483/780 [03:37<01:42,  2.90it/s] 62%|   | 484/780 [03:38<01:37,  3.05it/s] 62%|   | 485/780 [03:38<01:33,  3.16it/s] 62%|   | 486/780 [03:38<01:34,  3.12it/s] 62%|   | 487/780 [03:39<01:31,  3.21it/s] 63%|   | 488/780 [03:39<01:29,  3.28it/s] 63%|   | 489/780 [03:39<01:27,  3.33it/s] 63%|   | 490/780 [03:39<01:26,  3.36it/s] 63%|   | 491/780 [03:40<01:25,  3.38it/s] 63%|   | 492/780 [03:40<01:28,  3.24it/s] 63%|   | 493/780 [03:40<01:27,  3.30it/s] 63%|   | 494/780 [03:41<01:25,  3.34it/s] 63%|   | 495/780 [03:41<01:24,  3.36it/s] 64%|   | 496/780 [03:42<02:02,  2.32it/s] 64%|   | 497/780 [03:42<01:50,  2.56it/s] 64%|   | 498/780 [03:42<01:41,  2.78it/s] 64%|   | 499/780 [03:43<01:35,  2.95it/s] 64%|   | 500/780 [03:43<01:30,  3.08it/s]                                                  64%|   | 500/780 [03:43<01:30,  3.08it/s] 64%|   | 501/780 [03:43<01:34,  2.95it/s] 64%|   | 502/780 [03:44<01:30,  3.08it/s] 64%|   | 503/780 [03:44<01:26,  3.19it/s] 65%|   | 504/780 [03:44<01:24,  3.26it/s] 65%|   | 505/780 [03:44<01:23,  3.31it/s] 65%|   | 506/780 [03:45<01:21,  3.35it/s] 65%|   | 507/780 [03:45<01:20,  3.38it/s] 65%|   | 508/780 [03:45<01:20,  3.40it/s] 65%|   | 509/780 [03:46<01:28,  3.06it/s] 65%|   | 510/780 [03:46<01:25,  3.17it/s] 66%|   | 511/780 [03:46<01:30,  2.97it/s] 66%|   | 512/780 [03:47<01:26,  3.10it/s] 66%|   | 513/780 [03:47<01:23,  3.19it/s] 66%|   | 514/780 [03:47<01:21,  3.26it/s] 66%|   | 515/780 [03:48<01:20,  3.31it/s] 66%|   | 516/780 [03:48<01:18,  3.35it/s] 66%|   | 517/780 [03:48<01:17,  3.38it/s] 66%|   | 518/780 [03:48<01:17,  3.40it/s] 67%|   | 519/780 [03:49<01:16,  3.41it/s] 67%|   | 520/780 [03:49<01:15,  3.42it/s] 67%|   | 521/780 [03:49<01:15,  3.43it/s] 67%|   | 522/780 [03:50<01:15,  3.43it/s] 67%|   | 523/780 [03:50<01:14,  3.44it/s] 67%|   | 524/780 [03:50<01:14,  3.44it/s] 67%|   | 525/780 [03:50<01:14,  3.44it/s] 67%|   | 526/780 [03:51<01:13,  3.44it/s] 68%|   | 527/780 [03:51<01:13,  3.44it/s] 68%|   | 528/780 [03:51<01:13,  3.43it/s] 68%|   | 529/780 [03:52<01:13,  3.44it/s] 68%|   | 530/780 [03:52<01:18,  3.16it/s] 68%|   | 531/780 [03:52<01:16,  3.24it/s] 68%|   | 532/780 [03:53<01:15,  3.30it/s] 68%|   | 533/780 [03:53<01:13,  3.34it/s] 68%|   | 534/780 [03:53<01:12,  3.37it/s] 69%|   | 535/780 [03:53<01:12,  3.39it/s] 69%|   | 536/780 [03:54<01:11,  3.41it/s] 69%|   | 537/780 [03:54<01:11,  3.42it/s] 69%|   | 538/780 [03:54<01:10,  3.43it/s] 69%|   | 539/780 [03:55<01:10,  3.43it/s] 69%|   | 540/780 [03:55<01:09,  3.44it/s] 69%|   | 541/780 [03:55<01:14,  3.22it/s] 69%|   | 542/780 [03:56<01:12,  3.28it/s] 70%|   | 543/780 [03:56<01:11,  3.33it/s] 70%|   | 544/780 [03:56<01:10,  3.36it/s] 70%|   | 545/780 [03:56<01:09,  3.38it/s] 70%|   | 546/780 [03:57<01:08,  3.40it/s] 70%|   | 547/780 [03:57<01:08,  3.41it/s] 70%|   | 548/780 [03:57<01:07,  3.41it/s] 70%|   | 549/780 [03:58<01:07,  3.42it/s] 71%|   | 550/780 [03:58<01:07,  3.43it/s] 71%|   | 551/780 [03:58<01:06,  3.43it/s] 71%|   | 552/780 [03:58<01:08,  3.33it/s] 71%|   | 553/780 [03:59<01:07,  3.36it/s] 71%|   | 554/780 [03:59<01:06,  3.38it/s] 71%|   | 555/780 [03:59<01:06,  3.40it/s] 71%|  | 556/780 [04:00<01:05,  3.41it/s] 71%|  | 557/780 [04:00<01:05,  3.42it/s] 72%|  | 558/780 [04:00<01:04,  3.43it/s] 72%|  | 559/780 [04:01<01:04,  3.43it/s] 72%|  | 560/780 [04:01<01:04,  3.43it/s] 72%|  | 561/780 [04:01<01:03,  3.44it/s] 72%|  | 562/780 [04:01<01:03,  3.44it/s] 72%|  | 563/780 [04:02<01:04,  3.34it/s] 72%|  | 564/780 [04:02<01:04,  3.37it/s] 72%|  | 565/780 [04:02<01:03,  3.39it/s] 73%|  | 566/780 [04:03<01:04,  3.34it/s] 73%|  | 567/780 [04:03<01:03,  3.37it/s] 73%|  | 568/780 [04:03<01:02,  3.39it/s] 73%|  | 569/780 [04:03<01:01,  3.41it/s] 73%|  | 570/780 [04:04<01:01,  3.41it/s] 73%|  | 571/780 [04:04<01:04,  3.25it/s] 73%|  | 572/780 [04:04<01:02,  3.31it/s] 73%|  | 573/780 [04:05<01:03,  3.25it/s] 74%|  | 574/780 [04:05<01:02,  3.30it/s] 74%|  | 575/780 [04:05<01:01,  3.34it/s] 74%|  | 576/780 [04:06<01:00,  3.37it/s] 74%|  | 577/780 [04:06<00:59,  3.39it/s] 74%|  | 578/780 [04:06<00:59,  3.40it/s] 74%|  | 579/780 [04:06<00:58,  3.41it/s] 74%|  | 580/780 [04:07<00:58,  3.42it/s] 74%|  | 581/780 [04:07<00:58,  3.42it/s] 75%|  | 582/780 [04:07<00:57,  3.43it/s] 75%|  | 583/780 [04:08<00:57,  3.43it/s] 75%|  | 584/780 [04:08<00:58,  3.35it/s] 75%|  | 585/780 [04:08<00:57,  3.37it/s] 75%|  | 586/780 [04:09<00:57,  3.39it/s] 75%|  | 587/780 [04:09<00:56,  3.40it/s] 75%|  | 588/780 [04:09<00:56,  3.41it/s] 76%|  | 589/780 [04:09<00:55,  3.42it/s] 76%|  | 590/780 [04:10<00:55,  3.43it/s] 76%|  | 591/780 [04:10<00:55,  3.43it/s] 76%|  | 592/780 [04:10<00:54,  3.43it/s] 76%|  | 593/780 [04:11<00:54,  3.43it/s] 76%|  | 594/780 [04:11<00:54,  3.44it/s] 76%|  | 595/780 [04:11<00:55,  3.34it/s] 76%|  | 596/780 [04:11<00:54,  3.37it/s] 77%|  | 597/780 [04:12<00:53,  3.39it/s] 77%|  | 598/780 [04:12<00:53,  3.41it/s] 77%|  | 599/780 [04:12<00:52,  3.42it/s] 77%|  | 600/780 [04:13<00:52,  3.42it/s] 77%|  | 601/780 [04:13<00:52,  3.43it/s] 77%|  | 602/780 [04:13<00:51,  3.43it/s] 77%|  | 603/780 [04:13<00:51,  3.43it/s] 77%|  | 604/780 [04:14<00:51,  3.44it/s] 78%|  | 605/780 [04:14<00:50,  3.44it/s] 78%|  | 606/780 [04:14<00:51,  3.39it/s] 78%|  | 607/780 [04:15<00:50,  3.41it/s] 78%|  | 608/780 [04:15<00:50,  3.42it/s] 78%|  | 609/780 [04:15<00:49,  3.42it/s] 78%|  | 610/780 [04:16<00:49,  3.43it/s] 78%|  | 611/780 [04:16<00:49,  3.43it/s] 78%|  | 612/780 [04:16<00:48,  3.44it/s] 79%|  | 613/780 [04:16<00:48,  3.44it/s] 79%|  | 614/780 [04:17<00:48,  3.44it/s] 79%|  | 615/780 [04:17<00:48,  3.44it/s] 79%|  | 616/780 [04:17<00:47,  3.44it/s] 79%|  | 617/780 [04:18<00:48,  3.37it/s] 79%|  | 618/780 [04:18<00:47,  3.39it/s] 79%|  | 619/780 [04:18<00:47,  3.40it/s] 79%|  | 620/780 [04:18<00:46,  3.42it/s] 80%|  | 621/780 [04:19<00:46,  3.42it/s] 80%|  | 622/780 [04:19<00:46,  3.43it/s] 80%|  | 623/780 [04:19<00:45,  3.43it/s] 80%|  | 624/780 [04:20<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 10:40:04,720 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:40:04,720 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 10:40:04,720 >>   Batch size = 8
{'eval_loss': 1.1859288215637207, 'eval_runtime': 9.9644, 'eval_samples_per_second': 349.446, 'eval_steps_per_second': 43.756, 'epoch': 3.0}
{'loss': 0.4094, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.34it/s][A
  3%|         | 12/436 [00:00<00:08, 47.97it/s][A
  4%|         | 17/436 [00:00<00:09, 46.22it/s][A
  5%|         | 22/436 [00:00<00:09, 45.49it/s][A
  6%|         | 27/436 [00:00<00:09, 45.11it/s][A
  7%|         | 32/436 [00:00<00:09, 44.86it/s][A
  8%|         | 37/436 [00:00<00:08, 44.81it/s][A
 10%|         | 42/436 [00:00<00:08, 44.65it/s][A
 11%|         | 47/436 [00:01<00:08, 44.68it/s][A
 12%|        | 52/436 [00:01<00:08, 44.76it/s][A
 13%|        | 57/436 [00:01<00:08, 44.71it/s][A
 14%|        | 62/436 [00:01<00:08, 44.45it/s][A
 15%|        | 67/436 [00:01<00:08, 44.53it/s][A
 17%|        | 72/436 [00:01<00:08, 44.43it/s][A
 18%|        | 77/436 [00:01<00:08, 44.36it/s][A
 19%|        | 82/436 [00:01<00:07, 44.47it/s][A
 20%|        | 87/436 [00:01<00:07, 44.46it/s][A
 21%|        | 92/436 [00:02<00:07, 44.66it/s][A
 22%|       | 97/436 [00:02<00:07, 44.67it/s][A
 23%|       | 102/436 [00:02<00:07, 44.54it/s][A
 25%|       | 107/436 [00:02<00:07, 44.53it/s][A
 26%|       | 112/436 [00:02<00:07, 44.42it/s][A
 27%|       | 117/436 [00:02<00:07, 44.30it/s][A
 28%|       | 122/436 [00:02<00:07, 44.24it/s][A
 29%|       | 127/436 [00:02<00:06, 44.37it/s][A
 30%|       | 132/436 [00:02<00:06, 44.51it/s][A
 31%|      | 137/436 [00:03<00:06, 44.67it/s][A
 33%|      | 142/436 [00:03<00:06, 44.78it/s][A
 34%|      | 147/436 [00:03<00:06, 44.60it/s][A
 35%|      | 152/436 [00:03<00:06, 44.56it/s][A
 36%|      | 157/436 [00:03<00:06, 44.44it/s][A
 37%|      | 162/436 [00:03<00:06, 44.30it/s][A
 38%|      | 167/436 [00:03<00:06, 44.28it/s][A
 39%|      | 172/436 [00:03<00:05, 44.34it/s][A
 41%|      | 177/436 [00:03<00:05, 44.46it/s][A
 42%|     | 182/436 [00:04<00:05, 44.75it/s][A
 43%|     | 187/436 [00:04<00:05, 44.76it/s][A
 44%|     | 192/436 [00:04<00:05, 44.76it/s][A
 45%|     | 197/436 [00:04<00:05, 44.51it/s][A
 46%|     | 202/436 [00:04<00:05, 44.48it/s][A
 47%|     | 207/436 [00:04<00:05, 44.38it/s][A
 49%|     | 212/436 [00:04<00:05, 44.27it/s][A
 50%|     | 217/436 [00:04<00:04, 44.38it/s][A
 51%|     | 222/436 [00:04<00:04, 44.26it/s][A
 52%|    | 227/436 [00:05<00:04, 44.76it/s][A
 53%|    | 232/436 [00:05<00:04, 44.78it/s][A
 54%|    | 237/436 [00:05<00:04, 44.79it/s][A
 56%|    | 242/436 [00:05<00:04, 44.58it/s][A
 57%|    | 247/436 [00:05<00:04, 44.46it/s][A
 58%|    | 252/436 [00:05<00:04, 44.39it/s][A
 59%|    | 257/436 [00:05<00:04, 44.28it/s][A
 60%|    | 262/436 [00:05<00:03, 44.28it/s][A
 61%|    | 267/436 [00:05<00:03, 44.47it/s][A
 62%|   | 272/436 [00:06<00:03, 44.53it/s][A
 64%|   | 277/436 [00:06<00:03, 44.75it/s][A
 65%|   | 282/436 [00:06<00:03, 44.85it/s][A
 66%|   | 287/436 [00:06<00:03, 44.65it/s][A
 67%|   | 292/436 [00:06<00:03, 44.57it/s][A
 68%|   | 297/436 [00:06<00:03, 44.39it/s][A
 69%|   | 302/436 [00:06<00:03, 44.33it/s][A
 70%|   | 307/436 [00:06<00:02, 44.42it/s][A
 72%|  | 312/436 [00:06<00:02, 44.43it/s][A
 73%|  | 317/436 [00:07<00:02, 44.58it/s][A
 74%|  | 322/436 [00:07<00:02, 44.68it/s][A
 75%|  | 327/436 [00:07<00:02, 44.77it/s][A
 76%|  | 332/436 [00:07<00:02, 44.54it/s][A
 77%|  | 337/436 [00:07<00:02, 44.43it/s][A
 78%|  | 342/436 [00:07<00:02, 44.28it/s][A
 80%|  | 347/436 [00:07<00:02, 44.21it/s][A
 81%|  | 352/436 [00:07<00:01, 44.31it/s][A
 82%| | 357/436 [00:08<00:01, 44.47it/s][A
 83%| | 362/436 [00:08<00:01, 44.59it/s][A
 84%| | 367/436 [00:08<00:01, 44.74it/s][A
 85%| | 372/436 [00:08<00:01, 44.70it/s][A
 86%| | 377/436 [00:08<00:01, 44.59it/s][A
 88%| | 382/436 [00:08<00:01, 44.40it/s][A
 89%| | 387/436 [00:08<00:01, 44.27it/s][A
 90%| | 392/436 [00:08<00:00, 44.24it/s][A
 91%| | 397/436 [00:08<00:00, 44.39it/s][A
 92%|| 402/436 [00:09<00:00, 44.40it/s][A
 93%|| 407/436 [00:09<00:00, 44.66it/s][A
 94%|| 412/436 [00:09<00:00, 44.85it/s][A
 96%|| 417/436 [00:09<00:00, 44.76it/s][A
 97%|| 422/436 [00:09<00:00, 44.68it/s][A
 98%|| 427/436 [00:09<00:00, 44.40it/s][A
 99%|| 432/436 [00:09<00:00, 44.35it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.35it/s][A 80%|  | 624/780 [04:29<00:45,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:40:14,651 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 10:40:14,752 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:40:17,870 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:40:17,955 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:40:17,987 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:39<15:19,  5.93s/it] 80%|  | 626/780 [04:39<10:53,  4.25s/it] 80%|  | 627/780 [04:39<07:48,  3.06s/it] 81%|  | 628/780 [04:40<05:39,  2.23s/it] 81%|  | 629/780 [04:40<04:09,  1.65s/it] 81%|  | 630/780 [04:40<03:06,  1.24s/it] 81%|  | 631/780 [04:41<02:22,  1.04it/s] 81%|  | 632/780 [04:41<01:52,  1.32it/s] 81%|  | 633/780 [04:41<01:31,  1.61it/s] 81%| | 634/780 [04:41<01:18,  1.87it/s] 81%| | 635/780 [04:42<01:08,  2.11it/s] 82%| | 636/780 [04:42<01:00,  2.37it/s] 82%| | 637/780 [04:42<00:54,  2.60it/s] 82%| | 638/780 [04:43<00:50,  2.80it/s] 82%| | 639/780 [04:43<00:47,  2.95it/s] 82%| | 640/780 [04:43<00:45,  3.07it/s] 82%| | 641/780 [04:44<00:44,  3.16it/s] 82%| | 642/780 [04:44<00:42,  3.22it/s] 82%| | 643/780 [04:44<00:41,  3.27it/s] 83%| | 644/780 [04:44<00:41,  3.31it/s] 83%| | 645/780 [04:45<00:40,  3.33it/s] 83%| | 646/780 [04:45<00:40,  3.35it/s] 83%| | 647/780 [04:45<00:43,  3.08it/s] 83%| | 648/780 [04:46<00:44,  2.98it/s] 83%| | 649/780 [04:46<00:42,  3.09it/s] 83%| | 650/780 [04:46<00:40,  3.18it/s] 83%| | 651/780 [04:47<00:39,  3.24it/s] 84%| | 652/780 [04:47<00:39,  3.28it/s] 84%| | 653/780 [04:47<00:38,  3.31it/s] 84%| | 654/780 [04:48<00:37,  3.34it/s] 84%| | 655/780 [04:48<00:37,  3.36it/s] 84%| | 656/780 [04:48<00:36,  3.37it/s] 84%| | 657/780 [04:49<00:39,  3.12it/s] 84%| | 658/780 [04:49<00:38,  3.20it/s] 84%| | 659/780 [04:49<00:37,  3.25it/s] 85%| | 660/780 [04:49<00:36,  3.29it/s] 85%| | 661/780 [04:50<00:35,  3.32it/s] 85%| | 662/780 [04:50<00:35,  3.34it/s] 85%| | 663/780 [04:50<00:34,  3.36it/s] 85%| | 664/780 [04:51<00:34,  3.37it/s] 85%| | 665/780 [04:51<00:34,  3.37it/s] 85%| | 666/780 [04:51<00:33,  3.38it/s] 86%| | 667/780 [04:51<00:33,  3.37it/s] 86%| | 668/780 [04:52<00:33,  3.38it/s] 86%| | 669/780 [04:52<00:32,  3.38it/s] 86%| | 670/780 [04:52<00:32,  3.38it/s] 86%| | 671/780 [04:53<00:32,  3.38it/s] 86%| | 672/780 [04:53<00:31,  3.39it/s] 86%| | 673/780 [04:53<00:31,  3.38it/s] 86%| | 674/780 [04:54<00:31,  3.39it/s] 87%| | 675/780 [04:54<00:30,  3.39it/s] 87%| | 676/780 [04:54<00:30,  3.39it/s] 87%| | 677/780 [04:54<00:30,  3.39it/s] 87%| | 678/780 [04:55<00:30,  3.35it/s] 87%| | 679/780 [04:55<00:30,  3.36it/s] 87%| | 680/780 [04:55<00:29,  3.37it/s] 87%| | 681/780 [04:56<00:29,  3.38it/s] 87%| | 682/780 [04:56<00:28,  3.38it/s] 88%| | 683/780 [04:56<00:28,  3.38it/s] 88%| | 684/780 [04:56<00:28,  3.38it/s] 88%| | 685/780 [04:57<00:28,  3.38it/s] 88%| | 686/780 [04:57<00:27,  3.39it/s] 88%| | 687/780 [04:57<00:27,  3.39it/s] 88%| | 688/780 [04:58<00:27,  3.38it/s] 88%| | 689/780 [04:58<00:27,  3.36it/s] 88%| | 690/780 [04:58<00:26,  3.36it/s] 89%| | 691/780 [04:59<00:26,  3.37it/s] 89%| | 692/780 [04:59<00:26,  3.38it/s] 89%| | 693/780 [04:59<00:25,  3.38it/s] 89%| | 694/780 [04:59<00:25,  3.38it/s] 89%| | 695/780 [05:00<00:25,  3.39it/s] 89%| | 696/780 [05:00<00:24,  3.39it/s] 89%| | 697/780 [05:00<00:24,  3.39it/s] 89%| | 698/780 [05:01<00:24,  3.39it/s] 90%| | 699/780 [05:01<00:23,  3.39it/s] 90%| | 700/780 [05:01<00:23,  3.39it/s] 90%| | 701/780 [05:02<00:23,  3.39it/s] 90%| | 702/780 [05:02<00:22,  3.39it/s] 90%| | 703/780 [05:02<00:22,  3.39it/s] 90%| | 704/780 [05:02<00:22,  3.40it/s] 90%| | 705/780 [05:03<00:22,  3.38it/s] 91%| | 706/780 [05:03<00:21,  3.39it/s] 91%| | 707/780 [05:03<00:21,  3.39it/s] 91%| | 708/780 [05:04<00:21,  3.38it/s] 91%| | 709/780 [05:04<00:20,  3.38it/s] 91%| | 710/780 [05:04<00:20,  3.36it/s] 91%| | 711/780 [05:04<00:20,  3.37it/s] 91%|| 712/780 [05:05<00:20,  3.37it/s] 91%|| 713/780 [05:05<00:19,  3.38it/s] 92%|| 714/780 [05:05<00:19,  3.38it/s] 92%|| 715/780 [05:06<00:19,  3.39it/s] 92%|| 716/780 [05:06<00:18,  3.39it/s] 92%|| 717/780 [05:06<00:18,  3.39it/s] 92%|| 718/780 [05:07<00:18,  3.39it/s] 92%|| 719/780 [05:07<00:18,  3.38it/s] 92%|| 720/780 [05:07<00:17,  3.38it/s] 92%|| 721/780 [05:07<00:17,  3.39it/s] 93%|| 722/780 [05:08<00:17,  3.39it/s] 93%|| 723/780 [05:08<00:16,  3.39it/s] 93%|| 724/780 [05:08<00:16,  3.39it/s] 93%|| 725/780 [05:09<00:16,  3.39it/s] 93%|| 726/780 [05:09<00:15,  3.39it/s] 93%|| 727/780 [05:09<00:15,  3.39it/s] 93%|| 728/780 [05:09<00:15,  3.39it/s] 93%|| 729/780 [05:10<00:15,  3.39it/s] 94%|| 730/780 [05:10<00:14,  3.34it/s] 94%|| 731/780 [05:10<00:14,  3.36it/s] 94%|| 732/780 [05:11<00:15,  3.12it/s] 94%|| 733/780 [05:11<00:14,  3.19it/s] 94%|| 734/780 [05:11<00:14,  3.25it/s] 94%|| 735/780 [05:12<00:13,  3.29it/s] 94%|| 736/780 [05:12<00:13,  3.31it/s] 94%|| 737/780 [05:12<00:12,  3.34it/s] 95%|| 738/780 [05:13<00:12,  3.36it/s] 95%|| 739/780 [05:13<00:12,  3.36it/s] 95%|| 740/780 [05:13<00:11,  3.37it/s] 95%|| 741/780 [05:13<00:11,  3.38it/s] 95%|| 742/780 [05:14<00:11,  3.30it/s] 95%|| 743/780 [05:14<00:11,  3.33it/s] 95%|| 744/780 [05:14<00:10,  3.35it/s] 96%|| 745/780 [05:15<00:10,  3.36it/s] 96%|| 746/780 [05:15<00:10,  3.37it/s] 96%|| 747/780 [05:15<00:09,  3.38it/s] 96%|| 748/780 [05:16<00:09,  3.38it/s] 96%|| 749/780 [05:16<00:10,  3.06it/s] 96%|| 750/780 [05:16<00:09,  3.15it/s] 96%|| 751/780 [05:16<00:09,  3.22it/s] 96%|| 752/780 [05:17<00:08,  3.27it/s] 97%|| 753/780 [05:17<00:08,  3.30it/s] 97%|| 754/780 [05:17<00:07,  3.32it/s] 97%|| 755/780 [05:18<00:07,  3.34it/s] 97%|| 756/780 [05:18<00:07,  3.36it/s] 97%|| 757/780 [05:18<00:06,  3.36it/s] 97%|| 758/780 [05:19<00:06,  3.37it/s] 97%|| 759/780 [05:19<00:06,  3.29it/s] 97%|| 760/780 [05:19<00:06,  3.32it/s] 98%|| 761/780 [05:19<00:05,  3.34it/s] 98%|| 762/780 [05:20<00:05,  3.35it/s] 98%|| 763/780 [05:20<00:05,  3.36it/s] 98%|| 764/780 [05:20<00:04,  3.37it/s] 98%|| 765/780 [05:21<00:04,  3.37it/s] 98%|| 766/780 [05:21<00:04,  3.37it/s] 98%|| 767/780 [05:21<00:03,  3.38it/s] 98%|| 768/780 [05:22<00:03,  3.38it/s] 99%|| 769/780 [05:22<00:03,  3.38it/s] 99%|| 770/780 [05:22<00:02,  3.39it/s] 99%|| 771/780 [05:22<00:02,  3.40it/s] 99%|| 772/780 [05:23<00:02,  3.41it/s] 99%|| 773/780 [05:23<00:02,  3.42it/s] 99%|| 774/780 [05:23<00:01,  3.43it/s] 99%|| 775/780 [05:24<00:01,  3.43it/s] 99%|| 776/780 [05:24<00:01,  3.43it/s]100%|| 777/780 [05:24<00:00,  3.25it/s]100%|| 778/780 [05:25<00:00,  3.31it/s]100%|| 779/780 [05:25<00:00,  3.34it/s]100%|| 780/780 [05:25<00:00,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 10:41:10,144 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:41:10,144 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 10:41:10,144 >>   Batch size = 8
{'eval_loss': 1.1992919445037842, 'eval_runtime': 9.8228, 'eval_samples_per_second': 354.48, 'eval_steps_per_second': 44.386, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|         | 6/436 [00:00<00:07, 55.61it/s][A
  3%|         | 12/436 [00:00<00:08, 48.93it/s][A
  4%|         | 17/436 [00:00<00:08, 46.99it/s][A
  5%|         | 22/436 [00:00<00:08, 46.27it/s][A
  6%|         | 27/436 [00:00<00:08, 45.51it/s][A
  7%|         | 32/436 [00:00<00:08, 44.93it/s][A
  8%|         | 37/436 [00:00<00:09, 43.47it/s][A
 10%|         | 42/436 [00:00<00:08, 43.79it/s][A
 11%|         | 47/436 [00:01<00:08, 44.06it/s][A
 12%|        | 52/436 [00:01<00:08, 44.31it/s][A
 13%|        | 57/436 [00:01<00:08, 44.61it/s][A
 14%|        | 62/436 [00:01<00:08, 44.74it/s][A
 15%|        | 67/436 [00:01<00:08, 44.68it/s][A
 17%|        | 72/436 [00:01<00:08, 44.62it/s][A
 18%|        | 77/436 [00:01<00:08, 44.30it/s][A
 19%|        | 82/436 [00:01<00:08, 44.20it/s][A
 20%|        | 87/436 [00:01<00:07, 44.24it/s][A
 21%|        | 92/436 [00:02<00:07, 44.27it/s][A
 22%|       | 97/436 [00:02<00:07, 44.45it/s][A
 23%|       | 102/436 [00:02<00:07, 44.69it/s][A
 25%|       | 107/436 [00:02<00:07, 44.73it/s][A
 26%|       | 112/436 [00:02<00:07, 44.85it/s][A
 27%|       | 117/436 [00:02<00:07, 44.63it/s][A
 28%|       | 122/436 [00:02<00:07, 44.28it/s][A
 29%|       | 127/436 [00:02<00:06, 44.26it/s][A
 30%|       | 132/436 [00:02<00:06, 44.28it/s][A
 31%|      | 137/436 [00:03<00:06, 44.28it/s][A
 33%|      | 142/436 [00:03<00:06, 44.48it/s][A
 34%|      | 147/436 [00:03<00:06, 44.58it/s][A
 35%|      | 152/436 [00:03<00:06, 44.69it/s][A
 36%|      | 157/436 [00:03<00:06, 44.77it/s][A
 37%|      | 162/436 [00:03<00:06, 44.57it/s][A
 38%|      | 167/436 [00:03<00:06, 44.43it/s][A
 39%|      | 172/436 [00:03<00:06, 41.25it/s][A
 41%|      | 177/436 [00:03<00:06, 42.31it/s][A
 42%|     | 182/436 [00:04<00:05, 43.04it/s][A
 43%|     | 187/436 [00:04<00:05, 43.59it/s][A
 44%|     | 192/436 [00:04<00:05, 43.93it/s][A
 45%|     | 197/436 [00:04<00:05, 44.24it/s][A
 46%|     | 202/436 [00:04<00:05, 44.36it/s][A
 47%|     | 207/436 [00:04<00:05, 44.35it/s][A
 49%|     | 212/436 [00:04<00:05, 44.06it/s][A
 50%|     | 217/436 [00:04<00:04, 44.02it/s][A
 51%|     | 222/436 [00:04<00:04, 44.17it/s][A
 52%|    | 227/436 [00:05<00:04, 44.40it/s][A
 53%|    | 232/436 [00:05<00:04, 44.51it/s][A
 54%|    | 237/436 [00:05<00:04, 44.62it/s][A
 56%|    | 242/436 [00:05<00:04, 44.70it/s][A
 57%|    | 247/436 [00:05<00:04, 44.72it/s][A
 58%|    | 252/436 [00:05<00:04, 44.54it/s][A
 59%|    | 257/436 [00:05<00:04, 44.31it/s][A
 60%|    | 262/436 [00:05<00:03, 44.16it/s][A
 61%|    | 267/436 [00:06<00:03, 44.33it/s][A
 62%|   | 272/436 [00:06<00:03, 44.46it/s][A
 64%|   | 277/436 [00:06<00:03, 44.62it/s][A
 65%|   | 282/436 [00:06<00:03, 44.71it/s][A
 66%|   | 287/436 [00:06<00:03, 44.79it/s][A
 67%|   | 292/436 [00:06<00:03, 44.76it/s][A
 68%|   | 297/436 [00:06<00:03, 44.47it/s][A
 69%|   | 302/436 [00:06<00:03, 44.33it/s][A
 70%|   | 307/436 [00:06<00:02, 44.30it/s][A
 72%|  | 312/436 [00:07<00:02, 44.32it/s][A
 73%|  | 317/436 [00:07<00:02, 44.42it/s][A
 74%|  | 322/436 [00:07<00:02, 44.50it/s][A
 75%|  | 327/436 [00:07<00:02, 44.76it/s][A
 76%|  | 332/436 [00:07<00:02, 44.70it/s][A
 77%|  | 337/436 [00:07<00:02, 44.60it/s][A
 78%|  | 342/436 [00:07<00:02, 44.49it/s][A
 80%|  | 347/436 [00:07<00:02, 44.32it/s][A
 81%|  | 352/436 [00:07<00:01, 44.20it/s][A
 82%| | 357/436 [00:08<00:01, 44.39it/s][A
 83%| | 362/436 [00:08<00:01, 44.43it/s][A
 84%| | 367/436 [00:08<00:01, 44.66it/s][A
 85%| | 372/436 [00:08<00:01, 44.62it/s][A
 86%| | 377/436 [00:08<00:01, 44.70it/s][A
 88%| | 382/436 [00:08<00:01, 44.69it/s][A
 89%| | 387/436 [00:08<00:01, 44.51it/s][A
 90%| | 392/436 [00:08<00:00, 44.35it/s][A
 91%| | 397/436 [00:08<00:00, 40.60it/s][A
 92%|| 402/436 [00:09<00:00, 41.78it/s][A
 93%|| 407/436 [00:09<00:00, 42.80it/s][A
 94%|| 412/436 [00:09<00:00, 43.42it/s][A
 96%|| 417/436 [00:09<00:00, 43.93it/s][A
 97%|| 422/436 [00:09<00:00, 44.23it/s][A
 98%|| 427/436 [00:09<00:00, 44.25it/s][A
 99%|| 432/436 [00:09<00:00, 44.16it/s][A
                                                 [A                                                 
100%|| 436/436 [00:09<00:00, 44.16it/s][A100%|| 780/780 [05:35<00:00,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:41:20,367 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 10:41:20,609 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:41:24,208 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:41:24,374 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:41:24,470 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:41:36,280 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:41:36,356 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156 (score: 1.156178593635559).
                                                 100%|| 780/780 [06:06<00:00,  3.37it/s]100%|| 780/780 [06:06<00:00,  2.13it/s]
[INFO|trainer.py:1894] 2023-08-29 10:41:51,378 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 10:41:51,544 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:41:55,982 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:41:56,256 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:41:56,386 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:41:57,096 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:41:57,097 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:41:57,097 >>   train_loss               =     0.4019
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:41:57,097 >>   train_runtime            = 0:06:06.69
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:41:57,097 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:41:57,097 >>   train_samples_per_second =    136.353
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:41:57,097 >>   train_steps_per_second   =      2.127
{'eval_loss': 1.2083052396774292, 'eval_runtime': 9.8518, 'eval_samples_per_second': 353.438, 'eval_steps_per_second': 44.256, 'epoch': 5.0}
{'train_runtime': 366.6958, 'train_samples_per_second': 136.353, 'train_steps_per_second': 2.127, 'train_loss': 0.401934071076222, 'epoch': 5.0}
08/29/2023 10:41:57 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:41:57,505 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:41:57,505 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 10:41:57,505 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|         | 6/436 [00:00<00:07, 55.75it/s]  3%|         | 12/436 [00:00<00:08, 49.45it/s]  4%|         | 17/436 [00:00<00:08, 47.65it/s]  5%|         | 22/436 [00:00<00:08, 46.79it/s]  6%|         | 27/436 [00:00<00:08, 46.38it/s]  7%|         | 32/436 [00:00<00:08, 46.10it/s]  8%|         | 37/436 [00:00<00:08, 45.86it/s] 10%|         | 42/436 [00:00<00:08, 45.51it/s] 11%|         | 47/436 [00:01<00:08, 45.00it/s] 12%|        | 52/436 [00:01<00:08, 44.89it/s] 13%|        | 57/436 [00:01<00:08, 44.91it/s] 14%|        | 62/436 [00:01<00:08, 45.05it/s] 15%|        | 67/436 [00:01<00:08, 45.05it/s] 17%|        | 72/436 [00:01<00:08, 43.42it/s] 18%|        | 77/436 [00:01<00:08, 44.08it/s] 19%|        | 82/436 [00:01<00:07, 44.49it/s] 20%|        | 87/436 [00:01<00:07, 44.41it/s] 21%|        | 92/436 [00:02<00:07, 44.41it/s] 22%|       | 97/436 [00:02<00:07, 44.46it/s] 23%|       | 102/436 [00:02<00:07, 44.58it/s] 25%|       | 107/436 [00:02<00:07, 44.74it/s] 26%|       | 112/436 [00:02<00:07, 44.71it/s] 27%|       | 117/436 [00:02<00:07, 44.93it/s] 28%|       | 122/436 [00:02<00:06, 45.08it/s] 29%|       | 127/436 [00:02<00:06, 45.27it/s] 30%|       | 132/436 [00:02<00:06, 44.99it/s] 31%|      | 137/436 [00:03<00:06, 44.93it/s] 33%|      | 142/436 [00:03<00:06, 44.82it/s] 34%|      | 147/436 [00:03<00:06, 44.78it/s] 35%|      | 152/436 [00:03<00:06, 44.83it/s] 36%|      | 157/436 [00:03<00:06, 44.76it/s] 37%|      | 162/436 [00:03<00:06, 44.99it/s] 38%|      | 167/436 [00:03<00:05, 45.13it/s] 39%|      | 172/436 [00:03<00:05, 45.20it/s] 41%|      | 177/436 [00:03<00:05, 45.04it/s] 42%|     | 182/436 [00:04<00:05, 44.99it/s] 43%|     | 187/436 [00:04<00:05, 44.94it/s] 44%|     | 192/436 [00:04<00:05, 44.86it/s] 45%|     | 197/436 [00:04<00:05, 44.77it/s] 46%|     | 202/436 [00:04<00:05, 44.66it/s] 47%|     | 207/436 [00:04<00:05, 42.59it/s] 49%|     | 212/436 [00:04<00:05, 43.41it/s] 50%|     | 217/436 [00:04<00:04, 43.99it/s] 51%|     | 222/436 [00:04<00:04, 44.40it/s] 52%|    | 227/436 [00:05<00:04, 44.65it/s] 53%|    | 232/436 [00:05<00:04, 44.77it/s] 54%|    | 237/436 [00:05<00:04, 44.84it/s] 56%|    | 242/436 [00:05<00:04, 44.64it/s] 57%|    | 247/436 [00:05<00:04, 44.39it/s] 58%|    | 252/436 [00:05<00:04, 44.53it/s] 59%|    | 257/436 [00:05<00:04, 44.71it/s] 60%|    | 262/436 [00:05<00:03, 44.98it/s] 61%|    | 267/436 [00:05<00:03, 45.00it/s] 62%|   | 272/436 [00:06<00:03, 45.17it/s] 64%|   | 277/436 [00:06<00:03, 45.18it/s] 65%|   | 282/436 [00:06<00:03, 45.03it/s] 66%|   | 287/436 [00:06<00:03, 44.92it/s] 67%|   | 292/436 [00:06<00:03, 44.63it/s] 68%|   | 297/436 [00:06<00:03, 44.65it/s] 69%|   | 302/436 [00:06<00:02, 44.76it/s] 70%|   | 307/436 [00:06<00:02, 44.86it/s] 72%|  | 312/436 [00:06<00:02, 45.08it/s] 73%|  | 317/436 [00:07<00:02, 45.06it/s] 74%|  | 322/436 [00:07<00:02, 45.17it/s] 75%|  | 327/436 [00:07<00:02, 45.06it/s] 76%|  | 332/436 [00:07<00:02, 44.89it/s] 77%|  | 337/436 [00:07<00:02, 44.63it/s] 78%|  | 342/436 [00:07<00:02, 43.05it/s] 80%|  | 347/436 [00:07<00:02, 43.79it/s] 81%|  | 352/436 [00:07<00:01, 44.29it/s] 82%| | 357/436 [00:07<00:01, 44.56it/s] 83%| | 362/436 [00:08<00:01, 44.83it/s] 84%| | 367/436 [00:08<00:01, 44.92it/s] 85%| | 372/436 [00:08<00:01, 44.91it/s] 86%| | 377/436 [00:08<00:01, 44.80it/s] 88%| | 382/436 [00:08<00:01, 44.45it/s] 89%| | 387/436 [00:08<00:01, 44.55it/s] 90%| | 392/436 [00:08<00:00, 44.80it/s] 91%| | 397/436 [00:08<00:00, 44.94it/s] 92%|| 402/436 [00:08<00:00, 45.08it/s] 93%|| 407/436 [00:09<00:00, 45.14it/s] 94%|| 412/436 [00:09<00:00, 45.17it/s] 96%|| 417/436 [00:09<00:00, 45.06it/s] 97%|| 422/436 [00:09<00:00, 44.85it/s] 98%|| 427/436 [00:09<00:00, 44.56it/s] 99%|| 432/436 [00:09<00:00, 44.66it/s]100%|| 436/436 [00:09<00:00, 44.86it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:42:07,243 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   eval_loss               =     1.1562
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   eval_runtime            = 0:00:09.73
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   eval_samples_per_second =    357.594
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   eval_steps_per_second   =     44.776
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:42:07,243 >>   perplexity              =     3.1778
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:42:45,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:42:45,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:42:45,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:42:45,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:42:45,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:42:46,564 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:42:46,565 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:42:47,177 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:43:31,433 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:43:31,462 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:43:34,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:43:34,794 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:43:34,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:43:34,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:43:34,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:43:36,573 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:43:36,575 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:43:37,357 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:43:40,821 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:43:40,932 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.57it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:30,  1.50it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.48it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.47it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.57it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:47,  1.58it/s]Extractor Predicting: 73it [00:47,  1.63it/s]Extractor Predicting: 74it [00:48,  1.63it/s]Extractor Predicting: 75it [00:49,  1.59it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:51,  1.45it/s]Extractor Predicting: 80it [00:52,  1.47it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.47it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:57,  1.54it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.48it/s]Extractor Predicting: 91it [00:59,  1.47it/s]Extractor Predicting: 92it [01:00,  1.45it/s]Extractor Predicting: 93it [01:01,  1.50it/s]Extractor Predicting: 94it [01:01,  1.48it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:03,  1.53it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:04,  1.37it/s]Extractor Predicting: 99it [01:05,  1.35it/s]Extractor Predicting: 100it [01:06,  1.36it/s]Extractor Predicting: 101it [01:06,  1.41it/s]Extractor Predicting: 102it [01:07,  1.41it/s]Extractor Predicting: 103it [01:08,  1.41it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.43it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:11,  1.48it/s]Extractor Predicting: 109it [01:12,  1.50it/s]Extractor Predicting: 110it [01:13,  1.51it/s]Extractor Predicting: 111it [01:13,  1.56it/s]Extractor Predicting: 112it [01:14,  1.54it/s]Extractor Predicting: 113it [01:14,  1.54it/s]Extractor Predicting: 114it [01:15,  1.50it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:19,  1.49it/s]Extractor Predicting: 120it [01:19,  1.53it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.50it/s]Extractor Predicting: 123it [01:21,  1.47it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.50it/s]Extractor Predicting: 126it [01:23,  1.51it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:25,  1.49it/s]Extractor Predicting: 129it [01:25,  1.47it/s]Extractor Predicting: 130it [01:26,  1.46it/s]Extractor Predicting: 131it [01:27,  1.48it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:28,  1.47it/s]Extractor Predicting: 134it [01:29,  1.44it/s]Extractor Predicting: 135it [01:29,  1.46it/s]Extractor Predicting: 136it [01:30,  1.48it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:31,  1.48it/s]Extractor Predicting: 139it [01:32,  1.47it/s]Extractor Predicting: 140it [01:33,  1.45it/s]Extractor Predicting: 141it [01:33,  1.46it/s]Extractor Predicting: 142it [01:34,  1.50it/s]Extractor Predicting: 143it [01:35,  1.52it/s]Extractor Predicting: 143it [01:35,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:33,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:33,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:33,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:33,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:33,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:45:34,717 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:45:34,718 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:45:35,394 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:45:36,708 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:45:36,708 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:40,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:40,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:40,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:40,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:45:40,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:45:41,525 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:45:41,526 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:45:42,288 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:45:42,581 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:45:42,581 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.29377880184331795,
  "recall": 0.07323377369327973,
  "score": 0.11724137931034483,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.67it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:19,  1.50it/s]Extractor Predicting: 33it [00:20,  1.50it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:22,  1.49it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.48it/s]Extractor Predicting: 39it [00:24,  1.47it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.46it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:34,  1.49it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.53it/s]Extractor Predicting: 58it [00:37,  1.54it/s]Extractor Predicting: 59it [00:37,  1.52it/s]Extractor Predicting: 60it [00:38,  1.52it/s]Extractor Predicting: 61it [00:39,  1.51it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.58it/s]Extractor Predicting: 64it [00:41,  1.58it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:42,  1.52it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:44,  1.55it/s]Extractor Predicting: 70it [00:45,  1.56it/s]Extractor Predicting: 71it [00:45,  1.54it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:50,  1.44it/s]Extractor Predicting: 79it [00:51,  1.44it/s]Extractor Predicting: 80it [00:51,  1.45it/s]Extractor Predicting: 81it [00:52,  1.47it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:54,  1.49it/s]Extractor Predicting: 85it [00:55,  1.46it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:56,  1.52it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:57,  1.57it/s]Extractor Predicting: 90it [00:58,  1.53it/s]Extractor Predicting: 91it [00:58,  1.54it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [01:00,  1.52it/s]Extractor Predicting: 94it [01:00,  1.50it/s]Extractor Predicting: 95it [01:01,  1.49it/s]Extractor Predicting: 96it [01:02,  1.48it/s]Extractor Predicting: 97it [01:03,  1.50it/s]Extractor Predicting: 98it [01:03,  1.48it/s]Extractor Predicting: 99it [01:04,  1.50it/s]Extractor Predicting: 100it [01:05,  1.49it/s]Extractor Predicting: 101it [01:05,  1.46it/s]Extractor Predicting: 102it [01:06,  1.49it/s]Extractor Predicting: 103it [01:07,  1.51it/s]Extractor Predicting: 104it [01:07,  1.50it/s]Extractor Predicting: 105it [01:08,  1.50it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:09,  1.50it/s]Extractor Predicting: 108it [01:10,  1.44it/s]Extractor Predicting: 109it [01:11,  1.46it/s]Extractor Predicting: 110it [01:11,  1.47it/s]Extractor Predicting: 111it [01:12,  1.47it/s]Extractor Predicting: 112it [01:13,  1.47it/s]Extractor Predicting: 113it [01:13,  1.49it/s]Extractor Predicting: 114it [01:14,  1.50it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:16,  1.62it/s]Extractor Predicting: 118it [01:16,  1.66it/s]Extractor Predicting: 119it [01:17,  1.65it/s]Extractor Predicting: 120it [01:18,  1.65it/s]Extractor Predicting: 121it [01:18,  1.63it/s]Extractor Predicting: 122it [01:19,  1.61it/s]Extractor Predicting: 123it [01:19,  1.60it/s]Extractor Predicting: 124it [01:20,  1.62it/s]Extractor Predicting: 125it [01:21,  1.68it/s]Extractor Predicting: 126it [01:21,  1.67it/s]Extractor Predicting: 127it [01:22,  1.69it/s]Extractor Predicting: 128it [01:22,  1.71it/s]Extractor Predicting: 129it [01:23,  1.68it/s]Extractor Predicting: 130it [01:24,  1.67it/s]Extractor Predicting: 131it [01:24,  1.69it/s]Extractor Predicting: 132it [01:25,  1.70it/s]Extractor Predicting: 133it [01:25,  1.66it/s]Extractor Predicting: 134it [01:26,  1.68it/s]Extractor Predicting: 135it [01:27,  1.70it/s]Extractor Predicting: 136it [01:27,  1.74it/s]Extractor Predicting: 137it [01:28,  1.75it/s]Extractor Predicting: 138it [01:28,  1.71it/s]Extractor Predicting: 139it [01:29,  1.67it/s]Extractor Predicting: 140it [01:29,  1.69it/s]Extractor Predicting: 141it [01:30,  1.65it/s]Extractor Predicting: 142it [01:31,  1.62it/s]Extractor Predicting: 143it [01:31,  1.66it/s]Extractor Predicting: 144it [01:32,  1.63it/s]Extractor Predicting: 145it [01:33,  1.64it/s]Extractor Predicting: 146it [01:33,  1.65it/s]Extractor Predicting: 147it [01:34,  1.65it/s]Extractor Predicting: 148it [01:34,  1.63it/s]Extractor Predicting: 149it [01:35,  1.69it/s]Extractor Predicting: 150it [01:36,  1.63it/s]Extractor Predicting: 151it [01:36,  1.61it/s]Extractor Predicting: 152it [01:37,  1.65it/s]Extractor Predicting: 153it [01:37,  1.70it/s]Extractor Predicting: 154it [01:38,  1.71it/s]Extractor Predicting: 155it [01:39,  1.71it/s]Extractor Predicting: 156it [01:39,  1.66it/s]Extractor Predicting: 157it [01:40,  1.66it/s]Extractor Predicting: 158it [01:40,  1.68it/s]Extractor Predicting: 159it [01:41,  1.64it/s]Extractor Predicting: 160it [01:42,  1.65it/s]Extractor Predicting: 161it [01:42,  1.61it/s]Extractor Predicting: 162it [01:43,  1.62it/s]Extractor Predicting: 163it [01:43,  1.64it/s]Extractor Predicting: 164it [01:44,  1.64it/s]Extractor Predicting: 165it [01:45,  1.65it/s]Extractor Predicting: 166it [01:45,  1.65it/s]Extractor Predicting: 167it [01:46,  1.65it/s]Extractor Predicting: 168it [01:46,  1.62it/s]Extractor Predicting: 169it [01:47,  1.63it/s]Extractor Predicting: 170it [01:48,  1.60it/s]Extractor Predicting: 171it [01:48,  1.54it/s]Extractor Predicting: 172it [01:49,  1.62it/s]Extractor Predicting: 173it [01:50,  1.57it/s]Extractor Predicting: 174it [01:50,  1.57it/s]Extractor Predicting: 175it [01:51,  1.55it/s]Extractor Predicting: 176it [01:52,  1.52it/s]Extractor Predicting: 177it [01:52,  1.51it/s]Extractor Predicting: 178it [01:53,  1.51it/s]Extractor Predicting: 179it [01:54,  1.52it/s]Extractor Predicting: 180it [01:54,  1.56it/s]Extractor Predicting: 181it [01:55,  1.52it/s]Extractor Predicting: 182it [01:56,  1.51it/s]Extractor Predicting: 183it [01:56,  1.52it/s]Extractor Predicting: 184it [01:57,  1.51it/s]Extractor Predicting: 185it [01:58,  1.51it/s]Extractor Predicting: 186it [01:58,  1.48it/s]Extractor Predicting: 187it [01:59,  1.48it/s]Extractor Predicting: 188it [02:00,  1.53it/s]Extractor Predicting: 189it [02:00,  1.53it/s]Extractor Predicting: 190it [02:01,  1.54it/s]Extractor Predicting: 191it [02:02,  1.48it/s]Extractor Predicting: 192it [02:02,  1.50it/s]Extractor Predicting: 193it [02:03,  1.48it/s]Extractor Predicting: 194it [02:04,  1.31it/s]Extractor Predicting: 195it [02:05,  1.35it/s]Extractor Predicting: 196it [02:05,  1.40it/s]Extractor Predicting: 197it [02:06,  1.42it/s]Extractor Predicting: 198it [02:07,  1.42it/s]Extractor Predicting: 199it [02:07,  1.44it/s]Extractor Predicting: 200it [02:08,  1.45it/s]Extractor Predicting: 201it [02:09,  1.46it/s]Extractor Predicting: 202it [02:09,  1.48it/s]Extractor Predicting: 203it [02:10,  1.47it/s]Extractor Predicting: 204it [02:11,  1.50it/s]Extractor Predicting: 205it [02:11,  1.55it/s]Extractor Predicting: 206it [02:12,  1.55it/s]Extractor Predicting: 207it [02:13,  1.54it/s]Extractor Predicting: 208it [02:13,  1.51it/s]Extractor Predicting: 209it [02:14,  1.52it/s]Extractor Predicting: 210it [02:15,  1.55it/s]Extractor Predicting: 211it [02:15,  1.59it/s]Extractor Predicting: 212it [02:16,  1.59it/s]Extractor Predicting: 213it [02:16,  1.60it/s]Extractor Predicting: 214it [02:17,  1.58it/s]Extractor Predicting: 215it [02:18,  1.54it/s]Extractor Predicting: 216it [02:18,  1.56it/s]Extractor Predicting: 217it [02:19,  1.56it/s]Extractor Predicting: 218it [02:20,  1.58it/s]Extractor Predicting: 219it [02:20,  1.61it/s]Extractor Predicting: 220it [02:21,  1.59it/s]Extractor Predicting: 221it [02:21,  1.61it/s]Extractor Predicting: 222it [02:22,  1.63it/s]Extractor Predicting: 223it [02:23,  1.59it/s]Extractor Predicting: 224it [02:23,  1.60it/s]Extractor Predicting: 225it [02:24,  1.60it/s]Extractor Predicting: 226it [02:25,  1.55it/s]Extractor Predicting: 227it [02:25,  1.55it/s]Extractor Predicting: 228it [02:26,  1.54it/s]Extractor Predicting: 229it [02:27,  1.54it/s]Extractor Predicting: 230it [02:27,  1.53it/s]Extractor Predicting: 231it [02:28,  1.55it/s]Extractor Predicting: 232it [02:28,  1.59it/s]Extractor Predicting: 233it [02:29,  1.63it/s]Extractor Predicting: 234it [02:30,  1.65it/s]Extractor Predicting: 235it [02:30,  1.65it/s]Extractor Predicting: 236it [02:31,  1.70it/s]Extractor Predicting: 237it [02:31,  1.71it/s]Extractor Predicting: 238it [02:32,  1.71it/s]Extractor Predicting: 239it [02:33,  1.67it/s]Extractor Predicting: 240it [02:33,  1.70it/s]Extractor Predicting: 241it [02:34,  1.70it/s]Extractor Predicting: 242it [02:34,  1.71it/s]Extractor Predicting: 243it [02:35,  1.75it/s]Extractor Predicting: 244it [02:35,  1.73it/s]Extractor Predicting: 245it [02:36,  1.80it/s]Extractor Predicting: 246it [02:36,  1.83it/s]Extractor Predicting: 247it [02:37,  1.73it/s]Extractor Predicting: 248it [02:38,  1.69it/s]Extractor Predicting: 249it [02:38,  1.71it/s]Extractor Predicting: 250it [02:39,  1.72it/s]Extractor Predicting: 251it [02:39,  1.75it/s]Extractor Predicting: 252it [02:40,  1.76it/s]Extractor Predicting: 253it [02:41,  1.72it/s]Extractor Predicting: 254it [02:41,  1.71it/s]Extractor Predicting: 255it [02:42,  1.74it/s]Extractor Predicting: 256it [02:42,  1.74it/s]Extractor Predicting: 257it [02:43,  1.77it/s]Extractor Predicting: 258it [02:43,  1.77it/s]Extractor Predicting: 259it [02:44,  1.75it/s]Extractor Predicting: 260it [02:45,  1.74it/s]Extractor Predicting: 261it [02:45,  1.65it/s]Extractor Predicting: 262it [02:46,  1.66it/s]Extractor Predicting: 263it [02:47,  1.61it/s]Extractor Predicting: 264it [02:47,  1.55it/s]Extractor Predicting: 265it [02:48,  1.54it/s]Extractor Predicting: 266it [02:49,  1.55it/s]Extractor Predicting: 267it [02:49,  1.59it/s]Extractor Predicting: 268it [02:50,  1.59it/s]Extractor Predicting: 269it [02:50,  1.49it/s]Extractor Predicting: 270it [02:51,  1.49it/s]Extractor Predicting: 271it [02:52,  1.49it/s]Extractor Predicting: 272it [02:52,  1.53it/s]Extractor Predicting: 273it [02:53,  1.53it/s]Extractor Predicting: 274it [02:54,  1.47it/s]Extractor Predicting: 275it [02:54,  1.50it/s]Extractor Predicting: 276it [02:55,  1.51it/s]Extractor Predicting: 277it [02:56,  1.50it/s]Extractor Predicting: 278it [02:56,  1.51it/s]Extractor Predicting: 279it [02:57,  1.48it/s]Extractor Predicting: 280it [02:58,  1.48it/s]Extractor Predicting: 281it [02:59,  1.49it/s]Extractor Predicting: 282it [02:59,  1.50it/s]Extractor Predicting: 283it [03:00,  1.51it/s]Extractor Predicting: 284it [03:01,  1.46it/s]Extractor Predicting: 285it [03:01,  1.49it/s]Extractor Predicting: 286it [03:02,  1.50it/s]Extractor Predicting: 287it [03:03,  1.49it/s]Extractor Predicting: 288it [03:03,  1.55it/s]Extractor Predicting: 289it [03:04,  1.52it/s]Extractor Predicting: 290it [03:04,  1.55it/s]Extractor Predicting: 291it [03:05,  1.57it/s]Extractor Predicting: 292it [03:06,  1.57it/s]Extractor Predicting: 293it [03:06,  1.57it/s]Extractor Predicting: 294it [03:07,  1.59it/s]Extractor Predicting: 295it [03:08,  1.56it/s]Extractor Predicting: 296it [03:08,  1.56it/s]Extractor Predicting: 297it [03:09,  1.58it/s]Extractor Predicting: 298it [03:09,  1.63it/s]Extractor Predicting: 299it [03:10,  1.60it/s]Extractor Predicting: 300it [03:11,  1.59it/s]Extractor Predicting: 301it [03:12,  1.39it/s]Extractor Predicting: 302it [03:12,  1.42it/s]Extractor Predicting: 303it [03:13,  1.46it/s]Extractor Predicting: 304it [03:14,  1.48it/s]Extractor Predicting: 305it [03:14,  1.46it/s]Extractor Predicting: 306it [03:15,  1.50it/s]Extractor Predicting: 307it [03:16,  1.52it/s]Extractor Predicting: 308it [03:16,  1.53it/s]Extractor Predicting: 309it [03:17,  1.50it/s]Extractor Predicting: 310it [03:18,  1.50it/s]Extractor Predicting: 311it [03:18,  1.52it/s]Extractor Predicting: 312it [03:19,  1.53it/s]Extractor Predicting: 313it [03:20,  1.54it/s]Extractor Predicting: 314it [03:20,  1.53it/s]Extractor Predicting: 315it [03:21,  1.52it/s]Extractor Predicting: 316it [03:21,  1.53it/s]Extractor Predicting: 317it [03:22,  1.55it/s]Extractor Predicting: 318it [03:23,  1.59it/s]Extractor Predicting: 319it [03:23,  1.59it/s]Extractor Predicting: 320it [03:24,  1.56it/s]Extractor Predicting: 321it [03:25,  1.55it/s]Extractor Predicting: 322it [03:25,  1.55it/s]Extractor Predicting: 323it [03:26,  1.56it/s]Extractor Predicting: 324it [03:27,  1.57it/s]Extractor Predicting: 325it [03:27,  1.55it/s]Extractor Predicting: 326it [03:28,  1.56it/s]Extractor Predicting: 327it [03:28,  1.57it/s]Extractor Predicting: 328it [03:29,  1.56it/s]Extractor Predicting: 329it [03:30,  1.56it/s]Extractor Predicting: 330it [03:30,  1.52it/s]Extractor Predicting: 331it [03:31,  1.53it/s]Extractor Predicting: 332it [03:32,  1.56it/s]Extractor Predicting: 333it [03:32,  1.54it/s]Extractor Predicting: 334it [03:33,  1.57it/s]Extractor Predicting: 335it [03:34,  1.54it/s]Extractor Predicting: 336it [03:34,  1.57it/s]Extractor Predicting: 337it [03:35,  1.56it/s]Extractor Predicting: 338it [03:36,  1.57it/s]Extractor Predicting: 339it [03:36,  1.58it/s]Extractor Predicting: 340it [03:37,  1.56it/s]Extractor Predicting: 341it [03:37,  1.58it/s]Extractor Predicting: 342it [03:38,  1.60it/s]Extractor Predicting: 343it [03:39,  1.58it/s]Extractor Predicting: 344it [03:39,  1.61it/s]Extractor Predicting: 345it [03:40,  1.59it/s]Extractor Predicting: 346it [03:41,  1.59it/s]Extractor Predicting: 347it [03:41,  1.60it/s]Extractor Predicting: 348it [03:42,  1.54it/s]Extractor Predicting: 349it [03:43,  1.56it/s]Extractor Predicting: 350it [03:43,  1.55it/s]Extractor Predicting: 351it [03:44,  1.56it/s]Extractor Predicting: 352it [03:44,  1.56it/s]Extractor Predicting: 353it [03:45,  1.54it/s]Extractor Predicting: 354it [03:46,  1.55it/s]Extractor Predicting: 355it [03:46,  1.54it/s]Extractor Predicting: 356it [03:47,  1.56it/s]Extractor Predicting: 357it [03:48,  1.54it/s]Extractor Predicting: 358it [03:48,  1.55it/s]Extractor Predicting: 359it [03:49,  1.55it/s]Extractor Predicting: 360it [03:50,  1.56it/s]Extractor Predicting: 361it [03:50,  1.56it/s]Extractor Predicting: 362it [03:51,  1.60it/s]Extractor Predicting: 363it [03:52,  1.55it/s]Extractor Predicting: 364it [03:52,  1.59it/s]Extractor Predicting: 365it [03:53,  1.58it/s]Extractor Predicting: 366it [03:53,  1.59it/s]Extractor Predicting: 367it [03:54,  1.58it/s]Extractor Predicting: 368it [03:55,  1.56it/s]Extractor Predicting: 369it [03:55,  1.53it/s]Extractor Predicting: 370it [03:56,  1.54it/s]Extractor Predicting: 371it [03:57,  1.54it/s]Extractor Predicting: 372it [03:57,  1.57it/s]Extractor Predicting: 373it [03:58,  1.51it/s]Extractor Predicting: 374it [03:59,  1.56it/s]Extractor Predicting: 375it [03:59,  1.56it/s]Extractor Predicting: 376it [04:00,  1.55it/s]Extractor Predicting: 377it [04:00,  1.61it/s]Extractor Predicting: 378it [04:01,  1.58it/s]Extractor Predicting: 379it [04:02,  1.61it/s]Extractor Predicting: 380it [04:02,  1.59it/s]Extractor Predicting: 381it [04:03,  1.59it/s]Extractor Predicting: 382it [04:04,  1.58it/s]Extractor Predicting: 383it [04:04,  1.56it/s]Extractor Predicting: 384it [04:05,  1.55it/s]Extractor Predicting: 385it [04:06,  1.57it/s]Extractor Predicting: 386it [04:06,  1.50it/s]Extractor Predicting: 387it [04:07,  1.51it/s]Extractor Predicting: 388it [04:08,  1.48it/s]Extractor Predicting: 389it [04:08,  1.50it/s]Extractor Predicting: 390it [04:09,  1.52it/s]Extractor Predicting: 391it [04:10,  1.52it/s]Extractor Predicting: 392it [04:10,  1.55it/s]Extractor Predicting: 393it [04:11,  1.55it/s]Extractor Predicting: 394it [04:11,  1.56it/s]Extractor Predicting: 395it [04:12,  1.57it/s]Extractor Predicting: 396it [04:13,  1.52it/s]Extractor Predicting: 397it [04:13,  1.54it/s]Extractor Predicting: 398it [04:14,  1.35it/s]Extractor Predicting: 399it [04:15,  1.43it/s]Extractor Predicting: 400it [04:16,  1.44it/s]Extractor Predicting: 401it [04:16,  1.46it/s]Extractor Predicting: 402it [04:17,  1.48it/s]Extractor Predicting: 403it [04:18,  1.48it/s]Extractor Predicting: 404it [04:18,  1.51it/s]Extractor Predicting: 405it [04:19,  1.52it/s]Extractor Predicting: 406it [04:20,  1.54it/s]Extractor Predicting: 407it [04:20,  1.54it/s]Extractor Predicting: 408it [04:21,  1.53it/s]Extractor Predicting: 409it [04:22,  1.55it/s]Extractor Predicting: 410it [04:22,  1.56it/s]Extractor Predicting: 411it [04:23,  1.53it/s]Extractor Predicting: 412it [04:23,  1.56it/s]Extractor Predicting: 413it [04:24,  1.57it/s]Extractor Predicting: 414it [04:25,  1.56it/s]Extractor Predicting: 415it [04:25,  1.54it/s]Extractor Predicting: 416it [04:26,  1.55it/s]Extractor Predicting: 417it [04:27,  1.56it/s]Extractor Predicting: 418it [04:27,  1.59it/s]Extractor Predicting: 419it [04:28,  1.60it/s]Extractor Predicting: 420it [04:29,  1.61it/s]Extractor Predicting: 421it [04:29,  1.57it/s]Extractor Predicting: 422it [04:30,  1.58it/s]Extractor Predicting: 423it [04:30,  1.59it/s]Extractor Predicting: 424it [04:31,  1.59it/s]Extractor Predicting: 425it [04:32,  1.57it/s]Extractor Predicting: 426it [04:32,  1.59it/s]Extractor Predicting: 427it [04:33,  1.58it/s]Extractor Predicting: 428it [04:34,  1.54it/s]Extractor Predicting: 429it [04:34,  1.75it/s]Extractor Predicting: 429it [04:34,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:34,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:34,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:34,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:34,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:34,777 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:50:35,743 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:50:35,744 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:50:36,354 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:50:37,480 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:50:37,480 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:40,447 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:40,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:40,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:40,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:50:40,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:50:41,402 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:50:41,404 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:50:42,010 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:50:42,226 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:50:42,226 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.22614761476147616,
  "recall": 0.09774362964403813,
  "score": 0.1364932771967948,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.71it/s]Extractor Predicting: 5it [00:03,  1.58it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.26666666666666666,
  "recall": 0.03669724770642202,
  "score": 0.06451612903225808,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
