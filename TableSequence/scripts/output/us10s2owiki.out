Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:18<04:19, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:49, 17.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:54<03:41, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:12<03:18, 18.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:31<03:02, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:48<02:41, 17.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:09<02:30, 18.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:25<02:06, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:47<01:55, 19.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:07<01:37, 19.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:26<01:16, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:42<00:55, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:59<00:35, 17.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:19<00:18, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:46<00:00, 21.05s/it]Generating: 100%|██████████| 15/15 [04:46<00:00, 19.09s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8247282608695652, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.80078125, 'errors': {'', "('Sri Lanka', 'member of sports team', '', 'The 2008 Asian Cup , the first Asian Cup in Sri Lanka , was played in Malaysia by Asian team Akshay Kumar Sathy , who was later named in the team of Sri Lanka .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : notable work .', 'success_rate': 0.7903645833333334, 'errors': {'', "('Richard Durbin', 'notable work', '', 'He was a founding member of the Council on Foreign Relations with Richard Durbin in 1799 and is sometimes called the inventor of the printing press .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 358, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 429, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 476, 'raw': 640}
{'target': 600, 'success': 493, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 588, 'raw': 800}
{'target': 600, 'success': 610, 'raw': 832}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.7331730769230769, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Liberal Party of Canada', 'successful candidate', '', 'He was a member of the Liberal Party of Canada ( MP ) from 1994 to 1996 and was elected to Parliament from Ontario in 1996 .')", "('Progressive Conservative Party of Canada', 'successful candidate', '', 'He also ran for the federal government in the 2010 and 2012 elections as the Progressive Conservative Party of Canada ( PPC ) candidate for the Ottawa riding of Etobicoke .')", "('2008', 'successful candidate', '', 'At the 2008 Summer Olympics , she competed as a boxer in two Olympics in Athens ( 2008 ) and Budapest ( 2008 ) .')", "('Royal College of Music', 'successful candidate', '', 'Born in Leeds he studied in the Royal College of Music in Gloucestershire and also worked as a drummer .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : director .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 168, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 209, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 262, 'raw': 416}
{'target': 600, 'success': 278, 'raw': 448}
{'target': 600, 'success': 296, 'raw': 480}
{'target': 600, 'success': 318, 'raw': 512}
{'target': 600, 'success': 341, 'raw': 544}
{'target': 600, 'success': 362, 'raw': 576}
{'target': 600, 'success': 384, 'raw': 608}
{'target': 600, 'success': 407, 'raw': 640}
{'target': 600, 'success': 426, 'raw': 672}
{'target': 600, 'success': 447, 'raw': 704}
{'target': 600, 'success': 466, 'raw': 736}
{'target': 600, 'success': 491, 'raw': 768}
{'target': 600, 'success': 509, 'raw': 800}
{'target': 600, 'success': 534, 'raw': 832}
{'target': 600, 'success': 552, 'raw': 864}
{'target': 600, 'success': 577, 'raw': 896}
{'target': 600, 'success': 596, 'raw': 928}
{'target': 600, 'success': 616, 'raw': 960}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.6416666666666667, 'errors': {'', "('Pittsburgh Pirates', 'drafted by', '', 'He was selected at the third round by the Pittsburgh Pirates during the 2004 MLB Draft and was signed by the Cincinnati Reds in 2006 .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('Jack Kerouac', 'drafted by', '', 'He played 16 games for the St. Louis Blues and Colorado Avalanche during the 1970 NHL season , and in 1982 was traded to Detroit for goaltender Jack Kerouac .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8536931818181818, 'errors': {'', "('Korda', 'lyrics by', '', 'After a few weeks in Berlin , she sang in Korda at the Royal Opera with a different score and is now in Darmstadt and Hamburg singing in the Opera Hall .')"}}
['Relation : main subject . Context : The main characters in the series are The New Man , The Phantom of the Opera at the end of the third season and also a fictional character named The Young Prince of the Opera , played by Richard R. Noyes . Head Entity : The Phantom of Opera , Tail Entity : The New Man .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 81, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 149, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 259, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 309, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 379, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 476, 'raw': 672}
{'target': 600, 'success': 501, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 550, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7094907407407407, 'errors': {'', "('Gajapati', 'main subject', '', 'Sankari s son Sankari , better Known as Gajapati ( , ( R.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.7838541666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1532 ) , he purchased the lands of Lauterbach in Bavaria , near Leipzig ; in May 1534 , he made a grant to the Hanseatic Empire of Hanseatic descent . Head Entity : Hae , Tail Entity : Lauterbach .\n']
['Relation : occupant . Context : Later in the year ( 1532 ) , he purchased the lands of Lauterbach in Bavaria , near Leipzig ; in May 1534 , he made a grant to the Hanseatic Empire of Hanseatic descent . Head Entity : Hae , Tail Entity : Lauterbach .\n', 'Relation : occupant . Context : After he was drafted into the Army under his elder sister , Henry Sommers , he soon decided to join the United States Navy under James Buchanan II , under the command of Admiral Buchanan . Head Entity : Admiral Buchanan II , Tail Entity : James Buchanan .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 622, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 419, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 484, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : use .', 'success_rate': 0.7060185185185185, 'errors': {'', "('this', 'use', '', 'A number of variants of this theme have been used by The Beat in the 1960s ; one such is a theme in which a girl runs for life on the streets carrying a bag filled with chocolate candy .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 48, 'raw': 96}
{'target': 600, 'success': 65, 'raw': 128}
{'target': 600, 'success': 84, 'raw': 160}
{'target': 600, 'success': 100, 'raw': 192}
{'target': 600, 'success': 115, 'raw': 224}
{'target': 600, 'success': 130, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 168, 'raw': 320}
{'target': 600, 'success': 188, 'raw': 352}
{'target': 600, 'success': 205, 'raw': 384}
{'target': 600, 'success': 221, 'raw': 416}
{'target': 600, 'success': 241, 'raw': 448}
{'target': 600, 'success': 261, 'raw': 480}
{'target': 600, 'success': 277, 'raw': 512}
{'target': 600, 'success': 290, 'raw': 544}
{'target': 600, 'success': 309, 'raw': 576}
{'target': 600, 'success': 330, 'raw': 608}
{'target': 600, 'success': 343, 'raw': 640}
{'target': 600, 'success': 360, 'raw': 672}
{'target': 600, 'success': 373, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 421, 'raw': 800}
{'target': 600, 'success': 434, 'raw': 832}
{'target': 600, 'success': 450, 'raw': 864}
{'target': 600, 'success': 465, 'raw': 896}
{'target': 600, 'success': 485, 'raw': 928}
{'target': 600, 'success': 499, 'raw': 960}
{'target': 600, 'success': 520, 'raw': 992}
{'target': 600, 'success': 536, 'raw': 1024}
{'target': 600, 'success': 551, 'raw': 1056}
{'target': 600, 'success': 572, 'raw': 1088}
{'target': 600, 'success': 585, 'raw': 1120}
{'target': 600, 'success': 604, 'raw': 1152}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5243055555555556, 'errors': {'', "('John Cleese', 'voice type', '', 'Another member that was named after John Cleese is Henry Maclean , who was given the task of handling the music , alongside Bill Cunningham .')", "('Goodbye Daddy', 'voice type', '', 'Her other music is featured in the 2004 movie , Goodbye Daddy .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 14833
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14933, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:13, 13.95s/it]Extractor Estimating: 2it [00:17,  7.74s/it]Extractor Estimating: 3it [00:18,  4.52s/it]Extractor Estimating: 4it [00:20,  3.64s/it]Extractor Estimating: 5it [00:20,  2.55s/it]Extractor Estimating: 6it [00:21,  1.92s/it]Extractor Estimating: 7it [00:22,  1.49s/it]Extractor Estimating: 8it [00:22,  1.22s/it]Extractor Estimating: 9it [00:23,  1.03s/it]Extractor Estimating: 10it [00:24,  1.10it/s]Extractor Estimating: 11it [00:24,  1.22it/s]Extractor Estimating: 12it [00:25,  1.34it/s]Extractor Estimating: 13it [00:26,  1.38it/s]Extractor Estimating: 14it [00:26,  1.42it/s]Extractor Estimating: 15it [00:27,  1.48it/s]Extractor Estimating: 16it [00:27,  1.45it/s]Extractor Estimating: 17it [00:30,  1.20s/it]Extractor Estimating: 18it [00:31,  1.09s/it]Extractor Estimating: 19it [00:31,  1.03it/s]Extractor Estimating: 20it [00:32,  1.16it/s]Extractor Estimating: 21it [00:33,  1.26it/s]Extractor Estimating: 22it [00:33,  1.35it/s]Extractor Estimating: 23it [00:34,  1.36it/s]Extractor Estimating: 24it [00:35,  1.41it/s]Extractor Estimating: 25it [00:35,  1.42it/s]Extractor Estimating: 26it [00:36,  1.52it/s]Extractor Estimating: 27it [00:36,  1.58it/s]Extractor Estimating: 28it [00:37,  1.58it/s]Extractor Estimating: 29it [00:38,  1.63it/s]Extractor Estimating: 30it [00:38,  1.64it/s]Extractor Estimating: 31it [00:39,  1.63it/s]Extractor Estimating: 32it [00:39,  1.67it/s]Extractor Estimating: 33it [00:40,  1.65it/s]Extractor Estimating: 34it [00:41,  1.64it/s]Extractor Estimating: 35it [00:41,  1.65it/s]Extractor Estimating: 36it [00:42,  1.68it/s]Extractor Estimating: 37it [00:42,  1.66it/s]Extractor Estimating: 38it [00:43,  1.67it/s]Extractor Estimating: 39it [00:44,  1.65it/s]Extractor Estimating: 40it [00:44,  1.62it/s]Extractor Estimating: 41it [00:45,  1.61it/s]Extractor Estimating: 42it [00:46,  1.58it/s]Extractor Estimating: 43it [00:46,  1.56it/s]Extractor Estimating: 44it [00:47,  1.59it/s]Extractor Estimating: 45it [00:47,  1.64it/s]Extractor Estimating: 46it [00:48,  1.67it/s]Extractor Estimating: 47it [00:49,  1.68it/s]Extractor Estimating: 48it [00:49,  1.70it/s]Extractor Estimating: 49it [00:50,  1.65it/s]Extractor Estimating: 50it [00:50,  1.66it/s]Extractor Estimating: 51it [00:51,  1.57it/s]Extractor Estimating: 52it [00:52,  1.58it/s]Extractor Estimating: 53it [00:52,  1.57it/s]Extractor Estimating: 54it [00:53,  1.56it/s]Extractor Estimating: 55it [00:54,  1.55it/s]Extractor Estimating: 56it [00:54,  1.49it/s]Extractor Estimating: 57it [00:55,  1.46it/s]Extractor Estimating: 58it [00:56,  1.51it/s]Extractor Estimating: 59it [00:56,  1.52it/s]Extractor Estimating: 60it [00:57,  1.49it/s]Extractor Estimating: 61it [00:58,  1.46it/s]Extractor Estimating: 62it [00:59,  1.44it/s]Extractor Estimating: 63it [00:59,  1.45it/s]Extractor Estimating: 64it [01:00,  1.44it/s]Extractor Estimating: 65it [01:01,  1.47it/s]Extractor Estimating: 66it [01:01,  1.51it/s]Extractor Estimating: 67it [01:02,  1.49it/s]Extractor Estimating: 68it [01:03,  1.50it/s]Extractor Estimating: 69it [01:03,  1.47it/s]Extractor Estimating: 70it [01:04,  1.50it/s]Extractor Estimating: 71it [01:05,  1.51it/s]Extractor Estimating: 72it [01:05,  1.53it/s]Extractor Estimating: 73it [01:06,  1.54it/s]Extractor Estimating: 74it [01:07,  1.46it/s]Extractor Estimating: 75it [01:07,  1.50it/s]Extractor Estimating: 76it [01:08,  1.55it/s]Extractor Estimating: 77it [01:08,  1.56it/s]Extractor Estimating: 78it [01:09,  1.49it/s]Extractor Estimating: 79it [01:10,  1.50it/s]Extractor Estimating: 80it [01:11,  1.43it/s]Extractor Estimating: 81it [01:11,  1.48it/s]Extractor Estimating: 82it [01:12,  1.49it/s]Extractor Estimating: 83it [01:13,  1.51it/s]Extractor Estimating: 84it [01:13,  1.52it/s]Extractor Estimating: 85it [01:14,  1.52it/s]Extractor Estimating: 86it [01:14,  1.55it/s]Extractor Estimating: 87it [01:15,  1.54it/s]Extractor Estimating: 88it [01:16,  1.55it/s]Extractor Estimating: 89it [01:16,  1.56it/s]Extractor Estimating: 90it [01:17,  1.54it/s]Extractor Estimating: 91it [01:18,  1.53it/s]Extractor Estimating: 92it [01:18,  1.51it/s]Extractor Estimating: 93it [01:19,  1.51it/s]Extractor Estimating: 94it [01:20,  1.56it/s]Extractor Estimating: 95it [01:20,  1.44it/s]Extractor Estimating: 96it [01:21,  1.52it/s]Extractor Estimating: 97it [01:22,  1.57it/s]Extractor Estimating: 98it [01:22,  1.57it/s]Extractor Estimating: 99it [01:23,  1.59it/s]Extractor Estimating: 100it [01:24,  1.57it/s]Extractor Estimating: 101it [01:24,  1.61it/s]Extractor Estimating: 102it [01:25,  1.55it/s]Extractor Estimating: 103it [01:25,  1.62it/s]Extractor Estimating: 104it [01:28,  1.18s/it]Extractor Estimating: 105it [01:29,  1.03s/it]Extractor Estimating: 106it [01:29,  1.09it/s]Extractor Estimating: 107it [01:30,  1.19it/s]Extractor Estimating: 108it [01:30,  1.31it/s]Extractor Estimating: 109it [01:31,  1.41it/s]Extractor Estimating: 110it [01:32,  1.44it/s]Extractor Estimating: 111it [01:32,  1.50it/s]Extractor Estimating: 112it [01:33,  1.51it/s]Extractor Estimating: 113it [01:34,  1.54it/s]Extractor Estimating: 114it [01:34,  1.57it/s]Extractor Estimating: 115it [01:35,  1.61it/s]Extractor Estimating: 116it [01:35,  1.63it/s]Extractor Estimating: 117it [01:36,  1.61it/s]Extractor Estimating: 118it [01:37,  1.61it/s]Extractor Estimating: 119it [01:37,  1.60it/s]Extractor Estimating: 120it [01:38,  1.58it/s]Extractor Estimating: 121it [01:39,  1.61it/s]Extractor Estimating: 122it [01:39,  1.60it/s]Extractor Estimating: 123it [01:40,  1.60it/s]Extractor Estimating: 124it [01:40,  1.63it/s]Extractor Estimating: 125it [01:41,  1.62it/s]Extractor Estimating: 126it [01:42,  1.60it/s]Extractor Estimating: 127it [01:42,  1.56it/s]Extractor Estimating: 128it [01:43,  1.55it/s]Extractor Estimating: 129it [01:44,  1.50it/s]Extractor Estimating: 130it [01:44,  1.56it/s]Extractor Estimating: 131it [01:45,  1.61it/s]Extractor Estimating: 132it [01:46,  1.54it/s]Extractor Estimating: 133it [01:46,  1.51it/s]Extractor Estimating: 134it [01:47,  1.50it/s]Extractor Estimating: 135it [01:48,  1.47it/s]Extractor Estimating: 136it [01:48,  1.48it/s]Extractor Estimating: 137it [01:49,  1.49it/s]Extractor Estimating: 138it [01:50,  1.50it/s]Extractor Estimating: 139it [01:50,  1.50it/s]Extractor Estimating: 140it [01:51,  1.43it/s]Extractor Estimating: 141it [01:52,  1.41it/s]Extractor Estimating: 142it [01:52,  1.44it/s]Extractor Estimating: 143it [01:53,  1.48it/s]Extractor Estimating: 144it [01:54,  1.48it/s]Extractor Estimating: 145it [01:54,  1.46it/s]Extractor Estimating: 146it [01:55,  1.47it/s]Extractor Estimating: 147it [01:56,  1.48it/s]Extractor Estimating: 148it [01:57,  1.44it/s]Extractor Estimating: 149it [01:57,  1.46it/s]Extractor Estimating: 150it [01:58,  1.45it/s]Extractor Estimating: 151it [01:59,  1.49it/s]Extractor Estimating: 152it [01:59,  1.51it/s]Extractor Estimating: 153it [02:00,  1.51it/s]Extractor Estimating: 154it [02:01,  1.49it/s]Extractor Estimating: 155it [02:01,  1.52it/s]Extractor Estimating: 156it [02:02,  1.56it/s]Extractor Estimating: 157it [02:02,  1.54it/s]Extractor Estimating: 158it [02:03,  1.54it/s]Extractor Estimating: 159it [02:04,  1.56it/s]Extractor Estimating: 160it [02:04,  1.58it/s]Extractor Estimating: 161it [02:05,  1.55it/s]Extractor Estimating: 162it [02:06,  1.56it/s]Extractor Estimating: 163it [02:06,  1.56it/s]Extractor Estimating: 164it [02:07,  1.55it/s]Extractor Estimating: 165it [02:08,  1.54it/s]Extractor Estimating: 166it [02:08,  1.41it/s]Extractor Estimating: 167it [02:09,  1.46it/s]Extractor Estimating: 168it [02:10,  1.53it/s]Extractor Estimating: 169it [02:10,  1.55it/s]Extractor Estimating: 170it [02:11,  1.53it/s]Extractor Estimating: 171it [02:12,  1.55it/s]Extractor Estimating: 172it [02:12,  1.56it/s]Extractor Estimating: 173it [02:13,  1.56it/s]Extractor Estimating: 174it [02:13,  1.57it/s]Extractor Estimating: 175it [02:14,  1.56it/s]Extractor Estimating: 176it [02:15,  1.55it/s]Extractor Estimating: 177it [02:15,  1.50it/s]Extractor Estimating: 178it [02:16,  1.52it/s]Extractor Estimating: 179it [02:17,  1.54it/s]Extractor Estimating: 180it [02:17,  1.49it/s]Extractor Estimating: 181it [02:18,  1.47it/s]Extractor Estimating: 182it [02:19,  1.48it/s]Extractor Estimating: 183it [02:20,  1.45it/s]Extractor Estimating: 184it [02:20,  1.40it/s]Extractor Estimating: 185it [02:21,  1.39it/s]Extractor Estimating: 186it [02:22,  1.44it/s]Extractor Estimating: 187it [02:22,  1.44it/s]Extractor Estimating: 188it [02:23,  1.45it/s]Extractor Estimating: 189it [02:24,  1.50it/s]Extractor Estimating: 190it [02:24,  1.49it/s]Extractor Estimating: 191it [02:25,  1.44it/s]Extractor Estimating: 192it [02:26,  1.46it/s]Extractor Estimating: 193it [02:26,  1.44it/s]Extractor Estimating: 194it [02:27,  1.42it/s]Extractor Estimating: 195it [02:28,  1.46it/s]Extractor Estimating: 196it [02:28,  1.50it/s]Extractor Estimating: 197it [02:29,  1.52it/s]Extractor Estimating: 198it [02:30,  1.55it/s]Extractor Estimating: 199it [02:30,  1.51it/s]Extractor Estimating: 200it [02:31,  1.48it/s]Extractor Estimating: 201it [02:32,  1.50it/s]Extractor Estimating: 202it [02:32,  1.50it/s]Extractor Estimating: 203it [02:33,  1.48it/s]Extractor Estimating: 204it [02:34,  1.49it/s]Extractor Estimating: 205it [02:34,  1.50it/s]Extractor Estimating: 206it [02:35,  1.53it/s]Extractor Estimating: 207it [02:36,  1.43it/s]Extractor Estimating: 208it [02:37,  1.44it/s]Extractor Estimating: 209it [02:38,  1.27it/s]Extractor Estimating: 210it [02:38,  1.29it/s]Extractor Estimating: 211it [02:39,  1.37it/s]Extractor Estimating: 212it [02:40,  1.41it/s]Extractor Estimating: 213it [02:40,  1.43it/s]Extractor Estimating: 214it [02:41,  1.48it/s]Extractor Estimating: 215it [02:42,  1.43it/s]Extractor Estimating: 216it [02:42,  1.40it/s]Extractor Estimating: 217it [02:43,  1.39it/s]Extractor Estimating: 218it [02:44,  1.48it/s]Extractor Estimating: 219it [02:44,  1.51it/s]Extractor Estimating: 220it [02:45,  1.50it/s]Extractor Estimating: 221it [02:46,  1.55it/s]Extractor Estimating: 222it [02:46,  1.49it/s]Extractor Estimating: 223it [02:47,  1.46it/s]Extractor Estimating: 224it [02:48,  1.47it/s]Extractor Estimating: 225it [02:48,  1.47it/s]Extractor Estimating: 226it [02:49,  1.46it/s]Extractor Estimating: 227it [02:50,  1.50it/s]Extractor Estimating: 228it [02:50,  1.50it/s]Extractor Estimating: 229it [02:51,  1.50it/s]Extractor Estimating: 230it [02:52,  1.50it/s]Extractor Estimating: 231it [02:52,  1.49it/s]Extractor Estimating: 232it [02:53,  1.44it/s]Extractor Estimating: 233it [02:54,  1.42it/s]Extractor Estimating: 234it [02:55,  1.41it/s]Extractor Estimating: 235it [02:55,  1.43it/s]Extractor Estimating: 236it [02:56,  1.42it/s]Extractor Estimating: 237it [02:57,  1.38it/s]Extractor Estimating: 238it [02:58,  1.35it/s]Extractor Estimating: 239it [02:58,  1.28it/s]Extractor Estimating: 240it [02:59,  1.29it/s]Extractor Estimating: 241it [03:00,  1.30it/s]Extractor Estimating: 242it [03:01,  1.34it/s]Extractor Estimating: 243it [03:01,  1.36it/s]Extractor Estimating: 244it [03:02,  1.39it/s]Extractor Estimating: 245it [03:03,  1.38it/s]Extractor Estimating: 246it [03:04,  1.36it/s]Extractor Estimating: 247it [03:04,  1.36it/s]Extractor Estimating: 248it [03:05,  1.43it/s]Extractor Estimating: 249it [03:06,  1.41it/s]Extractor Estimating: 250it [03:06,  1.43it/s]Extractor Estimating: 251it [03:07,  1.44it/s]Extractor Estimating: 252it [03:08,  1.46it/s]Extractor Estimating: 253it [03:08,  1.55it/s]Extractor Estimating: 254it [03:09,  1.56it/s]Extractor Estimating: 255it [03:09,  1.54it/s]Extractor Estimating: 256it [03:10,  1.54it/s]Extractor Estimating: 257it [03:11,  1.52it/s]Extractor Estimating: 258it [03:12,  1.48it/s]Extractor Estimating: 259it [03:12,  1.51it/s]Extractor Estimating: 260it [03:13,  1.56it/s]Extractor Estimating: 261it [03:13,  1.54it/s]Extractor Estimating: 262it [03:14,  1.58it/s]Extractor Estimating: 263it [03:15,  1.55it/s]Extractor Estimating: 264it [03:15,  1.56it/s]Extractor Estimating: 265it [03:16,  1.57it/s]Extractor Estimating: 266it [03:17,  1.52it/s]Extractor Estimating: 267it [03:17,  1.51it/s]Extractor Estimating: 268it [03:18,  1.54it/s]Extractor Estimating: 269it [03:19,  1.56it/s]Extractor Estimating: 270it [03:19,  1.60it/s]Extractor Estimating: 271it [03:20,  1.53it/s]Extractor Estimating: 272it [03:21,  1.51it/s]Extractor Estimating: 273it [03:21,  1.51it/s]Extractor Estimating: 274it [03:22,  1.49it/s]Extractor Estimating: 275it [03:23,  1.48it/s]Extractor Estimating: 276it [03:23,  1.50it/s]Extractor Estimating: 277it [03:24,  1.57it/s]Extractor Estimating: 278it [03:24,  1.56it/s]Extractor Estimating: 279it [03:25,  1.58it/s]Extractor Estimating: 280it [03:26,  1.55it/s]Extractor Estimating: 281it [03:26,  1.53it/s]Extractor Estimating: 282it [03:27,  1.52it/s]Extractor Estimating: 283it [03:28,  1.59it/s]Extractor Estimating: 284it [03:28,  1.60it/s]Extractor Estimating: 285it [03:29,  1.62it/s]Extractor Estimating: 286it [03:29,  1.62it/s]Extractor Estimating: 287it [03:30,  1.60it/s]Extractor Estimating: 288it [03:31,  1.56it/s]Extractor Estimating: 289it [03:31,  1.56it/s]Extractor Estimating: 290it [03:32,  1.54it/s]Extractor Estimating: 291it [03:33,  1.45it/s]Extractor Estimating: 292it [03:34,  1.46it/s]Extractor Estimating: 293it [03:34,  1.52it/s]Extractor Estimating: 294it [03:35,  1.54it/s]Extractor Estimating: 295it [03:35,  1.60it/s]Extractor Estimating: 296it [03:36,  1.58it/s]Extractor Estimating: 297it [03:37,  1.63it/s]Extractor Estimating: 298it [03:37,  1.60it/s]Extractor Estimating: 299it [03:38,  1.61it/s]Extractor Estimating: 300it [03:38,  1.58it/s]Extractor Estimating: 301it [03:39,  1.59it/s]Extractor Estimating: 302it [03:40,  1.61it/s]Extractor Estimating: 303it [03:40,  1.58it/s]Extractor Estimating: 304it [03:41,  1.62it/s]Extractor Estimating: 305it [03:42,  1.61it/s]Extractor Estimating: 306it [03:42,  1.50it/s]Extractor Estimating: 307it [03:43,  1.52it/s]Extractor Estimating: 308it [03:44,  1.50it/s]Extractor Estimating: 309it [03:44,  1.53it/s]Extractor Estimating: 310it [03:45,  1.56it/s]Extractor Estimating: 311it [03:46,  1.58it/s]Extractor Estimating: 312it [03:46,  1.56it/s]Extractor Estimating: 313it [03:47,  1.58it/s]Extractor Estimating: 314it [03:48,  1.53it/s]Extractor Estimating: 315it [03:48,  1.56it/s]Extractor Estimating: 316it [03:49,  1.59it/s]Extractor Estimating: 317it [03:49,  1.57it/s]Extractor Estimating: 318it [03:50,  1.51it/s]Extractor Estimating: 319it [03:51,  1.52it/s]Extractor Estimating: 320it [03:51,  1.58it/s]Extractor Estimating: 321it [03:52,  1.50it/s]Extractor Estimating: 322it [03:53,  1.54it/s]Extractor Estimating: 323it [03:53,  1.57it/s]Extractor Estimating: 324it [03:54,  1.53it/s]Extractor Estimating: 325it [03:55,  1.54it/s]Extractor Estimating: 326it [03:55,  1.53it/s]Extractor Estimating: 327it [03:56,  1.57it/s]Extractor Estimating: 328it [03:57,  1.57it/s]Extractor Estimating: 329it [03:57,  1.56it/s]Extractor Estimating: 330it [03:58,  1.57it/s]Extractor Estimating: 331it [03:58,  1.57it/s]Extractor Estimating: 332it [03:59,  1.55it/s]Extractor Estimating: 333it [04:00,  1.53it/s]Extractor Estimating: 334it [04:00,  1.52it/s]Extractor Estimating: 335it [04:01,  1.57it/s]Extractor Estimating: 336it [04:02,  1.58it/s]Extractor Estimating: 337it [04:02,  1.58it/s]Extractor Estimating: 338it [04:03,  1.53it/s]Extractor Estimating: 339it [04:04,  1.56it/s]Extractor Estimating: 340it [04:04,  1.50it/s]Extractor Estimating: 341it [04:05,  1.53it/s]Extractor Estimating: 342it [04:06,  1.48it/s]Extractor Estimating: 343it [04:06,  1.51it/s]Extractor Estimating: 344it [04:07,  1.46it/s]Extractor Estimating: 345it [04:08,  1.49it/s]Extractor Estimating: 346it [04:08,  1.55it/s]Extractor Estimating: 347it [04:09,  1.54it/s]Extractor Estimating: 348it [04:10,  1.48it/s]Extractor Estimating: 349it [04:10,  1.53it/s]Extractor Estimating: 350it [04:11,  1.51it/s]Extractor Estimating: 351it [04:12,  1.56it/s]Extractor Estimating: 352it [04:12,  1.49it/s]Extractor Estimating: 353it [04:13,  1.49it/s]Extractor Estimating: 354it [04:14,  1.47it/s]Extractor Estimating: 355it [04:14,  1.49it/s]Extractor Estimating: 356it [04:15,  1.55it/s]Extractor Estimating: 357it [04:16,  1.56it/s]Extractor Estimating: 358it [04:16,  1.56it/s]Extractor Estimating: 359it [04:17,  1.55it/s]Extractor Estimating: 360it [04:17,  1.57it/s]Extractor Estimating: 361it [04:18,  1.53it/s]Extractor Estimating: 362it [04:19,  1.57it/s]Extractor Estimating: 363it [04:19,  1.64it/s]Extractor Estimating: 364it [04:20,  1.63it/s]Extractor Estimating: 365it [04:21,  1.62it/s]Extractor Estimating: 366it [04:21,  1.64it/s]Extractor Estimating: 367it [04:22,  1.60it/s]Extractor Estimating: 368it [04:22,  1.59it/s]Extractor Estimating: 369it [04:23,  1.57it/s]Extractor Estimating: 370it [04:24,  1.56it/s]Extractor Estimating: 371it [04:24,  1.56it/s]Extractor Estimating: 372it [04:25,  1.54it/s]Extractor Estimating: 373it [04:26,  1.57it/s]Extractor Estimating: 374it [04:26,  1.56it/s]Extractor Estimating: 375it [04:27,  1.49it/s]Extractor Estimating: 375it [04:27,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1545 mean pseudo reward: 0.9349236317993248
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 17386
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17486, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17486, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 35, avg_time 1.396, loss:500.8141
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 5, avg_time 1.087, loss:391.7929
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 40, avg_time 1.090, loss:338.2637
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 10, avg_time 1.089, loss:310.3393
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 45, avg_time 1.092, loss:280.3073
>> valid entity prec:0.5034, rec:0.4585, f1:0.4799
>> valid relation prec:0.0315, rec:0.0194, f1:0.0240
>> valid relation with NER prec:0.0315, rec:0.0194, f1:0.0240
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 15, avg_time 3.233, loss:251.0823
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 50, avg_time 1.086, loss:241.3039
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 20, avg_time 1.080, loss:220.9919
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 55, avg_time 1.092, loss:209.1129
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 25, avg_time 1.085, loss:209.1598
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5167, rec:0.4494, f1:0.4807
>> valid relation prec:0.0220, rec:0.0144, f1:0.0174
>> valid relation with NER prec:0.0220, rec:0.0144, f1:0.0174
new max entity f1 on valid!
g_step 1100, step 60, avg_time 3.227, loss:193.9703
g_step 1200, step 30, avg_time 1.080, loss:176.1496
g_step 1300, step 65, avg_time 1.088, loss:174.2145
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:52:23 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:52:23 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-52-23_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:52:24 - WARNING - datasets.builder -   Using custom data configuration default-912722184614ea09
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-912722184614ea09/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:52:24,864 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:24,865 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:52:24,866 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:24,867 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:52:24,876 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:52:24,883 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:52:24,883 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:52:24,883 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:52:24,883 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:52:24,883 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:52:24,883 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:52:25,035 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:52:28,177 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:52:28,302 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-912722184614ea09/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 19:52:28 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14f9d145ec20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.93ba/s]100%|██████████| 2/2 [00:00<00:00,  3.54ba/s]100%|██████████| 2/2 [00:00<00:00,  3.15ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.79ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.22ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.38ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.45ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.46ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.50ba/s]100%|██████████| 7/7 [00:01<00:00,  4.92ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  5.10ba/s]100%|██████████| 2/2 [00:00<00:00,  8.03ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  5.80ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.68ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  8.59ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  9.54ba/s]100%|██████████| 7/7 [00:00<00:00, 10.10ba/s]
[INFO|trainer.py:414] 2023-08-28 19:52:32,260 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:52:32,286 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:52:32,286 >>   Num examples = 1545
[INFO|trainer.py:1149] 2023-08-28 19:52:32,286 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:52:32,286 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:52:32,286 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:52:32,286 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:52:32,286 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:00<00:39,  3.01it/s]  2%|▏         | 2/120 [00:00<00:36,  3.27it/s]  2%|▎         | 3/120 [00:00<00:34,  3.37it/s]  3%|▎         | 4/120 [00:01<00:33,  3.42it/s]  4%|▍         | 5/120 [00:01<00:33,  3.44it/s]  5%|▌         | 6/120 [00:01<00:32,  3.46it/s]  6%|▌         | 7/120 [00:02<00:32,  3.47it/s]  7%|▋         | 8/120 [00:02<00:32,  3.47it/s]  8%|▊         | 9/120 [00:02<00:31,  3.48it/s]  8%|▊         | 10/120 [00:02<00:31,  3.48it/s]  9%|▉         | 11/120 [00:03<00:31,  3.48it/s] 10%|█         | 12/120 [00:03<00:30,  3.49it/s] 11%|█         | 13/120 [00:03<00:30,  3.49it/s] 12%|█▏        | 14/120 [00:04<00:30,  3.49it/s] 12%|█▎        | 15/120 [00:04<00:30,  3.49it/s] 13%|█▎        | 16/120 [00:04<00:30,  3.39it/s] 14%|█▍        | 17/120 [00:04<00:30,  3.42it/s] 15%|█▌        | 18/120 [00:05<00:29,  3.44it/s] 16%|█▌        | 19/120 [00:05<00:29,  3.45it/s] 17%|█▋        | 20/120 [00:05<00:28,  3.46it/s] 18%|█▊        | 21/120 [00:06<00:28,  3.47it/s] 18%|█▊        | 22/120 [00:06<00:28,  3.47it/s] 19%|█▉        | 23/120 [00:06<00:27,  3.47it/s] 20%|██        | 24/120 [00:06<00:27,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 19:52:39,286 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:52:39,286 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 19:52:39,286 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.73it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.04it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.41it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.77it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.29it/s][A
  4%|▍         | 33/782 [00:00<00:15, 48.07it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.80it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.66it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.62it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.64it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.53it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.43it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.47it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.48it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.42it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.44it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.44it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.38it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.38it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.38it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.42it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.45it/s][A
 15%|█▌        | 118/782 [00:02<00:15, 42.60it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 44.06it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 44.99it/s][A
 17%|█▋        | 133/782 [00:02<00:14, 45.81it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.40it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.77it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.03it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.20it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.15it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.26it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.13it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.28it/s][A
 23%|██▎       | 178/782 [00:03<00:14, 42.39it/s][A
 23%|██▎       | 183/782 [00:03<00:13, 43.87it/s][A
 24%|██▍       | 188/782 [00:04<00:13, 44.96it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 45.76it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.33it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.64it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.94it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.19it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.18it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.12it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.12it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.32it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.43it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.47it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.51it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.57it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.62it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.57it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.43it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.42it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.38it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 47.34it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.50it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.47it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.04it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.21it/s][A
 39%|███▉      | 308/782 [00:06<00:09, 47.42it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.47it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.39it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.33it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.36it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.39it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.49it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.41it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.48it/s][A
 45%|████▌     | 353/782 [00:07<00:10, 39.95it/s][A
 46%|████▌     | 358/782 [00:07<00:10, 42.06it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 43.57it/s][A
 47%|████▋     | 368/782 [00:07<00:09, 44.67it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 45.46it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.08it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.54it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.86it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.76it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.00it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.12it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.22it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.36it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.34it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 47.36it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.96it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.13it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.06it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.05it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.08it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.21it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.22it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.40it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.37it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.31it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.29it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.30it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.26it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.26it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.22it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.31it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.35it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.38it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 47.45it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.42it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.29it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.32it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.33it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.20it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.22it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.41it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.81it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 47.03it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.18it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.94it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.09it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.19it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.22it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.80it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.94it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.95it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.12it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.24it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.30it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.20it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.16it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.23it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.22it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.17it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.19it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.30it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 47.33it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.42it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.18it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.24it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.20it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.22it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.24it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.11it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.18it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.24it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.32it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.34it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.26it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.18it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.09it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.19it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.22it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.18it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.20it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 47.27it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.36it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.32it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.27it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.15it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.12it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:23<00:27,  3.48it/s]
100%|██████████| 782/782 [00:16<00:00, 47.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:52:56,021 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 19:52:56,120 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:53:01,082 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:53:01,143 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:53:01,158 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [00:40<16:07, 10.18s/it] 22%|██▏       | 26/120 [00:40<11:19,  7.23s/it] 22%|██▎       | 27/120 [00:40<07:58,  5.15s/it] 23%|██▎       | 28/120 [00:41<05:39,  3.69s/it] 24%|██▍       | 29/120 [00:41<04:02,  2.67s/it] 25%|██▌       | 30/120 [00:41<02:55,  1.95s/it] 26%|██▌       | 31/120 [00:41<02:09,  1.45s/it] 27%|██▋       | 32/120 [00:42<01:37,  1.10s/it] 28%|██▊       | 33/120 [00:42<01:14,  1.16it/s] 28%|██▊       | 34/120 [00:42<00:59,  1.45it/s] 29%|██▉       | 35/120 [00:43<00:48,  1.76it/s] 30%|███       | 36/120 [00:43<00:40,  2.07it/s] 31%|███       | 37/120 [00:43<00:36,  2.26it/s] 32%|███▏      | 38/120 [00:44<00:32,  2.53it/s] 32%|███▎      | 39/120 [00:44<00:29,  2.75it/s] 33%|███▎      | 40/120 [00:44<00:27,  2.94it/s] 34%|███▍      | 41/120 [00:44<00:25,  3.08it/s] 35%|███▌      | 42/120 [00:45<00:24,  3.19it/s] 36%|███▌      | 43/120 [00:45<00:23,  3.27it/s] 37%|███▋      | 44/120 [00:45<00:22,  3.33it/s] 38%|███▊      | 45/120 [00:46<00:22,  3.37it/s] 38%|███▊      | 46/120 [00:46<00:21,  3.41it/s] 39%|███▉      | 47/120 [00:46<00:21,  3.43it/s] 40%|████      | 48/120 [00:46<00:21,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 19:53:19,298 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:53:19,298 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 19:53:19,298 >>   Batch size = 8
{'eval_loss': 0.9170319437980652, 'eval_runtime': 16.6658, 'eval_samples_per_second': 375.2, 'eval_steps_per_second': 46.922, 'epoch': 0.98}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.86it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.14it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.21it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.46it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.12it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.86it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.63it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.51it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.47it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.38it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.37it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.41it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.30it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.26it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.28it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.24it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.27it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.28it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.29it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.26it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.32it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.33it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.26it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.30it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.34it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.31it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.20it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.30it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.06it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.28it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.22it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.31it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.24it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.29it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.31it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.30it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.23it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.27it/s][A
 25%|██▌       | 198/782 [00:04<00:14, 39.78it/s][A
 26%|██▌       | 203/782 [00:04<00:13, 41.70it/s][A
 27%|██▋       | 208/782 [00:04<00:13, 43.24it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 44.34it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 45.21it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 45.84it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.30it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.53it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.70it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.96it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.03it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.15it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.10it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.10it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.13it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.22it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.27it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 47.26it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.17it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.27it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.26it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.24it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.27it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.31it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.18it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.18it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.29it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.24it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.20it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.28it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.28it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.30it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.26it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.21it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.18it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.19it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.26it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.14it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.23it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.17it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.22it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.27it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.27it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.27it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.21it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.23it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.22it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.25it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.25it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.21it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.15it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.24it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.21it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.23it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.23it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.22it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.15it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.20it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.16it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.22it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.24it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.28it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.20it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.27it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 47.18it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.24it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.19it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.21it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.26it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.24it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.21it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.25it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.11it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.20it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.15it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.18it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.16it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.19it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.24it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.22it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.20it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.23it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.23it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.29it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.30it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.21it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.17it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.18it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.21it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.24it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.27it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.22it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.27it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.22it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.32it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.19it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.20it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.25it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 44.99it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 45.67it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.06it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.36it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.67it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.87it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.88it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.00it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.12it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.14it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.11it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.15it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.10it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 47.22it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.23it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.23it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.20it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.17it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.24it/s][A                                                
                                                 [A 40%|████      | 48/120 [01:03<00:21,  3.36it/s]
100%|██████████| 782/782 [00:16<00:00, 47.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:53:35,972 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 19:53:35,997 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:53:40,508 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:53:40,533 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:53:40,546 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [01:18<11:25,  9.65s/it] 42%|████▏     | 50/120 [01:18<07:58,  6.84s/it] 42%|████▎     | 51/120 [01:19<05:36,  4.87s/it] 43%|████▎     | 52/120 [01:19<03:57,  3.50s/it] 44%|████▍     | 53/120 [01:19<02:49,  2.53s/it] 45%|████▌     | 54/120 [01:19<02:02,  1.86s/it] 46%|████▌     | 55/120 [01:20<01:30,  1.39s/it] 47%|████▋     | 56/120 [01:20<01:07,  1.06s/it] 48%|████▊     | 57/120 [01:20<00:52,  1.21it/s] 48%|████▊     | 58/120 [01:21<00:41,  1.51it/s] 49%|████▉     | 59/120 [01:21<00:33,  1.82it/s] 50%|█████     | 60/120 [01:21<00:28,  2.12it/s] 51%|█████     | 61/120 [01:21<00:25,  2.32it/s] 52%|█████▏    | 62/120 [01:22<00:22,  2.58it/s] 52%|█████▎    | 63/120 [01:22<00:20,  2.80it/s] 53%|█████▎    | 64/120 [01:22<00:18,  2.97it/s] 54%|█████▍    | 65/120 [01:23<00:17,  3.11it/s] 55%|█████▌    | 66/120 [01:23<00:16,  3.21it/s] 56%|█████▌    | 67/120 [01:23<00:16,  3.29it/s] 57%|█████▋    | 68/120 [01:23<00:15,  3.34it/s] 57%|█████▊    | 69/120 [01:24<00:15,  3.38it/s] 58%|█████▊    | 70/120 [01:24<00:14,  3.41it/s] 59%|█████▉    | 71/120 [01:24<00:14,  3.43it/s] 60%|██████    | 72/120 [01:25<00:13,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 19:53:57,416 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:53:57,416 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 19:53:57,416 >>   Batch size = 8
{'eval_loss': 0.9357759952545166, 'eval_runtime': 16.639, 'eval_samples_per_second': 375.804, 'eval_steps_per_second': 46.998, 'epoch': 1.98}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.68it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.14it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.18it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.50it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.10it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.79it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.66it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.61it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.42it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.33it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.41it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.30it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.23it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.30it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.26it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.25it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.31it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.27it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.20it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.26it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.26it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.30it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.21it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.22it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.17it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.23it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.29it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.33it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.19it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.19it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.25it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.33it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.34it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.24it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.18it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.22it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.28it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.32it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.27it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.26it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.21it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.23it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.30it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.26it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.26it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.22it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.20it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.26it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.19it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.24it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.37it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.43it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.48it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.51it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.47it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.37it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.06it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.19it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.04it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.19it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.32it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.43it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.43it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.43it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.44it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.29it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.06it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.98it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.12it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.22it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.29it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.29it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.36it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.34it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.43it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.43it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.41it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.24it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.24it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.27it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.33it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.33it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.41it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.43it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.39it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.45it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.38it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.39it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.17it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.25it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.32it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.36it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.39it/s][A
 60%|██████    | 473/782 [00:09<00:06, 47.38it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.39it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.45it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.35it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.20it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.19it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.25it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.35it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.37it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.33it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.38it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.40it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.13it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.31it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.22it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.19it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.31it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.39it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.33it/s][A
 73%|███████▎  | 568/782 [00:11<00:04, 47.35it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.36it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.33it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.22it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.29it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.23it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.27it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.21it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.29it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.34it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.37it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.39it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.38it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.26it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.25it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.18it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.28it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.18it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.28it/s][A
 85%|████████▍ | 663/782 [00:13<00:02, 47.34it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.36it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.35it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.40it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.18it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.23it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.18it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.25it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.14it/s][A
 91%|█████████ | 708/782 [00:14<00:01, 47.25it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.30it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.31it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.35it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.29it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.26it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.17it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.30it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.32it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.23it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.17it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.29it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.34it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.30it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 45.58it/s][A                                                
                                                 [A 60%|██████    | 72/120 [01:41<00:13,  3.44it/s]
100%|██████████| 782/782 [00:16<00:00, 45.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:54:14,027 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 19:54:14,110 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:54:20,407 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:54:20,430 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:54:20,487 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [01:59<08:10, 10.43s/it] 62%|██████▏   | 74/120 [01:59<05:40,  7.41s/it] 62%|██████▎   | 75/120 [01:59<03:57,  5.27s/it] 63%|██████▎   | 76/120 [02:00<02:46,  3.78s/it] 64%|██████▍   | 77/120 [02:00<01:57,  2.73s/it] 65%|██████▌   | 78/120 [02:00<01:23,  2.00s/it] 66%|██████▌   | 79/120 [02:00<01:00,  1.48s/it] 67%|██████▋   | 80/120 [02:01<00:44,  1.12s/it] 68%|██████▊   | 81/120 [02:01<00:34,  1.15it/s] 68%|██████▊   | 82/120 [02:01<00:26,  1.43it/s] 69%|██████▉   | 83/120 [02:02<00:21,  1.74it/s] 70%|███████   | 84/120 [02:02<00:17,  2.05it/s] 71%|███████   | 85/120 [02:02<00:15,  2.33it/s] 72%|███████▏  | 86/120 [02:02<00:13,  2.59it/s] 72%|███████▎  | 87/120 [02:03<00:11,  2.80it/s] 73%|███████▎  | 88/120 [02:03<00:10,  2.98it/s] 74%|███████▍  | 89/120 [02:03<00:09,  3.11it/s] 75%|███████▌  | 90/120 [02:04<00:09,  3.22it/s] 76%|███████▌  | 91/120 [02:04<00:08,  3.29it/s] 77%|███████▋  | 92/120 [02:04<00:08,  3.35it/s] 78%|███████▊  | 93/120 [02:04<00:07,  3.39it/s] 78%|███████▊  | 94/120 [02:05<00:07,  3.41it/s] 79%|███████▉  | 95/120 [02:05<00:07,  3.43it/s] 80%|████████  | 96/120 [02:05<00:07,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 19:54:38,209 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:54:38,209 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 19:54:38,209 >>   Batch size = 8
{'eval_loss': 0.9321883320808411, 'eval_runtime': 16.5493, 'eval_samples_per_second': 377.842, 'eval_steps_per_second': 47.253, 'epoch': 2.98}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.87it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.19it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.23it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.46it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.18it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.90it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.75it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.53it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.44it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.45it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.46it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.38it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.31it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.27it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.26it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.35it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.37it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.34it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.25it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.27it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.28it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.30it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.24it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.25it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.22it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.11it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.21it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.24it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.18it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.22it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.21it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.28it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.38it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.32it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.32it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.39it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.49it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.47it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.36it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.36it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.19it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.19it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.16it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.27it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.40it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.36it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.38it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.33it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.23it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.29it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.32it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.33it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.25it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.33it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.10it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.12it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.26it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.23it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.22it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.16it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.25it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.30it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.15it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.21it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.24it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.35it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.31it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.37it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.26it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.19it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.33it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.31it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.19it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.29it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.18it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.41it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.36it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.40it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.31it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.22it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.28it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.31it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.26it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.22it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.34it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.39it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.40it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.32it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.31it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.21it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.17it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.29it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.28it/s][A
 60%|██████    | 473/782 [00:09<00:06, 47.23it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.32it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.34it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.37it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.37it/s][A
 64%|██████▎   | 498/782 [00:10<00:05, 47.35it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.26it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.22it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.34it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.28it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.21it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.31it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.32it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.39it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.28it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.28it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.31it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.25it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.21it/s][A
 73%|███████▎  | 568/782 [00:12<00:05, 41.12it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 42.88it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 44.08it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 45.06it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 45.78it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.33it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.68it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.81it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.76it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 46.73it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.83it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.08it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.16it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.23it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.34it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.41it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.42it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.26it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.13it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.10it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.04it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.21it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.21it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.36it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.41it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.46it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.28it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.19it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.09it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.21it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.20it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.20it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.21it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.33it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.41it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.36it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.25it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.11it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.10it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.15it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.21it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.18it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.28it/s][A                                                
                                                 [A 80%|████████  | 96/120 [02:22<00:07,  3.34it/s]
100%|██████████| 782/782 [00:16<00:00, 47.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:54:54,880 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 19:54:54,970 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:55:01,444 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:55:01,484 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:55:01,502 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [02:42<04:14, 11.06s/it] 82%|████████▏ | 98/120 [02:42<02:52,  7.83s/it] 82%|████████▎ | 99/120 [02:42<01:56,  5.57s/it] 83%|████████▎ | 100/120 [02:42<01:19,  3.98s/it] 84%|████████▍ | 101/120 [02:43<00:54,  2.87s/it] 85%|████████▌ | 102/120 [02:43<00:37,  2.10s/it] 86%|████████▌ | 103/120 [02:43<00:26,  1.55s/it] 87%|████████▋ | 104/120 [02:44<00:18,  1.17s/it] 88%|████████▊ | 105/120 [02:44<00:13,  1.10it/s] 88%|████████▊ | 106/120 [02:44<00:10,  1.39it/s] 89%|████████▉ | 107/120 [02:44<00:07,  1.69it/s] 90%|█████████ | 108/120 [02:45<00:05,  2.00it/s] 91%|█████████ | 109/120 [02:45<00:04,  2.26it/s] 92%|█████████▏| 110/120 [02:45<00:03,  2.53it/s] 92%|█████████▎| 111/120 [02:46<00:03,  2.76it/s] 93%|█████████▎| 112/120 [02:46<00:02,  2.94it/s] 94%|█████████▍| 113/120 [02:46<00:02,  3.09it/s] 95%|█████████▌| 114/120 [02:46<00:01,  3.20it/s] 96%|█████████▌| 115/120 [02:47<00:01,  3.28it/s] 97%|█████████▋| 116/120 [02:47<00:01,  3.34it/s] 98%|█████████▊| 117/120 [02:47<00:00,  3.39it/s] 98%|█████████▊| 118/120 [02:48<00:00,  3.42it/s] 99%|█████████▉| 119/120 [02:48<00:00,  3.44it/s]100%|██████████| 120/120 [02:48<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 19:55:20,973 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:55:20,973 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 19:55:20,973 >>   Batch size = 8
{'eval_loss': 0.9326030015945435, 'eval_runtime': 16.59, 'eval_samples_per_second': 376.915, 'eval_steps_per_second': 47.137, 'epoch': 3.98}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 58.39it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.28it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.47it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.69it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.35it/s][A
  4%|▍         | 33/782 [00:00<00:15, 48.14it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.92it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.61it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.39it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.49it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.57it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.42it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.51it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.55it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.54it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.52it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.20it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.27it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.27it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.27it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.40it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.41it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.39it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.41it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.43it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.47it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.05it/s][A
 18%|█▊        | 143/782 [00:02<00:13, 47.15it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.21it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.20it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.32it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.33it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.42it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.36it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.38it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.39it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.42it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.34it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.37it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.40it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.32it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.30it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.45it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.41it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.38it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.47it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.43it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.36it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.37it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.37it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.39it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.27it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.33it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.38it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.48it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.54it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.85it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.00it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.11it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.26it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.26it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.25it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.29it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.26it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.26it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.29it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.37it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.43it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.43it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.40it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.45it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.44it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.35it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.30it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.28it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.30it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.34it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.42it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.34it/s][A
 52%|█████▏    | 403/782 [00:08<00:07, 47.40it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.45it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.37it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.25it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.36it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.27it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.25it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.35it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.33it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.35it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.39it/s][A
 59%|█████▊    | 458/782 [00:09<00:07, 45.20it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 45.90it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.40it/s][A
 60%|██████    | 473/782 [00:09<00:06, 46.71it/s][A
 61%|██████    | 478/782 [00:10<00:06, 44.46it/s][A
 62%|██████▏   | 483/782 [00:10<00:08, 36.67it/s][A
 62%|██████▏   | 488/782 [00:10<00:07, 39.26it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 41.42it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 42.99it/s][A
 64%|██████▍   | 503/782 [00:10<00:06, 44.22it/s][A
 65%|██████▍   | 508/782 [00:10<00:06, 45.12it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 45.64it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.18it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.41it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.70it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.88it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.00it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.99it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.15it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.20it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.19it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.19it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.17it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.17it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.25it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.20it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.22it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.20it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.22it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.26it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.26it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.23it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.23it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.22it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.21it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.28it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.22it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.17it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.23it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.23it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 47.25it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.26it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.24it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.22it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.20it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.25it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.22it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.22it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.17it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.22it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.28it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.29it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.25it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.23it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.23it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.21it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.25it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.18it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.15it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 47.24it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.26it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.20it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.07it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.32it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.22it/s][A                                                 
                                                 [A100%|██████████| 120/120 [03:05<00:00,  3.39it/s]
100%|██████████| 782/782 [00:16<00:00, 47.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:55:37,635 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 19:55:37,711 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:55:45,069 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:55:45,123 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:55:45,140 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:55:56,999 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:55:57,005 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24 (score: 0.9170319437980652).
                                                 100%|██████████| 120/120 [03:32<00:00,  3.39it/s]100%|██████████| 120/120 [03:32<00:00,  1.77s/it]
[INFO|trainer.py:1894] 2023-08-28 19:56:04,845 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 19:56:05,377 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:56:11,652 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:56:11,798 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:56:11,844 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:56:12,338 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:12,338 >>   epoch                    =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:12,338 >>   train_loss               =     0.5891
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:12,339 >>   train_runtime            = 0:03:31.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:12,339 >>   train_samples            =       1545
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:12,339 >>   train_samples_per_second =     36.473
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:12,339 >>   train_steps_per_second   =      0.567
{'eval_loss': 0.9343780875205994, 'eval_runtime': 16.6495, 'eval_samples_per_second': 375.567, 'eval_steps_per_second': 46.968, 'epoch': 4.98}
{'train_runtime': 211.8023, 'train_samples_per_second': 36.473, 'train_steps_per_second': 0.567, 'train_loss': 0.5891287485758464, 'epoch': 4.98}
08/28/2023 19:56:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:56:12,390 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:56:12,390 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 19:56:12,390 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.43it/s]  2%|▏         | 12/782 [00:00<00:14, 51.55it/s]  2%|▏         | 18/782 [00:00<00:15, 49.77it/s]  3%|▎         | 24/782 [00:00<00:15, 48.94it/s]  4%|▎         | 29/782 [00:00<00:15, 48.57it/s]  4%|▍         | 34/782 [00:00<00:15, 48.30it/s]  5%|▍         | 39/782 [00:00<00:15, 48.11it/s]  6%|▌         | 44/782 [00:00<00:15, 48.06it/s]  6%|▋         | 49/782 [00:01<00:15, 47.85it/s]  7%|▋         | 54/782 [00:01<00:15, 47.76it/s]  8%|▊         | 59/782 [00:01<00:15, 47.71it/s]  8%|▊         | 64/782 [00:01<00:15, 47.69it/s]  9%|▉         | 69/782 [00:01<00:14, 47.66it/s]  9%|▉         | 74/782 [00:01<00:14, 47.58it/s] 10%|█         | 79/782 [00:01<00:14, 47.42it/s] 11%|█         | 84/782 [00:01<00:14, 47.68it/s] 11%|█▏        | 89/782 [00:01<00:14, 47.67it/s] 12%|█▏        | 94/782 [00:01<00:14, 47.66it/s] 13%|█▎        | 99/782 [00:02<00:14, 47.63it/s] 13%|█▎        | 104/782 [00:02<00:14, 47.62it/s] 14%|█▍        | 109/782 [00:02<00:14, 47.47it/s] 15%|█▍        | 114/782 [00:02<00:15, 43.46it/s] 15%|█▌        | 119/782 [00:02<00:14, 44.61it/s] 16%|█▌        | 124/782 [00:02<00:14, 45.47it/s] 16%|█▋        | 129/782 [00:02<00:14, 46.13it/s] 17%|█▋        | 134/782 [00:02<00:13, 46.55it/s] 18%|█▊        | 139/782 [00:02<00:13, 46.87it/s] 18%|█▊        | 144/782 [00:03<00:13, 47.11it/s] 19%|█▉        | 149/782 [00:03<00:13, 47.35it/s] 20%|█▉        | 154/782 [00:03<00:13, 47.23it/s] 20%|██        | 159/782 [00:03<00:13, 47.18it/s] 21%|██        | 164/782 [00:03<00:13, 47.35it/s] 22%|██▏       | 169/782 [00:03<00:12, 47.38it/s] 22%|██▏       | 174/782 [00:03<00:12, 47.44it/s] 23%|██▎       | 179/782 [00:03<00:12, 47.48it/s] 24%|██▎       | 184/782 [00:03<00:12, 47.61it/s] 24%|██▍       | 189/782 [00:03<00:12, 47.51it/s] 25%|██▍       | 194/782 [00:04<00:12, 47.61it/s] 25%|██▌       | 199/782 [00:04<00:12, 47.55it/s] 26%|██▌       | 204/782 [00:04<00:12, 47.50it/s] 27%|██▋       | 209/782 [00:04<00:12, 47.38it/s] 27%|██▋       | 214/782 [00:04<00:11, 47.47it/s] 28%|██▊       | 219/782 [00:04<00:11, 47.45it/s] 29%|██▊       | 224/782 [00:04<00:11, 47.54it/s] 29%|██▉       | 229/782 [00:04<00:11, 47.52it/s] 30%|██▉       | 234/782 [00:04<00:11, 47.47it/s] 31%|███       | 239/782 [00:05<00:11, 47.57it/s] 31%|███       | 244/782 [00:05<00:11, 47.54it/s] 32%|███▏      | 249/782 [00:05<00:11, 47.48it/s] 32%|███▏      | 254/782 [00:05<00:11, 47.43it/s] 33%|███▎      | 259/782 [00:05<00:15, 34.20it/s] 34%|███▍      | 264/782 [00:05<00:13, 37.39it/s] 34%|███▍      | 269/782 [00:05<00:12, 39.93it/s] 35%|███▌      | 274/782 [00:05<00:12, 41.98it/s] 36%|███▌      | 279/782 [00:06<00:11, 43.55it/s] 36%|███▋      | 284/782 [00:06<00:11, 44.76it/s] 37%|███▋      | 289/782 [00:06<00:10, 45.52it/s] 38%|███▊      | 294/782 [00:06<00:10, 46.02it/s] 38%|███▊      | 299/782 [00:06<00:10, 46.30it/s] 39%|███▉      | 304/782 [00:06<00:10, 46.63it/s] 40%|███▉      | 309/782 [00:06<00:10, 46.80it/s] 40%|████      | 314/782 [00:06<00:09, 47.05it/s] 41%|████      | 319/782 [00:06<00:09, 47.19it/s] 41%|████▏     | 324/782 [00:06<00:09, 47.35it/s] 42%|████▏     | 329/782 [00:07<00:09, 47.42it/s] 43%|████▎     | 334/782 [00:07<00:09, 47.47it/s] 43%|████▎     | 339/782 [00:07<00:09, 47.39it/s] 44%|████▍     | 344/782 [00:07<00:09, 47.41it/s] 45%|████▍     | 349/782 [00:07<00:09, 47.35it/s] 45%|████▌     | 354/782 [00:07<00:09, 47.25it/s] 46%|████▌     | 359/782 [00:07<00:08, 47.31it/s] 47%|████▋     | 364/782 [00:07<00:08, 47.40it/s] 47%|████▋     | 369/782 [00:07<00:08, 47.34it/s] 48%|████▊     | 374/782 [00:08<00:08, 47.46it/s] 48%|████▊     | 379/782 [00:08<00:08, 47.53it/s] 49%|████▉     | 384/782 [00:08<00:08, 47.55it/s] 50%|████▉     | 389/782 [00:08<00:08, 47.43it/s] 50%|█████     | 394/782 [00:08<00:08, 47.34it/s] 51%|█████     | 399/782 [00:08<00:08, 47.20it/s] 52%|█████▏    | 404/782 [00:08<00:07, 47.31it/s] 52%|█████▏    | 409/782 [00:08<00:07, 47.44it/s] 53%|█████▎    | 414/782 [00:08<00:07, 47.37it/s] 54%|█████▎    | 419/782 [00:08<00:07, 47.39it/s] 54%|█████▍    | 424/782 [00:09<00:07, 47.37it/s] 55%|█████▍    | 429/782 [00:09<00:07, 47.40it/s] 55%|█████▌    | 434/782 [00:09<00:07, 47.45it/s] 56%|█████▌    | 439/782 [00:09<00:07, 47.45it/s] 57%|█████▋    | 444/782 [00:09<00:07, 47.38it/s] 57%|█████▋    | 449/782 [00:09<00:07, 47.40it/s] 58%|█████▊    | 454/782 [00:09<00:06, 47.42it/s] 59%|█████▊    | 459/782 [00:09<00:06, 47.42it/s] 59%|█████▉    | 464/782 [00:09<00:06, 47.46it/s] 60%|█████▉    | 469/782 [00:10<00:06, 47.43it/s] 61%|██████    | 474/782 [00:10<00:06, 47.28it/s] 61%|██████▏   | 479/782 [00:10<00:06, 47.33it/s] 62%|██████▏   | 484/782 [00:10<00:06, 47.46it/s] 63%|██████▎   | 489/782 [00:10<00:06, 47.51it/s] 63%|██████▎   | 494/782 [00:10<00:06, 47.40it/s] 64%|██████▍   | 499/782 [00:10<00:05, 47.36it/s] 64%|██████▍   | 504/782 [00:10<00:05, 47.43it/s] 65%|██████▌   | 509/782 [00:10<00:05, 47.42it/s] 66%|██████▌   | 514/782 [00:10<00:05, 47.38it/s] 66%|██████▋   | 519/782 [00:11<00:05, 47.38it/s] 67%|██████▋   | 524/782 [00:11<00:05, 47.28it/s] 68%|██████▊   | 529/782 [00:11<00:05, 47.27it/s] 68%|██████▊   | 534/782 [00:11<00:05, 47.40it/s] 69%|██████▉   | 539/782 [00:11<00:05, 47.43it/s] 70%|██████▉   | 544/782 [00:11<00:05, 47.43it/s] 70%|███████   | 549/782 [00:11<00:04, 47.36it/s] 71%|███████   | 554/782 [00:11<00:04, 47.38it/s] 71%|███████▏  | 559/782 [00:11<00:04, 47.38it/s] 72%|███████▏  | 564/782 [00:12<00:04, 47.38it/s] 73%|███████▎  | 569/782 [00:12<00:04, 47.37it/s] 73%|███████▎  | 574/782 [00:12<00:04, 47.37it/s] 74%|███████▍  | 579/782 [00:12<00:04, 47.34it/s] 75%|███████▍  | 584/782 [00:12<00:04, 47.26it/s] 75%|███████▌  | 589/782 [00:12<00:04, 47.35it/s] 76%|███████▌  | 594/782 [00:12<00:03, 47.40it/s] 77%|███████▋  | 599/782 [00:12<00:03, 47.37it/s] 77%|███████▋  | 604/782 [00:12<00:03, 47.42it/s] 78%|███████▊  | 609/782 [00:12<00:03, 47.36it/s] 79%|███████▊  | 614/782 [00:13<00:03, 47.34it/s] 79%|███████▉  | 619/782 [00:13<00:03, 47.30it/s] 80%|███████▉  | 624/782 [00:13<00:03, 47.25it/s] 80%|████████  | 629/782 [00:13<00:03, 47.43it/s] 81%|████████  | 634/782 [00:13<00:03, 47.30it/s] 82%|████████▏ | 639/782 [00:13<00:03, 47.33it/s] 82%|████████▏ | 644/782 [00:13<00:02, 47.35it/s] 83%|████████▎ | 649/782 [00:13<00:02, 47.38it/s] 84%|████████▎ | 654/782 [00:13<00:02, 47.45it/s] 84%|████████▍ | 659/782 [00:14<00:02, 47.46it/s] 85%|████████▍ | 664/782 [00:14<00:02, 47.49it/s] 86%|████████▌ | 669/782 [00:14<00:02, 47.48it/s] 86%|████████▌ | 674/782 [00:14<00:02, 47.50it/s] 87%|████████▋ | 679/782 [00:14<00:02, 47.52it/s] 87%|████████▋ | 684/782 [00:14<00:02, 47.42it/s] 88%|████████▊ | 689/782 [00:14<00:02, 45.82it/s] 89%|████████▊ | 694/782 [00:14<00:01, 46.32it/s] 89%|████████▉ | 699/782 [00:14<00:01, 46.68it/s] 90%|█████████ | 704/782 [00:14<00:01, 46.94it/s] 91%|█████████ | 709/782 [00:15<00:01, 47.13it/s] 91%|█████████▏| 714/782 [00:15<00:01, 47.04it/s] 92%|█████████▏| 719/782 [00:15<00:01, 47.22it/s] 93%|█████████▎| 724/782 [00:15<00:01, 47.33it/s] 93%|█████████▎| 729/782 [00:15<00:01, 47.34it/s] 94%|█████████▍| 734/782 [00:15<00:01, 47.40it/s] 95%|█████████▍| 739/782 [00:15<00:00, 47.35it/s] 95%|█████████▌| 744/782 [00:15<00:00, 47.43it/s] 96%|█████████▌| 749/782 [00:15<00:00, 47.47it/s] 96%|█████████▋| 754/782 [00:16<00:00, 47.50it/s] 97%|█████████▋| 759/782 [00:16<00:00, 47.53it/s] 98%|█████████▊| 764/782 [00:16<00:00, 47.38it/s] 98%|█████████▊| 769/782 [00:16<00:00, 47.47it/s] 99%|█████████▉| 774/782 [00:16<00:00, 47.39it/s]100%|█████████▉| 779/782 [00:16<00:00, 47.49it/s]100%|██████████| 782/782 [00:16<00:00, 46.97it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:56:29,061 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,061 >>   epoch                   =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,061 >>   eval_loss               =      0.917
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,061 >>   eval_runtime            = 0:00:16.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,061 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,062 >>   eval_samples_per_second =    375.084
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,062 >>   eval_steps_per_second   =     46.908
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:56:29,062 >>   perplexity              =     2.5019
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:37,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:37,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:37,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:37,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:37,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:56:37,589 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:56:37,590 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:56:37,859 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:56:38,920 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:56:38,921 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:41,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:41,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:41,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:41,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:56:41,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:56:41,518 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:56:41,519 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:56:41,866 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:56:42,029 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:56:42,029 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.52it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:11,  1.56it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:13,  1.59it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.58it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:28,  1.34it/s]Extractor Predicting: 45it [00:29,  1.39it/s]Extractor Predicting: 46it [00:30,  1.42it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.37it/s]Extractor Predicting: 50it [00:32,  1.42it/s]Extractor Predicting: 51it [00:33,  1.44it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.35it/s]Extractor Predicting: 55it [00:36,  1.43it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.51it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.48it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.47it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:43,  1.46it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.43it/s]Extractor Predicting: 81it [00:53,  1.45it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.36it/s]Extractor Predicting: 84it [00:56,  1.39it/s]Extractor Predicting: 85it [00:56,  1.41it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.46it/s]Extractor Predicting: 88it [00:58,  1.47it/s]Extractor Predicting: 89it [00:59,  1.48it/s]Extractor Predicting: 90it [01:00,  1.49it/s]Extractor Predicting: 91it [01:00,  1.48it/s]Extractor Predicting: 92it [01:01,  1.48it/s]Extractor Predicting: 93it [01:02,  1.49it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:03,  1.49it/s]Extractor Predicting: 96it [01:04,  1.46it/s]Extractor Predicting: 97it [01:04,  1.46it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:06,  1.46it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.49it/s]Extractor Predicting: 102it [01:08,  1.50it/s]Extractor Predicting: 103it [01:08,  1.51it/s]Extractor Predicting: 104it [01:09,  1.52it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:11,  1.46it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:13,  1.48it/s]Extractor Predicting: 111it [01:14,  1.49it/s]Extractor Predicting: 112it [01:15,  1.49it/s]Extractor Predicting: 113it [01:15,  1.53it/s]Extractor Predicting: 114it [01:16,  1.52it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.51it/s]Extractor Predicting: 117it [01:18,  1.49it/s]Extractor Predicting: 118it [01:19,  1.49it/s]Extractor Predicting: 119it [01:19,  1.48it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:21,  1.49it/s]Extractor Predicting: 122it [01:21,  1.50it/s]Extractor Predicting: 123it [01:22,  1.54it/s]Extractor Predicting: 124it [01:23,  1.50it/s]Extractor Predicting: 125it [01:23,  1.48it/s]Extractor Predicting: 126it [01:24,  1.47it/s]Extractor Predicting: 127it [01:25,  1.46it/s]Extractor Predicting: 128it [01:25,  1.46it/s]Extractor Predicting: 129it [01:26,  1.47it/s]Extractor Predicting: 130it [01:27,  1.45it/s]Extractor Predicting: 131it [01:28,  1.29it/s]Extractor Predicting: 132it [01:28,  1.32it/s]Extractor Predicting: 133it [01:29,  1.37it/s]Extractor Predicting: 134it [01:30,  1.39it/s]Extractor Predicting: 135it [01:31,  1.32it/s]Extractor Predicting: 136it [01:31,  1.37it/s]Extractor Predicting: 137it [01:32,  1.40it/s]Extractor Predicting: 138it [01:33,  1.44it/s]Extractor Predicting: 139it [01:33,  1.45it/s]Extractor Predicting: 140it [01:34,  1.44it/s]Extractor Predicting: 141it [01:35,  1.48it/s]Extractor Predicting: 142it [01:35,  1.49it/s]Extractor Predicting: 143it [01:36,  1.47it/s]Extractor Predicting: 144it [01:37,  1.49it/s]Extractor Predicting: 145it [01:37,  1.45it/s]Extractor Predicting: 146it [01:38,  1.47it/s]Extractor Predicting: 147it [01:39,  1.47it/s]Extractor Predicting: 148it [01:39,  1.52it/s]Extractor Predicting: 149it [01:40,  1.52it/s]Extractor Predicting: 150it [01:41,  1.50it/s]Extractor Predicting: 151it [01:41,  1.48it/s]Extractor Predicting: 152it [01:42,  1.53it/s]Extractor Predicting: 153it [01:43,  1.51it/s]Extractor Predicting: 154it [01:43,  1.50it/s]Extractor Predicting: 155it [01:44,  1.51it/s]Extractor Predicting: 156it [01:45,  1.50it/s]Extractor Predicting: 157it [01:45,  1.55it/s]Extractor Predicting: 158it [01:46,  1.55it/s]Extractor Predicting: 159it [01:46,  1.56it/s]Extractor Predicting: 160it [01:47,  1.49it/s]Extractor Predicting: 161it [01:48,  1.48it/s]Extractor Predicting: 162it [01:49,  1.50it/s]Extractor Predicting: 163it [01:49,  1.53it/s]Extractor Predicting: 164it [01:50,  1.55it/s]Extractor Predicting: 165it [01:50,  1.59it/s]Extractor Predicting: 166it [01:51,  1.59it/s]Extractor Predicting: 167it [01:52,  1.58it/s]Extractor Predicting: 168it [01:53,  1.40it/s]Extractor Predicting: 169it [01:53,  1.46it/s]Extractor Predicting: 170it [01:54,  1.50it/s]Extractor Predicting: 171it [01:54,  1.50it/s]Extractor Predicting: 172it [01:55,  1.50it/s]Extractor Predicting: 173it [01:56,  1.54it/s]Extractor Predicting: 174it [01:56,  1.51it/s]Extractor Predicting: 175it [01:57,  1.53it/s]Extractor Predicting: 176it [01:58,  1.53it/s]Extractor Predicting: 177it [01:58,  1.54it/s]Extractor Predicting: 178it [01:59,  1.54it/s]Extractor Predicting: 179it [02:00,  1.55it/s]Extractor Predicting: 180it [02:00,  1.53it/s]Extractor Predicting: 181it [02:01,  1.53it/s]Extractor Predicting: 182it [02:02,  1.52it/s]Extractor Predicting: 183it [02:02,  1.54it/s]Extractor Predicting: 184it [02:03,  1.56it/s]Extractor Predicting: 185it [02:04,  1.54it/s]Extractor Predicting: 186it [02:04,  1.51it/s]Extractor Predicting: 187it [02:05,  1.56it/s]Extractor Predicting: 188it [02:06,  1.53it/s]Extractor Predicting: 189it [02:06,  1.54it/s]Extractor Predicting: 190it [02:07,  1.56it/s]Extractor Predicting: 191it [02:07,  1.59it/s]Extractor Predicting: 192it [02:08,  1.59it/s]Extractor Predicting: 193it [02:09,  1.57it/s]Extractor Predicting: 194it [02:09,  1.58it/s]Extractor Predicting: 195it [02:10,  1.58it/s]Extractor Predicting: 196it [02:11,  1.53it/s]Extractor Predicting: 197it [02:11,  1.57it/s]Extractor Predicting: 198it [02:12,  1.54it/s]Extractor Predicting: 199it [02:13,  1.53it/s]Extractor Predicting: 200it [02:13,  1.59it/s]Extractor Predicting: 201it [02:14,  1.60it/s]Extractor Predicting: 202it [02:14,  1.61it/s]Extractor Predicting: 203it [02:15,  1.62it/s]Extractor Predicting: 204it [02:16,  1.61it/s]Extractor Predicting: 205it [02:16,  1.60it/s]Extractor Predicting: 206it [02:17,  1.61it/s]Extractor Predicting: 207it [02:17,  1.62it/s]Extractor Predicting: 208it [02:18,  1.61it/s]Extractor Predicting: 209it [02:19,  1.59it/s]Extractor Predicting: 210it [02:19,  1.59it/s]Extractor Predicting: 211it [02:20,  1.60it/s]Extractor Predicting: 212it [02:21,  1.60it/s]Extractor Predicting: 213it [02:21,  1.58it/s]Extractor Predicting: 214it [02:22,  1.59it/s]Extractor Predicting: 215it [02:23,  1.57it/s]Extractor Predicting: 216it [02:23,  1.57it/s]Extractor Predicting: 217it [02:24,  1.57it/s]Extractor Predicting: 218it [02:24,  1.59it/s]Extractor Predicting: 219it [02:25,  1.47it/s]Extractor Predicting: 220it [02:26,  1.49it/s]Extractor Predicting: 221it [02:26,  1.52it/s]Extractor Predicting: 222it [02:27,  1.53it/s]Extractor Predicting: 223it [02:28,  1.55it/s]Extractor Predicting: 224it [02:29,  1.41it/s]Extractor Predicting: 225it [02:29,  1.45it/s]Extractor Predicting: 226it [02:30,  1.48it/s]Extractor Predicting: 227it [02:31,  1.52it/s]Extractor Predicting: 228it [02:31,  1.69it/s]Extractor Predicting: 228it [02:31,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:32,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:32,533 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:32,534 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:32,534 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:32,534 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:59:32,870 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:59:32,872 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:59:33,142 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:59:34,244 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:59:34,244 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:37,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:37,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:37,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:37,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:59:37,165 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:59:37,853 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:59:37,854 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:59:38,472 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:59:38,653 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:59:38,654 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.10241545893719807,
  "recall": 0.05085558931712778,
  "score": 0.06796324000854884,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.45it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:15,  1.45it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.45it/s]Extractor Predicting: 37it [00:24,  1.44it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.39it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:27,  1.37it/s]Extractor Predicting: 42it [00:28,  1.39it/s]Extractor Predicting: 43it [00:29,  1.41it/s]Extractor Predicting: 44it [00:29,  1.43it/s]Extractor Predicting: 45it [00:30,  1.42it/s]Extractor Predicting: 46it [00:31,  1.46it/s]Extractor Predicting: 47it [00:31,  1.58it/s]Extractor Predicting: 48it [00:32,  1.59it/s]Extractor Predicting: 49it [00:32,  1.63it/s]Extractor Predicting: 50it [00:33,  1.59it/s]Extractor Predicting: 51it [00:34,  1.58it/s]Extractor Predicting: 52it [00:34,  1.52it/s]Extractor Predicting: 53it [00:35,  1.45it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.37it/s]Extractor Predicting: 57it [00:38,  1.42it/s]Extractor Predicting: 58it [00:39,  1.45it/s]Extractor Predicting: 59it [00:39,  1.48it/s]Extractor Predicting: 60it [00:40,  1.52it/s]Extractor Predicting: 61it [00:41,  1.46it/s]Extractor Predicting: 62it [00:41,  1.47it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:43,  1.55it/s]Extractor Predicting: 66it [00:44,  1.52it/s]Extractor Predicting: 67it [00:45,  1.54it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:46,  1.56it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:48,  1.42it/s]Extractor Predicting: 73it [00:49,  1.45it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:51,  1.36it/s]Extractor Predicting: 77it [00:51,  1.41it/s]Extractor Predicting: 78it [00:52,  1.43it/s]Extractor Predicting: 79it [00:53,  1.47it/s]Extractor Predicting: 80it [00:53,  1.49it/s]Extractor Predicting: 81it [00:54,  1.43it/s]Extractor Predicting: 82it [00:55,  1.48it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.53it/s]Extractor Predicting: 85it [00:57,  1.53it/s]Extractor Predicting: 86it [00:57,  1.55it/s]Extractor Predicting: 87it [00:58,  1.55it/s]Extractor Predicting: 88it [00:59,  1.56it/s]Extractor Predicting: 89it [00:59,  1.58it/s]Extractor Predicting: 90it [01:00,  1.58it/s]Extractor Predicting: 91it [01:01,  1.55it/s]Extractor Predicting: 92it [01:01,  1.54it/s]Extractor Predicting: 93it [01:02,  1.55it/s]Extractor Predicting: 94it [01:02,  1.55it/s]Extractor Predicting: 95it [01:03,  1.57it/s]Extractor Predicting: 96it [01:04,  1.58it/s]Extractor Predicting: 97it [01:04,  1.56it/s]Extractor Predicting: 98it [01:05,  1.56it/s]Extractor Predicting: 99it [01:06,  1.54it/s]Extractor Predicting: 100it [01:06,  1.56it/s]Extractor Predicting: 101it [01:07,  1.57it/s]Extractor Predicting: 102it [01:08,  1.56it/s]Extractor Predicting: 103it [01:08,  1.57it/s]Extractor Predicting: 104it [01:09,  1.53it/s]Extractor Predicting: 105it [01:10,  1.53it/s]Extractor Predicting: 106it [01:10,  1.53it/s]Extractor Predicting: 107it [01:11,  1.56it/s]Extractor Predicting: 108it [01:11,  1.56it/s]Extractor Predicting: 109it [01:12,  1.56it/s]Extractor Predicting: 110it [01:13,  1.56it/s]Extractor Predicting: 111it [01:13,  1.57it/s]Extractor Predicting: 112it [01:14,  1.53it/s]Extractor Predicting: 113it [01:15,  1.57it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.55it/s]Extractor Predicting: 116it [01:17,  1.56it/s]Extractor Predicting: 117it [01:17,  1.58it/s]Extractor Predicting: 118it [01:18,  1.58it/s]Extractor Predicting: 119it [01:18,  1.63it/s]Extractor Predicting: 120it [01:19,  1.65it/s]Extractor Predicting: 121it [01:20,  1.65it/s]Extractor Predicting: 122it [01:20,  1.65it/s]Extractor Predicting: 123it [01:21,  1.66it/s]Extractor Predicting: 124it [01:21,  1.65it/s]Extractor Predicting: 125it [01:22,  1.59it/s]Extractor Predicting: 126it [01:23,  1.63it/s]Extractor Predicting: 127it [01:23,  1.58it/s]Extractor Predicting: 128it [01:24,  1.54it/s]Extractor Predicting: 129it [01:25,  1.54it/s]Extractor Predicting: 130it [01:25,  1.59it/s]Extractor Predicting: 131it [01:26,  1.57it/s]Extractor Predicting: 132it [01:27,  1.61it/s]Extractor Predicting: 133it [01:27,  1.60it/s]Extractor Predicting: 134it [01:28,  1.59it/s]Extractor Predicting: 135it [01:28,  1.59it/s]Extractor Predicting: 136it [01:29,  1.62it/s]Extractor Predicting: 137it [01:30,  1.60it/s]Extractor Predicting: 138it [01:30,  1.57it/s]Extractor Predicting: 139it [01:31,  1.57it/s]Extractor Predicting: 140it [01:32,  1.60it/s]Extractor Predicting: 141it [01:32,  1.62it/s]Extractor Predicting: 142it [01:33,  1.65it/s]Extractor Predicting: 143it [01:33,  1.64it/s]Extractor Predicting: 144it [01:34,  1.66it/s]Extractor Predicting: 145it [01:35,  1.63it/s]Extractor Predicting: 146it [01:35,  1.60it/s]Extractor Predicting: 147it [01:36,  1.62it/s]Extractor Predicting: 148it [01:36,  1.62it/s]Extractor Predicting: 149it [01:37,  1.57it/s]Extractor Predicting: 150it [01:38,  1.52it/s]Extractor Predicting: 151it [01:38,  1.54it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:40,  1.59it/s]Extractor Predicting: 154it [01:40,  1.56it/s]Extractor Predicting: 155it [01:41,  1.51it/s]Extractor Predicting: 156it [01:42,  1.51it/s]Extractor Predicting: 157it [01:42,  1.50it/s]Extractor Predicting: 158it [01:43,  1.54it/s]Extractor Predicting: 159it [01:44,  1.55it/s]Extractor Predicting: 160it [01:44,  1.57it/s]Extractor Predicting: 161it [01:45,  1.59it/s]Extractor Predicting: 162it [01:46,  1.60it/s]Extractor Predicting: 163it [01:46,  1.58it/s]Extractor Predicting: 164it [01:47,  1.61it/s]Extractor Predicting: 165it [01:47,  1.58it/s]Extractor Predicting: 166it [01:48,  1.58it/s]Extractor Predicting: 167it [01:49,  1.56it/s]Extractor Predicting: 168it [01:49,  1.57it/s]Extractor Predicting: 169it [01:50,  1.59it/s]Extractor Predicting: 170it [01:51,  1.42it/s]Extractor Predicting: 171it [01:52,  1.43it/s]Extractor Predicting: 172it [01:52,  1.46it/s]Extractor Predicting: 173it [01:53,  1.47it/s]Extractor Predicting: 174it [01:53,  1.50it/s]Extractor Predicting: 175it [01:54,  1.52it/s]Extractor Predicting: 176it [01:55,  1.52it/s]Extractor Predicting: 177it [01:55,  1.51it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:57,  1.52it/s]Extractor Predicting: 180it [01:57,  1.54it/s]Extractor Predicting: 181it [01:58,  1.58it/s]Extractor Predicting: 182it [01:59,  1.61it/s]Extractor Predicting: 183it [01:59,  1.51it/s]Extractor Predicting: 184it [02:00,  1.55it/s]Extractor Predicting: 185it [02:01,  1.57it/s]Extractor Predicting: 186it [02:01,  1.56it/s]Extractor Predicting: 187it [02:02,  1.60it/s]Extractor Predicting: 188it [02:02,  1.60it/s]Extractor Predicting: 189it [02:03,  1.56it/s]Extractor Predicting: 190it [02:04,  1.50it/s]Extractor Predicting: 191it [02:04,  1.53it/s]Extractor Predicting: 192it [02:05,  1.51it/s]Extractor Predicting: 193it [02:06,  1.51it/s]Extractor Predicting: 194it [02:06,  1.51it/s]Extractor Predicting: 195it [02:07,  1.49it/s]Extractor Predicting: 196it [02:08,  1.47it/s]Extractor Predicting: 197it [02:09,  1.49it/s]Extractor Predicting: 198it [02:09,  1.50it/s]Extractor Predicting: 199it [02:10,  1.49it/s]Extractor Predicting: 200it [02:11,  1.49it/s]Extractor Predicting: 201it [02:11,  1.49it/s]Extractor Predicting: 202it [02:12,  1.49it/s]Extractor Predicting: 203it [02:13,  1.50it/s]Extractor Predicting: 204it [02:13,  1.48it/s]Extractor Predicting: 205it [02:14,  1.47it/s]Extractor Predicting: 206it [02:15,  1.47it/s]Extractor Predicting: 207it [02:15,  1.44it/s]Extractor Predicting: 208it [02:16,  1.44it/s]Extractor Predicting: 209it [02:17,  1.49it/s]Extractor Predicting: 210it [02:17,  1.52it/s]Extractor Predicting: 211it [02:18,  1.52it/s]Extractor Predicting: 212it [02:19,  1.54it/s]Extractor Predicting: 213it [02:19,  1.47it/s]Extractor Predicting: 214it [02:20,  1.46it/s]Extractor Predicting: 215it [02:21,  1.47it/s]Extractor Predicting: 216it [02:21,  1.46it/s]Extractor Predicting: 217it [02:22,  1.44it/s]Extractor Predicting: 218it [02:23,  1.48it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:24,  1.50it/s]Extractor Predicting: 221it [02:25,  1.52it/s]Extractor Predicting: 222it [02:25,  1.47it/s]Extractor Predicting: 223it [02:26,  1.45it/s]Extractor Predicting: 224it [02:27,  1.43it/s]Extractor Predicting: 225it [02:28,  1.42it/s]Extractor Predicting: 226it [02:28,  1.43it/s]Extractor Predicting: 227it [02:29,  1.41it/s]Extractor Predicting: 228it [02:30,  1.40it/s]Extractor Predicting: 229it [02:30,  1.39it/s]Extractor Predicting: 230it [02:31,  1.41it/s]Extractor Predicting: 231it [02:32,  1.41it/s]Extractor Predicting: 232it [02:33,  1.41it/s]Extractor Predicting: 233it [02:33,  1.42it/s]Extractor Predicting: 234it [02:34,  1.43it/s]Extractor Predicting: 235it [02:35,  1.43it/s]Extractor Predicting: 236it [02:35,  1.43it/s]Extractor Predicting: 237it [02:36,  1.40it/s]Extractor Predicting: 238it [02:37,  1.40it/s]Extractor Predicting: 239it [02:37,  1.42it/s]Extractor Predicting: 240it [02:38,  1.45it/s]Extractor Predicting: 241it [02:39,  1.46it/s]Extractor Predicting: 242it [02:39,  1.47it/s]Extractor Predicting: 243it [02:40,  1.48it/s]Extractor Predicting: 244it [02:41,  1.45it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.54it/s]Extractor Predicting: 246it [02:42,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:32,792 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:32,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:32,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:32,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:32,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:02:33,491 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:02:33,492 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:34,048 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:35,118 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:35,118 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:38,115 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:38,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:38,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:38,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:38,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:02:38,907 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:02:38,908 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:39,637 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:39,790 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:39,790 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.31094527363184077,
  "recall": 0.1906779661016949,
  "score": 0.23639420046228202,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.73it/s]Extractor Predicting: 15it [00:09,  1.53it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:02:50,739 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:02:50,740 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:02:50,756 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:02:50,758 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:02:50,763 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:02:58,524 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:02:58,551 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:02:58,600 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:02:58,601 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:02:58,641 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:58,677 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:58,677 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:58,677 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:58,677 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:58,677 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:58,677 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.38596491228070173,
  "recall": 0.12359550561797752,
  "score": 0.18723404255319148,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:02:58,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:02:59,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:00,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:00,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:01,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:02,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:03,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:03,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:04,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:05,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:05,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:06,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:07,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:07,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:09,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:09,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:10,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:11,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:12,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:12,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:13,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:32, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-28 20:03:14,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:14,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:15,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:16,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:16,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:17,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:18,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:18,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:19,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:20,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:20,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:21,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:22,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:23,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:23,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:24,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:25,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:25,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:26,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:27,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:27,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:28,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:28,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:29,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:24, 15.76s/it][WARNING|generation_utils.py:914] 2023-08-28 20:03:30,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:31,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:31,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:32,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:33,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:33,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:34,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:35,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:36,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:37,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:37,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:38,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:39,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:40,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:41,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:41,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:42,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:43,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:44,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:44,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:45,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:46,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:46,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:17, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 20:03:47,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:48,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:48,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:49,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:50,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:50,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:51,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:52,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:53,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:53,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:54,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:55,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:55,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:56,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:57,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:58,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:58,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:03:59,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:00,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:00,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:01,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:03<02:52, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 20:04:02,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:02,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:03,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:04,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:04,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:05,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:06,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:06,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:07,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:08,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:08,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:09,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:09,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:10,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:11,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:11,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:12,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:13,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:14,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:14,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:15,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:16,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:16,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:18<02:35, 15.54s/it][WARNING|generation_utils.py:914] 2023-08-28 20:04:17,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:18,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:18,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:19,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:20,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:20,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:21,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:22,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:23,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:23,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:24,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:25,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:26,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:26,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:27,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:28,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:28,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:29,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:30,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:31,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:31,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:32,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:34<02:20, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-28 20:04:33,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:33,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:34,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:35,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:35,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:36,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:37,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:37,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:38,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:38,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:39,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:40,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:40,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:41,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:42,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:42,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:43,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:44,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:44,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:45,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:46,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:47<01:59, 14.96s/it][WARNING|generation_utils.py:914] 2023-08-28 20:04:46,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:47,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:48,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:48,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:49,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:50,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:50,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:51,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:52,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:53,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:53,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:54,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:55,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:56,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:56,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:57,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:58,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:59,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:04:59,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:00,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:01,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:02<01:45, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 20:05:01,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:02,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:03,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:04,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:04,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:05,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:06,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:07,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:07,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:08,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:09,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:10,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:10,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:11,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:12,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:12,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:13,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:14,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:14,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:15,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:16,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:17,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:18,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:19<01:33, 15.58s/it][WARNING|generation_utils.py:914] 2023-08-28 20:05:18,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:19,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:20,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:20,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:21,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:22,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:23,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:24,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:24,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:25,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:26,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:27,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:27,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:28,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:29,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:30,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:30,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:31,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:32,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:33,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:33,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:34,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:35,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:37<01:20, 16.16s/it][WARNING|generation_utils.py:914] 2023-08-28 20:05:36,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:36,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:37,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:38,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:39,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:39,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:40,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:41,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:42,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:42,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:43,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:44,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:44,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:45,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:46,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:46,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:47,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:48,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:49,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:49,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:50,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:51,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:52,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:52,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:54<01:05, 16.46s/it][WARNING|generation_utils.py:914] 2023-08-28 20:05:53,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:54,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:54,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:55,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:56,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:57,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:58,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:59,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:05:59,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:00,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:01,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:02,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:02,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:03,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:04,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:04,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:05,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:06,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:06,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:07,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:08,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:08,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:10<00:49, 16.41s/it][WARNING|generation_utils.py:914] 2023-08-28 20:06:09,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:10,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:10,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:11,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:12,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:13,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:14,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:15,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:16,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:16,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:17,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:18,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:18,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:19,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:20,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:21,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:21,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:22,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:23,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:23,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:25<00:32, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-28 20:06:24,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:25,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:26,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:27,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:27,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:28,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:29,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:29,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:30,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:31,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:31,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:32,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:33,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:33,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:34,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:35,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:35,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:36,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:37,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:38,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:39,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:39,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:40,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:42<00:16, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 20:06:41,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:42,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:42,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:43,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:44,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:44,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:45,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:46,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:46,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:47,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:48,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:49,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:49,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:50,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:51,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:51,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:52,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:53,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:53,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:54,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:55,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:56,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:56,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:57,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:58,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:58,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:06:59,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:00,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:00,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:01,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:02,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:03,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:03,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:04,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:06<00:00, 18.47s/it]Generating: 100%|██████████| 15/15 [04:06<00:00, 16.41s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:12,672 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:12,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:12,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:12,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:12,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:07:13,368 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:07:13,369 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:07:13,948 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:07:15,029 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:07:15,029 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:16,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:16,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:16,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:16,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:16,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:07:17,185 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:07:17,186 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:07:17,452 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:07:17,622 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:07:17,622 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8778409090909091, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.7890625, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8396739130434783, 'errors': {'', "('Academy of Music', 'notable work', '', 'She also studied theatre and poetry at the Academy of Music at the University of Leeds , where she received a Bachelor of Arts degree from the 1920s .')", 'too many values to unpack (expected 2)'}}
['Relation : owned by . Context : Later in the year , the couple built a new home for himself in Luton at the end of 2010 , in a development that would later be dubbed a luxury apartment building . Head Entity : Leuton , Tail Entity : Luton .\n']
['Relation : owned by . Context : Later in the year , the couple built a new home for himself in Luton at the end of 2010 , in a development that would later be dubbed a luxury apartment building . Head Entity : Leuton , Tail Entity : Luton .\n', 'Relation : owned by . Context : He was the son of Charles IV of France ( c. 1711 &ndash; 1711 ) and the eldest son of Baron James I of Wales ( 1758 &ndash; 1766 ) . Head Entity : Charles IV of France , Tail Entity : Charles IV of Wales .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : successful candidate . Context : Later in the year , the University of New Brunswick won the federal election , but at the end of 2010 , he was elected a sitting MP for the New Brunswick riding of Westmoreland . Head Entity : Nørreberg , Tail Entity : New Brunswick .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.8192934782608695, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : director .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8328804347826086, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupant . Context : Later in the year ( 1790 ) , he purchased the lands of Lauterbach in Bavaria , in which he made his first successful venture , in 1808 . Head Entity : Le Auterbach , Tail Entity : Lauterbach .\n']
['Relation : occupant . Context : Later in the year ( 1790 ) , he purchased the lands of Lauterbach in Bavaria , in which he made his first successful venture , in 1808 . Head Entity : Le Auterbach , Tail Entity : Lauterbach .\n', 'Relation : occupant . Context : After he was elected to the House of Commons , he was elected mayor of Sheffield in 1819 to fill seven years as Labour City Councillor . Head Entity : Labour City Councillor , Tail Entity : Mayor of Sheffield .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : organization directed by the office or position . Context : Following his leadership in the Communist Party under the leadership of the Central Committee , he was later the Chairman of the Central Committee of the Communist Workers Party . Head Entity : Chairman of the Communist Workers Party , Tail Entity : Communist Party .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8220108695652174, 'errors': {''}}
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on Fox s hit film adaptation of the 1957 novel of the same name by Robert E. Howard . Head Entity : Robert E. Howard , Tail Entity : actor .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on Fox s hit film adaptation of the 1957 novel of the same name by Robert E. Howard . Head Entity : Robert E. Howard , Tail Entity : actor .\n', 'Relation : voice type . Context : She also sang the famous theme song , The Big Song , by her friend and lyricist Fred Hampton for her debut album with the band , The Lion King . Head Entity : The Lion King , Tail Entity : The voice .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on Fox s hit film adaptation of the 1957 novel of the same name by Robert E. Howard . Head Entity : Robert E. Howard , Tail Entity : actor .\n', 'Relation : voice type . Context : She also sang the famous theme song , The Big Song , by her friend and lyricist Fred Hampton for her debut album with the band , The Lion King . Head Entity : The Lion King , Tail Entity : The voice .\n', 'Relation : voice type . Context : The next season of the Fox sitcom How I Met Your Mother , based on the play by the same name by the same name , began on November 13 , 2011 . Head Entity : How I Met Your Mother , Tail Entity : voice type .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on Fox s hit film adaptation of the 1957 novel of the same name by Robert E. Howard . Head Entity : Robert E. Howard , Tail Entity : actor .\n', 'Relation : voice type . Context : She also sang the famous theme song , The Big Song , by her friend and lyricist Fred Hampton for her debut album with the band , The Lion King . Head Entity : The Lion King , Tail Entity : The voice .\n', 'Relation : voice type . Context : The next season of the Fox sitcom How I Met Your Mother , based on the play by the same name by the same name , began on November 13 , 2011 . Head Entity : How I Met Your Mother , Tail Entity : voice type .\n', 'Relation : voice type . Context : He is best remembered for his role in the 2007 television series Game of Thrones , in which he voices the dragon Khal Drogo and his wife, Sansa Stark . Head Entity : Game of Thrones , Tail Entity : voice type .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on Fox s hit film adaptation of the 1957 novel of the same name by Robert E. Howard . Head Entity : Robert E. Howard , Tail Entity : actor .\n', 'Relation : voice type . Context : She also sang the famous theme song , The Big Song , by her friend and lyricist Fred Hampton for her debut album with the band , The Lion King . Head Entity : The Lion King , Tail Entity : The voice .\n', 'Relation : voice type . Context : The next season of the Fox sitcom How I Met Your Mother , based on the play by the same name by the same name , began on November 13 , 2011 . Head Entity : How I Met Your Mother , Tail Entity : voice type .\n', 'Relation : voice type . Context : He is best remembered for his role in the 2007 television series Game of Thrones , in which he voices the dragon Khal Drogo and his wife, Sansa Stark . Head Entity : Game of Thrones , Tail Entity : voice type .\n', 'Relation : voice type . Context : She made her television debut in 2008 with the musical series The Voice of the Voice of the Voice of the Voice of the Voice of the Voice of the Voice of the Voice of the voice in the original musical adaptation . Head Entity : The Voice of the Voice of the Voice in the original musical adaptation , Tail Entity : voice type .\n']
['Relation : voice type . Context : Later in the year ( 1790 ) , he played the part of Leland at the London premiere of George Bernard Shaw s play One Life to Live , in which he portrayed a married man . Head Entity : One Life to Live , Tail Entity : male .\n', 'Relation : voice type . Context : He was best known for playing the roles of John Fincher ( the role of John Fincher ) and Bill Murray . Head Entity : John Fincher , Tail Entity : voice type .\n', 'Relation : voice type . Context : Her film credits include The Girl Who Loved the Joker ( 2004 ) , The Evil Dead ( 2005 ) and The Evil Dead 2 ( 2006 ) . Head Entity : The Evil Dead , Tail Entity : voice type .\n', 'Relation : voice type . Context : In 1994 , she appeared on Fox s hit film adaptation of the 1957 novel of the same name by Robert E. Howard . Head Entity : Robert E. Howard , Tail Entity : actor .\n', 'Relation : voice type . Context : She also sang the famous theme song , The Big Song , by her friend and lyricist Fred Hampton for her debut album with the band , The Lion King . Head Entity : The Lion King , Tail Entity : The voice .\n', 'Relation : voice type . Context : The next season of the Fox sitcom How I Met Your Mother , based on the play by the same name by the same name , began on November 13 , 2011 . Head Entity : How I Met Your Mother , Tail Entity : voice type .\n', 'Relation : voice type . Context : He is best remembered for his role in the 2007 television series Game of Thrones , in which he voices the dragon Khal Drogo and his wife, Sansa Stark . Head Entity : Game of Thrones , Tail Entity : voice type .\n', 'Relation : voice type . Context : She made her television debut in 2008 with the musical series The Voice of the Voice of the Voice of the Voice of the Voice of the Voice of the Voice of the Voice of the voice in the original musical adaptation . Head Entity : The Voice of the Voice of the Voice in the original musical adaptation , Tail Entity : voice type .\n', 'Relation : voice type . Context : He appeared in the musical Eurythmics short film , the same year . Head Entity : Eurythmics , Tail Entity : voice type .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 26, 'raw': 64}
{'target': 600, 'success': 48, 'raw': 96}
{'target': 600, 'success': 68, 'raw': 128}
{'target': 600, 'success': 87, 'raw': 160}
{'target': 600, 'success': 106, 'raw': 192}
{'target': 600, 'success': 125, 'raw': 224}
{'target': 600, 'success': 142, 'raw': 256}
{'target': 600, 'success': 152, 'raw': 288}
{'target': 600, 'success': 168, 'raw': 320}
{'target': 600, 'success': 182, 'raw': 352}
{'target': 600, 'success': 200, 'raw': 384}
{'target': 600, 'success': 220, 'raw': 416}
{'target': 600, 'success': 235, 'raw': 448}
{'target': 600, 'success': 250, 'raw': 480}
{'target': 600, 'success': 259, 'raw': 512}
{'target': 600, 'success': 275, 'raw': 544}
{'target': 600, 'success': 293, 'raw': 576}
{'target': 600, 'success': 310, 'raw': 608}
{'target': 600, 'success': 327, 'raw': 640}
{'target': 600, 'success': 353, 'raw': 672}
{'target': 600, 'success': 370, 'raw': 704}
{'target': 600, 'success': 387, 'raw': 736}
{'target': 600, 'success': 401, 'raw': 768}
{'target': 600, 'success': 419, 'raw': 800}
{'target': 600, 'success': 436, 'raw': 832}
{'target': 600, 'success': 454, 'raw': 864}
{'target': 600, 'success': 476, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 519, 'raw': 960}
{'target': 600, 'success': 546, 'raw': 992}
{'target': 600, 'success': 565, 'raw': 1024}
{'target': 600, 'success': 582, 'raw': 1056}
{'target': 600, 'success': 601, 'raw': 1088}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5523897058823529, 'errors': {'', "('How I Met Your Mother', 'voice type', '', 'In 2006 , he appeared as the antagonist in a Broadway production of How I Met Your Mother and starred in an episode of the American soap opera A Christmas Carol .')", "('Bob Dylan', 'voice type', '', 'The album cover featured the line, based on the voice of American singer Bob Dylan in the song Rockin Time .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 10548
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10648, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.68it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.58it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.63it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.62it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.59it/s]Extractor Estimating: 12it [00:07,  1.57it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:10,  1.52it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.66it/s]Extractor Estimating: 23it [00:14,  1.69it/s]Extractor Estimating: 24it [00:14,  1.64it/s]Extractor Estimating: 25it [00:15,  1.63it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.66it/s]Extractor Estimating: 28it [00:17,  1.65it/s]Extractor Estimating: 29it [00:17,  1.68it/s]Extractor Estimating: 30it [00:18,  1.72it/s]Extractor Estimating: 31it [00:19,  1.75it/s]Extractor Estimating: 32it [00:19,  1.72it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:20,  1.69it/s]Extractor Estimating: 35it [00:21,  1.69it/s]Extractor Estimating: 36it [00:22,  1.68it/s]Extractor Estimating: 37it [00:22,  1.64it/s]Extractor Estimating: 38it [00:23,  1.70it/s]Extractor Estimating: 39it [00:23,  1.67it/s]Extractor Estimating: 40it [00:24,  1.71it/s]Extractor Estimating: 41it [00:25,  1.61it/s]Extractor Estimating: 42it [00:25,  1.62it/s]Extractor Estimating: 43it [00:26,  1.63it/s]Extractor Estimating: 44it [00:26,  1.63it/s]Extractor Estimating: 45it [00:27,  1.68it/s]Extractor Estimating: 46it [00:28,  1.71it/s]Extractor Estimating: 47it [00:28,  1.72it/s]Extractor Estimating: 48it [00:29,  1.71it/s]Extractor Estimating: 49it [00:29,  1.67it/s]Extractor Estimating: 50it [00:30,  1.65it/s]Extractor Estimating: 51it [00:31,  1.66it/s]Extractor Estimating: 52it [00:31,  1.67it/s]Extractor Estimating: 53it [00:32,  1.63it/s]Extractor Estimating: 54it [00:32,  1.61it/s]Extractor Estimating: 55it [00:33,  1.61it/s]Extractor Estimating: 56it [00:34,  1.52it/s]Extractor Estimating: 57it [00:34,  1.51it/s]Extractor Estimating: 58it [00:35,  1.53it/s]Extractor Estimating: 59it [00:36,  1.41it/s]Extractor Estimating: 60it [00:37,  1.43it/s]Extractor Estimating: 61it [00:37,  1.44it/s]Extractor Estimating: 62it [00:38,  1.46it/s]Extractor Estimating: 63it [00:39,  1.48it/s]Extractor Estimating: 64it [00:39,  1.52it/s]Extractor Estimating: 65it [00:40,  1.56it/s]Extractor Estimating: 66it [00:40,  1.54it/s]Extractor Estimating: 67it [00:41,  1.51it/s]Extractor Estimating: 68it [00:42,  1.51it/s]Extractor Estimating: 69it [00:42,  1.55it/s]Extractor Estimating: 70it [00:43,  1.53it/s]Extractor Estimating: 71it [00:44,  1.55it/s]Extractor Estimating: 72it [00:44,  1.60it/s]Extractor Estimating: 73it [00:45,  1.56it/s]Extractor Estimating: 74it [00:46,  1.60it/s]Extractor Estimating: 75it [00:46,  1.61it/s]Extractor Estimating: 76it [00:47,  1.66it/s]Extractor Estimating: 77it [00:47,  1.64it/s]Extractor Estimating: 78it [00:48,  1.66it/s]Extractor Estimating: 79it [00:49,  1.65it/s]Extractor Estimating: 80it [00:49,  1.50it/s]Extractor Estimating: 81it [00:50,  1.51it/s]Extractor Estimating: 82it [00:51,  1.54it/s]Extractor Estimating: 83it [00:51,  1.54it/s]Extractor Estimating: 84it [00:52,  1.61it/s]Extractor Estimating: 85it [00:53,  1.61it/s]Extractor Estimating: 86it [00:53,  1.59it/s]Extractor Estimating: 87it [00:54,  1.63it/s]Extractor Estimating: 88it [00:54,  1.65it/s]Extractor Estimating: 89it [00:55,  1.69it/s]Extractor Estimating: 90it [00:55,  1.73it/s]Extractor Estimating: 91it [00:56,  1.64it/s]Extractor Estimating: 92it [00:57,  1.64it/s]Extractor Estimating: 93it [00:57,  1.68it/s]Extractor Estimating: 94it [00:58,  1.68it/s]Extractor Estimating: 95it [00:58,  1.65it/s]Extractor Estimating: 96it [00:59,  1.63it/s]Extractor Estimating: 97it [01:00,  1.63it/s]Extractor Estimating: 98it [01:00,  1.67it/s]Extractor Estimating: 99it [01:01,  1.66it/s]Extractor Estimating: 100it [01:02,  1.66it/s]Extractor Estimating: 101it [01:02,  1.69it/s]Extractor Estimating: 102it [01:03,  1.69it/s]Extractor Estimating: 103it [01:03,  1.73it/s]Extractor Estimating: 104it [01:04,  1.74it/s]Extractor Estimating: 105it [01:04,  1.68it/s]Extractor Estimating: 106it [01:05,  1.71it/s]Extractor Estimating: 107it [01:06,  1.75it/s]Extractor Estimating: 108it [01:06,  1.75it/s]Extractor Estimating: 109it [01:07,  1.74it/s]Extractor Estimating: 110it [01:07,  1.76it/s]Extractor Estimating: 111it [01:08,  1.80it/s]Extractor Estimating: 112it [01:08,  1.80it/s]Extractor Estimating: 113it [01:09,  1.80it/s]Extractor Estimating: 114it [01:09,  1.78it/s]Extractor Estimating: 115it [01:10,  1.77it/s]Extractor Estimating: 116it [01:11,  1.75it/s]Extractor Estimating: 117it [01:11,  1.76it/s]Extractor Estimating: 118it [01:12,  1.77it/s]Extractor Estimating: 119it [01:12,  1.80it/s]Extractor Estimating: 120it [01:13,  1.80it/s]Extractor Estimating: 121it [01:13,  1.78it/s]Extractor Estimating: 122it [01:14,  1.63it/s]Extractor Estimating: 123it [01:15,  1.72it/s]Extractor Estimating: 124it [01:15,  1.74it/s]Extractor Estimating: 125it [01:16,  1.72it/s]Extractor Estimating: 126it [01:16,  1.68it/s]Extractor Estimating: 127it [01:17,  1.71it/s]Extractor Estimating: 128it [01:18,  1.68it/s]Extractor Estimating: 129it [01:18,  1.73it/s]Extractor Estimating: 130it [01:19,  1.76it/s]Extractor Estimating: 131it [01:19,  1.70it/s]Extractor Estimating: 132it [01:20,  1.69it/s]Extractor Estimating: 133it [01:20,  1.71it/s]Extractor Estimating: 134it [01:21,  1.67it/s]Extractor Estimating: 135it [01:22,  1.63it/s]Extractor Estimating: 136it [01:22,  1.55it/s]Extractor Estimating: 137it [01:23,  1.57it/s]Extractor Estimating: 138it [01:24,  1.55it/s]Extractor Estimating: 139it [01:24,  1.53it/s]Extractor Estimating: 140it [01:25,  1.59it/s]Extractor Estimating: 141it [01:26,  1.58it/s]Extractor Estimating: 142it [01:26,  1.59it/s]Extractor Estimating: 143it [01:27,  1.56it/s]Extractor Estimating: 144it [01:28,  1.48it/s]Extractor Estimating: 145it [01:28,  1.55it/s]Extractor Estimating: 146it [01:29,  1.60it/s]Extractor Estimating: 147it [01:29,  1.60it/s]Extractor Estimating: 148it [01:30,  1.66it/s]Extractor Estimating: 149it [01:31,  1.68it/s]Extractor Estimating: 150it [01:31,  1.67it/s]Extractor Estimating: 151it [01:32,  1.72it/s]Extractor Estimating: 152it [01:32,  1.64it/s]Extractor Estimating: 153it [01:33,  1.62it/s]Extractor Estimating: 154it [01:34,  1.63it/s]Extractor Estimating: 155it [01:34,  1.57it/s]Extractor Estimating: 156it [01:35,  1.65it/s]Extractor Estimating: 157it [01:36,  1.65it/s]Extractor Estimating: 158it [01:36,  1.62it/s]Extractor Estimating: 159it [01:37,  1.67it/s]Extractor Estimating: 160it [01:37,  1.66it/s]Extractor Estimating: 161it [01:38,  1.64it/s]Extractor Estimating: 162it [01:39,  1.66it/s]Extractor Estimating: 163it [01:39,  1.64it/s]Extractor Estimating: 164it [01:40,  1.65it/s]Extractor Estimating: 165it [01:40,  1.62it/s]Extractor Estimating: 166it [01:41,  1.62it/s]Extractor Estimating: 167it [01:42,  1.60it/s]Extractor Estimating: 168it [01:42,  1.56it/s]Extractor Estimating: 169it [01:43,  1.58it/s]Extractor Estimating: 170it [01:44,  1.59it/s]Extractor Estimating: 171it [01:44,  1.61it/s]Extractor Estimating: 172it [01:45,  1.61it/s]Extractor Estimating: 173it [01:45,  1.65it/s]Extractor Estimating: 174it [01:46,  1.63it/s]Extractor Estimating: 175it [01:47,  1.65it/s]Extractor Estimating: 176it [01:47,  1.64it/s]Extractor Estimating: 177it [01:48,  1.60it/s]Extractor Estimating: 178it [01:48,  1.64it/s]Extractor Estimating: 179it [01:49,  1.62it/s]Extractor Estimating: 180it [01:50,  1.58it/s]Extractor Estimating: 181it [01:50,  1.57it/s]Extractor Estimating: 182it [01:51,  1.60it/s]Extractor Estimating: 183it [01:52,  1.52it/s]Extractor Estimating: 184it [01:52,  1.54it/s]Extractor Estimating: 185it [01:53,  1.51it/s]Extractor Estimating: 186it [01:54,  1.55it/s]Extractor Estimating: 187it [01:54,  1.52it/s]Extractor Estimating: 188it [01:55,  1.55it/s]Extractor Estimating: 189it [01:56,  1.51it/s]Extractor Estimating: 190it [01:56,  1.56it/s]Extractor Estimating: 191it [01:57,  1.56it/s]Extractor Estimating: 192it [01:58,  1.54it/s]Extractor Estimating: 193it [01:58,  1.54it/s]Extractor Estimating: 194it [01:59,  1.56it/s]Extractor Estimating: 195it [02:00,  1.52it/s]Extractor Estimating: 196it [02:00,  1.51it/s]Extractor Estimating: 197it [02:01,  1.54it/s]Extractor Estimating: 198it [02:01,  1.59it/s]Extractor Estimating: 199it [02:02,  1.61it/s]Extractor Estimating: 200it [02:03,  1.63it/s]Extractor Estimating: 201it [02:03,  1.58it/s]Extractor Estimating: 202it [02:04,  1.59it/s]Extractor Estimating: 203it [02:04,  1.62it/s]Extractor Estimating: 204it [02:05,  1.57it/s]Extractor Estimating: 205it [02:06,  1.59it/s]Extractor Estimating: 206it [02:07,  1.44it/s]Extractor Estimating: 207it [02:07,  1.49it/s]Extractor Estimating: 208it [02:08,  1.52it/s]Extractor Estimating: 209it [02:08,  1.57it/s]Extractor Estimating: 210it [02:09,  1.61it/s]Extractor Estimating: 211it [02:10,  1.58it/s]Extractor Estimating: 212it [02:10,  1.54it/s]Extractor Estimating: 213it [02:11,  1.55it/s]Extractor Estimating: 214it [02:12,  1.54it/s]Extractor Estimating: 215it [02:12,  1.58it/s]Extractor Estimating: 216it [02:13,  1.64it/s]Extractor Estimating: 217it [02:13,  1.63it/s]Extractor Estimating: 218it [02:14,  1.60it/s]Extractor Estimating: 219it [02:15,  1.63it/s]Extractor Estimating: 220it [02:15,  1.58it/s]Extractor Estimating: 221it [02:16,  1.60it/s]Extractor Estimating: 222it [02:17,  1.62it/s]Extractor Estimating: 223it [02:17,  1.60it/s]Extractor Estimating: 224it [02:18,  1.61it/s]Extractor Estimating: 225it [02:18,  1.63it/s]Extractor Estimating: 226it [02:19,  1.59it/s]Extractor Estimating: 227it [02:20,  1.59it/s]Extractor Estimating: 228it [02:20,  1.56it/s]Extractor Estimating: 229it [02:21,  1.50it/s]Extractor Estimating: 230it [02:22,  1.48it/s]Extractor Estimating: 231it [02:23,  1.46it/s]Extractor Estimating: 232it [02:23,  1.51it/s]Extractor Estimating: 233it [02:24,  1.52it/s]Extractor Estimating: 234it [02:25,  1.47it/s]Extractor Estimating: 235it [02:25,  1.47it/s]Extractor Estimating: 236it [02:26,  1.48it/s]Extractor Estimating: 237it [02:27,  1.47it/s]Extractor Estimating: 238it [02:27,  1.53it/s]Extractor Estimating: 239it [02:28,  1.55it/s]Extractor Estimating: 240it [02:29,  1.48it/s]Extractor Estimating: 241it [02:29,  1.52it/s]Extractor Estimating: 242it [02:30,  1.52it/s]Extractor Estimating: 243it [02:30,  1.53it/s]Extractor Estimating: 244it [02:31,  1.52it/s]Extractor Estimating: 245it [02:32,  1.54it/s]Extractor Estimating: 246it [02:32,  1.51it/s]Extractor Estimating: 247it [02:33,  1.53it/s]Extractor Estimating: 248it [02:34,  1.52it/s]Extractor Estimating: 249it [02:34,  1.54it/s]Extractor Estimating: 250it [02:35,  1.48it/s]Extractor Estimating: 251it [02:36,  1.52it/s]Extractor Estimating: 252it [02:36,  1.54it/s]Extractor Estimating: 253it [02:37,  1.55it/s]Extractor Estimating: 254it [02:38,  1.60it/s]Extractor Estimating: 255it [02:38,  1.60it/s]Extractor Estimating: 256it [02:39,  1.56it/s]Extractor Estimating: 257it [02:39,  1.55it/s]Extractor Estimating: 258it [02:40,  1.54it/s]Extractor Estimating: 259it [02:41,  1.51it/s]Extractor Estimating: 260it [02:41,  1.55it/s]Extractor Estimating: 261it [02:42,  1.56it/s]Extractor Estimating: 262it [02:43,  1.59it/s]Extractor Estimating: 263it [02:43,  1.63it/s]Extractor Estimating: 264it [02:44,  1.63it/s]Extractor Estimating: 265it [02:45,  1.60it/s]Extractor Estimating: 266it [02:45,  1.59it/s]Extractor Estimating: 267it [02:46,  1.58it/s]Extractor Estimating: 268it [02:47,  1.54it/s]Extractor Estimating: 269it [02:47,  1.58it/s]Extractor Estimating: 270it [02:48,  1.57it/s]Extractor Estimating: 271it [02:48,  1.55it/s]Extractor Estimating: 272it [02:49,  1.58it/s]Extractor Estimating: 273it [02:50,  1.52it/s]Extractor Estimating: 274it [02:50,  1.55it/s]Extractor Estimating: 275it [02:51,  1.54it/s]Extractor Estimating: 276it [02:52,  1.56it/s]Extractor Estimating: 277it [02:52,  1.54it/s]Extractor Estimating: 278it [02:53,  1.57it/s]Extractor Estimating: 279it [02:54,  1.40it/s]Extractor Estimating: 280it [02:54,  1.47it/s]Extractor Estimating: 281it [02:55,  1.57it/s]Extractor Estimating: 282it [02:56,  1.41it/s]Extractor Estimating: 283it [02:56,  1.49it/s]Extractor Estimating: 284it [02:57,  1.51it/s]Extractor Estimating: 285it [02:58,  1.55it/s]Extractor Estimating: 286it [02:58,  1.57it/s]Extractor Estimating: 287it [02:59,  1.60it/s]Extractor Estimating: 288it [02:59,  1.67it/s]Extractor Estimating: 289it [03:00,  1.71it/s]Extractor Estimating: 290it [03:01,  1.68it/s]Extractor Estimating: 291it [03:01,  1.68it/s]Extractor Estimating: 292it [03:02,  1.66it/s]Extractor Estimating: 293it [03:02,  1.71it/s]Extractor Estimating: 294it [03:03,  1.71it/s]Extractor Estimating: 295it [03:03,  1.72it/s]Extractor Estimating: 296it [03:04,  1.71it/s]Extractor Estimating: 297it [03:05,  1.66it/s]Extractor Estimating: 298it [03:05,  1.70it/s]Extractor Estimating: 299it [03:06,  1.62it/s]Extractor Estimating: 300it [03:07,  1.62it/s]Extractor Estimating: 301it [03:07,  1.61it/s]Extractor Estimating: 302it [03:08,  1.62it/s]Extractor Estimating: 303it [03:09,  1.54it/s]Extractor Estimating: 304it [03:09,  1.56it/s]Extractor Estimating: 305it [03:10,  1.62it/s]Extractor Estimating: 306it [03:10,  1.65it/s]Extractor Estimating: 307it [03:11,  1.65it/s]Extractor Estimating: 308it [03:12,  1.57it/s]Extractor Estimating: 309it [03:12,  1.60it/s]Extractor Estimating: 310it [03:13,  1.58it/s]Extractor Estimating: 311it [03:13,  1.61it/s]Extractor Estimating: 312it [03:14,  1.63it/s]Extractor Estimating: 313it [03:15,  1.62it/s]Extractor Estimating: 314it [03:15,  1.64it/s]Extractor Estimating: 315it [03:16,  1.68it/s]Extractor Estimating: 316it [03:16,  1.65it/s]Extractor Estimating: 317it [03:17,  1.62it/s]Extractor Estimating: 318it [03:18,  1.62it/s]Extractor Estimating: 319it [03:18,  1.56it/s]Extractor Estimating: 320it [03:19,  1.58it/s]Extractor Estimating: 321it [03:20,  1.57it/s]Extractor Estimating: 322it [03:20,  1.55it/s]Extractor Estimating: 323it [03:21,  1.52it/s]Extractor Estimating: 324it [03:22,  1.57it/s]Extractor Estimating: 325it [03:22,  1.55it/s]Extractor Estimating: 326it [03:23,  1.51it/s]Extractor Estimating: 327it [03:24,  1.48it/s]Extractor Estimating: 328it [03:24,  1.51it/s]Extractor Estimating: 329it [03:25,  1.58it/s]Extractor Estimating: 330it [03:25,  1.61it/s]Extractor Estimating: 331it [03:26,  1.61it/s]Extractor Estimating: 332it [03:27,  1.61it/s]Extractor Estimating: 333it [03:27,  1.64it/s]Extractor Estimating: 334it [03:28,  1.63it/s]Extractor Estimating: 335it [03:29,  1.62it/s]Extractor Estimating: 336it [03:29,  1.61it/s]Extractor Estimating: 337it [03:30,  1.60it/s]Extractor Estimating: 338it [03:30,  1.58it/s]Extractor Estimating: 339it [03:31,  1.59it/s]Extractor Estimating: 340it [03:32,  1.61it/s]Extractor Estimating: 341it [03:32,  1.53it/s]Extractor Estimating: 342it [03:33,  1.56it/s]Extractor Estimating: 343it [03:34,  1.52it/s]Extractor Estimating: 344it [03:34,  1.57it/s]Extractor Estimating: 345it [03:35,  1.58it/s]Extractor Estimating: 346it [03:36,  1.60it/s]Extractor Estimating: 347it [03:36,  1.57it/s]Extractor Estimating: 348it [03:37,  1.58it/s]Extractor Estimating: 349it [03:37,  1.59it/s]Extractor Estimating: 350it [03:38,  1.57it/s]Extractor Estimating: 351it [03:39,  1.57it/s]Extractor Estimating: 352it [03:39,  1.62it/s]Extractor Estimating: 353it [03:40,  1.62it/s]Extractor Estimating: 354it [03:41,  1.62it/s]Extractor Estimating: 355it [03:41,  1.57it/s]Extractor Estimating: 356it [03:42,  1.58it/s]Extractor Estimating: 357it [03:42,  1.57it/s]Extractor Estimating: 358it [03:43,  1.60it/s]Extractor Estimating: 359it [03:44,  1.63it/s]Extractor Estimating: 360it [03:44,  1.58it/s]Extractor Estimating: 361it [03:45,  1.60it/s]Extractor Estimating: 362it [03:46,  1.62it/s]Extractor Estimating: 363it [03:46,  1.62it/s]Extractor Estimating: 364it [03:47,  1.44it/s]Extractor Estimating: 365it [03:48,  1.50it/s]Extractor Estimating: 366it [03:48,  1.55it/s]Extractor Estimating: 367it [03:49,  1.60it/s]Extractor Estimating: 368it [03:49,  1.57it/s]Extractor Estimating: 369it [03:50,  1.54it/s]Extractor Estimating: 370it [03:51,  1.58it/s]Extractor Estimating: 371it [03:51,  1.55it/s]Extractor Estimating: 372it [03:52,  1.59it/s]Extractor Estimating: 373it [03:53,  1.56it/s]Extractor Estimating: 374it [03:53,  1.79it/s]Extractor Estimating: 374it [03:53,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:29,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:29,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:29,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:29,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:29,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:11:29,591 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:11:29,592 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:11:29,861 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:11:30,962 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:11:30,962 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:33,143 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:33,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:33,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:33,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:11:33,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:11:33,558 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:11:33,559 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:11:33,833 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:11:34,005 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:11:34,005 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:14:26,297 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:14:26,300 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 3027 mean pseudo reward: 0.9657666679868042
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 17762
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17862, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17862, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.065, loss:559.6211
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 73, avg_time 1.050, loss:489.9488
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 46, avg_time 1.071, loss:496.6917
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 19, avg_time 1.053, loss:440.2590
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 119, avg_time 1.063, loss:443.2762
>> valid entity prec:0.4958, rec:0.4975, f1:0.4967
>> valid relation prec:0.0378, rec:0.0242, f1:0.0295
>> valid relation with NER prec:0.0378, rec:0.0242, f1:0.0295
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 92, avg_time 3.214, loss:413.2674
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 65, avg_time 1.051, loss:389.5999
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 38, avg_time 1.071, loss:407.7818
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 11, avg_time 1.053, loss:380.5366
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 111, avg_time 1.069, loss:390.5854
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4846, rec:0.5063, f1:0.4952
>> valid relation prec:0.0383, rec:0.0219, f1:0.0279
>> valid relation with NER prec:0.0383, rec:0.0219, f1:0.0279
g_step 1100, step 84, avg_time 3.195, loss:362.9568
g_step 1200, step 57, avg_time 1.063, loss:348.9920
g_step 1300, step 30, avg_time 1.055, loss:342.2356
g_step 1400, step 3, avg_time 1.064, loss:333.5326
g_step 1500, step 103, avg_time 1.061, loss:318.9581
>> valid entity prec:0.4835, rec:0.4599, f1:0.4714
>> valid relation prec:0.0429, rec:0.0253, f1:0.0318
>> valid relation with NER prec:0.0429, rec:0.0253, f1:0.0318
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 76, avg_time 3.220, loss:292.3438
g_step 1700, step 49, avg_time 1.056, loss:300.4833
g_step 1800, step 22, avg_time 1.064, loss:288.2510
g_step 1900, step 122, avg_time 1.061, loss:274.4775
g_step 2000, step 95, avg_time 1.061, loss:245.5549
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4830, rec:0.4821, f1:0.4825
>> valid relation prec:0.0611, rec:0.0387, f1:0.0474
>> valid relation with NER prec:0.0611, rec:0.0387, f1:0.0474
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 68, avg_time 3.215, loss:244.3989
g_step 2200, step 41, avg_time 1.054, loss:242.3170
g_step 2300, step 14, avg_time 1.071, loss:235.7092
g_step 2400, step 114, avg_time 1.057, loss:231.0984
g_step 2500, step 87, avg_time 1.053, loss:221.7002
>> valid entity prec:0.5080, rec:0.4855, f1:0.4965
>> valid relation prec:0.0438, rec:0.0296, f1:0.0353
>> valid relation with NER prec:0.0438, rec:0.0296, f1:0.0353
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:14:26 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:14:26 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-14-26_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:14:27 - WARNING - datasets.builder -   Using custom data configuration default-ddabb47666556855
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ddabb47666556855/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:14:27,664 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:14:27,665 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:14:27,665 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:14:27,666 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:14:27,687 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:27,697 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:27,697 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:27,697 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:27,697 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:27,697 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:14:27,697 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:14:27,851 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:14:31,030 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:14:31,030 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ddabb47666556855/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.18ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.08ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.70ba/s]100%|██████████| 4/4 [00:00<00:00,  4.64ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.18ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.37ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.43ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.48ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.48ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.49ba/s]100%|██████████| 7/7 [00:01<00:00,  4.96ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.01ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.26ba/s]100%|██████████| 4/4 [00:00<00:00, 11.58ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  7.83ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.73ba/s] 71%|███████▏  | 5/7 [00:00<00:00, 10.37ba/s]100%|██████████| 7/7 [00:00<00:00, 12.33ba/s]100%|██████████| 7/7 [00:00<00:00, 11.34ba/s]
[INFO|trainer.py:414] 2023-08-28 21:14:34,766 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:14:34,772 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:14:34,772 >>   Num examples = 3029
[INFO|trainer.py:1149] 2023-08-28 21:14:34,772 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:14:34,773 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:14:34,773 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:14:34,773 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:14:34,773 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:09,  3.37it/s]  1%|          | 2/235 [00:00<01:07,  3.44it/s]  1%|▏         | 3/235 [00:00<01:06,  3.47it/s]  2%|▏         | 4/235 [00:01<01:06,  3.48it/s]  2%|▏         | 5/235 [00:01<01:06,  3.48it/s]  3%|▎         | 6/235 [00:01<01:05,  3.49it/s]  3%|▎         | 7/235 [00:02<01:05,  3.49it/s]  3%|▎         | 8/235 [00:02<01:04,  3.49it/s]  4%|▍         | 9/235 [00:02<01:04,  3.50it/s]  4%|▍         | 10/235 [00:02<01:04,  3.50it/s]  5%|▍         | 11/235 [00:03<01:04,  3.49it/s]  5%|▌         | 12/235 [00:03<01:03,  3.49it/s]  6%|▌         | 13/235 [00:03<01:03,  3.50it/s]  6%|▌         | 14/235 [00:04<01:03,  3.50it/s]  6%|▋         | 15/235 [00:04<01:02,  3.50it/s]  7%|▋         | 16/235 [00:04<01:03,  3.46it/s]  7%|▋         | 17/235 [00:04<01:02,  3.47it/s]  8%|▊         | 18/235 [00:05<01:02,  3.48it/s]  8%|▊         | 19/235 [00:05<01:01,  3.48it/s]  9%|▊         | 20/235 [00:05<01:01,  3.49it/s]  9%|▉         | 21/235 [00:06<01:01,  3.49it/s]  9%|▉         | 22/235 [00:06<01:01,  3.49it/s] 10%|▉         | 23/235 [00:06<01:00,  3.49it/s] 10%|█         | 24/235 [00:06<01:00,  3.49it/s] 11%|█         | 25/235 [00:07<01:00,  3.49it/s] 11%|█         | 26/235 [00:07<00:59,  3.50it/s] 11%|█▏        | 27/235 [00:07<01:05,  3.18it/s] 12%|█▏        | 28/235 [00:08<01:03,  3.25it/s] 12%|█▏        | 29/235 [00:08<01:02,  3.32it/s] 13%|█▎        | 30/235 [00:08<01:00,  3.36it/s] 13%|█▎        | 31/235 [00:08<01:00,  3.40it/s] 14%|█▎        | 32/235 [00:09<00:59,  3.42it/s] 14%|█▍        | 33/235 [00:09<01:00,  3.32it/s] 14%|█▍        | 34/235 [00:09<00:59,  3.37it/s] 15%|█▍        | 35/235 [00:10<00:58,  3.40it/s] 15%|█▌        | 36/235 [00:10<00:58,  3.43it/s] 16%|█▌        | 37/235 [00:10<00:57,  3.44it/s] 16%|█▌        | 38/235 [00:11<00:57,  3.44it/s] 17%|█▋        | 39/235 [00:11<00:56,  3.46it/s] 17%|█▋        | 40/235 [00:11<00:56,  3.47it/s] 17%|█▋        | 41/235 [00:11<00:55,  3.47it/s] 18%|█▊        | 42/235 [00:12<00:55,  3.48it/s] 18%|█▊        | 43/235 [00:12<00:55,  3.48it/s] 19%|█▊        | 44/235 [00:12<00:54,  3.48it/s] 19%|█▉        | 45/235 [00:13<00:54,  3.48it/s] 20%|█▉        | 46/235 [00:13<00:54,  3.48it/s] 20%|██        | 47/235 [00:13<00:53,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 21:14:48,438 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:14:48,438 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 21:14:48,438 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.97it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.08it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.41it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.66it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.28it/s][A
  4%|▍         | 33/782 [00:00<00:17, 43.47it/s][A
  5%|▍         | 38/782 [00:00<00:16, 44.70it/s][A
  5%|▌         | 43/782 [00:00<00:16, 45.38it/s][A
  6%|▌         | 48/782 [00:01<00:15, 45.99it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.43it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.85it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.01it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.23it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.39it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.39it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.44it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.36it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.43it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.43it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.40it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.32it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.48it/s][A
 15%|█▌        | 118/782 [00:02<00:13, 47.48it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.53it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.55it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.10it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.23it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.19it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.33it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.35it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.41it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.44it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.41it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.46it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.43it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.51it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.53it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.46it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.35it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.41it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.47it/s][A
 27%|██▋       | 213/782 [00:04<00:11, 47.51it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.43it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.48it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.54it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.53it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.38it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.40it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.38it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.40it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.48it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.33it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.35it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.39it/s][A
 36%|███▌      | 278/782 [00:05<00:11, 45.13it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 45.84it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.34it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.68it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.83it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.08it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.25it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.22it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.13it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.13it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.06it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.28it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.37it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.49it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.40it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.55it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.35it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.27it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.31it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.28it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.27it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.33it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.39it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.43it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.46it/s][A
 52%|█████▏    | 403/782 [00:08<00:07, 47.48it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.38it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.27it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.18it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.19it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.28it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.35it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.42it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.41it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.39it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.27it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.32it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.26it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.23it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.24it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.25it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.15it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.14it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.12it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.15it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.23it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.31it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 47.16it/s][A
 66%|██████▌   | 518/782 [00:11<00:08, 30.12it/s][A
 67%|██████▋   | 523/782 [00:11<00:07, 33.80it/s][A
 68%|██████▊   | 528/782 [00:11<00:06, 37.00it/s][A
 68%|██████▊   | 533/782 [00:11<00:06, 39.57it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 41.51it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 43.11it/s][A
 70%|███████   | 548/782 [00:11<00:05, 44.33it/s][A
 71%|███████   | 553/782 [00:11<00:05, 45.12it/s][A
 71%|███████▏  | 558/782 [00:12<00:04, 45.80it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.22it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.52it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.69it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.90it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.92it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.06it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.15it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.23it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.20it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 47.28it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.28it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.24it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.29it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.35it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.26it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.25it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.26it/s][A
 83%|████████▎ | 648/782 [00:14<00:02, 47.28it/s][A
 84%|████████▎ | 653/782 [00:14<00:06, 19.73it/s][A
 84%|████████▍ | 658/782 [00:14<00:05, 23.90it/s][A
 85%|████████▍ | 663/782 [00:14<00:04, 28.07it/s][A
 85%|████████▌ | 668/782 [00:14<00:03, 32.01it/s][A
 86%|████████▌ | 673/782 [00:14<00:03, 35.50it/s][A
 87%|████████▋ | 678/782 [00:15<00:02, 38.42it/s][A
 87%|████████▋ | 683/782 [00:15<00:02, 40.81it/s][A
 88%|████████▊ | 688/782 [00:15<00:02, 42.57it/s][A
 89%|████████▊ | 693/782 [00:15<00:02, 43.75it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 44.72it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 45.45it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 45.98it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.28it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.61it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.86it/s][A
 93%|█████████▎| 728/782 [00:16<00:01, 47.03it/s][A
 94%|█████████▎| 733/782 [00:16<00:01, 47.11it/s][A
 94%|█████████▍| 738/782 [00:16<00:00, 47.07it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.91it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.84it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.89it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 39.66it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 41.63it/s][A
 98%|█████████▊| 768/782 [00:17<00:00, 43.01it/s][A
 99%|█████████▉| 773/782 [00:17<00:00, 44.19it/s][A
 99%|█████████▉| 778/782 [00:17<00:00, 45.13it/s][A
                                                 [A                                                
100%|██████████| 782/782 [00:17<00:00, 45.13it/s][A 20%|██        | 47/235 [00:31<00:53,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:15:06,668 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 21:15:07,722 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:15:19,500 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:15:20,852 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:15:21,015 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [00:56<40:47, 13.09s/it] 21%|██        | 49/235 [00:56<28:40,  9.25s/it] 21%|██▏       | 50/235 [00:57<20:13,  6.56s/it] 22%|██▏       | 51/235 [00:57<14:20,  4.68s/it] 22%|██▏       | 52/235 [00:57<10:14,  3.36s/it] 23%|██▎       | 53/235 [00:58<07:23,  2.44s/it] 23%|██▎       | 54/235 [00:58<05:24,  1.79s/it] 23%|██▎       | 55/235 [00:58<04:01,  1.34s/it] 24%|██▍       | 56/235 [00:58<03:03,  1.02s/it] 24%|██▍       | 57/235 [00:59<02:22,  1.25it/s] 25%|██▍       | 58/235 [00:59<01:54,  1.54it/s] 25%|██▌       | 59/235 [00:59<01:34,  1.85it/s] 26%|██▌       | 60/235 [01:00<01:21,  2.15it/s] 26%|██▌       | 61/235 [01:00<01:11,  2.43it/s] 26%|██▋       | 62/235 [01:00<01:04,  2.67it/s] 27%|██▋       | 63/235 [01:00<00:59,  2.88it/s] 27%|██▋       | 64/235 [01:01<00:56,  3.04it/s] 28%|██▊       | 65/235 [01:01<00:53,  3.16it/s] 28%|██▊       | 66/235 [01:01<00:51,  3.25it/s] 29%|██▊       | 67/235 [01:02<00:50,  3.32it/s] 29%|██▉       | 68/235 [01:02<00:49,  3.37it/s] 29%|██▉       | 69/235 [01:02<00:48,  3.41it/s] 30%|██▉       | 70/235 [01:02<00:48,  3.43it/s] 30%|███       | 71/235 [01:03<00:51,  3.21it/s] 31%|███       | 72/235 [01:03<00:49,  3.29it/s] 31%|███       | 73/235 [01:03<00:48,  3.35it/s] 31%|███▏      | 74/235 [01:04<00:47,  3.39it/s] 32%|███▏      | 75/235 [01:04<00:46,  3.42it/s] 32%|███▏      | 76/235 [01:04<00:46,  3.44it/s] 33%|███▎      | 77/235 [01:04<00:45,  3.45it/s] 33%|███▎      | 78/235 [01:05<00:45,  3.46it/s] 34%|███▎      | 79/235 [01:05<00:44,  3.47it/s] 34%|███▍      | 80/235 [01:05<00:44,  3.47it/s] 34%|███▍      | 81/235 [01:06<00:44,  3.48it/s] 35%|███▍      | 82/235 [01:06<00:44,  3.44it/s] 35%|███▌      | 83/235 [01:06<00:43,  3.46it/s] 36%|███▌      | 84/235 [01:06<00:43,  3.47it/s] 36%|███▌      | 85/235 [01:07<00:43,  3.47it/s] 37%|███▋      | 86/235 [01:07<00:42,  3.48it/s] 37%|███▋      | 87/235 [01:07<00:42,  3.45it/s] 37%|███▋      | 88/235 [01:08<00:42,  3.46it/s] 38%|███▊      | 89/235 [01:08<00:42,  3.46it/s] 38%|███▊      | 90/235 [01:08<00:41,  3.47it/s] 39%|███▊      | 91/235 [01:08<00:41,  3.47it/s] 39%|███▉      | 92/235 [01:09<00:41,  3.48it/s] 40%|███▉      | 93/235 [01:09<00:41,  3.45it/s] 40%|████      | 94/235 [01:09<00:40,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:15:44,687 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:15:44,687 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 21:15:44,687 >>   Batch size = 8
{'eval_loss': 0.9446118474006653, 'eval_runtime': 17.3717, 'eval_samples_per_second': 359.954, 'eval_steps_per_second': 45.016, 'epoch': 0.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.63it/s][A
  2%|▏         | 12/782 [00:00<00:14, 51.35it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.28it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.63it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.16it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.90it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.79it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.65it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.51it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.45it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.32it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.38it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.44it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.39it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.37it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.29it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.31it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.32it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.36it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.35it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.35it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.19it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.22it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.25it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.32it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.30it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.30it/s][A
 18%|█▊        | 143/782 [00:02<00:13, 47.32it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.30it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.25it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.31it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.33it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.32it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.32it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.13it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.28it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.32it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.37it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.35it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.42it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.27it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.27it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.31it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.26it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.22it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.23it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.20it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.23it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.26it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.32it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.28it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.32it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.59it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.74it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.95it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.09it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.22it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.26it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.21it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.23it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.26it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.31it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.37it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.28it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.34it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.24it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.27it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.33it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.34it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.32it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.29it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.24it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.26it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.28it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.27it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.27it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.27it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.23it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.21it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.23it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.14it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.14it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.22it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.24it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.27it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.25it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.22it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.22it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.25it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.32it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.25it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.29it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.20it/s][A
 60%|██████    | 473/782 [00:09<00:06, 47.22it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.29it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.28it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.20it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.29it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.32it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.16it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.08it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.12it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.16it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.23it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.23it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.28it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.14it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.18it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.27it/s][A
 71%|███████   | 553/782 [00:11<00:05, 45.31it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 45.87it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 46.26it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.55it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.78it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.93it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.94it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.09it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.11it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.12it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.16it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.26it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.30it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.23it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.17it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.27it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.23it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.21it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.23it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.25it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.34it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.28it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.28it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.33it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.29it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.33it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.32it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.27it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.20it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.18it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.25it/s][A
 91%|█████████ | 708/782 [00:14<00:01, 47.27it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.27it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.25it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.20it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.21it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.20it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.23it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.16it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.24it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.20it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.25it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.00it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.13it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.13it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.17it/s][A
                                                 [A                                                
100%|██████████| 782/782 [00:16<00:00, 47.17it/s][A 40%|████      | 94/235 [01:26<00:40,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:16:01,365 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 21:16:01,471 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:16:06,405 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:16:06,445 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:16:06,461 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [01:46<26:16, 11.26s/it] 41%|████      | 96/235 [01:47<18:31,  8.00s/it] 41%|████▏     | 97/235 [01:47<13:04,  5.68s/it] 42%|████▏     | 98/235 [01:47<09:16,  4.06s/it] 42%|████▏     | 99/235 [01:47<06:38,  2.93s/it] 43%|████▎     | 100/235 [01:48<04:48,  2.14s/it] 43%|████▎     | 101/235 [01:48<03:32,  1.58s/it] 43%|████▎     | 102/235 [01:48<02:38,  1.19s/it] 44%|████▍     | 103/235 [01:49<02:01,  1.08it/s] 44%|████▍     | 104/235 [01:49<01:35,  1.37it/s] 45%|████▍     | 105/235 [01:49<01:17,  1.67it/s] 45%|████▌     | 106/235 [01:49<01:05,  1.98it/s] 46%|████▌     | 107/235 [01:50<00:59,  2.15it/s] 46%|████▌     | 108/235 [01:50<00:52,  2.43it/s] 46%|████▋     | 109/235 [01:50<00:47,  2.67it/s] 47%|████▋     | 110/235 [01:51<00:43,  2.87it/s] 47%|████▋     | 111/235 [01:51<00:40,  3.03it/s] 48%|████▊     | 112/235 [01:51<00:38,  3.16it/s] 48%|████▊     | 113/235 [01:52<00:37,  3.25it/s] 49%|████▊     | 114/235 [01:52<00:36,  3.32it/s] 49%|████▉     | 115/235 [01:52<00:35,  3.36it/s] 49%|████▉     | 116/235 [01:52<00:35,  3.40it/s] 50%|████▉     | 117/235 [01:53<00:34,  3.42it/s] 50%|█████     | 118/235 [01:53<00:34,  3.41it/s] 51%|█████     | 119/235 [01:53<00:33,  3.44it/s] 51%|█████     | 120/235 [01:54<00:34,  3.36it/s] 51%|█████▏    | 121/235 [01:54<00:33,  3.40it/s] 52%|█████▏    | 122/235 [01:54<00:33,  3.42it/s] 52%|█████▏    | 123/235 [01:54<00:32,  3.44it/s] 53%|█████▎    | 124/235 [01:55<00:32,  3.45it/s] 53%|█████▎    | 125/235 [01:55<00:31,  3.46it/s] 54%|█████▎    | 126/235 [01:55<00:31,  3.47it/s] 54%|█████▍    | 127/235 [01:56<00:31,  3.47it/s] 54%|█████▍    | 128/235 [01:56<00:30,  3.47it/s] 55%|█████▍    | 129/235 [01:56<00:30,  3.48it/s] 55%|█████▌    | 130/235 [01:57<00:31,  3.38it/s] 56%|█████▌    | 131/235 [01:57<00:30,  3.41it/s] 56%|█████▌    | 132/235 [01:57<00:30,  3.43it/s] 57%|█████▋    | 133/235 [01:57<00:29,  3.45it/s] 57%|█████▋    | 134/235 [01:58<00:29,  3.46it/s] 57%|█████▋    | 135/235 [01:58<00:28,  3.47it/s] 58%|█████▊    | 136/235 [01:58<00:28,  3.47it/s] 58%|█████▊    | 137/235 [01:59<00:28,  3.47it/s] 59%|█████▊    | 138/235 [01:59<00:27,  3.48it/s] 59%|█████▉    | 139/235 [01:59<00:27,  3.48it/s] 60%|█████▉    | 140/235 [01:59<00:27,  3.48it/s] 60%|██████    | 141/235 [02:00<00:28,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 21:16:35,035 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:16:35,035 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 21:16:35,035 >>   Batch size = 8
{'eval_loss': 0.9537084698677063, 'eval_runtime': 16.6067, 'eval_samples_per_second': 376.535, 'eval_steps_per_second': 47.089, 'epoch': 1.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 58.07it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.04it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.24it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.58it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.22it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.97it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.73it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.55it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.52it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.45it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.48it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.38it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.40it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.27it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.30it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.34it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.33it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.33it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.39it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.26it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.29it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.24it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.27it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.25it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.19it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.49it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.77it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.97it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.09it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.15it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.09it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.04it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.13it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.19it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.20it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.26it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.18it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.24it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.24it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.29it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.29it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.27it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.27it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.25it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.27it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.29it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.24it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.26it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.29it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.30it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.25it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.24it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.28it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.28it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.28it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.28it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.28it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.27it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.35it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.39it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.27it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.28it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.34it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.31it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.32it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.36it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.38it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.25it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.29it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.33it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 45.05it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 45.76it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.19it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.51it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 46.78it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.92it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.07it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.19it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.21it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.26it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.33it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.28it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.09it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.43it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.71it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.91it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.07it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.09it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.16it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.24it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.24it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.29it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.33it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.24it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.23it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.27it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.35it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.23it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.32it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.36it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.33it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.28it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.24it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.19it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.09it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.08it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.08it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.15it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.22it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.19it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.19it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 44.75it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 45.49it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 45.95it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.27it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.66it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.83it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.06it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.11it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.21it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.26it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.26it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.33it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.34it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.27it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.32it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.35it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.34it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.38it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.37it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.33it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.38it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.37it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.36it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.42it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.31it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.21it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.34it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.37it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.32it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 40.59it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 42.41it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 43.79it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 44.84it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 45.63it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.19it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.56it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.82it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.76it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.80it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.87it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.99it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.10it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.10it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.16it/s][A                                                 
                                                 [A 60%|██████    | 141/235 [02:16<00:28,  3.32it/s]
100%|██████████| 782/782 [00:16<00:00, 47.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:16:51,728 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 21:16:51,743 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:17:01,444 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:17:02,149 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:17:02,244 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [02:41<19:42, 12.72s/it] 61%|██████    | 143/235 [02:42<13:53,  9.06s/it] 61%|██████▏   | 144/235 [02:42<09:45,  6.43s/it] 62%|██████▏   | 145/235 [02:43<06:52,  4.59s/it] 62%|██████▏   | 146/235 [02:43<04:53,  3.30s/it] 63%|██████▎   | 147/235 [02:43<03:30,  2.39s/it] 63%|██████▎   | 148/235 [02:43<02:33,  1.76s/it] 63%|██████▎   | 149/235 [02:44<01:53,  1.32s/it] 64%|██████▍   | 150/235 [02:44<01:25,  1.01s/it] 64%|██████▍   | 151/235 [02:44<01:06,  1.26it/s] 65%|██████▍   | 152/235 [02:45<00:53,  1.56it/s] 65%|██████▌   | 153/235 [02:45<00:45,  1.79it/s] 66%|██████▌   | 154/235 [02:45<00:38,  2.10it/s] 66%|██████▌   | 155/235 [02:45<00:33,  2.38it/s] 66%|██████▋   | 156/235 [02:46<00:30,  2.63it/s] 67%|██████▋   | 157/235 [02:46<00:27,  2.84it/s] 67%|██████▋   | 158/235 [02:46<00:25,  3.00it/s] 68%|██████▊   | 159/235 [02:47<00:24,  3.13it/s] 68%|██████▊   | 160/235 [02:47<00:23,  3.23it/s] 69%|██████▊   | 161/235 [02:47<00:22,  3.30it/s] 69%|██████▉   | 162/235 [02:47<00:21,  3.35it/s] 69%|██████▉   | 163/235 [02:48<00:21,  3.39it/s] 70%|██████▉   | 164/235 [02:48<00:27,  2.59it/s] 70%|███████   | 165/235 [02:49<00:24,  2.81it/s] 71%|███████   | 166/235 [02:49<00:23,  2.98it/s] 71%|███████   | 167/235 [02:49<00:21,  3.12it/s] 71%|███████▏  | 168/235 [02:50<00:20,  3.22it/s] 72%|███████▏  | 169/235 [02:50<00:20,  3.29it/s] 72%|███████▏  | 170/235 [02:50<00:19,  3.35it/s] 73%|███████▎  | 171/235 [02:50<00:18,  3.39it/s] 73%|███████▎  | 172/235 [02:51<00:18,  3.42it/s] 74%|███████▎  | 173/235 [02:51<00:18,  3.44it/s] 74%|███████▍  | 174/235 [02:51<00:18,  3.31it/s] 74%|███████▍  | 175/235 [02:52<00:17,  3.36it/s] 75%|███████▍  | 176/235 [02:52<00:17,  3.39it/s] 75%|███████▌  | 177/235 [02:52<00:16,  3.42it/s] 76%|███████▌  | 178/235 [02:52<00:16,  3.44it/s] 76%|███████▌  | 179/235 [02:53<00:16,  3.45it/s] 77%|███████▋  | 180/235 [02:53<00:15,  3.46it/s] 77%|███████▋  | 181/235 [02:53<00:15,  3.47it/s] 77%|███████▋  | 182/235 [02:54<00:15,  3.47it/s] 78%|███████▊  | 183/235 [02:54<00:14,  3.48it/s] 78%|███████▊  | 184/235 [02:54<00:14,  3.48it/s] 79%|███████▊  | 185/235 [02:54<00:14,  3.40it/s] 79%|███████▉  | 186/235 [02:55<00:14,  3.42it/s] 80%|███████▉  | 187/235 [02:55<00:15,  3.18it/s] 80%|████████  | 188/235 [02:55<00:14,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 21:17:30,712 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:17:30,712 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 21:17:30,712 >>   Batch size = 8
{'eval_loss': 0.9648634195327759, 'eval_runtime': 16.6861, 'eval_samples_per_second': 374.744, 'eval_steps_per_second': 46.865, 'epoch': 2.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.40it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.07it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.36it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.66it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.17it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.94it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.69it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.68it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.62it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.53it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.51it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.47it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.43it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.40it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.39it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.39it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.32it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.34it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.34it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.31it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.36it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.35it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.29it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.33it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.32it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.35it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.25it/s][A
 18%|█▊        | 143/782 [00:02<00:13, 47.31it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.34it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.39it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.37it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.38it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.25it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.31it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.36it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.32it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.34it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.39it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.31it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.32it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.39it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.37it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.29it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.29it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.29it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.32it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.35it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.32it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.31it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.87it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.99it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.12it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.21it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.25it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.23it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.29it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.24it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.37it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.39it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.47it/s][A
 39%|███▉      | 308/782 [00:06<00:09, 47.54it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.54it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.46it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.43it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.40it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.29it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.28it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.32it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.32it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.47it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.51it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.44it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.47it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.41it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.25it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.24it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.36it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.33it/s][A
 51%|█████     | 398/782 [00:08<00:08, 42.86it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 44.16it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 45.12it/s][A
 53%|█████▎    | 413/782 [00:08<00:08, 45.88it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.40it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.75it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.00it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.17it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.02it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.98it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.02it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.20it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.35it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.36it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.46it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.52it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.56it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.40it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.22it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.29it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.17it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.29it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.40it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.45it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.51it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.58it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.48it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.39it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.29it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.05it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.09it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.20it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.28it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.34it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.46it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.45it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.34it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.35it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.18it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.23it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.32it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.40it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.35it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.44it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.48it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.44it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.40it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.30it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.31it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.29it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.23it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.37it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.41it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.40it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.43it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.34it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.24it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.23it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.32it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.30it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.29it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.36it/s][A
 91%|█████████ | 708/782 [00:14<00:01, 47.37it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.41it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.39it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.32it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.27it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.26it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.27it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.31it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.32it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.27it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.40it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.44it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.38it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.22it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.31it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.31it/s][A 80%|████████  | 188/235 [03:12<00:14,  3.27it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:17:47,328 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 21:17:47,358 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:17:52,301 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:17:52,316 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:17:52,324 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [03:31<08:14, 10.76s/it] 81%|████████  | 190/235 [03:31<05:45,  7.68s/it] 81%|████████▏ | 191/235 [03:31<04:00,  5.46s/it] 82%|████████▏ | 192/235 [03:32<02:49,  3.93s/it] 82%|████████▏ | 193/235 [03:32<02:02,  2.92s/it] 83%|████████▎ | 194/235 [03:33<01:27,  2.13s/it] 83%|████████▎ | 195/235 [03:33<01:03,  1.58s/it] 83%|████████▎ | 196/235 [03:33<00:46,  1.19s/it] 84%|████████▍ | 197/235 [03:33<00:34,  1.09it/s] 84%|████████▍ | 198/235 [03:34<00:26,  1.37it/s] 85%|████████▍ | 199/235 [03:34<00:23,  1.51it/s] 85%|████████▌ | 200/235 [03:34<00:19,  1.81it/s] 86%|████████▌ | 201/235 [03:35<00:16,  2.12it/s] 86%|████████▌ | 202/235 [03:35<00:13,  2.40it/s] 86%|████████▋ | 203/235 [03:35<00:12,  2.65it/s] 87%|████████▋ | 204/235 [03:36<00:10,  2.86it/s] 87%|████████▋ | 205/235 [03:36<00:09,  3.02it/s] 88%|████████▊ | 206/235 [03:36<00:09,  3.15it/s] 88%|████████▊ | 207/235 [03:36<00:08,  3.24it/s] 89%|████████▊ | 208/235 [03:37<00:08,  3.31it/s] 89%|████████▉ | 209/235 [03:37<00:08,  3.13it/s] 89%|████████▉ | 210/235 [03:37<00:07,  3.23it/s] 90%|████████▉ | 211/235 [03:38<00:07,  3.31it/s] 90%|█████████ | 212/235 [03:38<00:06,  3.36it/s] 91%|█████████ | 213/235 [03:38<00:06,  3.39it/s] 91%|█████████ | 214/235 [03:39<00:06,  3.42it/s] 91%|█████████▏| 215/235 [03:39<00:05,  3.44it/s] 92%|█████████▏| 216/235 [03:39<00:05,  3.45it/s] 92%|█████████▏| 217/235 [03:39<00:05,  3.46it/s] 93%|█████████▎| 218/235 [03:40<00:04,  3.47it/s] 93%|█████████▎| 219/235 [03:40<00:04,  3.47it/s] 94%|█████████▎| 220/235 [03:40<00:04,  3.47it/s] 94%|█████████▍| 221/235 [03:41<00:04,  3.47it/s] 94%|█████████▍| 222/235 [03:41<00:03,  3.47it/s] 95%|█████████▍| 223/235 [03:41<00:03,  3.47it/s] 95%|█████████▌| 224/235 [03:41<00:03,  3.48it/s] 96%|█████████▌| 225/235 [03:42<00:02,  3.48it/s] 96%|█████████▌| 226/235 [03:42<00:02,  3.48it/s] 97%|█████████▋| 227/235 [03:42<00:02,  3.48it/s] 97%|█████████▋| 228/235 [03:43<00:02,  3.48it/s] 97%|█████████▋| 229/235 [03:43<00:01,  3.48it/s] 98%|█████████▊| 230/235 [03:43<00:01,  3.48it/s] 98%|█████████▊| 231/235 [03:44<00:01,  2.38it/s] 99%|█████████▊| 232/235 [03:44<00:01,  2.63it/s] 99%|█████████▉| 233/235 [03:44<00:00,  2.84it/s]100%|█████████▉| 234/235 [03:45<00:00,  3.01it/s]100%|██████████| 235/235 [03:45<00:00,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 21:18:20,310 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:18:20,310 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 21:18:20,310 >>   Batch size = 8
{'eval_loss': 0.974417507648468, 'eval_runtime': 16.5939, 'eval_samples_per_second': 376.826, 'eval_steps_per_second': 47.126, 'epoch': 3.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.97it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.02it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.33it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.62it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.36it/s][A
  4%|▍         | 33/782 [00:00<00:15, 48.15it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.96it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.85it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.64it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.52it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.45it/s][A
  8%|▊         | 63/782 [00:01<00:17, 41.27it/s][A
  9%|▊         | 68/782 [00:01<00:16, 43.01it/s][A
  9%|▉         | 73/782 [00:01<00:15, 44.34it/s][A
 10%|▉         | 78/782 [00:01<00:15, 45.26it/s][A
 11%|█         | 83/782 [00:01<00:15, 45.87it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.34it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.62it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.85it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.64it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.73it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.82it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.96it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 47.07it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.14it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.26it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.38it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.47it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.30it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.37it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.25it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.19it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.34it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.48it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.49it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.55it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.59it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.39it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.33it/s][A
 26%|██▌       | 203/782 [00:04<00:13, 43.89it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 44.88it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 45.62it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.21it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.59it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.91it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.14it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.25it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.17it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.07it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.14it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.28it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.26it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.32it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.37it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.29it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 47.52it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.44it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.37it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.33it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.33it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.30it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.30it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.40it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.35it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.49it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.50it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.39it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.39it/s][A
 45%|████▍     | 348/782 [00:07<00:11, 38.96it/s][A
 45%|████▌     | 353/782 [00:07<00:10, 41.19it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 42.82it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 44.21it/s][A
 47%|████▋     | 368/782 [00:07<00:09, 45.13it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 45.83it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.38it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.74it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.68it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.79it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.98it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.21it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.26it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.18it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.35it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 47.47it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.51it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.36it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.28it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.22it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.24it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.35it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.38it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.28it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 47.35it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.42it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.44it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.40it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 43.19it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 44.37it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 45.32it/s][A
 64%|██████▍   | 503/782 [00:10<00:06, 46.01it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.49it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.74it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 47.01it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.19it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.99it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.99it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.12it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.11it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.22it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.33it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.45it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 47.40it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.46it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.32it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.17it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.22it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.28it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.25it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.31it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.40it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.46it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.51it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.24it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.24it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.22it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.18it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.27it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.27it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.29it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.33it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 47.48it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.29it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.74it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.93it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.90it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.00it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.11it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.18it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.23it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 47.31it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.25it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.26it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.28it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.35it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.20it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.26it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.37it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.32it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.36it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 47.29it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.33it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.27it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.26it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.29it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.21it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.21it/s][A100%|██████████| 235/235 [04:02<00:00,  3.13it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:18:37,025 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 21:18:37,049 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:18:42,190 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:18:42,210 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:18:42,233 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:18:52,245 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:18:52,249 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47 (score: 0.9446118474006653).
                                                 100%|██████████| 235/235 [04:22<00:00,  3.13it/s]100%|██████████| 235/235 [04:22<00:00,  1.12s/it]
[INFO|trainer.py:1894] 2023-08-28 21:18:56,876 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 21:18:56,896 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:01,334 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:01,877 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:01,979 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:19:03,224 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:03,224 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:03,224 >>   train_loss               =     0.4567
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:03,224 >>   train_runtime            = 0:04:22.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:03,224 >>   train_samples            =       3029
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:03,224 >>   train_samples_per_second =     57.784
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:03,224 >>   train_steps_per_second   =      0.897
{'eval_loss': 0.9781278967857361, 'eval_runtime': 16.7033, 'eval_samples_per_second': 374.356, 'eval_steps_per_second': 46.817, 'epoch': 4.99}
{'train_runtime': 262.0971, 'train_samples_per_second': 57.784, 'train_steps_per_second': 0.897, 'train_loss': 0.4566680908203125, 'epoch': 4.99}
08/28/2023 21:19:04 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:19:04,654 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:04,654 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 21:19:04,654 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.75it/s]  2%|▏         | 12/782 [00:00<00:14, 51.75it/s]  2%|▏         | 18/782 [00:00<00:15, 49.80it/s]  3%|▎         | 24/782 [00:00<00:15, 48.98it/s]  4%|▎         | 29/782 [00:00<00:16, 46.62it/s]  4%|▍         | 34/782 [00:00<00:16, 44.88it/s]  5%|▍         | 39/782 [00:00<00:16, 45.89it/s]  6%|▌         | 44/782 [00:00<00:15, 46.32it/s]  6%|▋         | 49/782 [00:01<00:15, 46.66it/s]  7%|▋         | 54/782 [00:01<00:15, 46.86it/s]  8%|▊         | 59/782 [00:01<00:15, 45.86it/s]  8%|▊         | 64/782 [00:01<00:15, 45.29it/s]  9%|▉         | 69/782 [00:01<00:16, 43.77it/s]  9%|▉         | 74/782 [00:01<00:15, 44.91it/s] 10%|█         | 79/782 [00:01<00:15, 45.61it/s] 11%|█         | 84/782 [00:01<00:15, 46.09it/s] 11%|█▏        | 89/782 [00:01<00:14, 46.56it/s] 12%|█▏        | 94/782 [00:02<00:17, 39.79it/s] 13%|█▎        | 99/782 [00:02<00:21, 31.60it/s] 13%|█▎        | 103/782 [00:02<00:20, 32.57it/s] 14%|█▎        | 107/782 [00:02<00:20, 33.50it/s] 14%|█▍        | 111/782 [00:02<00:19, 34.22it/s] 15%|█▍        | 116/782 [00:02<00:17, 37.71it/s] 15%|█▌        | 121/782 [00:02<00:16, 40.43it/s] 16%|█▌        | 126/782 [00:02<00:15, 42.44it/s] 17%|█▋        | 131/782 [00:03<00:14, 43.94it/s] 17%|█▋        | 136/782 [00:03<00:14, 44.92it/s] 18%|█▊        | 141/782 [00:03<00:14, 45.77it/s] 19%|█▊        | 146/782 [00:03<00:13, 46.31it/s] 19%|█▉        | 151/782 [00:03<00:13, 46.67it/s] 20%|█▉        | 156/782 [00:03<00:14, 41.84it/s] 21%|██        | 161/782 [00:03<00:14, 43.38it/s] 21%|██        | 166/782 [00:03<00:13, 44.53it/s] 22%|██▏       | 171/782 [00:03<00:13, 45.43it/s] 23%|██▎       | 176/782 [00:04<00:13, 46.10it/s] 23%|██▎       | 181/782 [00:04<00:12, 46.49it/s] 24%|██▍       | 186/782 [00:04<00:12, 46.77it/s] 24%|██▍       | 191/782 [00:04<00:12, 47.00it/s] 25%|██▌       | 196/782 [00:04<00:12, 47.07it/s] 26%|██▌       | 201/782 [00:04<00:12, 47.21it/s] 26%|██▋       | 206/782 [00:04<00:12, 47.29it/s] 27%|██▋       | 211/782 [00:04<00:12, 47.35it/s] 28%|██▊       | 216/782 [00:04<00:11, 47.38it/s] 28%|██▊       | 221/782 [00:05<00:11, 47.46it/s] 29%|██▉       | 226/782 [00:05<00:11, 47.48it/s] 30%|██▉       | 231/782 [00:05<00:11, 47.54it/s] 30%|███       | 236/782 [00:05<00:11, 47.51it/s] 31%|███       | 241/782 [00:05<00:11, 47.38it/s] 31%|███▏      | 246/782 [00:05<00:11, 47.41it/s] 32%|███▏      | 251/782 [00:05<00:11, 47.46it/s] 33%|███▎      | 256/782 [00:05<00:11, 47.37it/s] 33%|███▎      | 261/782 [00:05<00:10, 47.44it/s] 34%|███▍      | 266/782 [00:05<00:10, 47.49it/s] 35%|███▍      | 271/782 [00:06<00:10, 47.50it/s] 35%|███▌      | 276/782 [00:06<00:10, 47.53it/s] 36%|███▌      | 281/782 [00:06<00:10, 47.48it/s] 37%|███▋      | 286/782 [00:06<00:10, 47.48it/s] 37%|███▋      | 291/782 [00:06<00:10, 47.47it/s] 38%|███▊      | 296/782 [00:06<00:10, 47.45it/s] 38%|███▊      | 301/782 [00:06<00:17, 27.26it/s] 39%|███▉      | 306/782 [00:07<00:15, 31.30it/s] 40%|███▉      | 311/782 [00:07<00:13, 34.87it/s] 40%|████      | 316/782 [00:07<00:12, 37.88it/s] 41%|████      | 321/782 [00:07<00:11, 40.34it/s] 42%|████▏     | 326/782 [00:07<00:10, 42.30it/s] 42%|████▏     | 331/782 [00:07<00:10, 43.80it/s] 43%|████▎     | 336/782 [00:07<00:09, 44.92it/s] 44%|████▎     | 341/782 [00:07<00:09, 45.28it/s] 44%|████▍     | 346/782 [00:07<00:09, 45.82it/s] 45%|████▍     | 351/782 [00:08<00:09, 46.29it/s] 46%|████▌     | 356/782 [00:08<00:09, 46.58it/s] 46%|████▌     | 361/782 [00:08<00:08, 46.95it/s] 47%|████▋     | 366/782 [00:08<00:08, 47.17it/s] 47%|████▋     | 371/782 [00:08<00:08, 47.28it/s] 48%|████▊     | 376/782 [00:08<00:08, 47.41it/s] 49%|████▊     | 381/782 [00:08<00:08, 47.48it/s] 49%|████▉     | 386/782 [00:08<00:08, 47.29it/s] 50%|█████     | 391/782 [00:08<00:08, 47.14it/s] 51%|█████     | 396/782 [00:08<00:08, 47.24it/s] 51%|█████▏    | 401/782 [00:09<00:08, 47.34it/s] 52%|█████▏    | 406/782 [00:09<00:07, 47.34it/s] 53%|█████▎    | 411/782 [00:09<00:07, 47.33it/s] 53%|█████▎    | 416/782 [00:09<00:07, 47.34it/s] 54%|█████▍    | 421/782 [00:09<00:07, 47.47it/s] 54%|█████▍    | 426/782 [00:09<00:07, 47.54it/s] 55%|█████▌    | 431/782 [00:09<00:07, 44.38it/s] 56%|█████▌    | 436/782 [00:09<00:07, 45.28it/s] 56%|█████▋    | 441/782 [00:09<00:07, 45.92it/s] 57%|█████▋    | 446/782 [00:10<00:07, 46.39it/s] 58%|█████▊    | 451/782 [00:10<00:07, 46.76it/s] 58%|█████▊    | 456/782 [00:10<00:06, 47.01it/s] 59%|█████▉    | 461/782 [00:10<00:06, 47.18it/s] 60%|█████▉    | 466/782 [00:10<00:06, 47.34it/s] 60%|██████    | 471/782 [00:10<00:06, 47.09it/s] 61%|██████    | 476/782 [00:10<00:06, 46.99it/s] 62%|██████▏   | 481/782 [00:10<00:06, 47.05it/s] 62%|██████▏   | 486/782 [00:10<00:06, 47.11it/s] 63%|██████▎   | 491/782 [00:10<00:06, 47.10it/s] 63%|██████▎   | 496/782 [00:11<00:06, 47.11it/s] 64%|██████▍   | 501/782 [00:11<00:05, 47.26it/s] 65%|██████▍   | 506/782 [00:11<00:05, 47.35it/s] 65%|██████▌   | 511/782 [00:11<00:05, 47.43it/s] 66%|██████▌   | 516/782 [00:11<00:05, 47.33it/s] 67%|██████▋   | 521/782 [00:11<00:05, 47.18it/s] 67%|██████▋   | 526/782 [00:11<00:05, 47.20it/s] 68%|██████▊   | 531/782 [00:11<00:05, 47.25it/s] 69%|██████▊   | 536/782 [00:11<00:05, 47.39it/s] 69%|██████▉   | 541/782 [00:12<00:05, 47.40it/s] 70%|██████▉   | 546/782 [00:12<00:04, 47.39it/s] 70%|███████   | 551/782 [00:12<00:04, 47.43it/s] 71%|███████   | 556/782 [00:12<00:04, 47.50it/s] 72%|███████▏  | 561/782 [00:12<00:04, 47.56it/s] 72%|███████▏  | 566/782 [00:12<00:04, 47.48it/s] 73%|███████▎  | 571/782 [00:12<00:04, 47.29it/s] 74%|███████▎  | 576/782 [00:12<00:06, 32.22it/s] 74%|███████▍  | 581/782 [00:13<00:05, 35.60it/s] 75%|███████▍  | 586/782 [00:13<00:05, 38.48it/s] 76%|███████▌  | 591/782 [00:13<00:04, 40.78it/s] 76%|███████▌  | 596/782 [00:13<00:04, 42.51it/s] 77%|███████▋  | 601/782 [00:13<00:04, 43.91it/s] 77%|███████▋  | 606/782 [00:13<00:03, 44.93it/s] 78%|███████▊  | 611/782 [00:13<00:03, 45.57it/s] 79%|███████▉  | 616/782 [00:13<00:03, 46.07it/s] 79%|███████▉  | 621/782 [00:13<00:03, 46.49it/s] 80%|████████  | 626/782 [00:13<00:03, 46.75it/s] 81%|████████  | 631/782 [00:14<00:03, 46.95it/s] 81%|████████▏ | 636/782 [00:14<00:03, 47.08it/s] 82%|████████▏ | 641/782 [00:14<00:02, 47.15it/s] 83%|████████▎ | 646/782 [00:14<00:02, 47.26it/s] 83%|████████▎ | 651/782 [00:14<00:02, 47.32it/s] 84%|████████▍ | 656/782 [00:14<00:02, 47.30it/s] 85%|████████▍ | 661/782 [00:14<00:02, 47.32it/s] 85%|████████▌ | 666/782 [00:14<00:02, 47.34it/s] 86%|████████▌ | 671/782 [00:14<00:02, 47.25it/s] 86%|████████▋ | 676/782 [00:15<00:02, 47.35it/s] 87%|████████▋ | 681/782 [00:15<00:02, 47.22it/s] 88%|████████▊ | 686/782 [00:15<00:02, 47.25it/s] 88%|████████▊ | 691/782 [00:15<00:01, 47.30it/s] 89%|████████▉ | 696/782 [00:15<00:01, 47.31it/s] 90%|████████▉ | 701/782 [00:15<00:01, 47.32it/s] 90%|█████████ | 706/782 [00:15<00:01, 47.36it/s] 91%|█████████ | 711/782 [00:15<00:01, 42.62it/s] 92%|█████████▏| 716/782 [00:15<00:01, 43.94it/s] 92%|█████████▏| 721/782 [00:16<00:01, 44.82it/s] 93%|█████████▎| 726/782 [00:16<00:01, 45.67it/s] 93%|█████████▎| 731/782 [00:16<00:01, 46.18it/s] 94%|█████████▍| 736/782 [00:16<00:00, 46.55it/s] 95%|█████████▍| 741/782 [00:16<00:00, 46.76it/s] 95%|█████████▌| 746/782 [00:16<00:00, 46.99it/s] 96%|█████████▌| 751/782 [00:16<00:00, 47.04it/s] 97%|█████████▋| 756/782 [00:16<00:00, 47.13it/s] 97%|█████████▋| 761/782 [00:16<00:00, 47.10it/s] 98%|█████████▊| 766/782 [00:16<00:00, 47.14it/s] 99%|█████████▊| 771/782 [00:17<00:00, 47.13it/s] 99%|█████████▉| 776/782 [00:17<00:00, 47.01it/s]100%|█████████▉| 781/782 [00:17<00:00, 47.08it/s]100%|██████████| 782/782 [00:17<00:00, 45.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:19:22,012 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   eval_loss               =     0.9446
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   eval_runtime            = 0:00:17.35
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   eval_samples_per_second =    360.246
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   eval_steps_per_second   =     45.052
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:19:22,012 >>   perplexity              =     2.5718
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:41,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:41,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:41,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:41,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:41,042 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:19:42,670 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:19:42,672 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:19:43,369 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:19:44,417 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:19:44,427 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:47,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:47,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:47,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:47,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:19:47,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:19:48,083 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:19:48,118 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:19:48,683 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:19:48,838 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:19:48,838 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.43it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:14,  1.54it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:20,  1.50it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:22,  1.50it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:27,  1.21it/s]Extractor Predicting: 41it [00:27,  1.29it/s]Extractor Predicting: 42it [00:28,  1.36it/s]Extractor Predicting: 43it [00:29,  1.41it/s]Extractor Predicting: 44it [00:29,  1.43it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.53it/s]Extractor Predicting: 49it [00:33,  1.43it/s]Extractor Predicting: 50it [00:33,  1.46it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.49it/s]Extractor Predicting: 55it [00:37,  1.53it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:39,  1.53it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:40,  1.49it/s]Extractor Predicting: 61it [00:41,  1.47it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.49it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.41it/s]Extractor Predicting: 67it [00:45,  1.45it/s]Extractor Predicting: 68it [00:45,  1.49it/s]Extractor Predicting: 69it [00:46,  1.47it/s]Extractor Predicting: 70it [00:47,  1.47it/s]Extractor Predicting: 71it [00:47,  1.46it/s]Extractor Predicting: 72it [00:48,  1.48it/s]Extractor Predicting: 73it [00:49,  1.46it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:51,  1.50it/s]Extractor Predicting: 77it [00:52,  1.47it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:53,  1.49it/s]Extractor Predicting: 80it [00:54,  1.48it/s]Extractor Predicting: 81it [00:54,  1.48it/s]Extractor Predicting: 82it [00:55,  1.51it/s]Extractor Predicting: 83it [00:56,  1.48it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:57,  1.48it/s]Extractor Predicting: 86it [00:58,  1.48it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:59,  1.50it/s]Extractor Predicting: 89it [01:00,  1.50it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.36it/s]Extractor Predicting: 92it [01:02,  1.39it/s]Extractor Predicting: 93it [01:02,  1.42it/s]Extractor Predicting: 94it [01:03,  1.45it/s]Extractor Predicting: 95it [01:04,  1.47it/s]Extractor Predicting: 96it [01:04,  1.45it/s]Extractor Predicting: 97it [01:05,  1.44it/s]Extractor Predicting: 98it [01:06,  1.45it/s]Extractor Predicting: 99it [01:07,  1.44it/s]Extractor Predicting: 100it [01:07,  1.47it/s]Extractor Predicting: 101it [01:08,  1.47it/s]Extractor Predicting: 102it [01:09,  1.49it/s]Extractor Predicting: 103it [01:09,  1.50it/s]Extractor Predicting: 104it [01:10,  1.51it/s]Extractor Predicting: 105it [01:11,  1.49it/s]Extractor Predicting: 106it [01:11,  1.49it/s]Extractor Predicting: 107it [01:12,  1.46it/s]Extractor Predicting: 108it [01:13,  1.45it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:14,  1.48it/s]Extractor Predicting: 111it [01:15,  1.38it/s]Extractor Predicting: 112it [01:15,  1.41it/s]Extractor Predicting: 113it [01:16,  1.46it/s]Extractor Predicting: 114it [01:17,  1.47it/s]Extractor Predicting: 115it [01:17,  1.47it/s]Extractor Predicting: 116it [01:18,  1.47it/s]Extractor Predicting: 117it [01:19,  1.46it/s]Extractor Predicting: 118it [01:19,  1.47it/s]Extractor Predicting: 119it [01:20,  1.46it/s]Extractor Predicting: 120it [01:21,  1.47it/s]Extractor Predicting: 121it [01:22,  1.42it/s]Extractor Predicting: 122it [01:22,  1.45it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:24,  1.47it/s]Extractor Predicting: 125it [01:24,  1.45it/s]Extractor Predicting: 126it [01:25,  1.43it/s]Extractor Predicting: 127it [01:26,  1.44it/s]Extractor Predicting: 128it [01:26,  1.44it/s]Extractor Predicting: 129it [01:27,  1.45it/s]Extractor Predicting: 130it [01:28,  1.44it/s]Extractor Predicting: 131it [01:28,  1.44it/s]Extractor Predicting: 132it [01:29,  1.44it/s]Extractor Predicting: 133it [01:30,  1.46it/s]Extractor Predicting: 134it [01:31,  1.45it/s]Extractor Predicting: 135it [01:31,  1.44it/s]Extractor Predicting: 136it [01:32,  1.46it/s]Extractor Predicting: 137it [01:33,  1.46it/s]Extractor Predicting: 138it [01:33,  1.49it/s]Extractor Predicting: 139it [01:34,  1.48it/s]Extractor Predicting: 140it [01:35,  1.48it/s]Extractor Predicting: 141it [01:35,  1.46it/s]Extractor Predicting: 142it [01:36,  1.48it/s]Extractor Predicting: 143it [01:37,  1.46it/s]Extractor Predicting: 144it [01:37,  1.48it/s]Extractor Predicting: 145it [01:38,  1.49it/s]Extractor Predicting: 146it [01:39,  1.50it/s]Extractor Predicting: 147it [01:39,  1.49it/s]Extractor Predicting: 148it [01:40,  1.54it/s]Extractor Predicting: 149it [01:41,  1.53it/s]Extractor Predicting: 150it [01:41,  1.52it/s]Extractor Predicting: 151it [01:42,  1.49it/s]Extractor Predicting: 152it [01:43,  1.54it/s]Extractor Predicting: 153it [01:43,  1.52it/s]Extractor Predicting: 154it [01:44,  1.51it/s]Extractor Predicting: 155it [01:45,  1.52it/s]Extractor Predicting: 156it [01:45,  1.48it/s]Extractor Predicting: 157it [01:46,  1.53it/s]Extractor Predicting: 158it [01:47,  1.54it/s]Extractor Predicting: 159it [01:47,  1.55it/s]Extractor Predicting: 160it [01:48,  1.50it/s]Extractor Predicting: 161it [01:49,  1.48it/s]Extractor Predicting: 162it [01:49,  1.50it/s]Extractor Predicting: 163it [01:50,  1.53it/s]Extractor Predicting: 164it [01:50,  1.55it/s]Extractor Predicting: 165it [01:51,  1.59it/s]Extractor Predicting: 166it [01:52,  1.59it/s]Extractor Predicting: 167it [01:52,  1.57it/s]Extractor Predicting: 168it [01:53,  1.54it/s]Extractor Predicting: 169it [01:54,  1.56it/s]Extractor Predicting: 170it [01:54,  1.57it/s]Extractor Predicting: 171it [01:55,  1.55it/s]Extractor Predicting: 172it [01:56,  1.54it/s]Extractor Predicting: 173it [01:56,  1.56it/s]Extractor Predicting: 174it [01:57,  1.53it/s]Extractor Predicting: 175it [01:58,  1.54it/s]Extractor Predicting: 176it [01:58,  1.54it/s]Extractor Predicting: 177it [01:59,  1.54it/s]Extractor Predicting: 178it [01:59,  1.54it/s]Extractor Predicting: 179it [02:00,  1.55it/s]Extractor Predicting: 180it [02:01,  1.54it/s]Extractor Predicting: 181it [02:02,  1.40it/s]Extractor Predicting: 182it [02:02,  1.42it/s]Extractor Predicting: 183it [02:03,  1.47it/s]Extractor Predicting: 184it [02:04,  1.50it/s]Extractor Predicting: 185it [02:04,  1.50it/s]Extractor Predicting: 186it [02:05,  1.45it/s]Extractor Predicting: 187it [02:06,  1.51it/s]Extractor Predicting: 188it [02:06,  1.49it/s]Extractor Predicting: 189it [02:07,  1.51it/s]Extractor Predicting: 190it [02:08,  1.54it/s]Extractor Predicting: 191it [02:08,  1.56it/s]Extractor Predicting: 192it [02:09,  1.58it/s]Extractor Predicting: 193it [02:09,  1.55it/s]Extractor Predicting: 194it [02:10,  1.56it/s]Extractor Predicting: 195it [02:11,  1.55it/s]Extractor Predicting: 196it [02:11,  1.52it/s]Extractor Predicting: 197it [02:12,  1.56it/s]Extractor Predicting: 198it [02:13,  1.54it/s]Extractor Predicting: 199it [02:13,  1.53it/s]Extractor Predicting: 200it [02:14,  1.59it/s]Extractor Predicting: 201it [02:15,  1.60it/s]Extractor Predicting: 202it [02:15,  1.60it/s]Extractor Predicting: 203it [02:16,  1.61it/s]Extractor Predicting: 204it [02:16,  1.59it/s]Extractor Predicting: 205it [02:17,  1.58it/s]Extractor Predicting: 206it [02:18,  1.61it/s]Extractor Predicting: 207it [02:18,  1.62it/s]Extractor Predicting: 208it [02:19,  1.60it/s]Extractor Predicting: 209it [02:20,  1.56it/s]Extractor Predicting: 210it [02:20,  1.57it/s]Extractor Predicting: 211it [02:21,  1.58it/s]Extractor Predicting: 212it [02:21,  1.59it/s]Extractor Predicting: 213it [02:22,  1.57it/s]Extractor Predicting: 214it [02:23,  1.59it/s]Extractor Predicting: 215it [02:23,  1.57it/s]Extractor Predicting: 216it [02:24,  1.57it/s]Extractor Predicting: 217it [02:25,  1.57it/s]Extractor Predicting: 218it [02:25,  1.58it/s]Extractor Predicting: 219it [02:26,  1.55it/s]Extractor Predicting: 220it [02:27,  1.54it/s]Extractor Predicting: 221it [02:27,  1.56it/s]Extractor Predicting: 222it [02:28,  1.56it/s]Extractor Predicting: 223it [02:29,  1.57it/s]Extractor Predicting: 224it [02:29,  1.57it/s]Extractor Predicting: 225it [02:30,  1.56it/s]Extractor Predicting: 226it [02:30,  1.56it/s]Extractor Predicting: 227it [02:31,  1.58it/s]Extractor Predicting: 228it [02:31,  1.75it/s]Extractor Predicting: 228it [02:31,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:31,155 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:31,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:31,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:31,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:31,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:22:31,770 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:22:31,771 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:32,456 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:33,487 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:33,487 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:36,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:36,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:36,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:36,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:36,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:22:37,206 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:22:37,208 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:37,805 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:37,950 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:37,950 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.12049203242941012,
  "recall": 0.06892691508076124,
  "score": 0.08769074262461851,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:09,  1.47it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.46it/s]Extractor Predicting: 28it [00:18,  1.44it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.44it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:25,  1.41it/s]Extractor Predicting: 39it [00:26,  1.38it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:28,  1.44it/s]Extractor Predicting: 43it [00:29,  1.44it/s]Extractor Predicting: 44it [00:29,  1.45it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:31,  1.59it/s]Extractor Predicting: 48it [00:32,  1.60it/s]Extractor Predicting: 49it [00:32,  1.63it/s]Extractor Predicting: 50it [00:33,  1.59it/s]Extractor Predicting: 51it [00:34,  1.59it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:37,  1.54it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.55it/s]Extractor Predicting: 61it [00:40,  1.58it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 64it [00:42,  1.55it/s]Extractor Predicting: 65it [00:43,  1.57it/s]Extractor Predicting: 66it [00:43,  1.56it/s]Extractor Predicting: 67it [00:44,  1.55it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:45,  1.56it/s]Extractor Predicting: 70it [00:46,  1.57it/s]Extractor Predicting: 71it [00:47,  1.59it/s]Extractor Predicting: 72it [00:47,  1.58it/s]Extractor Predicting: 73it [00:48,  1.56it/s]Extractor Predicting: 74it [00:48,  1.56it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.57it/s]Extractor Predicting: 77it [00:50,  1.54it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.40it/s]Extractor Predicting: 80it [00:53,  1.43it/s]Extractor Predicting: 81it [00:53,  1.44it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.54it/s]Extractor Predicting: 86it [00:56,  1.55it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:58,  1.54it/s]Extractor Predicting: 89it [00:58,  1.55it/s]Extractor Predicting: 90it [00:59,  1.57it/s]Extractor Predicting: 91it [01:00,  1.53it/s]Extractor Predicting: 92it [01:00,  1.55it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.55it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:04,  1.55it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:06,  1.54it/s]Extractor Predicting: 101it [01:06,  1.55it/s]Extractor Predicting: 102it [01:07,  1.56it/s]Extractor Predicting: 103it [01:07,  1.56it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:11,  1.55it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:13,  1.56it/s]Extractor Predicting: 112it [01:13,  1.57it/s]Extractor Predicting: 113it [01:14,  1.59it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:18,  1.62it/s]Extractor Predicting: 120it [01:18,  1.63it/s]Extractor Predicting: 121it [01:19,  1.63it/s]Extractor Predicting: 122it [01:20,  1.64it/s]Extractor Predicting: 123it [01:20,  1.65it/s]Extractor Predicting: 124it [01:21,  1.65it/s]Extractor Predicting: 125it [01:21,  1.59it/s]Extractor Predicting: 126it [01:22,  1.62it/s]Extractor Predicting: 127it [01:23,  1.59it/s]Extractor Predicting: 128it [01:23,  1.55it/s]Extractor Predicting: 129it [01:24,  1.55it/s]Extractor Predicting: 130it [01:25,  1.58it/s]Extractor Predicting: 131it [01:25,  1.56it/s]Extractor Predicting: 132it [01:26,  1.60it/s]Extractor Predicting: 133it [01:26,  1.59it/s]Extractor Predicting: 134it [01:27,  1.58it/s]Extractor Predicting: 135it [01:28,  1.58it/s]Extractor Predicting: 136it [01:28,  1.61it/s]Extractor Predicting: 137it [01:29,  1.59it/s]Extractor Predicting: 138it [01:30,  1.49it/s]Extractor Predicting: 139it [01:30,  1.51it/s]Extractor Predicting: 140it [01:31,  1.56it/s]Extractor Predicting: 141it [01:32,  1.60it/s]Extractor Predicting: 142it [01:32,  1.63it/s]Extractor Predicting: 143it [01:33,  1.62it/s]Extractor Predicting: 144it [01:33,  1.63it/s]Extractor Predicting: 145it [01:34,  1.62it/s]Extractor Predicting: 146it [01:35,  1.60it/s]Extractor Predicting: 147it [01:35,  1.62it/s]Extractor Predicting: 148it [01:36,  1.57it/s]Extractor Predicting: 149it [01:37,  1.54it/s]Extractor Predicting: 150it [01:37,  1.50it/s]Extractor Predicting: 151it [01:38,  1.53it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:39,  1.43it/s]Extractor Predicting: 154it [01:40,  1.45it/s]Extractor Predicting: 155it [01:41,  1.43it/s]Extractor Predicting: 156it [01:41,  1.47it/s]Extractor Predicting: 157it [01:42,  1.47it/s]Extractor Predicting: 158it [01:43,  1.47it/s]Extractor Predicting: 159it [01:43,  1.49it/s]Extractor Predicting: 160it [01:44,  1.53it/s]Extractor Predicting: 161it [01:45,  1.56it/s]Extractor Predicting: 162it [01:45,  1.58it/s]Extractor Predicting: 163it [01:46,  1.55it/s]Extractor Predicting: 164it [01:47,  1.58it/s]Extractor Predicting: 165it [01:47,  1.56it/s]Extractor Predicting: 166it [01:48,  1.59it/s]Extractor Predicting: 167it [01:48,  1.57it/s]Extractor Predicting: 168it [01:49,  1.57it/s]Extractor Predicting: 169it [01:50,  1.59it/s]Extractor Predicting: 170it [01:50,  1.57it/s]Extractor Predicting: 171it [01:51,  1.55it/s]Extractor Predicting: 172it [01:52,  1.56it/s]Extractor Predicting: 173it [01:52,  1.53it/s]Extractor Predicting: 174it [01:53,  1.54it/s]Extractor Predicting: 175it [01:54,  1.54it/s]Extractor Predicting: 176it [01:54,  1.55it/s]Extractor Predicting: 177it [01:55,  1.53it/s]Extractor Predicting: 178it [01:56,  1.50it/s]Extractor Predicting: 179it [01:56,  1.53it/s]Extractor Predicting: 180it [01:57,  1.55it/s]Extractor Predicting: 181it [01:57,  1.58it/s]Extractor Predicting: 182it [01:58,  1.61it/s]Extractor Predicting: 183it [01:59,  1.56it/s]Extractor Predicting: 184it [01:59,  1.59it/s]Extractor Predicting: 185it [02:00,  1.45it/s]Extractor Predicting: 186it [02:01,  1.48it/s]Extractor Predicting: 187it [02:01,  1.53it/s]Extractor Predicting: 188it [02:02,  1.56it/s]Extractor Predicting: 189it [02:03,  1.52it/s]Extractor Predicting: 190it [02:03,  1.47it/s]Extractor Predicting: 191it [02:04,  1.49it/s]Extractor Predicting: 192it [02:05,  1.48it/s]Extractor Predicting: 193it [02:05,  1.49it/s]Extractor Predicting: 194it [02:06,  1.50it/s]Extractor Predicting: 195it [02:07,  1.47it/s]Extractor Predicting: 196it [02:08,  1.46it/s]Extractor Predicting: 197it [02:08,  1.47it/s]Extractor Predicting: 198it [02:09,  1.49it/s]Extractor Predicting: 199it [02:10,  1.48it/s]Extractor Predicting: 200it [02:10,  1.48it/s]Extractor Predicting: 201it [02:11,  1.48it/s]Extractor Predicting: 202it [02:12,  1.48it/s]Extractor Predicting: 203it [02:12,  1.49it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:14,  1.45it/s]Extractor Predicting: 206it [02:14,  1.45it/s]Extractor Predicting: 207it [02:15,  1.42it/s]Extractor Predicting: 208it [02:16,  1.42it/s]Extractor Predicting: 209it [02:16,  1.47it/s]Extractor Predicting: 210it [02:17,  1.50it/s]Extractor Predicting: 211it [02:18,  1.50it/s]Extractor Predicting: 212it [02:18,  1.52it/s]Extractor Predicting: 213it [02:19,  1.51it/s]Extractor Predicting: 214it [02:20,  1.48it/s]Extractor Predicting: 215it [02:20,  1.48it/s]Extractor Predicting: 216it [02:21,  1.47it/s]Extractor Predicting: 217it [02:22,  1.45it/s]Extractor Predicting: 218it [02:22,  1.48it/s]Extractor Predicting: 219it [02:23,  1.50it/s]Extractor Predicting: 220it [02:24,  1.49it/s]Extractor Predicting: 221it [02:24,  1.52it/s]Extractor Predicting: 222it [02:25,  1.48it/s]Extractor Predicting: 223it [02:26,  1.45it/s]Extractor Predicting: 224it [02:27,  1.43it/s]Extractor Predicting: 225it [02:27,  1.41it/s]Extractor Predicting: 226it [02:28,  1.42it/s]Extractor Predicting: 227it [02:29,  1.40it/s]Extractor Predicting: 228it [02:29,  1.39it/s]Extractor Predicting: 229it [02:30,  1.39it/s]Extractor Predicting: 230it [02:31,  1.39it/s]Extractor Predicting: 231it [02:32,  1.39it/s]Extractor Predicting: 232it [02:32,  1.40it/s]Extractor Predicting: 233it [02:33,  1.41it/s]Extractor Predicting: 234it [02:34,  1.42it/s]Extractor Predicting: 235it [02:34,  1.42it/s]Extractor Predicting: 236it [02:35,  1.42it/s]Extractor Predicting: 237it [02:36,  1.39it/s]Extractor Predicting: 238it [02:37,  1.39it/s]Extractor Predicting: 239it [02:37,  1.42it/s]Extractor Predicting: 240it [02:38,  1.45it/s]Extractor Predicting: 241it [02:39,  1.46it/s]Extractor Predicting: 242it [02:39,  1.47it/s]Extractor Predicting: 243it [02:40,  1.48it/s]Extractor Predicting: 244it [02:41,  1.45it/s]Extractor Predicting: 245it [02:41,  1.48it/s]Extractor Predicting: 246it [02:42,  1.53it/s]Extractor Predicting: 246it [02:42,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:33,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:33,528 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:33,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:33,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:33,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:25:34,362 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:25:34,363 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:35,154 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:36,171 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:36,172 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:39,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:25:40,391 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:25:40,392 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:41,024 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:41,166 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:41,167 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2786727578744891,
  "recall": 0.1964406779661017,
  "score": 0.23044040163038076,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.72it/s]Extractor Predicting: 15it [00:09,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:25:51,770 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:51,771 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:25:51,777 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:51,778 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:25:51,783 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:25:58,286 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:25:58,291 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:25:58,305 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:58,305 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:25:58,311 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:58,314 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:58,314 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:58,314 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:58,314 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:58,314 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:58,314 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.40077821011673154,
  "recall": 0.1446629213483146,
  "score": 0.21259029927760575,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:25:58,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:59,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:25:59,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:00,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:01,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:02,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:02,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:03,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:04,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:04,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:05,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:06,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:06,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:07,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:08,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:08,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:09,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:10,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:11,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:11,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:12,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:26, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:13,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:14,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:14,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:15,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:16,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:16,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:17,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:18,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:18,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:19,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:20,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:20,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:21,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:22,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:22,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:23,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:24,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:25,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:25,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:26,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:27,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:10, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:27,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:28,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:29,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:30,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:31,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:31,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:32,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:33,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:34,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:34,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:35,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:36,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:37,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:38,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:38,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:39,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:40,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:40,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:41,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:42,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:43,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:45<03:02, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:43,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:44,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:45,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:46,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:46,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:47,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:48,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:48,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:49,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:50,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:51,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:51,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:52,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:53,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:54,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:54,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:55,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:56,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:57,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:57,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:59<02:45, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-28 21:26:58,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:59,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:26:59,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:00,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:00,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:01,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:02,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:02,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:03,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:03,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:04,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:05,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:06,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:06,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:08,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:08,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:09,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:09,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:10,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:11,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:13<02:24, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:11,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:12,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:13,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:13,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:14,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:15,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:16,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:16,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:17,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:18,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:19,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:19,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:20,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:21,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:21,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:22,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:23,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:24,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:24,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:25,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:26,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:28<02:13, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:27,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:28,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:28,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:29,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:29,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:30,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:31,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:31,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:32,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:33,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:33,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:34,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:35,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:35,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:36,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:37,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:38,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:38,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:39,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:39,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:42<01:55, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:40,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:41,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:42,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:43,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:43,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:44,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:45,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:46,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:46,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:47,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:48,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:49,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:49,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:50,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:51,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:52,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:52,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:53,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:54,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:54,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:57<01:41, 14.49s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:55,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:56,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:57,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:57,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:58,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:59,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:59,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:00,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:01,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:01,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:02,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:03,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:03,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:04,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:05,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:06,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:06,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:07,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:08,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:08,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:09,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:10,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:12<01:28, 14.75s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:10,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:11,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:12,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:13,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:14,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:15,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:15,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:16,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:17,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:18,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:18,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:19,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:20,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:20,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:21,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:22,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:23,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:24,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:24,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:25,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:26,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:28<01:16, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:27,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:27,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:28,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:29,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:30,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:30,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:31,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:32,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:33,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:33,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:34,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:35,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:36,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:36,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:37,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:38,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:38,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:39,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:40,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:41,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:41,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:42,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:44<01:01, 15.47s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:43,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:44,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:44,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:45,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:46,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:46,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:47,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:48,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:48,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:49,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:50,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:51,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:51,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:52,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:53,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:54,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:54,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:55,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:55,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:56,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:58<00:45, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:57,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:57,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:58,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:59,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:00,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:01,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:01,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:02,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:03,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:04,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:04,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:05,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:06,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:07,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:07,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:08,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:09,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:09,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:10,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:11,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:12,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:14<00:30, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:12,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:13,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:14,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:14,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:15,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:16,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:16,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:17,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:18,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:19,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:19,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:20,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:21,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:22,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:22,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:23,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:24,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:24,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:25,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:26,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:27,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:29<00:15, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:28,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:28,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:29,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:30,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:30,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:31,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:32,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:32,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:33,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:34,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:35,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:35,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:36,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:37,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:37,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:38,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:38,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:39,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:40,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:40,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:41,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:42,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:42,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:43,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:44,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:44,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:45,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:46,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:46,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:48<00:00, 16.48s/it]Generating: 100%|██████████| 15/15 [03:48<00:00, 15.26s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:58,217 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:58,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:58,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:58,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:58,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:29:59,299 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:29:59,300 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:00,608 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:01,694 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:01,794 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:07,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:08,074 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:08,074 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:08,074 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:08,074 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:30:09,333 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:30:09,334 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:10,641 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:10,798 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:10,846 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : director .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9166666666666666, 'errors': {''}}
['Relation : occupant . Context : Later in the year , the house was bought by a young French designer who had recently moved to Los Angeles , where he was a student in the history department of the American Academy of Fine Arts . Head Entity : Louis , Tail Entity : Louis .\n']
['Relation : occupant . Context : Later in the year , the house was bought by a young French designer who had recently moved to Los Angeles , where he was a student in the history department of the American Academy of Fine Arts . Head Entity : Louis , Tail Entity : Louis .\n', 'Relation : occupant . Context : After the death of King Henry IV of England , the city entered an extensive Roman occupation , under his command of the house of Henry I of Scotland . Head Entity : Henry IV of England , Tail Entity : Henry IV of Scotland .\n']
['Relation : occupant . Context : Later in the year , the house was bought by a young French designer who had recently moved to Los Angeles , where he was a student in the history department of the American Academy of Fine Arts . Head Entity : Louis , Tail Entity : Louis .\n', 'Relation : occupant . Context : After the death of King Henry IV of England , the city entered an extensive Roman occupation , under his command of the house of Henry I of Scotland . Head Entity : Henry IV of England , Tail Entity : Henry IV of Scotland .\n', 'Relation : occupant . Context : This was the first of four such luxury mansions built by the French emperor of the 17th century and the first of two luxury mansions built by the modern Italian emperor . Head Entity : Emperor of the 17th century , Tail Entity : the French .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.9390625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : use .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 186, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 283, 'raw': 448}
{'target': 600, 'success': 307, 'raw': 480}
{'target': 600, 'success': 333, 'raw': 512}
{'target': 600, 'success': 358, 'raw': 544}
{'target': 600, 'success': 379, 'raw': 576}
{'target': 600, 'success': 399, 'raw': 608}
{'target': 600, 'success': 420, 'raw': 640}
{'target': 600, 'success': 440, 'raw': 672}
{'target': 600, 'success': 460, 'raw': 704}
{'target': 600, 'success': 484, 'raw': 736}
{'target': 600, 'success': 504, 'raw': 768}
{'target': 600, 'success': 524, 'raw': 800}
{'target': 600, 'success': 543, 'raw': 832}
{'target': 600, 'success': 565, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 612, 'raw': 928}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6594827586206896, 'errors': {'', "('The Voice of America', 'voice type', '', 'He won the lead role on the hit series The Voice of America with his dramatic role in the 2010 comedy film The Voice of America .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 9199
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9299, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.43it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.61it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.54it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.60it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:10,  1.63it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.54it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.52it/s]Extractor Estimating: 23it [00:14,  1.55it/s]Extractor Estimating: 24it [00:15,  1.49it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:16,  1.53it/s]Extractor Estimating: 27it [00:17,  1.52it/s]Extractor Estimating: 28it [00:17,  1.56it/s]Extractor Estimating: 29it [00:18,  1.50it/s]Extractor Estimating: 30it [00:19,  1.57it/s]Extractor Estimating: 31it [00:19,  1.60it/s]Extractor Estimating: 32it [00:20,  1.55it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:21,  1.53it/s]Extractor Estimating: 35it [00:22,  1.59it/s]Extractor Estimating: 36it [00:23,  1.59it/s]Extractor Estimating: 37it [00:23,  1.58it/s]Extractor Estimating: 38it [00:24,  1.60it/s]Extractor Estimating: 39it [00:25,  1.50it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:26,  1.61it/s]Extractor Estimating: 42it [00:26,  1.58it/s]Extractor Estimating: 43it [00:27,  1.62it/s]Extractor Estimating: 44it [00:28,  1.60it/s]Extractor Estimating: 45it [00:28,  1.62it/s]Extractor Estimating: 46it [00:29,  1.53it/s]Extractor Estimating: 47it [00:30,  1.51it/s]Extractor Estimating: 48it [00:30,  1.55it/s]Extractor Estimating: 49it [00:31,  1.60it/s]Extractor Estimating: 50it [00:31,  1.62it/s]Extractor Estimating: 51it [00:32,  1.50it/s]Extractor Estimating: 52it [00:33,  1.53it/s]Extractor Estimating: 53it [00:33,  1.55it/s]Extractor Estimating: 54it [00:34,  1.54it/s]Extractor Estimating: 55it [00:35,  1.53it/s]Extractor Estimating: 56it [00:35,  1.54it/s]Extractor Estimating: 57it [00:36,  1.51it/s]Extractor Estimating: 58it [00:37,  1.48it/s]Extractor Estimating: 59it [00:37,  1.51it/s]Extractor Estimating: 60it [00:38,  1.58it/s]Extractor Estimating: 61it [00:39,  1.55it/s]Extractor Estimating: 62it [00:39,  1.51it/s]Extractor Estimating: 63it [00:40,  1.54it/s]Extractor Estimating: 64it [00:41,  1.54it/s]Extractor Estimating: 65it [00:41,  1.56it/s]Extractor Estimating: 66it [00:42,  1.58it/s]Extractor Estimating: 67it [00:42,  1.60it/s]Extractor Estimating: 68it [00:43,  1.60it/s]Extractor Estimating: 69it [00:44,  1.61it/s]Extractor Estimating: 70it [00:44,  1.62it/s]Extractor Estimating: 71it [00:45,  1.61it/s]Extractor Estimating: 72it [00:46,  1.59it/s]Extractor Estimating: 73it [00:46,  1.60it/s]Extractor Estimating: 74it [00:47,  1.61it/s]Extractor Estimating: 75it [00:47,  1.65it/s]Extractor Estimating: 76it [00:48,  1.65it/s]Extractor Estimating: 77it [00:49,  1.57it/s]Extractor Estimating: 78it [00:49,  1.57it/s]Extractor Estimating: 79it [00:50,  1.58it/s]Extractor Estimating: 80it [00:51,  1.56it/s]Extractor Estimating: 81it [00:51,  1.57it/s]Extractor Estimating: 82it [00:52,  1.60it/s]Extractor Estimating: 83it [00:52,  1.61it/s]Extractor Estimating: 84it [00:53,  1.58it/s]Extractor Estimating: 85it [00:54,  1.63it/s]Extractor Estimating: 86it [00:54,  1.61it/s]Extractor Estimating: 87it [00:55,  1.57it/s]Extractor Estimating: 88it [00:56,  1.57it/s]Extractor Estimating: 89it [00:56,  1.60it/s]Extractor Estimating: 90it [00:57,  1.58it/s]Extractor Estimating: 91it [00:57,  1.60it/s]Extractor Estimating: 92it [00:58,  1.45it/s]Extractor Estimating: 93it [00:59,  1.51it/s]Extractor Estimating: 94it [01:00,  1.53it/s]Extractor Estimating: 95it [01:00,  1.51it/s]Extractor Estimating: 96it [01:01,  1.51it/s]Extractor Estimating: 97it [01:02,  1.38it/s]Extractor Estimating: 98it [01:02,  1.44it/s]Extractor Estimating: 99it [01:03,  1.47it/s]Extractor Estimating: 100it [01:04,  1.51it/s]Extractor Estimating: 101it [01:04,  1.58it/s]Extractor Estimating: 102it [01:05,  1.34it/s]Extractor Estimating: 103it [01:06,  1.45it/s]Extractor Estimating: 104it [01:06,  1.56it/s]Extractor Estimating: 105it [01:07,  1.61it/s]Extractor Estimating: 106it [01:07,  1.64it/s]Extractor Estimating: 107it [01:08,  1.69it/s]Extractor Estimating: 108it [01:09,  1.71it/s]Extractor Estimating: 109it [01:09,  1.73it/s]Extractor Estimating: 110it [01:10,  1.74it/s]Extractor Estimating: 111it [01:10,  1.64it/s]Extractor Estimating: 112it [01:11,  1.66it/s]Extractor Estimating: 113it [01:12,  1.63it/s]Extractor Estimating: 114it [01:12,  1.65it/s]Extractor Estimating: 115it [01:13,  1.69it/s]Extractor Estimating: 116it [01:13,  1.71it/s]Extractor Estimating: 117it [01:14,  1.62it/s]Extractor Estimating: 118it [01:15,  1.66it/s]Extractor Estimating: 119it [01:15,  1.69it/s]Extractor Estimating: 120it [01:16,  1.72it/s]Extractor Estimating: 121it [01:16,  1.65it/s]Extractor Estimating: 122it [01:17,  1.62it/s]Extractor Estimating: 123it [01:18,  1.67it/s]Extractor Estimating: 124it [01:18,  1.68it/s]Extractor Estimating: 125it [01:19,  1.73it/s]Extractor Estimating: 126it [01:19,  1.68it/s]Extractor Estimating: 127it [01:20,  1.54it/s]Extractor Estimating: 128it [01:21,  1.59it/s]Extractor Estimating: 129it [01:21,  1.57it/s]Extractor Estimating: 130it [01:22,  1.54it/s]Extractor Estimating: 131it [01:23,  1.53it/s]Extractor Estimating: 132it [01:23,  1.54it/s]Extractor Estimating: 133it [01:24,  1.57it/s]Extractor Estimating: 134it [01:25,  1.62it/s]Extractor Estimating: 135it [01:25,  1.60it/s]Extractor Estimating: 136it [01:26,  1.61it/s]Extractor Estimating: 137it [01:27,  1.39it/s]Extractor Estimating: 138it [01:27,  1.46it/s]Extractor Estimating: 139it [01:28,  1.50it/s]Extractor Estimating: 140it [01:29,  1.55it/s]Extractor Estimating: 141it [01:29,  1.60it/s]Extractor Estimating: 142it [01:30,  1.33it/s]Extractor Estimating: 143it [01:31,  1.40it/s]Extractor Estimating: 144it [01:31,  1.44it/s]Extractor Estimating: 145it [01:32,  1.48it/s]Extractor Estimating: 146it [01:33,  1.52it/s]Extractor Estimating: 147it [01:34,  1.44it/s]Extractor Estimating: 148it [01:34,  1.51it/s]Extractor Estimating: 149it [01:35,  1.47it/s]Extractor Estimating: 150it [01:35,  1.49it/s]Extractor Estimating: 151it [01:36,  1.53it/s]Extractor Estimating: 152it [01:37,  1.42it/s]Extractor Estimating: 153it [01:38,  1.47it/s]Extractor Estimating: 154it [01:38,  1.53it/s]Extractor Estimating: 155it [01:39,  1.57it/s]Extractor Estimating: 156it [01:39,  1.63it/s]Extractor Estimating: 157it [01:40,  1.58it/s]Extractor Estimating: 158it [01:41,  1.44it/s]Extractor Estimating: 159it [01:41,  1.46it/s]Extractor Estimating: 160it [01:42,  1.50it/s]Extractor Estimating: 161it [01:43,  1.52it/s]Extractor Estimating: 162it [01:43,  1.55it/s]Extractor Estimating: 163it [01:44,  1.55it/s]Extractor Estimating: 164it [01:45,  1.60it/s]Extractor Estimating: 165it [01:45,  1.66it/s]Extractor Estimating: 166it [01:46,  1.65it/s]Extractor Estimating: 167it [01:46,  1.67it/s]Extractor Estimating: 168it [01:47,  1.67it/s]Extractor Estimating: 169it [01:48,  1.61it/s]Extractor Estimating: 170it [01:48,  1.63it/s]Extractor Estimating: 171it [01:49,  1.65it/s]Extractor Estimating: 172it [01:49,  1.65it/s]Extractor Estimating: 173it [01:50,  1.64it/s]Extractor Estimating: 174it [01:51,  1.60it/s]Extractor Estimating: 175it [01:51,  1.57it/s]Extractor Estimating: 176it [01:52,  1.57it/s]Extractor Estimating: 177it [01:53,  1.54it/s]Extractor Estimating: 178it [01:53,  1.61it/s]Extractor Estimating: 179it [01:54,  1.51it/s]Extractor Estimating: 180it [01:55,  1.51it/s]Extractor Estimating: 181it [01:55,  1.49it/s]Extractor Estimating: 182it [01:56,  1.49it/s]Extractor Estimating: 183it [01:57,  1.53it/s]Extractor Estimating: 184it [01:57,  1.50it/s]Extractor Estimating: 185it [01:58,  1.47it/s]Extractor Estimating: 186it [01:59,  1.46it/s]Extractor Estimating: 187it [01:59,  1.45it/s]Extractor Estimating: 188it [02:00,  1.48it/s]Extractor Estimating: 189it [02:01,  1.48it/s]Extractor Estimating: 190it [02:01,  1.47it/s]Extractor Estimating: 191it [02:02,  1.53it/s]Extractor Estimating: 192it [02:03,  1.52it/s]Extractor Estimating: 193it [02:03,  1.54it/s]Extractor Estimating: 194it [02:04,  1.53it/s]Extractor Estimating: 195it [02:05,  1.51it/s]Extractor Estimating: 196it [02:05,  1.45it/s]Extractor Estimating: 197it [02:06,  1.47it/s]Extractor Estimating: 198it [02:07,  1.46it/s]Extractor Estimating: 199it [02:07,  1.47it/s]Extractor Estimating: 200it [02:08,  1.37it/s]Extractor Estimating: 201it [02:09,  1.41it/s]Extractor Estimating: 202it [02:09,  1.48it/s]Extractor Estimating: 203it [02:10,  1.53it/s]Extractor Estimating: 204it [02:11,  1.57it/s]Extractor Estimating: 205it [02:11,  1.59it/s]Extractor Estimating: 206it [02:12,  1.54it/s]Extractor Estimating: 207it [02:13,  1.59it/s]Extractor Estimating: 208it [02:13,  1.60it/s]Extractor Estimating: 209it [02:14,  1.58it/s]Extractor Estimating: 210it [02:15,  1.50it/s]Extractor Estimating: 211it [02:15,  1.55it/s]Extractor Estimating: 212it [02:16,  1.55it/s]Extractor Estimating: 213it [02:16,  1.57it/s]Extractor Estimating: 214it [02:17,  1.59it/s]Extractor Estimating: 215it [02:18,  1.61it/s]Extractor Estimating: 216it [02:18,  1.64it/s]Extractor Estimating: 217it [02:19,  1.67it/s]Extractor Estimating: 218it [02:19,  1.66it/s]Extractor Estimating: 219it [02:20,  1.67it/s]Extractor Estimating: 220it [02:21,  1.69it/s]Extractor Estimating: 221it [02:21,  1.69it/s]Extractor Estimating: 222it [02:22,  1.65it/s]Extractor Estimating: 223it [02:22,  1.63it/s]Extractor Estimating: 224it [02:23,  1.66it/s]Extractor Estimating: 225it [02:24,  1.63it/s]Extractor Estimating: 226it [02:24,  1.55it/s]Extractor Estimating: 227it [02:25,  1.51it/s]Extractor Estimating: 228it [02:26,  1.44it/s]Extractor Estimating: 229it [02:27,  1.40it/s]Extractor Estimating: 230it [02:27,  1.45it/s]Extractor Estimating: 231it [02:28,  1.44it/s]Extractor Estimating: 232it [02:29,  1.48it/s]Extractor Estimating: 233it [02:29,  1.51it/s]Extractor Estimating: 234it [02:30,  1.51it/s]Extractor Estimating: 235it [02:31,  1.52it/s]Extractor Estimating: 236it [02:31,  1.48it/s]Extractor Estimating: 237it [02:32,  1.50it/s]Extractor Estimating: 238it [02:33,  1.52it/s]Extractor Estimating: 239it [02:33,  1.54it/s]Extractor Estimating: 240it [02:34,  1.57it/s]Extractor Estimating: 241it [02:34,  1.56it/s]Extractor Estimating: 242it [02:35,  1.55it/s]Extractor Estimating: 243it [02:36,  1.57it/s]Extractor Estimating: 244it [02:36,  1.54it/s]Extractor Estimating: 245it [02:37,  1.54it/s]Extractor Estimating: 246it [02:38,  1.51it/s]Extractor Estimating: 247it [02:38,  1.52it/s]Extractor Estimating: 248it [02:39,  1.53it/s]Extractor Estimating: 249it [02:40,  1.49it/s]Extractor Estimating: 250it [02:40,  1.51it/s]Extractor Estimating: 251it [02:41,  1.52it/s]Extractor Estimating: 252it [02:42,  1.52it/s]Extractor Estimating: 253it [02:43,  1.39it/s]Extractor Estimating: 254it [02:43,  1.42it/s]Extractor Estimating: 255it [02:44,  1.50it/s]Extractor Estimating: 256it [02:44,  1.53it/s]Extractor Estimating: 257it [02:45,  1.55it/s]Extractor Estimating: 258it [02:46,  1.41it/s]Extractor Estimating: 259it [02:46,  1.51it/s]Extractor Estimating: 260it [02:47,  1.54it/s]Extractor Estimating: 261it [02:48,  1.54it/s]Extractor Estimating: 262it [02:48,  1.54it/s]Extractor Estimating: 263it [02:49,  1.49it/s]Extractor Estimating: 264it [02:50,  1.52it/s]Extractor Estimating: 265it [02:50,  1.49it/s]Extractor Estimating: 266it [02:51,  1.51it/s]Extractor Estimating: 267it [02:52,  1.51it/s]Extractor Estimating: 268it [02:53,  1.30it/s]Extractor Estimating: 269it [02:53,  1.35it/s]Extractor Estimating: 270it [02:54,  1.38it/s]Extractor Estimating: 271it [02:55,  1.41it/s]Extractor Estimating: 272it [02:55,  1.45it/s]Extractor Estimating: 273it [02:56,  1.47it/s]Extractor Estimating: 274it [02:57,  1.48it/s]Extractor Estimating: 275it [02:57,  1.49it/s]Extractor Estimating: 276it [02:58,  1.54it/s]Extractor Estimating: 277it [02:59,  1.48it/s]Extractor Estimating: 278it [02:59,  1.48it/s]Extractor Estimating: 279it [03:00,  1.53it/s]Extractor Estimating: 280it [03:01,  1.57it/s]Extractor Estimating: 281it [03:01,  1.63it/s]Extractor Estimating: 282it [03:02,  1.51it/s]Extractor Estimating: 283it [03:03,  1.51it/s]Extractor Estimating: 284it [03:03,  1.51it/s]Extractor Estimating: 285it [03:04,  1.53it/s]Extractor Estimating: 286it [03:05,  1.53it/s]Extractor Estimating: 287it [03:05,  1.55it/s]Extractor Estimating: 288it [03:06,  1.64it/s]Extractor Estimating: 289it [03:06,  1.64it/s]Extractor Estimating: 290it [03:07,  1.64it/s]Extractor Estimating: 291it [03:08,  1.65it/s]Extractor Estimating: 292it [03:08,  1.65it/s]Extractor Estimating: 293it [03:09,  1.66it/s]Extractor Estimating: 294it [03:09,  1.63it/s]Extractor Estimating: 295it [03:10,  1.67it/s]Extractor Estimating: 296it [03:10,  1.68it/s]Extractor Estimating: 297it [03:11,  1.62it/s]Extractor Estimating: 298it [03:12,  1.57it/s]Extractor Estimating: 299it [03:12,  1.64it/s]Extractor Estimating: 300it [03:13,  1.68it/s]Extractor Estimating: 301it [03:14,  1.68it/s]Extractor Estimating: 302it [03:14,  1.59it/s]Extractor Estimating: 303it [03:15,  1.57it/s]Extractor Estimating: 304it [03:16,  1.52it/s]Extractor Estimating: 305it [03:16,  1.58it/s]Extractor Estimating: 306it [03:17,  1.56it/s]Extractor Estimating: 307it [03:17,  1.60it/s]Extractor Estimating: 308it [03:18,  1.59it/s]Extractor Estimating: 309it [03:19,  1.56it/s]Extractor Estimating: 310it [03:19,  1.55it/s]Extractor Estimating: 311it [03:20,  1.62it/s]Extractor Estimating: 312it [03:21,  1.62it/s]Extractor Estimating: 313it [03:21,  1.56it/s]Extractor Estimating: 314it [03:22,  1.56it/s]Extractor Estimating: 315it [03:23,  1.54it/s]Extractor Estimating: 316it [03:23,  1.54it/s]Extractor Estimating: 317it [03:24,  1.55it/s]Extractor Estimating: 318it [03:24,  1.59it/s]Extractor Estimating: 319it [03:25,  1.59it/s]Extractor Estimating: 320it [03:26,  1.62it/s]Extractor Estimating: 321it [03:26,  1.61it/s]Extractor Estimating: 322it [03:27,  1.59it/s]Extractor Estimating: 323it [03:28,  1.59it/s]Extractor Estimating: 324it [03:28,  1.63it/s]Extractor Estimating: 325it [03:29,  1.60it/s]Extractor Estimating: 326it [03:29,  1.61it/s]Extractor Estimating: 327it [03:30,  1.65it/s]Extractor Estimating: 328it [03:31,  1.68it/s]Extractor Estimating: 329it [03:31,  1.66it/s]Extractor Estimating: 330it [03:32,  1.65it/s]Extractor Estimating: 331it [03:32,  1.62it/s]Extractor Estimating: 332it [03:33,  1.60it/s]Extractor Estimating: 333it [03:34,  1.54it/s]Extractor Estimating: 334it [03:34,  1.55it/s]Extractor Estimating: 335it [03:35,  1.54it/s]Extractor Estimating: 336it [03:36,  1.52it/s]Extractor Estimating: 337it [03:36,  1.57it/s]Extractor Estimating: 338it [03:37,  1.51it/s]Extractor Estimating: 339it [03:38,  1.55it/s]Extractor Estimating: 340it [03:38,  1.52it/s]Extractor Estimating: 341it [03:39,  1.50it/s]Extractor Estimating: 342it [03:40,  1.55it/s]Extractor Estimating: 343it [03:40,  1.55it/s]Extractor Estimating: 344it [03:41,  1.58it/s]Extractor Estimating: 345it [03:42,  1.56it/s]Extractor Estimating: 346it [03:42,  1.56it/s]Extractor Estimating: 347it [03:43,  1.53it/s]Extractor Estimating: 348it [03:44,  1.47it/s]Extractor Estimating: 349it [03:44,  1.51it/s]Extractor Estimating: 350it [03:45,  1.58it/s]Extractor Estimating: 351it [03:45,  1.60it/s]Extractor Estimating: 352it [03:46,  1.57it/s]Extractor Estimating: 353it [03:47,  1.55it/s]Extractor Estimating: 354it [03:47,  1.57it/s]Extractor Estimating: 355it [03:48,  1.58it/s]Extractor Estimating: 356it [03:49,  1.59it/s]Extractor Estimating: 357it [03:49,  1.64it/s]Extractor Estimating: 358it [03:50,  1.67it/s]Extractor Estimating: 359it [03:50,  1.67it/s]Extractor Estimating: 360it [03:51,  1.64it/s]Extractor Estimating: 361it [03:52,  1.64it/s]Extractor Estimating: 362it [03:52,  1.64it/s]Extractor Estimating: 363it [03:53,  1.60it/s]Extractor Estimating: 364it [03:53,  1.60it/s]Extractor Estimating: 365it [03:54,  1.65it/s]Extractor Estimating: 366it [03:55,  1.64it/s]Extractor Estimating: 367it [03:55,  1.65it/s]Extractor Estimating: 368it [03:56,  1.50it/s]Extractor Estimating: 369it [03:57,  1.55it/s]Extractor Estimating: 370it [03:57,  1.61it/s]Extractor Estimating: 371it [03:58,  1.60it/s]Extractor Estimating: 372it [03:58,  1.59it/s]Extractor Estimating: 373it [04:00,  1.29it/s]Extractor Estimating: 374it [04:00,  1.38it/s]Extractor Estimating: 375it [04:01,  1.58it/s]Extractor Estimating: 375it [04:01,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:42,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:42,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:42,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:42,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:42,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:34:43,397 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:34:43,398 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:43,964 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:45,024 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:45,024 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:50,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:50,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:50,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:50,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:50,620 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:34:51,829 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:34:51,831 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:52,598 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:52,750 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:52,750 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:09:28,519 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:09:28,556 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4499 mean pseudo reward: 0.9774097356760905
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 18128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18228, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.102, loss:593.3827
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 12, avg_time 1.120, loss:551.3501
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 112, avg_time 1.101, loss:483.5240
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 24, avg_time 1.110, loss:487.1334
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 124, avg_time 1.097, loss:449.3390
>> valid entity prec:0.5013, rec:0.4912, f1:0.4962
>> valid relation prec:0.0565, rec:0.0328, f1:0.0415
>> valid relation with NER prec:0.0565, rec:0.0328, f1:0.0415
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 3.265, loss:412.6547
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 1.118, loss:429.3587
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 48, avg_time 1.105, loss:435.7479
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 148, avg_time 1.110, loss:395.1128
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 60, avg_time 1.104, loss:401.0192
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5001, rec:0.4872, f1:0.4936
>> valid relation prec:0.0312, rec:0.0168, f1:0.0219
>> valid relation with NER prec:0.0312, rec:0.0168, f1:0.0219
g_step 1100, step 160, avg_time 3.277, loss:422.1115
g_step 1200, step 72, avg_time 1.086, loss:379.3399
g_step 1300, step 172, avg_time 1.110, loss:378.4478
g_step 1400, step 84, avg_time 1.102, loss:340.6315
g_step 1500, step 184, avg_time 1.109, loss:368.7275
>> valid entity prec:0.4952, rec:0.4754, f1:0.4851
>> valid relation prec:0.0395, rec:0.0205, f1:0.0270
>> valid relation with NER prec:0.0395, rec:0.0205, f1:0.0270
g_step 1600, step 96, avg_time 3.279, loss:320.1813
g_step 1700, step 8, avg_time 1.096, loss:346.6668
g_step 1800, step 108, avg_time 1.101, loss:305.2717
g_step 1900, step 20, avg_time 1.098, loss:321.2822
g_step 2000, step 120, avg_time 1.110, loss:291.0141
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4967, rec:0.4750, f1:0.4856
>> valid relation prec:0.0312, rec:0.0173, f1:0.0222
>> valid relation with NER prec:0.0312, rec:0.0173, f1:0.0222
g_step 2100, step 32, avg_time 3.258, loss:311.9236
g_step 2200, step 132, avg_time 1.117, loss:282.5512
g_step 2300, step 44, avg_time 1.108, loss:285.9994
g_step 2400, step 144, avg_time 1.096, loss:268.8744
g_step 2500, step 56, avg_time 1.122, loss:268.5385
>> valid entity prec:0.5155, rec:0.4673, f1:0.4902
>> valid relation prec:0.0535, rec:0.0341, f1:0.0416
>> valid relation with NER prec:0.0535, rec:0.0341, f1:0.0416
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 156, avg_time 3.249, loss:278.2694
g_step 2700, step 68, avg_time 1.102, loss:255.1100
g_step 2800, step 168, avg_time 1.110, loss:265.9572
g_step 2900, step 80, avg_time 1.112, loss:252.4083
g_step 3000, step 180, avg_time 1.105, loss:253.3248
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4936, rec:0.4576, f1:0.4749
>> valid relation prec:0.0364, rec:0.0229, f1:0.0281
>> valid relation with NER prec:0.0364, rec:0.0229, f1:0.0281
g_step 3100, step 92, avg_time 3.267, loss:220.7599
g_step 3200, step 4, avg_time 1.106, loss:241.6624
g_step 3300, step 104, avg_time 1.114, loss:224.6518
g_step 3400, step 16, avg_time 1.102, loss:246.0709
g_step 3500, step 116, avg_time 1.110, loss:206.0982
>> valid entity prec:0.4988, rec:0.4804, f1:0.4894
>> valid relation prec:0.0466, rec:0.0310, f1:0.0373
>> valid relation with NER prec:0.0466, rec:0.0310, f1:0.0373
g_step 3600, step 28, avg_time 3.268, loss:196.2389
g_step 3700, step 128, avg_time 1.107, loss:216.2918
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:09:28 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:09:28 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-09-28_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:09:30 - WARNING - datasets.builder -   Using custom data configuration default-7954914e0e6c9d09
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7954914e0e6c9d09/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:09:30,989 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:09:30,990 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:09:30,990 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:09:30,991 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:09:31,003 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:31,007 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:31,007 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:31,007 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:31,007 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:31,007 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:31,007 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:09:31,153 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:09:34,360 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:09:34,365 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7954914e0e6c9d09/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.19ba/s] 40%|████      | 2/5 [00:00<00:00,  3.99ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.33ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.50ba/s]100%|██████████| 5/5 [00:01<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  4.29ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.98ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.28ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.37ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.42ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.43ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.45ba/s]100%|██████████| 7/7 [00:01<00:00,  4.91ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.70ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.09ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.67ba/s]100%|██████████| 5/5 [00:00<00:00, 10.77ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.76ba/s] 43%|████▎     | 3/7 [00:00<00:00, 10.05ba/s] 71%|███████▏  | 5/7 [00:00<00:00, 10.34ba/s]100%|██████████| 7/7 [00:00<00:00, 12.35ba/s]100%|██████████| 7/7 [00:00<00:00, 11.47ba/s]
[INFO|trainer.py:414] 2023-08-28 23:09:38,465 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:09:38,476 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:09:38,476 >>   Num examples = 4501
[INFO|trainer.py:1149] 2023-08-28 23:09:38,476 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:09:38,476 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:09:38,476 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:09:38,476 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:09:38,476 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:00<01:43,  3.36it/s]  1%|          | 2/350 [00:00<01:41,  3.43it/s]  1%|          | 3/350 [00:00<01:40,  3.46it/s]  1%|          | 4/350 [00:01<01:39,  3.47it/s]  1%|▏         | 5/350 [00:01<01:39,  3.48it/s]  2%|▏         | 6/350 [00:01<01:38,  3.48it/s]  2%|▏         | 7/350 [00:02<01:38,  3.48it/s]  2%|▏         | 8/350 [00:02<01:38,  3.47it/s]  3%|▎         | 9/350 [00:02<01:38,  3.48it/s]  3%|▎         | 10/350 [00:02<01:37,  3.48it/s]  3%|▎         | 11/350 [00:03<01:37,  3.48it/s]  3%|▎         | 12/350 [00:03<01:37,  3.48it/s]  4%|▎         | 13/350 [00:03<01:36,  3.49it/s]  4%|▍         | 14/350 [00:04<01:36,  3.48it/s]  4%|▍         | 15/350 [00:04<01:36,  3.48it/s]  5%|▍         | 16/350 [00:04<01:35,  3.48it/s]  5%|▍         | 17/350 [00:04<01:35,  3.48it/s]  5%|▌         | 18/350 [00:05<01:35,  3.48it/s]  5%|▌         | 19/350 [00:05<01:35,  3.46it/s]  6%|▌         | 20/350 [00:05<01:35,  3.46it/s]  6%|▌         | 21/350 [00:06<01:34,  3.47it/s]  6%|▋         | 22/350 [00:06<01:34,  3.47it/s]  7%|▋         | 23/350 [00:06<01:33,  3.48it/s]  7%|▋         | 24/350 [00:06<01:33,  3.48it/s]  7%|▋         | 25/350 [00:07<01:33,  3.48it/s]  7%|▋         | 26/350 [00:07<01:33,  3.48it/s]  8%|▊         | 27/350 [00:07<01:32,  3.48it/s]  8%|▊         | 28/350 [00:08<01:32,  3.48it/s]  8%|▊         | 29/350 [00:08<01:32,  3.48it/s]  9%|▊         | 30/350 [00:08<01:32,  3.46it/s]  9%|▉         | 31/350 [00:08<01:32,  3.46it/s]  9%|▉         | 32/350 [00:09<01:31,  3.47it/s]  9%|▉         | 33/350 [00:09<01:31,  3.47it/s] 10%|▉         | 34/350 [00:09<01:30,  3.48it/s] 10%|█         | 35/350 [00:10<01:30,  3.48it/s] 10%|█         | 36/350 [00:10<01:30,  3.48it/s] 11%|█         | 37/350 [00:10<01:29,  3.48it/s] 11%|█         | 38/350 [00:10<01:29,  3.48it/s] 11%|█         | 39/350 [00:11<01:29,  3.48it/s] 11%|█▏        | 40/350 [00:11<01:29,  3.48it/s] 12%|█▏        | 41/350 [00:11<01:29,  3.46it/s] 12%|█▏        | 42/350 [00:12<01:29,  3.46it/s] 12%|█▏        | 43/350 [00:12<01:28,  3.46it/s] 13%|█▎        | 44/350 [00:12<01:28,  3.47it/s] 13%|█▎        | 45/350 [00:12<01:27,  3.47it/s] 13%|█▎        | 46/350 [00:13<01:27,  3.47it/s] 13%|█▎        | 47/350 [00:13<01:27,  3.47it/s] 14%|█▎        | 48/350 [00:13<01:26,  3.47it/s] 14%|█▍        | 49/350 [00:14<01:26,  3.47it/s] 14%|█▍        | 50/350 [00:14<01:26,  3.47it/s] 15%|█▍        | 51/350 [00:14<01:26,  3.47it/s] 15%|█▍        | 52/350 [00:14<01:26,  3.46it/s] 15%|█▌        | 53/350 [00:15<01:25,  3.47it/s] 15%|█▌        | 54/350 [00:15<01:25,  3.47it/s] 16%|█▌        | 55/350 [00:15<01:24,  3.47it/s] 16%|█▌        | 56/350 [00:16<01:24,  3.47it/s] 16%|█▋        | 57/350 [00:16<01:24,  3.47it/s] 17%|█▋        | 58/350 [00:16<01:24,  3.47it/s] 17%|█▋        | 59/350 [00:16<01:23,  3.47it/s] 17%|█▋        | 60/350 [00:17<01:23,  3.47it/s] 17%|█▋        | 61/350 [00:17<01:23,  3.47it/s] 18%|█▊        | 62/350 [00:17<01:22,  3.47it/s] 18%|█▊        | 63/350 [00:18<01:23,  3.45it/s] 18%|█▊        | 64/350 [00:18<01:22,  3.46it/s] 19%|█▊        | 65/350 [00:18<01:22,  3.46it/s] 19%|█▉        | 66/350 [00:19<01:21,  3.47it/s] 19%|█▉        | 67/350 [00:19<01:21,  3.47it/s] 19%|█▉        | 68/350 [00:19<01:21,  3.47it/s] 20%|█▉        | 69/350 [00:19<01:20,  3.47it/s] 20%|██        | 70/350 [00:20<01:20,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 23:09:58,683 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:09:58,683 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 23:09:58,683 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.88it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.27it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.55it/s][A
  3%|▎         | 23/782 [00:00<00:16, 46.83it/s][A
  4%|▎         | 28/782 [00:00<00:16, 47.12it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.10it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.07it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.97it/s][A
  6%|▌         | 48/782 [00:01<00:15, 47.01it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.16it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.09it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.18it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.25it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.35it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.25it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.25it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.11it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.06it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.17it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.28it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.23it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.30it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.32it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.24it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.18it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.24it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.20it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.05it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.21it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.24it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.25it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.23it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.16it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.16it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.10it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.07it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.04it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.98it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.08it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.19it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.18it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.21it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.21it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.17it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.22it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.16it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.17it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.15it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.16it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.19it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.21it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.29it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.15it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.20it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.07it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.25it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.17it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.18it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.24it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.28it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.24it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.29it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.06it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.15it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.10it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.20it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.03it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.15it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.18it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.13it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.19it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.25it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.18it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.03it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.20it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.12it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.02it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.17it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.10it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.18it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.22it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.21it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.14it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.19it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.17it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.07it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.13it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.11it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.22it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.25it/s][A
 59%|█████▊    | 458/782 [00:09<00:07, 46.04it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.43it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.64it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.86it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.91it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.99it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.03it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.06it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.07it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.08it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.14it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.19it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.17it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.25it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.10it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.07it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.15it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.12it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.09it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.23it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.28it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.16it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.18it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.15it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.12it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.10it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.16it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.11it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.09it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.17it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.17it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.19it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.11it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.10it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.15it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.17it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.14it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.11it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.13it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.14it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.20it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.18it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.20it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.13it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.13it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.17it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.11it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.06it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.09it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.18it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.14it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.11it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.09it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.02it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.03it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.17it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.15it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.08it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 44.72it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 45.47it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.02it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.30it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.61it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.78it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.96it/s][A                                                
                                                 [A 20%|██        | 70/350 [00:36<01:20,  3.47it/s]
100%|██████████| 782/782 [00:16<00:00, 46.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:10:15,389 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 23:10:15,427 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:10:19,052 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:10:19,069 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:10:19,079 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [00:48<40:00,  8.60s/it] 21%|██        | 72/350 [00:48<28:18,  6.11s/it] 21%|██        | 73/350 [00:48<20:08,  4.36s/it] 21%|██        | 74/350 [00:49<14:26,  3.14s/it] 21%|██▏       | 75/350 [00:49<10:28,  2.28s/it] 22%|██▏       | 76/350 [00:49<07:41,  1.68s/it] 22%|██▏       | 77/350 [00:49<05:45,  1.27s/it] 22%|██▏       | 78/350 [00:50<04:24,  1.03it/s] 23%|██▎       | 79/350 [00:50<03:27,  1.30it/s] 23%|██▎       | 80/350 [00:50<02:48,  1.61it/s] 23%|██▎       | 81/350 [00:51<02:20,  1.92it/s] 23%|██▎       | 82/350 [00:51<02:01,  2.21it/s] 24%|██▎       | 83/350 [00:51<01:48,  2.45it/s] 24%|██▍       | 84/350 [00:51<01:38,  2.69it/s] 24%|██▍       | 85/350 [00:52<01:31,  2.89it/s] 25%|██▍       | 86/350 [00:52<01:26,  3.04it/s] 25%|██▍       | 87/350 [00:52<01:23,  3.16it/s] 25%|██▌       | 88/350 [00:53<01:20,  3.25it/s] 25%|██▌       | 89/350 [00:53<01:18,  3.32it/s] 26%|██▌       | 90/350 [00:53<01:17,  3.36it/s] 26%|██▌       | 91/350 [00:53<01:16,  3.39it/s] 26%|██▋       | 92/350 [00:54<01:15,  3.42it/s] 27%|██▋       | 93/350 [00:54<01:14,  3.44it/s] 27%|██▋       | 94/350 [00:54<01:14,  3.44it/s] 27%|██▋       | 95/350 [00:55<01:13,  3.45it/s] 27%|██▋       | 96/350 [00:55<01:13,  3.46it/s] 28%|██▊       | 97/350 [00:55<01:13,  3.47it/s] 28%|██▊       | 98/350 [00:55<01:12,  3.47it/s] 28%|██▊       | 99/350 [00:56<01:12,  3.47it/s] 29%|██▊       | 100/350 [00:56<01:11,  3.47it/s] 29%|██▉       | 101/350 [00:56<01:11,  3.47it/s] 29%|██▉       | 102/350 [00:57<01:11,  3.47it/s] 29%|██▉       | 103/350 [00:57<01:11,  3.48it/s] 30%|██▉       | 104/350 [00:57<01:10,  3.48it/s] 30%|███       | 105/350 [00:57<01:10,  3.46it/s] 30%|███       | 106/350 [00:58<01:10,  3.46it/s] 31%|███       | 107/350 [00:58<01:10,  3.47it/s] 31%|███       | 108/350 [00:58<01:09,  3.47it/s] 31%|███       | 109/350 [00:59<01:09,  3.47it/s] 31%|███▏      | 110/350 [00:59<01:09,  3.48it/s] 32%|███▏      | 111/350 [00:59<01:08,  3.48it/s] 32%|███▏      | 112/350 [00:59<01:08,  3.48it/s] 32%|███▏      | 113/350 [01:00<01:08,  3.48it/s] 33%|███▎      | 114/350 [01:00<01:07,  3.48it/s] 33%|███▎      | 115/350 [01:00<01:07,  3.48it/s] 33%|███▎      | 116/350 [01:01<01:07,  3.46it/s] 33%|███▎      | 117/350 [01:01<01:07,  3.47it/s] 34%|███▎      | 118/350 [01:01<01:06,  3.47it/s] 34%|███▍      | 119/350 [01:01<01:06,  3.47it/s] 34%|███▍      | 120/350 [01:02<01:06,  3.47it/s] 35%|███▍      | 121/350 [01:02<01:05,  3.47it/s] 35%|███▍      | 122/350 [01:02<01:05,  3.47it/s] 35%|███▌      | 123/350 [01:03<01:05,  3.47it/s] 35%|███▌      | 124/350 [01:03<01:05,  3.47it/s] 36%|███▌      | 125/350 [01:03<01:04,  3.48it/s] 36%|███▌      | 126/350 [01:04<01:04,  3.47it/s] 36%|███▋      | 127/350 [01:04<01:04,  3.46it/s] 37%|███▋      | 128/350 [01:04<01:04,  3.46it/s] 37%|███▋      | 129/350 [01:04<01:03,  3.47it/s] 37%|███▋      | 130/350 [01:05<01:03,  3.47it/s] 37%|███▋      | 131/350 [01:05<01:03,  3.47it/s] 38%|███▊      | 132/350 [01:05<01:02,  3.47it/s] 38%|███▊      | 133/350 [01:06<01:02,  3.47it/s] 38%|███▊      | 134/350 [01:06<01:02,  3.48it/s] 39%|███▊      | 135/350 [01:06<01:01,  3.48it/s] 39%|███▉      | 136/350 [01:06<01:01,  3.48it/s] 39%|███▉      | 137/350 [01:07<01:01,  3.48it/s] 39%|███▉      | 138/350 [01:07<01:01,  3.46it/s] 40%|███▉      | 139/350 [01:07<01:00,  3.47it/s] 40%|████      | 140/350 [01:08<01:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 23:10:46,569 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:10:46,569 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 23:10:46,569 >>   Batch size = 8
{'eval_loss': 0.9789218306541443, 'eval_runtime': 16.656, 'eval_samples_per_second': 375.421, 'eval_steps_per_second': 46.95, 'epoch': 0.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.93it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.25it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.40it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.67it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.27it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.95it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.55it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.17it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.16it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.19it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.31it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.27it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.27it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.35it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.38it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.20it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.09it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.09it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.05it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.86it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.07it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.11it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.12it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.24it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.17it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.11it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.14it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.07it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.98it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.17it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.20it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.21it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.14it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.24it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.14it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.16it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.13it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.07it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.14it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.18it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.15it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.04it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.04it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.02it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.89it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.87it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.99it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.98it/s][A
 32%|███▏      | 248/782 [00:05<00:12, 42.61it/s][A
 32%|███▏      | 253/782 [00:05<00:12, 43.97it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 44.84it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 45.47it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.09it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.42it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.69it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.88it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.76it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.81it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.97it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.99it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.16it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.06it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.10it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.20it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.22it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.10it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.99it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.01it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.13it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.17it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.18it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.12it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.14it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.20it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.08it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.09it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 44.70it/s][A
 50%|█████     | 393/782 [00:08<00:08, 45.41it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.02it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.40it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.35it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.72it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.89it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.90it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.71it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.65it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.61it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.76it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.87it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.90it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.92it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.98it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.97it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.92it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.95it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.89it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.94it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.05it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.16it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.22it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.12it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.26it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 47.10it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.09it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.04it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 43.13it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 44.29it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 45.12it/s][A
 70%|███████   | 548/782 [00:11<00:05, 45.68it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.20it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.47it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.73it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.86it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.78it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.74it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.86it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.00it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.03it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.05it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.20it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.12it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.27it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.05it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.89it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.90it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.00it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.03it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.14it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.10it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.20it/s][A
 84%|████████▍ | 658/782 [00:14<00:03, 41.31it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 42.92it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 44.07it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 44.98it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 45.61it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.09it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.52it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.77it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.55it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.63it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.76it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.90it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.96it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.03it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.10it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.07it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.15it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.96it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.85it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.91it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.97it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.01it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.10it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.20it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.18it/s][A                                                 
                                                 [A 40%|████      | 140/350 [01:24<01:00,  3.47it/s]
100%|██████████| 782/782 [00:16<00:00, 47.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:11:03,377 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 23:11:03,443 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:11:07,211 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:11:07,245 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:11:07,254 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [01:36<30:22,  8.72s/it] 41%|████      | 142/350 [01:36<21:28,  6.20s/it] 41%|████      | 143/350 [01:37<15:15,  4.42s/it] 41%|████      | 144/350 [01:37<10:55,  3.18s/it] 41%|████▏     | 145/350 [01:37<07:54,  2.31s/it] 42%|████▏     | 146/350 [01:37<05:47,  1.71s/it] 42%|████▏     | 147/350 [01:38<04:19,  1.28s/it] 42%|████▏     | 148/350 [01:38<03:18,  1.02it/s] 43%|████▎     | 149/350 [01:38<02:35,  1.29it/s] 43%|████▎     | 150/350 [01:39<02:05,  1.59it/s] 43%|████▎     | 151/350 [01:39<01:44,  1.90it/s] 43%|████▎     | 152/350 [01:39<01:29,  2.20it/s] 44%|████▎     | 153/350 [01:39<01:21,  2.41it/s] 44%|████▍     | 154/350 [01:40<01:13,  2.65it/s] 44%|████▍     | 155/350 [01:40<01:08,  2.86it/s] 45%|████▍     | 156/350 [01:40<01:04,  3.02it/s] 45%|████▍     | 157/350 [01:41<01:01,  3.14it/s] 45%|████▌     | 158/350 [01:41<00:59,  3.23it/s] 45%|████▌     | 159/350 [01:41<00:57,  3.30it/s] 46%|████▌     | 160/350 [01:41<00:56,  3.35it/s] 46%|████▌     | 161/350 [01:42<00:55,  3.39it/s] 46%|████▋     | 162/350 [01:42<00:55,  3.42it/s] 47%|████▋     | 163/350 [01:42<00:54,  3.43it/s] 47%|████▋     | 164/350 [01:43<00:54,  3.43it/s] 47%|████▋     | 165/350 [01:43<00:53,  3.45it/s] 47%|████▋     | 166/350 [01:43<00:53,  3.46it/s] 48%|████▊     | 167/350 [01:43<00:52,  3.46it/s] 48%|████▊     | 168/350 [01:44<00:52,  3.47it/s] 48%|████▊     | 169/350 [01:44<00:52,  3.47it/s] 49%|████▊     | 170/350 [01:44<00:51,  3.47it/s] 49%|████▉     | 171/350 [01:45<00:51,  3.47it/s] 49%|████▉     | 172/350 [01:45<00:51,  3.47it/s] 49%|████▉     | 173/350 [01:45<00:50,  3.47it/s] 50%|████▉     | 174/350 [01:45<00:50,  3.47it/s] 50%|█████     | 175/350 [01:46<00:51,  3.42it/s] 50%|█████     | 176/350 [01:46<00:50,  3.44it/s] 51%|█████     | 177/350 [01:46<00:50,  3.45it/s] 51%|█████     | 178/350 [01:47<00:49,  3.46it/s] 51%|█████     | 179/350 [01:47<00:49,  3.46it/s] 51%|█████▏    | 180/350 [01:47<00:49,  3.47it/s] 52%|█████▏    | 181/350 [01:48<00:48,  3.47it/s] 52%|█████▏    | 182/350 [01:48<00:48,  3.47it/s] 52%|█████▏    | 183/350 [01:48<00:48,  3.47it/s] 53%|█████▎    | 184/350 [01:48<00:47,  3.47it/s] 53%|█████▎    | 185/350 [01:49<00:47,  3.47it/s] 53%|█████▎    | 186/350 [01:49<00:47,  3.47it/s] 53%|█████▎    | 187/350 [01:49<00:46,  3.47it/s] 54%|█████▎    | 188/350 [01:50<00:46,  3.47it/s] 54%|█████▍    | 189/350 [01:50<00:46,  3.47it/s] 54%|█████▍    | 190/350 [01:50<00:46,  3.47it/s] 55%|█████▍    | 191/350 [01:50<00:45,  3.47it/s] 55%|█████▍    | 192/350 [01:51<00:45,  3.47it/s] 55%|█████▌    | 193/350 [01:51<00:45,  3.47it/s] 55%|█████▌    | 194/350 [01:51<00:44,  3.47it/s] 56%|█████▌    | 195/350 [01:52<00:44,  3.47it/s] 56%|█████▌    | 196/350 [01:52<00:44,  3.47it/s] 56%|█████▋    | 197/350 [01:52<00:44,  3.44it/s] 57%|█████▋    | 198/350 [01:52<00:44,  3.44it/s] 57%|█████▋    | 199/350 [01:53<00:43,  3.45it/s] 57%|█████▋    | 200/350 [01:53<00:43,  3.46it/s] 57%|█████▋    | 201/350 [01:53<00:43,  3.46it/s] 58%|█████▊    | 202/350 [01:54<00:42,  3.46it/s] 58%|█████▊    | 203/350 [01:54<00:42,  3.47it/s] 58%|█████▊    | 204/350 [01:54<00:42,  3.47it/s] 59%|█████▊    | 205/350 [01:54<00:41,  3.47it/s] 59%|█████▉    | 206/350 [01:55<00:41,  3.47it/s] 59%|█████▉    | 207/350 [01:55<00:41,  3.47it/s] 59%|█████▉    | 208/350 [01:55<00:40,  3.47it/s] 60%|█████▉    | 209/350 [01:56<00:41,  3.41it/s] 60%|██████    | 210/350 [01:56<00:40,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 23:11:34,923 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:11:34,923 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 23:11:34,923 >>   Batch size = 8
{'eval_loss': 0.9954032897949219, 'eval_runtime': 16.7795, 'eval_samples_per_second': 372.656, 'eval_steps_per_second': 46.604, 'epoch': 1.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.98it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.13it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.20it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.52it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.16it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.92it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.54it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.10it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.11it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.12it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.06it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.09it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.09it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.18it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.29it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.26it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.12it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.06it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.16it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.10it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.13it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.15it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.90it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.94it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.10it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.09it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.05it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.12it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.16it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.06it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.14it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.25it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.12it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.20it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.19it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.11it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.07it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.17it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.21it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.10it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.06it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.16it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.15it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.20it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.12it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.06it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.04it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.13it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.15it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.12it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.10it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.08it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.15it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.21it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.11it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.08it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.03it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.11it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.15it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.17it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.08it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.13it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.15it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.14it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.15it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.01it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.09it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.21it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.14it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.04it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.16it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.16it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.18it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.10it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.12it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.07it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.17it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.15it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.07it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.08it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.96it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.06it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.12it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.04it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.05it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.12it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.13it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.05it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.99it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.15it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.07it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.05it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.12it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.02it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.03it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.15it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.11it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.10it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.08it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.15it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.15it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.98it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.03it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.08it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.07it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.13it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.13it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.12it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.02it/s][A
 71%|███████   | 553/782 [00:11<00:05, 44.56it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 45.25it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 45.75it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.07it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.31it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.60it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.85it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.96it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.81it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.75it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.82it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.94it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.97it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.05it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.01it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.07it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.16it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.05it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.98it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.03it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.07it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.15it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.15it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.17it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.05it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.12it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.14it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.05it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.96it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.02it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.05it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.15it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.11it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.98it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.02it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.01it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.82it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.85it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.79it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.75it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 46.81it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.74it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.78it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.86it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.95it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.93it/s][A                                                 
                                                 [A 60%|██████    | 210/350 [02:13<00:40,  3.43it/s]
100%|██████████| 782/782 [00:16<00:00, 46.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:11:51,782 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 23:11:52,461 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:11:59,505 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:11:59,789 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:11:59,798 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [02:29<23:10, 10.00s/it] 61%|██████    | 212/350 [02:29<16:18,  7.09s/it] 61%|██████    | 213/350 [02:29<11:31,  5.05s/it] 61%|██████    | 214/350 [02:29<08:12,  3.62s/it] 61%|██████▏   | 215/350 [02:30<05:53,  2.62s/it] 62%|██████▏   | 216/350 [02:30<04:17,  1.92s/it] 62%|██████▏   | 217/350 [02:30<03:10,  1.43s/it] 62%|██████▏   | 218/350 [02:31<02:23,  1.09s/it] 63%|██████▎   | 219/350 [02:31<01:51,  1.18it/s] 63%|██████▎   | 220/350 [02:31<01:28,  1.47it/s] 63%|██████▎   | 221/350 [02:31<01:12,  1.78it/s] 63%|██████▎   | 222/350 [02:32<01:01,  2.09it/s] 64%|██████▎   | 223/350 [02:32<00:53,  2.36it/s] 64%|██████▍   | 224/350 [02:32<00:48,  2.62it/s] 64%|██████▍   | 225/350 [02:33<00:44,  2.83it/s] 65%|██████▍   | 226/350 [02:33<00:41,  2.99it/s] 65%|██████▍   | 227/350 [02:33<00:39,  3.13it/s] 65%|██████▌   | 228/350 [02:33<00:37,  3.22it/s] 65%|██████▌   | 229/350 [02:34<00:36,  3.30it/s] 66%|██████▌   | 230/350 [02:34<00:35,  3.35it/s] 66%|██████▌   | 231/350 [02:34<00:35,  3.39it/s] 66%|██████▋   | 232/350 [02:35<00:34,  3.41it/s] 67%|██████▋   | 233/350 [02:35<00:34,  3.43it/s] 67%|██████▋   | 234/350 [02:35<00:33,  3.43it/s] 67%|██████▋   | 235/350 [02:35<00:33,  3.44it/s] 67%|██████▋   | 236/350 [02:36<00:33,  3.45it/s] 68%|██████▊   | 237/350 [02:36<00:32,  3.46it/s] 68%|██████▊   | 238/350 [02:36<00:32,  3.46it/s] 68%|██████▊   | 239/350 [02:37<00:32,  3.47it/s] 69%|██████▊   | 240/350 [02:37<00:31,  3.47it/s] 69%|██████▉   | 241/350 [02:37<00:31,  3.47it/s] 69%|██████▉   | 242/350 [02:37<00:31,  3.47it/s] 69%|██████▉   | 243/350 [02:38<00:30,  3.47it/s] 70%|██████▉   | 244/350 [02:38<00:30,  3.47it/s] 70%|███████   | 245/350 [02:38<00:30,  3.46it/s] 70%|███████   | 246/350 [02:39<00:30,  3.46it/s] 71%|███████   | 247/350 [02:39<00:29,  3.47it/s] 71%|███████   | 248/350 [02:39<00:29,  3.47it/s] 71%|███████   | 249/350 [02:40<00:29,  3.47it/s] 71%|███████▏  | 250/350 [02:40<00:28,  3.47it/s] 72%|███████▏  | 251/350 [02:40<00:28,  3.47it/s] 72%|███████▏  | 252/350 [02:40<00:28,  3.47it/s] 72%|███████▏  | 253/350 [02:41<00:27,  3.47it/s] 73%|███████▎  | 254/350 [02:41<00:27,  3.47it/s] 73%|███████▎  | 255/350 [02:41<00:27,  3.47it/s] 73%|███████▎  | 256/350 [02:42<00:27,  3.45it/s] 73%|███████▎  | 257/350 [02:42<00:26,  3.45it/s] 74%|███████▎  | 258/350 [02:42<00:26,  3.46it/s] 74%|███████▍  | 259/350 [02:42<00:26,  3.46it/s] 74%|███████▍  | 260/350 [02:43<00:25,  3.46it/s] 75%|███████▍  | 261/350 [02:43<00:25,  3.47it/s] 75%|███████▍  | 262/350 [02:43<00:25,  3.47it/s] 75%|███████▌  | 263/350 [02:44<00:25,  3.47it/s] 75%|███████▌  | 264/350 [02:44<00:24,  3.47it/s] 76%|███████▌  | 265/350 [02:44<00:24,  3.47it/s] 76%|███████▌  | 266/350 [02:44<00:24,  3.48it/s] 76%|███████▋  | 267/350 [02:45<00:24,  3.39it/s] 77%|███████▋  | 268/350 [02:45<00:24,  3.41it/s] 77%|███████▋  | 269/350 [02:45<00:23,  3.43it/s] 77%|███████▋  | 270/350 [02:46<00:23,  3.44it/s] 77%|███████▋  | 271/350 [02:46<00:22,  3.45it/s] 78%|███████▊  | 272/350 [02:46<00:22,  3.45it/s] 78%|███████▊  | 273/350 [02:46<00:22,  3.46it/s] 78%|███████▊  | 274/350 [02:47<00:21,  3.46it/s] 79%|███████▊  | 275/350 [02:47<00:21,  3.47it/s] 79%|███████▉  | 276/350 [02:47<00:21,  3.46it/s] 79%|███████▉  | 277/350 [02:48<00:21,  3.47it/s] 79%|███████▉  | 278/350 [02:48<00:20,  3.45it/s] 80%|███████▉  | 279/350 [02:48<00:20,  3.46it/s] 80%|████████  | 280/350 [02:48<00:20,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 23:12:27,501 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:12:27,501 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 23:12:27,501 >>   Batch size = 8
{'eval_loss': 1.0109115839004517, 'eval_runtime': 16.6693, 'eval_samples_per_second': 375.12, 'eval_steps_per_second': 46.912, 'epoch': 2.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 7/782 [00:00<00:13, 58.88it/s][A
  2%|▏         | 13/782 [00:00<00:14, 51.79it/s][A
  2%|▏         | 19/782 [00:00<00:15, 49.92it/s][A
  3%|▎         | 25/782 [00:00<00:15, 48.80it/s][A
  4%|▍         | 30/782 [00:00<00:15, 48.25it/s][A
  4%|▍         | 35/782 [00:00<00:15, 47.84it/s][A
  5%|▌         | 40/782 [00:00<00:15, 47.47it/s][A
  6%|▌         | 45/782 [00:00<00:15, 47.29it/s][A
  6%|▋         | 50/782 [00:01<00:15, 47.35it/s][A
  7%|▋         | 55/782 [00:01<00:17, 41.52it/s][A
  8%|▊         | 60/782 [00:01<00:16, 42.81it/s][A
  8%|▊         | 65/782 [00:01<00:16, 44.11it/s][A
  9%|▉         | 70/782 [00:01<00:15, 44.94it/s][A
 10%|▉         | 75/782 [00:01<00:15, 45.68it/s][A
 10%|█         | 80/782 [00:01<00:15, 46.14it/s][A
 11%|█         | 85/782 [00:01<00:15, 46.39it/s][A
 12%|█▏        | 90/782 [00:01<00:14, 46.75it/s][A
 12%|█▏        | 95/782 [00:02<00:14, 46.98it/s][A
 13%|█▎        | 100/782 [00:02<00:14, 46.46it/s][A
 13%|█▎        | 105/782 [00:02<00:14, 46.63it/s][A
 14%|█▍        | 110/782 [00:02<00:14, 46.91it/s][A
 15%|█▍        | 115/782 [00:02<00:14, 47.00it/s][A
 15%|█▌        | 120/782 [00:02<00:14, 47.09it/s][A
 16%|█▌        | 125/782 [00:02<00:13, 47.08it/s][A
 17%|█▋        | 130/782 [00:02<00:13, 47.09it/s][A
 17%|█▋        | 135/782 [00:02<00:13, 47.09it/s][A
 18%|█▊        | 140/782 [00:02<00:13, 47.26it/s][A
 19%|█▊        | 145/782 [00:03<00:13, 47.11it/s][A
 19%|█▉        | 150/782 [00:03<00:13, 47.11it/s][A
 20%|█▉        | 155/782 [00:03<00:13, 47.11it/s][A
 20%|██        | 160/782 [00:03<00:13, 47.16it/s][A
 21%|██        | 165/782 [00:03<00:13, 47.27it/s][A
 22%|██▏       | 170/782 [00:03<00:12, 47.26it/s][A
 22%|██▏       | 175/782 [00:03<00:12, 47.27it/s][A
 23%|██▎       | 180/782 [00:03<00:12, 47.16it/s][A
 24%|██▎       | 185/782 [00:03<00:12, 47.13it/s][A
 24%|██▍       | 190/782 [00:04<00:12, 47.19it/s][A
 25%|██▍       | 195/782 [00:04<00:12, 47.17it/s][A
 26%|██▌       | 200/782 [00:04<00:12, 47.16it/s][A
 26%|██▌       | 205/782 [00:04<00:12, 47.15it/s][A
 27%|██▋       | 210/782 [00:04<00:12, 47.20it/s][A
 27%|██▋       | 215/782 [00:04<00:12, 47.22it/s][A
 28%|██▊       | 220/782 [00:04<00:11, 47.26it/s][A
 29%|██▉       | 225/782 [00:04<00:11, 47.24it/s][A
 29%|██▉       | 230/782 [00:04<00:11, 47.18it/s][A
 30%|███       | 235/782 [00:05<00:11, 47.11it/s][A
 31%|███       | 240/782 [00:05<00:11, 47.18it/s][A
 31%|███▏      | 245/782 [00:05<00:11, 47.10it/s][A
 32%|███▏      | 250/782 [00:05<00:11, 47.11it/s][A
 33%|███▎      | 255/782 [00:05<00:11, 47.12it/s][A
 33%|███▎      | 260/782 [00:05<00:11, 47.19it/s][A
 34%|███▍      | 265/782 [00:05<00:10, 47.22it/s][A
 35%|███▍      | 270/782 [00:05<00:10, 47.20it/s][A
 35%|███▌      | 275/782 [00:05<00:10, 47.17it/s][A
 36%|███▌      | 280/782 [00:05<00:10, 47.26it/s][A
 36%|███▋      | 285/782 [00:06<00:10, 47.15it/s][A
 37%|███▋      | 290/782 [00:06<00:10, 47.23it/s][A
 38%|███▊      | 295/782 [00:06<00:10, 47.11it/s][A
 38%|███▊      | 300/782 [00:06<00:10, 47.06it/s][A
 39%|███▉      | 305/782 [00:06<00:10, 47.15it/s][A
 40%|███▉      | 310/782 [00:06<00:10, 47.16it/s][A
 40%|████      | 315/782 [00:06<00:09, 47.18it/s][A
 41%|████      | 320/782 [00:06<00:09, 47.13it/s][A
 42%|████▏     | 325/782 [00:06<00:09, 47.20it/s][A
 42%|████▏     | 330/782 [00:07<00:09, 47.12it/s][A
 43%|████▎     | 335/782 [00:07<00:09, 47.13it/s][A
 43%|████▎     | 340/782 [00:07<00:09, 47.17it/s][A
 44%|████▍     | 345/782 [00:07<00:09, 47.14it/s][A
 45%|████▍     | 350/782 [00:07<00:09, 47.08it/s][A
 45%|████▌     | 355/782 [00:07<00:09, 47.15it/s][A
 46%|████▌     | 360/782 [00:07<00:08, 47.17it/s][A
 47%|████▋     | 365/782 [00:07<00:08, 47.18it/s][A
 47%|████▋     | 370/782 [00:07<00:08, 47.23it/s][A
 48%|████▊     | 375/782 [00:07<00:08, 47.21it/s][A
 49%|████▊     | 380/782 [00:08<00:08, 47.12it/s][A
 49%|████▉     | 385/782 [00:08<00:08, 47.10it/s][A
 50%|████▉     | 390/782 [00:08<00:08, 47.17it/s][A
 51%|█████     | 395/782 [00:08<00:08, 47.15it/s][A
 51%|█████     | 400/782 [00:08<00:08, 47.05it/s][A
 52%|█████▏    | 405/782 [00:08<00:08, 47.12it/s][A
 52%|█████▏    | 410/782 [00:08<00:07, 47.16it/s][A
 53%|█████▎    | 415/782 [00:08<00:07, 47.14it/s][A
 54%|█████▎    | 420/782 [00:08<00:07, 47.22it/s][A
 54%|█████▍    | 425/782 [00:09<00:07, 47.28it/s][A
 55%|█████▍    | 430/782 [00:09<00:07, 47.11it/s][A
 56%|█████▌    | 435/782 [00:09<00:07, 47.12it/s][A
 56%|█████▋    | 440/782 [00:09<00:07, 47.12it/s][A
 57%|█████▋    | 445/782 [00:09<00:07, 47.15it/s][A
 58%|█████▊    | 450/782 [00:09<00:07, 47.14it/s][A
 58%|█████▊    | 455/782 [00:09<00:06, 47.13it/s][A
 59%|█████▉    | 460/782 [00:09<00:06, 47.19it/s][A
 59%|█████▉    | 465/782 [00:09<00:06, 47.16it/s][A
 60%|██████    | 470/782 [00:09<00:06, 47.23it/s][A
 61%|██████    | 475/782 [00:10<00:06, 47.26it/s][A
 61%|██████▏   | 480/782 [00:10<00:06, 47.08it/s][A
 62%|██████▏   | 485/782 [00:10<00:06, 47.05it/s][A
 63%|██████▎   | 490/782 [00:10<00:06, 47.20it/s][A
 63%|██████▎   | 495/782 [00:10<00:06, 47.22it/s][A
 64%|██████▍   | 500/782 [00:10<00:05, 47.10it/s][A
 65%|██████▍   | 505/782 [00:10<00:05, 47.16it/s][A
 65%|██████▌   | 510/782 [00:10<00:05, 47.13it/s][A
 66%|██████▌   | 515/782 [00:10<00:05, 47.15it/s][A
 66%|██████▋   | 520/782 [00:11<00:05, 47.24it/s][A
 67%|██████▋   | 525/782 [00:11<00:05, 47.23it/s][A
 68%|██████▊   | 530/782 [00:11<00:05, 47.03it/s][A
 68%|██████▊   | 535/782 [00:11<00:05, 46.96it/s][A
 69%|██████▉   | 540/782 [00:11<00:05, 47.07it/s][A
 70%|██████▉   | 545/782 [00:11<00:05, 47.00it/s][A
 70%|███████   | 550/782 [00:11<00:04, 47.10it/s][A
 71%|███████   | 555/782 [00:11<00:04, 47.19it/s][A
 72%|███████▏  | 560/782 [00:11<00:04, 47.06it/s][A
 72%|███████▏  | 565/782 [00:12<00:04, 47.09it/s][A
 73%|███████▎  | 570/782 [00:12<00:04, 47.20it/s][A
 74%|███████▎  | 575/782 [00:12<00:04, 47.18it/s][A
 74%|███████▍  | 580/782 [00:12<00:04, 47.04it/s][A
 75%|███████▍  | 585/782 [00:12<00:04, 47.14it/s][A
 75%|███████▌  | 590/782 [00:12<00:04, 47.18it/s][A
 76%|███████▌  | 595/782 [00:12<00:03, 47.09it/s][A
 77%|███████▋  | 600/782 [00:12<00:03, 47.18it/s][A
 77%|███████▋  | 605/782 [00:12<00:03, 47.20it/s][A
 78%|███████▊  | 610/782 [00:12<00:03, 47.07it/s][A
 79%|███████▊  | 615/782 [00:13<00:03, 47.10it/s][A
 79%|███████▉  | 620/782 [00:13<00:03, 47.17it/s][A
 80%|███████▉  | 625/782 [00:13<00:03, 47.19it/s][A
 81%|████████  | 630/782 [00:13<00:03, 47.07it/s][A
 81%|████████  | 635/782 [00:13<00:03, 47.16it/s][A
 82%|████████▏ | 640/782 [00:13<00:03, 47.08it/s][A
 82%|████████▏ | 645/782 [00:13<00:02, 47.02it/s][A
 83%|████████▎ | 650/782 [00:13<00:02, 47.12it/s][A
 84%|████████▍ | 655/782 [00:13<00:02, 47.17it/s][A
 84%|████████▍ | 660/782 [00:14<00:02, 47.02it/s][A
 85%|████████▌ | 665/782 [00:14<00:02, 47.06it/s][A
 86%|████████▌ | 670/782 [00:14<00:02, 47.13it/s][A
 86%|████████▋ | 675/782 [00:14<00:02, 47.12it/s][A
 87%|████████▋ | 680/782 [00:14<00:02, 46.87it/s][A
 88%|████████▊ | 685/782 [00:14<00:02, 46.99it/s][A
 88%|████████▊ | 690/782 [00:14<00:01, 46.95it/s][A
 89%|████████▉ | 695/782 [00:14<00:01, 46.97it/s][A
 90%|████████▉ | 700/782 [00:14<00:01, 47.10it/s][A
 90%|█████████ | 705/782 [00:14<00:01, 47.16it/s][A
 91%|█████████ | 710/782 [00:15<00:01, 47.09it/s][A
 91%|█████████▏| 715/782 [00:15<00:01, 47.13it/s][A
 92%|█████████▏| 720/782 [00:15<00:01, 47.08it/s][A
 93%|█████████▎| 725/782 [00:15<00:01, 47.02it/s][A
 93%|█████████▎| 730/782 [00:15<00:01, 47.15it/s][A
 94%|█████████▍| 735/782 [00:15<00:00, 47.14it/s][A
 95%|█████████▍| 740/782 [00:15<00:00, 46.99it/s][A
 95%|█████████▌| 745/782 [00:15<00:00, 47.11it/s][A
 96%|█████████▌| 750/782 [00:15<00:00, 47.16it/s][A
 97%|█████████▋| 755/782 [00:16<00:00, 47.12it/s][A
 97%|█████████▋| 760/782 [00:16<00:00, 47.13it/s][A
 98%|█████████▊| 765/782 [00:16<00:00, 47.12it/s][A
 98%|█████████▊| 770/782 [00:16<00:00, 47.02it/s][A
 99%|█████████▉| 775/782 [00:16<00:00, 46.89it/s][A
100%|█████████▉| 780/782 [00:16<00:00, 46.91it/s][A                                                 
                                                 [A 80%|████████  | 280/350 [03:05<00:20,  3.46it/s]
100%|██████████| 782/782 [00:16<00:00, 46.91it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:12:44,202 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 23:12:44,256 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:12:49,608 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:12:49,624 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:12:49,633 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [03:25<12:58, 11.29s/it] 81%|████████  | 282/350 [03:26<09:03,  7.99s/it] 81%|████████  | 283/350 [03:26<06:20,  5.68s/it] 81%|████████  | 284/350 [03:26<04:27,  4.06s/it] 81%|████████▏ | 285/350 [03:27<03:10,  2.93s/it] 82%|████████▏ | 286/350 [03:27<02:16,  2.14s/it] 82%|████████▏ | 287/350 [03:27<01:39,  1.58s/it] 82%|████████▏ | 288/350 [03:27<01:13,  1.19s/it] 83%|████████▎ | 289/350 [03:28<00:56,  1.08it/s] 83%|████████▎ | 290/350 [03:28<00:43,  1.37it/s] 83%|████████▎ | 291/350 [03:28<00:35,  1.67it/s] 83%|████████▎ | 292/350 [03:29<00:29,  1.98it/s] 84%|████████▎ | 293/350 [03:29<00:25,  2.28it/s] 84%|████████▍ | 294/350 [03:29<00:22,  2.54it/s] 84%|████████▍ | 295/350 [03:29<00:19,  2.76it/s] 85%|████████▍ | 296/350 [03:30<00:18,  2.94it/s] 85%|████████▍ | 297/350 [03:30<00:17,  3.09it/s] 85%|████████▌ | 298/350 [03:30<00:16,  3.19it/s] 85%|████████▌ | 299/350 [03:31<00:15,  3.28it/s] 86%|████████▌ | 300/350 [03:31<00:15,  3.32it/s] 86%|████████▌ | 301/350 [03:31<00:14,  3.37it/s] 86%|████████▋ | 302/350 [03:31<00:14,  3.40it/s] 87%|████████▋ | 303/350 [03:32<00:13,  3.43it/s] 87%|████████▋ | 304/350 [03:32<00:13,  3.44it/s] 87%|████████▋ | 305/350 [03:32<00:13,  3.45it/s] 87%|████████▋ | 306/350 [03:33<00:12,  3.46it/s] 88%|████████▊ | 307/350 [03:33<00:12,  3.47it/s] 88%|████████▊ | 308/350 [03:33<00:12,  3.47it/s] 88%|████████▊ | 309/350 [03:33<00:11,  3.48it/s] 89%|████████▊ | 310/350 [03:34<00:11,  3.48it/s] 89%|████████▉ | 311/350 [03:34<00:11,  3.46it/s] 89%|████████▉ | 312/350 [03:34<00:10,  3.47it/s] 89%|████████▉ | 313/350 [03:35<00:10,  3.47it/s] 90%|████████▉ | 314/350 [03:35<00:10,  3.47it/s] 90%|█████████ | 315/350 [03:35<00:10,  3.48it/s] 90%|█████████ | 316/350 [03:35<00:09,  3.48it/s] 91%|█████████ | 317/350 [03:36<00:09,  3.48it/s] 91%|█████████ | 318/350 [03:36<00:09,  3.48it/s] 91%|█████████ | 319/350 [03:36<00:08,  3.48it/s] 91%|█████████▏| 320/350 [03:37<00:08,  3.48it/s] 92%|█████████▏| 321/350 [03:37<00:08,  3.48it/s] 92%|█████████▏| 322/350 [03:37<00:08,  3.41it/s] 92%|█████████▏| 323/350 [03:38<00:07,  3.43it/s] 93%|█████████▎| 324/350 [03:38<00:07,  3.44it/s] 93%|█████████▎| 325/350 [03:38<00:07,  3.46it/s] 93%|█████████▎| 326/350 [03:38<00:06,  3.46it/s] 93%|█████████▎| 327/350 [03:39<00:06,  3.46it/s] 94%|█████████▎| 328/350 [03:39<00:06,  3.47it/s] 94%|█████████▍| 329/350 [03:39<00:06,  3.47it/s] 94%|█████████▍| 330/350 [03:40<00:05,  3.47it/s] 95%|█████████▍| 331/350 [03:40<00:05,  3.47it/s] 95%|█████████▍| 332/350 [03:40<00:05,  3.47it/s] 95%|█████████▌| 333/350 [03:40<00:04,  3.43it/s] 95%|█████████▌| 334/350 [03:41<00:04,  3.44it/s] 96%|█████████▌| 335/350 [03:41<00:04,  3.45it/s] 96%|█████████▌| 336/350 [03:41<00:04,  3.46it/s] 96%|█████████▋| 337/350 [03:42<00:03,  3.47it/s] 97%|█████████▋| 338/350 [03:42<00:03,  3.47it/s] 97%|█████████▋| 339/350 [03:42<00:03,  3.47it/s] 97%|█████████▋| 340/350 [03:42<00:02,  3.47it/s] 97%|█████████▋| 341/350 [03:43<00:02,  3.47it/s] 98%|█████████▊| 342/350 [03:43<00:02,  3.48it/s] 98%|█████████▊| 343/350 [03:43<00:02,  3.47it/s] 98%|█████████▊| 344/350 [03:44<00:01,  3.46it/s] 99%|█████████▊| 345/350 [03:44<00:01,  3.46it/s] 99%|█████████▉| 346/350 [03:44<00:01,  3.47it/s] 99%|█████████▉| 347/350 [03:44<00:00,  3.47it/s] 99%|█████████▉| 348/350 [03:45<00:00,  3.47it/s]100%|█████████▉| 349/350 [03:45<00:00,  3.47it/s]100%|██████████| 350/350 [03:45<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 23:13:24,282 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:13:24,282 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 23:13:24,282 >>   Batch size = 8
{'eval_loss': 1.0211551189422607, 'eval_runtime': 16.6812, 'eval_samples_per_second': 374.853, 'eval_steps_per_second': 46.879, 'epoch': 3.99}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 58.00it/s][A
  2%|▏         | 12/782 [00:00<00:14, 51.37it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.39it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.64it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.15it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.88it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.71it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.39it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.23it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.25it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.27it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.28it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.35it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.29it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.26it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.34it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.23it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.16it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.18it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.17it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.12it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.18it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.25it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.30it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.17it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.14it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.18it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.17it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.12it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.15it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.28it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.27it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.28it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.24it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.21it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.16it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.14it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.19it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.27it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.20it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.86it/s][A
 27%|██▋       | 213/782 [00:04<00:13, 41.95it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 43.47it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 44.51it/s][A
 29%|██▉       | 228/782 [00:04<00:12, 45.30it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 45.81it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.35it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.70it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.83it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.98it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.93it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.85it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.01it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.01it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.15it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 47.17it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.32it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.32it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.26it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.15it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.13it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.16it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.11it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.19it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.20it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.28it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.28it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.29it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.25it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.12it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 47.06it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.11it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.12it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.15it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.19it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.19it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.24it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.28it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.21it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.11it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.13it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.13it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.17it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.21it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.18it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.17it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.16it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.24it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.19it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.04it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.11it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.21it/s][A
 60%|█████▉    | 468/782 [00:09<00:07, 44.76it/s][A
 60%|██████    | 473/782 [00:10<00:06, 45.57it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.15it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.44it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.70it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.91it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.00it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.03it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.95it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.85it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.88it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.05it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.08it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.17it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.17it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.23it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.26it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.12it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.10it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.02it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.95it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.13it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.17it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.19it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.21it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.20it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.14it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.13it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.07it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.55it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.78it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.92it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.00it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.12it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.11it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.16it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.08it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.93it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 46.90it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.98it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.10it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.13it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.18it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.20it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.19it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.11it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.11it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.94it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.96it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.08it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.12it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.16it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.21it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.22it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.16it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.02it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.03it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 47.03it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.95it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.02it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.11it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.14it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.12it/s][A                                                 
                                                 [A100%|██████████| 350/350 [04:02<00:00,  3.47it/s]
100%|██████████| 782/782 [00:16<00:00, 47.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:13:40,949 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 23:13:40,978 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:13:44,471 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:13:44,513 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:13:44,541 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:13:55,360 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:13:55,366 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70 (score: 0.9789218306541443).
                                                 100%|██████████| 350/350 [04:24<00:00,  3.47it/s]100%|██████████| 350/350 [04:24<00:00,  1.32it/s]
[INFO|trainer.py:1894] 2023-08-28 23:14:03,147 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 23:14:03,162 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:14:06,689 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:14:06,729 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:14:06,742 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:14:06,954 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:06,954 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:06,954 >>   train_loss               =     0.4152
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:06,954 >>   train_runtime            = 0:04:24.64
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:06,954 >>   train_samples            =       4501
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:06,954 >>   train_samples_per_second =     85.037
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:06,954 >>   train_steps_per_second   =      1.323
{'eval_loss': 1.0202841758728027, 'eval_runtime': 16.6383, 'eval_samples_per_second': 375.819, 'eval_steps_per_second': 47.0, 'epoch': 4.99}
{'train_runtime': 264.6492, 'train_samples_per_second': 85.037, 'train_steps_per_second': 1.323, 'train_loss': 0.41521026611328127, 'epoch': 4.99}
08/28/2023 23:14:06 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:14:06,991 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:14:06,991 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-28 23:14:06,991 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.55it/s]  2%|▏         | 12/782 [00:00<00:14, 51.64it/s]  2%|▏         | 18/782 [00:00<00:15, 49.73it/s]  3%|▎         | 24/782 [00:00<00:15, 48.76it/s]  4%|▎         | 29/782 [00:00<00:15, 48.35it/s]  4%|▍         | 34/782 [00:00<00:15, 48.20it/s]  5%|▍         | 39/782 [00:00<00:15, 47.97it/s]  6%|▌         | 44/782 [00:00<00:15, 47.79it/s]  6%|▋         | 49/782 [00:01<00:15, 47.57it/s]  7%|▋         | 54/782 [00:01<00:15, 47.49it/s]  8%|▊         | 59/782 [00:01<00:15, 47.52it/s]  8%|▊         | 64/782 [00:01<00:15, 47.54it/s]  9%|▉         | 69/782 [00:01<00:15, 47.46it/s]  9%|▉         | 74/782 [00:01<00:14, 47.52it/s] 10%|█         | 79/782 [00:01<00:14, 47.52it/s] 11%|█         | 84/782 [00:01<00:14, 47.43it/s] 11%|█▏        | 89/782 [00:01<00:14, 47.45it/s] 12%|█▏        | 94/782 [00:01<00:14, 47.38it/s] 13%|█▎        | 99/782 [00:02<00:14, 47.34it/s] 13%|█▎        | 104/782 [00:02<00:14, 47.37it/s] 14%|█▍        | 109/782 [00:02<00:14, 47.36it/s] 15%|█▍        | 114/782 [00:02<00:14, 47.34it/s] 15%|█▌        | 119/782 [00:02<00:13, 47.42it/s] 16%|█▌        | 124/782 [00:02<00:13, 47.43it/s] 16%|█▋        | 129/782 [00:02<00:13, 47.38it/s] 17%|█▋        | 134/782 [00:02<00:13, 47.43it/s] 18%|█▊        | 139/782 [00:02<00:13, 47.35it/s] 18%|█▊        | 144/782 [00:03<00:13, 47.37it/s] 19%|█▉        | 149/782 [00:03<00:13, 47.42it/s] 20%|█▉        | 154/782 [00:03<00:13, 47.42it/s] 20%|██        | 159/782 [00:03<00:13, 47.29it/s] 21%|██        | 164/782 [00:03<00:13, 47.36it/s] 22%|██▏       | 169/782 [00:03<00:12, 47.43it/s] 22%|██▏       | 174/782 [00:03<00:12, 47.30it/s] 23%|██▎       | 179/782 [00:03<00:12, 47.42it/s] 24%|██▎       | 184/782 [00:03<00:12, 47.37it/s] 24%|██▍       | 189/782 [00:03<00:12, 47.30it/s] 25%|██▍       | 194/782 [00:04<00:12, 45.51it/s] 25%|██▌       | 199/782 [00:04<00:12, 46.05it/s] 26%|██▌       | 204/782 [00:04<00:12, 46.44it/s] 27%|██▋       | 209/782 [00:04<00:12, 46.74it/s] 27%|██▋       | 214/782 [00:04<00:12, 46.95it/s] 28%|██▊       | 219/782 [00:04<00:11, 47.07it/s] 29%|██▊       | 224/782 [00:04<00:11, 47.25it/s] 29%|██▉       | 229/782 [00:04<00:11, 47.26it/s] 30%|██▉       | 234/782 [00:04<00:11, 47.16it/s] 31%|███       | 239/782 [00:05<00:11, 47.31it/s] 31%|███       | 244/782 [00:05<00:11, 47.27it/s] 32%|███▏      | 249/782 [00:05<00:11, 47.23it/s] 32%|███▏      | 254/782 [00:05<00:11, 47.25it/s] 33%|███▎      | 259/782 [00:05<00:11, 47.33it/s] 34%|███▍      | 264/782 [00:05<00:10, 47.38it/s] 34%|███▍      | 269/782 [00:05<00:10, 47.41it/s] 35%|███▌      | 274/782 [00:05<00:10, 47.47it/s] 36%|███▌      | 279/782 [00:05<00:10, 47.39it/s] 36%|███▋      | 284/782 [00:05<00:10, 47.28it/s] 37%|███▋      | 289/782 [00:06<00:10, 47.30it/s] 38%|███▊      | 294/782 [00:06<00:10, 47.29it/s] 38%|███▊      | 299/782 [00:06<00:10, 47.25it/s] 39%|███▉      | 304/782 [00:06<00:10, 47.19it/s] 40%|███▉      | 309/782 [00:06<00:09, 47.33it/s] 40%|████      | 314/782 [00:06<00:09, 47.39it/s] 41%|████      | 319/782 [00:06<00:09, 47.36it/s] 41%|████▏     | 324/782 [00:06<00:09, 47.39it/s] 42%|████▏     | 329/782 [00:06<00:09, 47.37it/s] 43%|████▎     | 334/782 [00:07<00:09, 47.28it/s] 43%|████▎     | 339/782 [00:07<00:09, 47.04it/s] 44%|████▍     | 344/782 [00:07<00:09, 47.18it/s] 45%|████▍     | 349/782 [00:07<00:09, 47.12it/s] 45%|████▌     | 354/782 [00:07<00:09, 47.10it/s] 46%|████▌     | 359/782 [00:07<00:08, 47.29it/s] 47%|████▋     | 364/782 [00:07<00:08, 47.28it/s] 47%|████▋     | 369/782 [00:07<00:08, 47.31it/s] 48%|████▊     | 374/782 [00:07<00:08, 47.34it/s] 48%|████▊     | 379/782 [00:07<00:08, 47.30it/s] 49%|████▉     | 384/782 [00:08<00:08, 47.32it/s] 50%|████▉     | 389/782 [00:08<00:08, 47.29it/s] 50%|█████     | 394/782 [00:08<00:08, 47.29it/s] 51%|█████     | 399/782 [00:08<00:08, 47.24it/s] 52%|█████▏    | 404/782 [00:08<00:08, 47.24it/s] 52%|█████▏    | 409/782 [00:08<00:07, 47.27it/s] 53%|█████▎    | 414/782 [00:08<00:07, 47.31it/s] 54%|█████▎    | 419/782 [00:08<00:07, 47.38it/s] 54%|█████▍    | 424/782 [00:08<00:07, 47.28it/s] 55%|█████▍    | 429/782 [00:09<00:07, 47.31it/s] 55%|█████▌    | 434/782 [00:09<00:07, 47.38it/s] 56%|█████▌    | 439/782 [00:09<00:07, 47.25it/s] 57%|█████▋    | 444/782 [00:09<00:07, 47.25it/s] 57%|█████▋    | 449/782 [00:09<00:07, 47.22it/s] 58%|█████▊    | 454/782 [00:09<00:06, 47.24it/s] 59%|█████▊    | 459/782 [00:09<00:06, 47.19it/s] 59%|█████▉    | 464/782 [00:09<00:06, 47.37it/s] 60%|█████▉    | 469/782 [00:09<00:06, 47.34it/s] 61%|██████    | 474/782 [00:10<00:06, 47.25it/s] 61%|██████▏   | 479/782 [00:10<00:06, 47.35it/s] 62%|██████▏   | 484/782 [00:10<00:06, 47.08it/s] 63%|██████▎   | 489/782 [00:10<00:06, 47.09it/s] 63%|██████▎   | 494/782 [00:10<00:06, 47.06it/s] 64%|██████▍   | 499/782 [00:10<00:06, 47.16it/s] 64%|██████▍   | 504/782 [00:10<00:05, 47.18it/s] 65%|██████▌   | 509/782 [00:10<00:05, 47.23it/s] 66%|██████▌   | 514/782 [00:10<00:05, 47.30it/s] 66%|██████▋   | 519/782 [00:10<00:05, 47.25it/s] 67%|██████▋   | 524/782 [00:11<00:05, 47.25it/s] 68%|██████▊   | 529/782 [00:11<00:05, 47.25it/s] 68%|██████▊   | 534/782 [00:11<00:05, 47.31it/s] 69%|██████▉   | 539/782 [00:11<00:05, 47.23it/s] 70%|██████▉   | 544/782 [00:11<00:05, 47.17it/s] 70%|███████   | 549/782 [00:11<00:04, 47.21it/s] 71%|███████   | 554/782 [00:11<00:04, 47.27it/s] 71%|███████▏  | 559/782 [00:11<00:04, 47.29it/s] 72%|███████▏  | 564/782 [00:11<00:04, 47.30it/s] 73%|███████▎  | 569/782 [00:12<00:04, 47.33it/s] 73%|███████▎  | 574/782 [00:12<00:04, 47.25it/s] 74%|███████▍  | 579/782 [00:12<00:04, 47.22it/s] 75%|███████▍  | 584/782 [00:12<00:04, 47.31it/s] 75%|███████▌  | 589/782 [00:12<00:04, 47.25it/s] 76%|███████▌  | 594/782 [00:12<00:03, 47.13it/s] 77%|███████▋  | 599/782 [00:12<00:03, 47.23it/s] 77%|███████▋  | 604/782 [00:12<00:03, 47.25it/s] 78%|███████▊  | 609/782 [00:12<00:03, 47.22it/s] 79%|███████▊  | 614/782 [00:12<00:03, 47.24it/s] 79%|███████▉  | 619/782 [00:13<00:03, 47.22it/s] 80%|███████▉  | 624/782 [00:13<00:03, 47.25it/s] 80%|████████  | 629/782 [00:13<00:03, 47.18it/s] 81%|████████  | 634/782 [00:13<00:03, 47.28it/s] 82%|████████▏ | 639/782 [00:13<00:03, 47.11it/s] 82%|████████▏ | 644/782 [00:13<00:02, 47.19it/s] 83%|████████▎ | 649/782 [00:13<00:02, 47.33it/s] 84%|████████▎ | 654/782 [00:13<00:02, 47.26it/s] 84%|████████▍ | 659/782 [00:13<00:02, 47.21it/s] 85%|████████▍ | 664/782 [00:14<00:02, 47.24it/s] 86%|████████▌ | 669/782 [00:14<00:02, 47.28it/s] 86%|████████▌ | 674/782 [00:14<00:02, 47.19it/s] 87%|████████▋ | 679/782 [00:14<00:02, 47.20it/s] 87%|████████▋ | 684/782 [00:14<00:02, 47.25it/s] 88%|████████▊ | 689/782 [00:14<00:01, 47.25it/s] 89%|████████▊ | 694/782 [00:14<00:01, 47.31it/s] 89%|████████▉ | 699/782 [00:14<00:01, 47.30it/s] 90%|█████████ | 704/782 [00:14<00:01, 47.37it/s] 91%|█████████ | 709/782 [00:14<00:01, 47.30it/s] 91%|█████████▏| 714/782 [00:15<00:01, 47.24it/s] 92%|█████████▏| 719/782 [00:15<00:01, 47.32it/s] 93%|█████████▎| 724/782 [00:15<00:01, 47.41it/s] 93%|█████████▎| 729/782 [00:15<00:01, 47.28it/s] 94%|█████████▍| 734/782 [00:15<00:01, 47.30it/s] 95%|█████████▍| 739/782 [00:15<00:00, 47.35it/s] 95%|█████████▌| 744/782 [00:15<00:00, 47.17it/s] 96%|█████████▌| 749/782 [00:15<00:00, 47.27it/s] 96%|█████████▋| 754/782 [00:15<00:00, 47.32it/s] 97%|█████████▋| 759/782 [00:16<00:00, 47.35it/s] 98%|█████████▊| 764/782 [00:16<00:00, 47.24it/s] 98%|█████████▊| 769/782 [00:16<00:00, 47.32it/s] 99%|█████████▉| 774/782 [00:16<00:00, 47.39it/s]100%|█████████▉| 779/782 [00:16<00:00, 47.32it/s]100%|██████████| 782/782 [00:16<00:00, 47.33it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:14:23,537 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   eval_loss               =     0.9789
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   eval_runtime            = 0:00:16.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   eval_samples_per_second =    377.929
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   eval_steps_per_second   =     47.264
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:14:23,537 >>   perplexity              =     2.6616
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:30,199 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:30,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:30,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:30,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:30,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:14:30,572 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:14:30,573 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:14:30,841 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:14:31,901 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:14:31,901 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:33,281 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:33,290 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:33,290 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:33,290 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:14:33,290 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:14:33,638 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:14:33,639 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:14:34,337 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:14:34,535 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:14:34,535 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:08,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:26,  1.48it/s]Extractor Predicting: 40it [00:26,  1.43it/s]Extractor Predicting: 41it [00:27,  1.45it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:30,  1.50it/s]Extractor Predicting: 46it [00:30,  1.49it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.55it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:33,  1.46it/s]Extractor Predicting: 51it [00:34,  1.46it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.51it/s]Extractor Predicting: 61it [00:40,  1.48it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:44,  1.47it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:46,  1.49it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:48,  1.49it/s]Extractor Predicting: 73it [00:49,  1.47it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:51,  1.50it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:53,  1.49it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:54,  1.49it/s]Extractor Predicting: 82it [00:55,  1.51it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.49it/s]Extractor Predicting: 85it [00:57,  1.48it/s]Extractor Predicting: 86it [00:57,  1.48it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [01:00,  1.49it/s]Extractor Predicting: 91it [01:01,  1.35it/s]Extractor Predicting: 92it [01:02,  1.37it/s]Extractor Predicting: 93it [01:02,  1.41it/s]Extractor Predicting: 94it [01:03,  1.44it/s]Extractor Predicting: 95it [01:04,  1.46it/s]Extractor Predicting: 96it [01:04,  1.44it/s]Extractor Predicting: 97it [01:05,  1.44it/s]Extractor Predicting: 98it [01:06,  1.44it/s]Extractor Predicting: 99it [01:06,  1.44it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:08,  1.46it/s]Extractor Predicting: 102it [01:08,  1.48it/s]Extractor Predicting: 103it [01:09,  1.49it/s]Extractor Predicting: 104it [01:10,  1.50it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:11,  1.48it/s]Extractor Predicting: 107it [01:12,  1.45it/s]Extractor Predicting: 108it [01:12,  1.45it/s]Extractor Predicting: 109it [01:13,  1.45it/s]Extractor Predicting: 110it [01:14,  1.47it/s]Extractor Predicting: 111it [01:14,  1.49it/s]Extractor Predicting: 112it [01:15,  1.48it/s]Extractor Predicting: 113it [01:16,  1.52it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:17,  1.49it/s]Extractor Predicting: 116it [01:18,  1.49it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:19,  1.48it/s]Extractor Predicting: 119it [01:20,  1.47it/s]Extractor Predicting: 120it [01:21,  1.48it/s]Extractor Predicting: 121it [01:21,  1.47it/s]Extractor Predicting: 122it [01:22,  1.49it/s]Extractor Predicting: 123it [01:22,  1.52it/s]Extractor Predicting: 124it [01:23,  1.48it/s]Extractor Predicting: 125it [01:24,  1.46it/s]Extractor Predicting: 126it [01:25,  1.48it/s]Extractor Predicting: 127it [01:25,  1.47it/s]Extractor Predicting: 128it [01:26,  1.46it/s]Extractor Predicting: 129it [01:27,  1.47it/s]Extractor Predicting: 130it [01:27,  1.45it/s]Extractor Predicting: 131it [01:28,  1.44it/s]Extractor Predicting: 132it [01:29,  1.43it/s]Extractor Predicting: 133it [01:29,  1.45it/s]Extractor Predicting: 134it [01:30,  1.45it/s]Extractor Predicting: 135it [01:31,  1.44it/s]Extractor Predicting: 136it [01:31,  1.45it/s]Extractor Predicting: 137it [01:32,  1.46it/s]Extractor Predicting: 138it [01:33,  1.48it/s]Extractor Predicting: 139it [01:33,  1.48it/s]Extractor Predicting: 140it [01:34,  1.47it/s]Extractor Predicting: 141it [01:35,  1.49it/s]Extractor Predicting: 142it [01:35,  1.49it/s]Extractor Predicting: 143it [01:36,  1.47it/s]Extractor Predicting: 144it [01:37,  1.49it/s]Extractor Predicting: 145it [01:38,  1.49it/s]Extractor Predicting: 146it [01:38,  1.50it/s]Extractor Predicting: 147it [01:39,  1.48it/s]Extractor Predicting: 148it [01:39,  1.53it/s]Extractor Predicting: 149it [01:40,  1.52it/s]Extractor Predicting: 150it [01:41,  1.51it/s]Extractor Predicting: 151it [01:42,  1.48it/s]Extractor Predicting: 152it [01:42,  1.52it/s]Extractor Predicting: 153it [01:43,  1.50it/s]Extractor Predicting: 154it [01:43,  1.50it/s]Extractor Predicting: 155it [01:44,  1.51it/s]Extractor Predicting: 156it [01:45,  1.50it/s]Extractor Predicting: 157it [01:45,  1.54it/s]Extractor Predicting: 158it [01:46,  1.54it/s]Extractor Predicting: 159it [01:47,  1.55it/s]Extractor Predicting: 160it [01:47,  1.50it/s]Extractor Predicting: 161it [01:48,  1.49it/s]Extractor Predicting: 162it [01:49,  1.51it/s]Extractor Predicting: 163it [01:49,  1.53it/s]Extractor Predicting: 164it [01:50,  1.55it/s]Extractor Predicting: 165it [01:51,  1.58it/s]Extractor Predicting: 166it [01:51,  1.59it/s]Extractor Predicting: 167it [01:52,  1.57it/s]Extractor Predicting: 168it [01:53,  1.54it/s]Extractor Predicting: 169it [01:53,  1.56it/s]Extractor Predicting: 170it [01:54,  1.57it/s]Extractor Predicting: 171it [01:54,  1.55it/s]Extractor Predicting: 172it [01:55,  1.53it/s]Extractor Predicting: 173it [01:56,  1.56it/s]Extractor Predicting: 174it [01:56,  1.52it/s]Extractor Predicting: 175it [01:57,  1.53it/s]Extractor Predicting: 176it [01:58,  1.53it/s]Extractor Predicting: 177it [01:58,  1.54it/s]Extractor Predicting: 178it [01:59,  1.53it/s]Extractor Predicting: 179it [02:00,  1.39it/s]Extractor Predicting: 180it [02:01,  1.42it/s]Extractor Predicting: 181it [02:01,  1.46it/s]Extractor Predicting: 182it [02:02,  1.46it/s]Extractor Predicting: 183it [02:03,  1.49it/s]Extractor Predicting: 184it [02:03,  1.52it/s]Extractor Predicting: 185it [02:04,  1.51it/s]Extractor Predicting: 186it [02:05,  1.48it/s]Extractor Predicting: 187it [02:05,  1.52it/s]Extractor Predicting: 188it [02:06,  1.50it/s]Extractor Predicting: 189it [02:06,  1.52it/s]Extractor Predicting: 190it [02:07,  1.54it/s]Extractor Predicting: 191it [02:08,  1.56it/s]Extractor Predicting: 192it [02:08,  1.57it/s]Extractor Predicting: 193it [02:09,  1.54it/s]Extractor Predicting: 194it [02:10,  1.56it/s]Extractor Predicting: 195it [02:10,  1.56it/s]Extractor Predicting: 196it [02:11,  1.53it/s]Extractor Predicting: 197it [02:12,  1.56it/s]Extractor Predicting: 198it [02:12,  1.54it/s]Extractor Predicting: 199it [02:13,  1.52it/s]Extractor Predicting: 200it [02:14,  1.58it/s]Extractor Predicting: 201it [02:14,  1.59it/s]Extractor Predicting: 202it [02:15,  1.60it/s]Extractor Predicting: 203it [02:15,  1.60it/s]Extractor Predicting: 204it [02:16,  1.60it/s]Extractor Predicting: 205it [02:17,  1.58it/s]Extractor Predicting: 206it [02:17,  1.60it/s]Extractor Predicting: 207it [02:18,  1.62it/s]Extractor Predicting: 208it [02:19,  1.59it/s]Extractor Predicting: 209it [02:19,  1.57it/s]Extractor Predicting: 210it [02:20,  1.57it/s]Extractor Predicting: 211it [02:20,  1.58it/s]Extractor Predicting: 212it [02:21,  1.59it/s]Extractor Predicting: 213it [02:22,  1.56it/s]Extractor Predicting: 214it [02:22,  1.59it/s]Extractor Predicting: 215it [02:23,  1.57it/s]Extractor Predicting: 216it [02:24,  1.56it/s]Extractor Predicting: 217it [02:24,  1.56it/s]Extractor Predicting: 218it [02:25,  1.57it/s]Extractor Predicting: 219it [02:26,  1.55it/s]Extractor Predicting: 220it [02:26,  1.54it/s]Extractor Predicting: 221it [02:27,  1.56it/s]Extractor Predicting: 222it [02:27,  1.55it/s]Extractor Predicting: 223it [02:28,  1.55it/s]Extractor Predicting: 224it [02:29,  1.57it/s]Extractor Predicting: 225it [02:29,  1.56it/s]Extractor Predicting: 226it [02:30,  1.56it/s]Extractor Predicting: 227it [02:31,  1.57it/s]Extractor Predicting: 228it [02:31,  1.72it/s]Extractor Predicting: 228it [02:31,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:14,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:14,588 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:14,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:14,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:14,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:17:15,317 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:17:15,318 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:17:15,594 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:17:16,642 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:17:16,642 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:18,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:18,378 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:18,378 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:18,379 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:18,379 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:17:19,132 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:17:19,133 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:17:19,396 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:17:19,551 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:17:19,551 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.11318242343541944,
  "recall": 0.05437390052774668,
  "score": 0.07345792373339095,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:09,  1.46it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.44it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.37it/s]Extractor Predicting: 40it [00:27,  1.41it/s]Extractor Predicting: 41it [00:27,  1.43it/s]Extractor Predicting: 42it [00:28,  1.44it/s]Extractor Predicting: 43it [00:29,  1.34it/s]Extractor Predicting: 44it [00:29,  1.37it/s]Extractor Predicting: 45it [00:30,  1.37it/s]Extractor Predicting: 46it [00:31,  1.42it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:32,  1.56it/s]Extractor Predicting: 49it [00:33,  1.58it/s]Extractor Predicting: 50it [00:33,  1.55it/s]Extractor Predicting: 51it [00:34,  1.55it/s]Extractor Predicting: 52it [00:35,  1.50it/s]Extractor Predicting: 53it [00:35,  1.43it/s]Extractor Predicting: 54it [00:36,  1.48it/s]Extractor Predicting: 55it [00:37,  1.53it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:38,  1.52it/s]Extractor Predicting: 58it [00:39,  1.51it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.54it/s]Extractor Predicting: 61it [00:41,  1.57it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.54it/s]Extractor Predicting: 64it [00:42,  1.55it/s]Extractor Predicting: 65it [00:43,  1.56it/s]Extractor Predicting: 66it [00:44,  1.55it/s]Extractor Predicting: 67it [00:44,  1.55it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:46,  1.55it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:47,  1.58it/s]Extractor Predicting: 72it [00:48,  1.58it/s]Extractor Predicting: 73it [00:48,  1.56it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:50,  1.52it/s]Extractor Predicting: 76it [00:50,  1.56it/s]Extractor Predicting: 77it [00:51,  1.55it/s]Extractor Predicting: 78it [00:52,  1.52it/s]Extractor Predicting: 79it [00:52,  1.54it/s]Extractor Predicting: 80it [00:53,  1.53it/s]Extractor Predicting: 81it [00:54,  1.51it/s]Extractor Predicting: 82it [00:54,  1.54it/s]Extractor Predicting: 83it [00:55,  1.53it/s]Extractor Predicting: 84it [00:55,  1.56it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:57,  1.55it/s]Extractor Predicting: 87it [00:57,  1.56it/s]Extractor Predicting: 88it [00:58,  1.56it/s]Extractor Predicting: 89it [00:59,  1.57it/s]Extractor Predicting: 90it [00:59,  1.58it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:01,  1.55it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.55it/s]Extractor Predicting: 95it [01:02,  1.56it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:04,  1.55it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:06,  1.54it/s]Extractor Predicting: 101it [01:06,  1.55it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:08,  1.56it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:10,  1.52it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:11,  1.55it/s]Extractor Predicting: 109it [01:12,  1.55it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:13,  1.56it/s]Extractor Predicting: 112it [01:13,  1.57it/s]Extractor Predicting: 113it [01:14,  1.59it/s]Extractor Predicting: 114it [01:15,  1.54it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:17,  1.59it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:18,  1.63it/s]Extractor Predicting: 120it [01:18,  1.63it/s]Extractor Predicting: 121it [01:19,  1.62it/s]Extractor Predicting: 122it [01:20,  1.64it/s]Extractor Predicting: 123it [01:20,  1.64it/s]Extractor Predicting: 124it [01:21,  1.64it/s]Extractor Predicting: 125it [01:22,  1.59it/s]Extractor Predicting: 126it [01:22,  1.62it/s]Extractor Predicting: 127it [01:23,  1.59it/s]Extractor Predicting: 128it [01:24,  1.53it/s]Extractor Predicting: 129it [01:24,  1.54it/s]Extractor Predicting: 130it [01:25,  1.58it/s]Extractor Predicting: 131it [01:25,  1.56it/s]Extractor Predicting: 132it [01:26,  1.60it/s]Extractor Predicting: 133it [01:27,  1.59it/s]Extractor Predicting: 134it [01:27,  1.58it/s]Extractor Predicting: 135it [01:28,  1.59it/s]Extractor Predicting: 136it [01:29,  1.62it/s]Extractor Predicting: 137it [01:29,  1.60it/s]Extractor Predicting: 138it [01:30,  1.55it/s]Extractor Predicting: 139it [01:30,  1.55it/s]Extractor Predicting: 140it [01:31,  1.59it/s]Extractor Predicting: 141it [01:32,  1.45it/s]Extractor Predicting: 142it [01:33,  1.52it/s]Extractor Predicting: 143it [01:33,  1.53it/s]Extractor Predicting: 144it [01:34,  1.56it/s]Extractor Predicting: 145it [01:34,  1.56it/s]Extractor Predicting: 146it [01:35,  1.56it/s]Extractor Predicting: 147it [01:36,  1.58it/s]Extractor Predicting: 148it [01:36,  1.59it/s]Extractor Predicting: 149it [01:37,  1.54it/s]Extractor Predicting: 150it [01:38,  1.50it/s]Extractor Predicting: 151it [01:38,  1.52it/s]Extractor Predicting: 152it [01:39,  1.54it/s]Extractor Predicting: 153it [01:40,  1.56it/s]Extractor Predicting: 154it [01:40,  1.53it/s]Extractor Predicting: 155it [01:41,  1.48it/s]Extractor Predicting: 156it [01:42,  1.50it/s]Extractor Predicting: 157it [01:42,  1.49it/s]Extractor Predicting: 158it [01:43,  1.53it/s]Extractor Predicting: 159it [01:44,  1.54it/s]Extractor Predicting: 160it [01:44,  1.56it/s]Extractor Predicting: 161it [01:45,  1.58it/s]Extractor Predicting: 162it [01:45,  1.59it/s]Extractor Predicting: 163it [01:46,  1.56it/s]Extractor Predicting: 164it [01:47,  1.59it/s]Extractor Predicting: 165it [01:47,  1.57it/s]Extractor Predicting: 166it [01:48,  1.59it/s]Extractor Predicting: 167it [01:49,  1.57it/s]Extractor Predicting: 168it [01:49,  1.57it/s]Extractor Predicting: 169it [01:50,  1.58it/s]Extractor Predicting: 170it [01:51,  1.56it/s]Extractor Predicting: 171it [01:51,  1.55it/s]Extractor Predicting: 172it [01:52,  1.55it/s]Extractor Predicting: 173it [01:53,  1.53it/s]Extractor Predicting: 174it [01:53,  1.54it/s]Extractor Predicting: 175it [01:54,  1.54it/s]Extractor Predicting: 176it [01:54,  1.55it/s]Extractor Predicting: 177it [01:55,  1.53it/s]Extractor Predicting: 178it [01:56,  1.50it/s]Extractor Predicting: 179it [01:56,  1.53it/s]Extractor Predicting: 180it [01:57,  1.55it/s]Extractor Predicting: 181it [01:58,  1.58it/s]Extractor Predicting: 182it [01:58,  1.60it/s]Extractor Predicting: 183it [01:59,  1.59it/s]Extractor Predicting: 184it [01:59,  1.61it/s]Extractor Predicting: 185it [02:00,  1.61it/s]Extractor Predicting: 186it [02:01,  1.59it/s]Extractor Predicting: 187it [02:01,  1.61it/s]Extractor Predicting: 188it [02:02,  1.62it/s]Extractor Predicting: 189it [02:03,  1.57it/s]Extractor Predicting: 190it [02:03,  1.51it/s]Extractor Predicting: 191it [02:04,  1.52it/s]Extractor Predicting: 192it [02:05,  1.50it/s]Extractor Predicting: 193it [02:05,  1.51it/s]Extractor Predicting: 194it [02:06,  1.51it/s]Extractor Predicting: 195it [02:07,  1.48it/s]Extractor Predicting: 196it [02:07,  1.47it/s]Extractor Predicting: 197it [02:08,  1.48it/s]Extractor Predicting: 198it [02:09,  1.50it/s]Extractor Predicting: 199it [02:09,  1.49it/s]Extractor Predicting: 200it [02:10,  1.49it/s]Extractor Predicting: 201it [02:11,  1.48it/s]Extractor Predicting: 202it [02:11,  1.48it/s]Extractor Predicting: 203it [02:12,  1.49it/s]Extractor Predicting: 204it [02:13,  1.48it/s]Extractor Predicting: 205it [02:13,  1.47it/s]Extractor Predicting: 206it [02:14,  1.46it/s]Extractor Predicting: 207it [02:15,  1.43it/s]Extractor Predicting: 208it [02:16,  1.43it/s]Extractor Predicting: 209it [02:16,  1.49it/s]Extractor Predicting: 210it [02:17,  1.51it/s]Extractor Predicting: 211it [02:18,  1.52it/s]Extractor Predicting: 212it [02:18,  1.38it/s]Extractor Predicting: 213it [02:19,  1.40it/s]Extractor Predicting: 214it [02:20,  1.40it/s]Extractor Predicting: 215it [02:20,  1.42it/s]Extractor Predicting: 216it [02:21,  1.42it/s]Extractor Predicting: 217it [02:22,  1.40it/s]Extractor Predicting: 218it [02:23,  1.44it/s]Extractor Predicting: 219it [02:23,  1.46it/s]Extractor Predicting: 220it [02:24,  1.47it/s]Extractor Predicting: 221it [02:25,  1.49it/s]Extractor Predicting: 222it [02:25,  1.44it/s]Extractor Predicting: 223it [02:26,  1.43it/s]Extractor Predicting: 224it [02:27,  1.41it/s]Extractor Predicting: 225it [02:27,  1.40it/s]Extractor Predicting: 226it [02:28,  1.41it/s]Extractor Predicting: 227it [02:29,  1.39it/s]Extractor Predicting: 228it [02:30,  1.38it/s]Extractor Predicting: 229it [02:30,  1.38it/s]Extractor Predicting: 230it [02:31,  1.40it/s]Extractor Predicting: 231it [02:32,  1.40it/s]Extractor Predicting: 232it [02:32,  1.41it/s]Extractor Predicting: 233it [02:33,  1.40it/s]Extractor Predicting: 234it [02:34,  1.42it/s]Extractor Predicting: 235it [02:35,  1.42it/s]Extractor Predicting: 236it [02:35,  1.42it/s]Extractor Predicting: 237it [02:36,  1.39it/s]Extractor Predicting: 238it [02:37,  1.39it/s]Extractor Predicting: 239it [02:37,  1.42it/s]Extractor Predicting: 240it [02:38,  1.45it/s]Extractor Predicting: 241it [02:39,  1.45it/s]Extractor Predicting: 242it [02:39,  1.46it/s]Extractor Predicting: 243it [02:40,  1.47it/s]Extractor Predicting: 244it [02:41,  1.44it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.52it/s]Extractor Predicting: 246it [02:42,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:10,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:10,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:10,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:10,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:10,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:20:11,608 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:20:11,610 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:20:12,226 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:20:13,267 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:20:13,267 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:16,140 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:16,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:16,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:16,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:20:16,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:20:16,902 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:20:16,906 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:20:17,479 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:20:17,675 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:20:17,675 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.32905344879068377,
  "recall": 0.18677966101694915,
  "score": 0.2382960320034598,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.71it/s]Extractor Predicting: 15it [00:10,  1.50it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:20:28,354 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:20:28,355 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:20:28,360 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:20:28,361 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:20:28,367 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:20:33,011 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:20:33,018 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:20:33,074 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:20:33,075 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:20:33,084 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:33,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:33,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:33,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:33,096 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:33,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:20:33,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4444444444444444,
  "recall": 0.12359550561797752,
  "score": 0.1934065934065934,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:20:33,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:34,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:34,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:35,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:36,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:36,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:37,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:38,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:39,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:39,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:40,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:41,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:42,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:42,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:43,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:44,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:44,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:45,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:46,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:46,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:47,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:29, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-28 23:20:48,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:48,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:49,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:50,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:50,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:51,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:52,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:53,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:53,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:54,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:55,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:56,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:56,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:57,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:58,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:58,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:20:59,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:00,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:00,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:01,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:06, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-28 23:21:02,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:03,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:03,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:04,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:05,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:06,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:07,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:08,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:08,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:09,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:10,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:10,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:11,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:12,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:12,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:13,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:14,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:15,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:16,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:16,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:17,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<03:01, 15.11s/it][WARNING|generation_utils.py:914] 2023-08-28 23:21:18,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:18,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:19,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:20,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:21,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:21,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:22,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:23,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:23,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:24,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:25,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:25,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:26,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:27,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:28,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:28,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:29,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:30,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:30,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:31,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:58<02:41, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-28 23:21:32,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:32,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:33,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:34,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:34,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:35,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:35,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:36,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:37,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:37,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:38,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:39,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:39,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:40,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:41,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:41,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:42,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:43,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:43,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:44,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:21, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-28 23:21:45,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:46,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:46,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:47,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:48,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:49,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:49,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:50,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:51,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:52,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:52,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:53,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:54,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:55,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:55,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:56,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:57,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:57,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:58,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:59,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:08, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:00,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:00,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:01,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:02,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:02,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:03,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:04,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:04,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:05,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:06,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:06,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:07,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:08,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:08,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:09,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:10,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:11,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:11,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:12,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:13,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:40<01:52, 14.08s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:13,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:14,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:15,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:15,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:16,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:17,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:17,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:18,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:19,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:19,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:20,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:21,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:22,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:22,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:23,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:24,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:25,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:25,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:26,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:27,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:54<01:38, 14.08s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:27,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:28,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:29,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:30,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:30,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:31,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:32,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:33,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:33,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:34,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:35,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:36,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:36,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:37,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:38,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:38,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:39,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:40,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:40,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:41,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:42,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:43,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:10<01:28, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:43,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:44,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:45,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:45,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:46,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:47,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:48,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:48,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:49,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:50,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:51,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:51,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:52,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:53,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:54,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:54,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:55,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:56,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:57,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:58,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:58,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:26<01:15, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:59,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:00,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:00,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:01,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:02,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:03,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:03,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:04,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:05,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:05,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:06,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:07,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:07,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:08,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:09,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:10,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:10,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:11,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:12,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:12,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:13,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:14,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:42<01:01, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-28 23:23:15,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:15,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:16,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:17,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:18,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:18,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:19,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:20,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:21,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:21,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:22,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:23,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:24,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:24,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:25,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:26,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:26,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:27,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:28,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:28,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:56<00:44, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-28 23:23:29,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:30,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:30,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:31,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:32,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:33,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:33,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:34,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:35,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:35,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:36,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:37,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:37,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:38,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:39,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:40,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:40,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:41,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:42,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:42,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:10<00:29, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-28 23:23:43,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:44,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:45,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:45,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:46,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:46,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:47,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:48,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:48,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:49,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:50,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:50,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:51,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:52,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:52,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:53,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:54,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:55,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:55,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:56,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:56,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:24<00:14, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-28 23:23:57,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:58,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:58,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:59,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:00,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:01,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:01,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:02,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:03,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:03,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:04,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:05,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:06,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:07,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:07,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:08,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:09,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:10,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:10,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:11,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:12,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:12,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:13,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:40<00:00, 15.12s/it]Generating: 100%|██████████| 15/15 [03:40<00:00, 14.73s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:19,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:19,865 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:19,865 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:19,865 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:19,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:24:21,124 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:24:21,125 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:24:21,756 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:24:22,846 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:24:22,850 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:26,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:26,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:26,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:26,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:24:26,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:24:27,518 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:24:27,519 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:24:28,781 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:24:28,947 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:24:28,947 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9671875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9196428571428571, 'errors': {'', "('Wilhelm Dzogchen', 'notable work', '', 'In 1789 he studied under Pierre Jules van der Rohe , a French professor in mathematics and his translation into German by Wilhelm Dzogchen .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8764204545454546, 'errors': {''}}
['Relation : mother . Context : Later in life , he married his second wife , a young princess of the dynasty at the end of the fourth century BC , after her first death . Head Entity : princess of the dynasty , Tail Entity : mother .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : use .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : voice type .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 8196
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8296, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.49it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.60it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:10,  1.61it/s]Extractor Estimating: 17it [00:10,  1.62it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.63it/s]Extractor Estimating: 20it [00:12,  1.57it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:13,  1.59it/s]Extractor Estimating: 23it [00:14,  1.57it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:17,  1.59it/s]Extractor Estimating: 28it [00:17,  1.60it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:19,  1.60it/s]Extractor Estimating: 31it [00:19,  1.61it/s]Extractor Estimating: 32it [00:20,  1.61it/s]Extractor Estimating: 33it [00:20,  1.60it/s]Extractor Estimating: 34it [00:21,  1.59it/s]Extractor Estimating: 35it [00:22,  1.56it/s]Extractor Estimating: 36it [00:22,  1.55it/s]Extractor Estimating: 37it [00:23,  1.53it/s]Extractor Estimating: 38it [00:24,  1.54it/s]Extractor Estimating: 39it [00:24,  1.55it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:26,  1.56it/s]Extractor Estimating: 42it [00:26,  1.57it/s]Extractor Estimating: 43it [00:27,  1.48it/s]Extractor Estimating: 44it [00:28,  1.46it/s]Extractor Estimating: 45it [00:28,  1.48it/s]Extractor Estimating: 46it [00:29,  1.45it/s]Extractor Estimating: 47it [00:30,  1.52it/s]Extractor Estimating: 48it [00:30,  1.56it/s]Extractor Estimating: 49it [00:31,  1.57it/s]Extractor Estimating: 50it [00:31,  1.58it/s]Extractor Estimating: 51it [00:32,  1.57it/s]Extractor Estimating: 52it [00:33,  1.57it/s]Extractor Estimating: 53it [00:33,  1.61it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:35,  1.63it/s]Extractor Estimating: 56it [00:35,  1.60it/s]Extractor Estimating: 57it [00:36,  1.58it/s]Extractor Estimating: 58it [00:36,  1.62it/s]Extractor Estimating: 59it [00:37,  1.64it/s]Extractor Estimating: 60it [00:38,  1.66it/s]Extractor Estimating: 61it [00:38,  1.61it/s]Extractor Estimating: 62it [00:39,  1.54it/s]Extractor Estimating: 63it [00:40,  1.55it/s]Extractor Estimating: 64it [00:40,  1.59it/s]Extractor Estimating: 65it [00:41,  1.63it/s]Extractor Estimating: 66it [00:41,  1.63it/s]Extractor Estimating: 67it [00:42,  1.63it/s]Extractor Estimating: 68it [00:43,  1.59it/s]Extractor Estimating: 69it [00:43,  1.55it/s]Extractor Estimating: 70it [00:44,  1.59it/s]Extractor Estimating: 71it [00:45,  1.61it/s]Extractor Estimating: 72it [00:45,  1.63it/s]Extractor Estimating: 73it [00:46,  1.61it/s]Extractor Estimating: 74it [00:46,  1.62it/s]Extractor Estimating: 75it [00:47,  1.67it/s]Extractor Estimating: 76it [00:48,  1.63it/s]Extractor Estimating: 77it [00:48,  1.61it/s]Extractor Estimating: 78it [00:49,  1.61it/s]Extractor Estimating: 79it [00:49,  1.65it/s]Extractor Estimating: 80it [00:50,  1.64it/s]Extractor Estimating: 81it [00:51,  1.70it/s]Extractor Estimating: 82it [00:51,  1.67it/s]Extractor Estimating: 83it [00:52,  1.67it/s]Extractor Estimating: 84it [00:52,  1.71it/s]Extractor Estimating: 85it [00:53,  1.72it/s]Extractor Estimating: 86it [00:54,  1.70it/s]Extractor Estimating: 87it [00:54,  1.70it/s]Extractor Estimating: 88it [00:55,  1.64it/s]Extractor Estimating: 89it [00:55,  1.62it/s]Extractor Estimating: 90it [00:56,  1.62it/s]Extractor Estimating: 91it [00:57,  1.61it/s]Extractor Estimating: 92it [00:57,  1.64it/s]Extractor Estimating: 93it [00:58,  1.58it/s]Extractor Estimating: 94it [00:59,  1.60it/s]Extractor Estimating: 95it [00:59,  1.65it/s]Extractor Estimating: 96it [01:00,  1.63it/s]Extractor Estimating: 97it [01:00,  1.59it/s]Extractor Estimating: 98it [01:01,  1.63it/s]Extractor Estimating: 99it [01:02,  1.62it/s]Extractor Estimating: 100it [01:02,  1.66it/s]Extractor Estimating: 101it [01:03,  1.73it/s]Extractor Estimating: 102it [01:03,  1.75it/s]Extractor Estimating: 103it [01:04,  1.75it/s]Extractor Estimating: 104it [01:04,  1.76it/s]Extractor Estimating: 105it [01:05,  1.65it/s]Extractor Estimating: 106it [01:06,  1.68it/s]Extractor Estimating: 107it [01:06,  1.70it/s]Extractor Estimating: 108it [01:07,  1.70it/s]Extractor Estimating: 109it [01:07,  1.78it/s]Extractor Estimating: 110it [01:08,  1.79it/s]Extractor Estimating: 111it [01:08,  1.80it/s]Extractor Estimating: 112it [01:09,  1.83it/s]Extractor Estimating: 113it [01:09,  1.89it/s]Extractor Estimating: 114it [01:10,  1.85it/s]Extractor Estimating: 115it [01:11,  1.87it/s]Extractor Estimating: 116it [01:11,  1.85it/s]Extractor Estimating: 117it [01:12,  1.90it/s]Extractor Estimating: 118it [01:12,  1.88it/s]Extractor Estimating: 119it [01:13,  1.81it/s]Extractor Estimating: 120it [01:13,  1.84it/s]Extractor Estimating: 121it [01:14,  1.88it/s]Extractor Estimating: 122it [01:14,  1.87it/s]Extractor Estimating: 123it [01:15,  1.81it/s]Extractor Estimating: 124it [01:15,  1.86it/s]Extractor Estimating: 125it [01:16,  1.76it/s]Extractor Estimating: 126it [01:17,  1.75it/s]Extractor Estimating: 127it [01:17,  1.64it/s]Extractor Estimating: 128it [01:18,  1.64it/s]Extractor Estimating: 129it [01:18,  1.70it/s]Extractor Estimating: 130it [01:19,  1.65it/s]Extractor Estimating: 131it [01:20,  1.59it/s]Extractor Estimating: 132it [01:20,  1.62it/s]Extractor Estimating: 133it [01:21,  1.65it/s]Extractor Estimating: 134it [01:22,  1.62it/s]Extractor Estimating: 135it [01:22,  1.63it/s]Extractor Estimating: 136it [01:23,  1.63it/s]Extractor Estimating: 137it [01:23,  1.62it/s]Extractor Estimating: 138it [01:24,  1.61it/s]Extractor Estimating: 139it [01:25,  1.64it/s]Extractor Estimating: 140it [01:25,  1.65it/s]Extractor Estimating: 141it [01:26,  1.62it/s]Extractor Estimating: 142it [01:27,  1.58it/s]Extractor Estimating: 143it [01:27,  1.59it/s]Extractor Estimating: 144it [01:28,  1.63it/s]Extractor Estimating: 145it [01:28,  1.66it/s]Extractor Estimating: 146it [01:29,  1.61it/s]Extractor Estimating: 147it [01:30,  1.64it/s]Extractor Estimating: 148it [01:30,  1.66it/s]Extractor Estimating: 149it [01:31,  1.66it/s]Extractor Estimating: 150it [01:31,  1.62it/s]Extractor Estimating: 151it [01:32,  1.62it/s]Extractor Estimating: 152it [01:33,  1.65it/s]Extractor Estimating: 153it [01:33,  1.60it/s]Extractor Estimating: 154it [01:34,  1.58it/s]Extractor Estimating: 155it [01:35,  1.62it/s]Extractor Estimating: 156it [01:35,  1.61it/s]Extractor Estimating: 157it [01:36,  1.59it/s]Extractor Estimating: 158it [01:36,  1.66it/s]Extractor Estimating: 159it [01:37,  1.66it/s]Extractor Estimating: 160it [01:38,  1.59it/s]Extractor Estimating: 161it [01:38,  1.58it/s]Extractor Estimating: 162it [01:39,  1.53it/s]Extractor Estimating: 163it [01:40,  1.59it/s]Extractor Estimating: 164it [01:40,  1.59it/s]Extractor Estimating: 165it [01:41,  1.51it/s]Extractor Estimating: 166it [01:42,  1.54it/s]Extractor Estimating: 167it [01:42,  1.53it/s]Extractor Estimating: 168it [01:43,  1.55it/s]Extractor Estimating: 169it [01:44,  1.46it/s]Extractor Estimating: 170it [01:44,  1.47it/s]Extractor Estimating: 171it [01:45,  1.53it/s]Extractor Estimating: 172it [01:46,  1.53it/s]Extractor Estimating: 173it [01:46,  1.53it/s]Extractor Estimating: 174it [01:47,  1.52it/s]Extractor Estimating: 175it [01:47,  1.59it/s]Extractor Estimating: 176it [01:48,  1.42it/s]Extractor Estimating: 177it [01:49,  1.41it/s]Extractor Estimating: 178it [01:50,  1.38it/s]Extractor Estimating: 179it [01:50,  1.41it/s]Extractor Estimating: 180it [01:51,  1.40it/s]Extractor Estimating: 181it [01:52,  1.44it/s]Extractor Estimating: 182it [01:52,  1.45it/s]Extractor Estimating: 183it [01:53,  1.51it/s]Extractor Estimating: 184it [01:54,  1.52it/s]Extractor Estimating: 185it [01:54,  1.55it/s]Extractor Estimating: 186it [01:55,  1.52it/s]Extractor Estimating: 187it [01:56,  1.49it/s]Extractor Estimating: 188it [01:56,  1.49it/s]Extractor Estimating: 189it [01:57,  1.50it/s]Extractor Estimating: 190it [01:58,  1.47it/s]Extractor Estimating: 191it [01:58,  1.51it/s]Extractor Estimating: 192it [01:59,  1.47it/s]Extractor Estimating: 193it [02:00,  1.43it/s]Extractor Estimating: 194it [02:01,  1.44it/s]Extractor Estimating: 195it [02:01,  1.44it/s]Extractor Estimating: 196it [02:02,  1.48it/s]Extractor Estimating: 197it [02:02,  1.54it/s]Extractor Estimating: 198it [02:03,  1.51it/s]Extractor Estimating: 199it [02:04,  1.52it/s]Extractor Estimating: 200it [02:04,  1.52it/s]Extractor Estimating: 201it [02:05,  1.54it/s]Extractor Estimating: 202it [02:06,  1.60it/s]Extractor Estimating: 203it [02:06,  1.56it/s]Extractor Estimating: 204it [02:07,  1.49it/s]Extractor Estimating: 205it [02:08,  1.51it/s]Extractor Estimating: 206it [02:08,  1.56it/s]Extractor Estimating: 207it [02:09,  1.54it/s]Extractor Estimating: 208it [02:10,  1.51it/s]Extractor Estimating: 209it [02:10,  1.52it/s]Extractor Estimating: 210it [02:11,  1.58it/s]Extractor Estimating: 211it [02:12,  1.59it/s]Extractor Estimating: 212it [02:12,  1.53it/s]Extractor Estimating: 213it [02:13,  1.58it/s]Extractor Estimating: 214it [02:13,  1.62it/s]Extractor Estimating: 215it [02:14,  1.59it/s]Extractor Estimating: 216it [02:15,  1.58it/s]Extractor Estimating: 217it [02:15,  1.62it/s]Extractor Estimating: 218it [02:16,  1.66it/s]Extractor Estimating: 219it [02:16,  1.63it/s]Extractor Estimating: 220it [02:17,  1.66it/s]Extractor Estimating: 221it [02:18,  1.68it/s]Extractor Estimating: 222it [02:18,  1.73it/s]Extractor Estimating: 223it [02:19,  1.73it/s]Extractor Estimating: 224it [02:19,  1.69it/s]Extractor Estimating: 225it [02:20,  1.69it/s]Extractor Estimating: 226it [02:21,  1.71it/s]Extractor Estimating: 227it [02:21,  1.60it/s]Extractor Estimating: 228it [02:22,  1.61it/s]Extractor Estimating: 229it [02:22,  1.63it/s]Extractor Estimating: 230it [02:23,  1.56it/s]Extractor Estimating: 231it [02:24,  1.58it/s]Extractor Estimating: 232it [02:24,  1.60it/s]Extractor Estimating: 233it [02:25,  1.57it/s]Extractor Estimating: 234it [02:26,  1.61it/s]Extractor Estimating: 235it [02:26,  1.57it/s]Extractor Estimating: 236it [02:27,  1.59it/s]Extractor Estimating: 237it [02:28,  1.58it/s]Extractor Estimating: 238it [02:28,  1.54it/s]Extractor Estimating: 239it [02:29,  1.52it/s]Extractor Estimating: 240it [02:30,  1.55it/s]Extractor Estimating: 241it [02:30,  1.53it/s]Extractor Estimating: 242it [02:31,  1.50it/s]Extractor Estimating: 243it [02:32,  1.41it/s]Extractor Estimating: 244it [02:32,  1.43it/s]Extractor Estimating: 245it [02:33,  1.46it/s]Extractor Estimating: 246it [02:34,  1.49it/s]Extractor Estimating: 247it [02:34,  1.48it/s]Extractor Estimating: 248it [02:35,  1.50it/s]Extractor Estimating: 249it [02:36,  1.50it/s]Extractor Estimating: 250it [02:36,  1.50it/s]Extractor Estimating: 251it [02:37,  1.54it/s]Extractor Estimating: 252it [02:38,  1.56it/s]Extractor Estimating: 253it [02:38,  1.56it/s]Extractor Estimating: 254it [02:39,  1.57it/s]Extractor Estimating: 255it [02:39,  1.61it/s]Extractor Estimating: 256it [02:40,  1.61it/s]Extractor Estimating: 257it [02:41,  1.65it/s]Extractor Estimating: 258it [02:41,  1.61it/s]Extractor Estimating: 259it [02:42,  1.57it/s]Extractor Estimating: 260it [02:43,  1.51it/s]Extractor Estimating: 261it [02:43,  1.58it/s]Extractor Estimating: 262it [02:44,  1.59it/s]Extractor Estimating: 263it [02:44,  1.61it/s]Extractor Estimating: 264it [02:45,  1.58it/s]Extractor Estimating: 265it [02:46,  1.62it/s]Extractor Estimating: 266it [02:46,  1.56it/s]Extractor Estimating: 267it [02:47,  1.62it/s]Extractor Estimating: 268it [02:48,  1.56it/s]Extractor Estimating: 269it [02:48,  1.58it/s]Extractor Estimating: 270it [02:49,  1.60it/s]Extractor Estimating: 271it [02:50,  1.59it/s]Extractor Estimating: 272it [02:50,  1.60it/s]Extractor Estimating: 273it [02:51,  1.52it/s]Extractor Estimating: 274it [02:51,  1.58it/s]Extractor Estimating: 275it [02:52,  1.58it/s]Extractor Estimating: 276it [02:53,  1.67it/s]Extractor Estimating: 277it [02:53,  1.61it/s]Extractor Estimating: 278it [02:54,  1.63it/s]Extractor Estimating: 279it [02:54,  1.63it/s]Extractor Estimating: 280it [02:55,  1.63it/s]Extractor Estimating: 281it [02:56,  1.64it/s]Extractor Estimating: 282it [02:56,  1.66it/s]Extractor Estimating: 283it [02:57,  1.66it/s]Extractor Estimating: 284it [02:57,  1.63it/s]Extractor Estimating: 285it [02:58,  1.64it/s]Extractor Estimating: 286it [02:59,  1.64it/s]Extractor Estimating: 287it [02:59,  1.56it/s]Extractor Estimating: 288it [03:00,  1.55it/s]Extractor Estimating: 289it [03:01,  1.60it/s]Extractor Estimating: 290it [03:01,  1.59it/s]Extractor Estimating: 291it [03:02,  1.63it/s]Extractor Estimating: 292it [03:02,  1.62it/s]Extractor Estimating: 293it [03:03,  1.66it/s]Extractor Estimating: 294it [03:04,  1.70it/s]Extractor Estimating: 295it [03:04,  1.67it/s]Extractor Estimating: 296it [03:05,  1.64it/s]Extractor Estimating: 297it [03:06,  1.62it/s]Extractor Estimating: 298it [03:06,  1.61it/s]Extractor Estimating: 299it [03:07,  1.65it/s]Extractor Estimating: 300it [03:07,  1.69it/s]Extractor Estimating: 301it [03:08,  1.67it/s]Extractor Estimating: 302it [03:08,  1.68it/s]Extractor Estimating: 303it [03:09,  1.70it/s]Extractor Estimating: 304it [03:10,  1.68it/s]Extractor Estimating: 305it [03:10,  1.61it/s]Extractor Estimating: 306it [03:11,  1.59it/s]Extractor Estimating: 307it [03:12,  1.58it/s]Extractor Estimating: 308it [03:12,  1.63it/s]Extractor Estimating: 309it [03:13,  1.65it/s]Extractor Estimating: 310it [03:13,  1.65it/s]Extractor Estimating: 311it [03:14,  1.66it/s]Extractor Estimating: 312it [03:15,  1.58it/s]Extractor Estimating: 313it [03:15,  1.61it/s]Extractor Estimating: 314it [03:16,  1.61it/s]Extractor Estimating: 315it [03:17,  1.59it/s]Extractor Estimating: 316it [03:17,  1.62it/s]Extractor Estimating: 317it [03:18,  1.62it/s]Extractor Estimating: 318it [03:18,  1.60it/s]Extractor Estimating: 319it [03:19,  1.60it/s]Extractor Estimating: 320it [03:20,  1.64it/s]Extractor Estimating: 321it [03:20,  1.66it/s]Extractor Estimating: 322it [03:21,  1.52it/s]Extractor Estimating: 323it [03:22,  1.58it/s]Extractor Estimating: 324it [03:22,  1.58it/s]Extractor Estimating: 325it [03:23,  1.62it/s]Extractor Estimating: 326it [03:23,  1.64it/s]Extractor Estimating: 327it [03:24,  1.61it/s]Extractor Estimating: 328it [03:25,  1.63it/s]Extractor Estimating: 329it [03:25,  1.65it/s]Extractor Estimating: 330it [03:26,  1.67it/s]Extractor Estimating: 331it [03:26,  1.67it/s]Extractor Estimating: 332it [03:27,  1.67it/s]Extractor Estimating: 333it [03:28,  1.70it/s]Extractor Estimating: 334it [03:28,  1.68it/s]Extractor Estimating: 335it [03:29,  1.69it/s]Extractor Estimating: 336it [03:29,  1.69it/s]Extractor Estimating: 337it [03:30,  1.70it/s]Extractor Estimating: 338it [03:30,  1.70it/s]Extractor Estimating: 339it [03:31,  1.69it/s]Extractor Estimating: 340it [03:32,  1.71it/s]Extractor Estimating: 341it [03:32,  1.65it/s]Extractor Estimating: 342it [03:33,  1.65it/s]Extractor Estimating: 343it [03:34,  1.61it/s]Extractor Estimating: 344it [03:34,  1.64it/s]Extractor Estimating: 345it [03:35,  1.72it/s]Extractor Estimating: 346it [03:35,  1.68it/s]Extractor Estimating: 347it [03:36,  1.70it/s]Extractor Estimating: 348it [03:36,  1.67it/s]Extractor Estimating: 349it [03:37,  1.69it/s]Extractor Estimating: 350it [03:38,  1.68it/s]Extractor Estimating: 351it [03:38,  1.66it/s]Extractor Estimating: 352it [03:39,  1.62it/s]Extractor Estimating: 353it [03:40,  1.58it/s]Extractor Estimating: 354it [03:40,  1.56it/s]Extractor Estimating: 355it [03:41,  1.60it/s]Extractor Estimating: 356it [03:42,  1.59it/s]Extractor Estimating: 357it [03:42,  1.61it/s]Extractor Estimating: 358it [03:43,  1.62it/s]Extractor Estimating: 359it [03:43,  1.67it/s]Extractor Estimating: 360it [03:44,  1.65it/s]Extractor Estimating: 361it [03:44,  1.66it/s]Extractor Estimating: 362it [03:45,  1.62it/s]Extractor Estimating: 363it [03:46,  1.68it/s]Extractor Estimating: 364it [03:46,  1.69it/s]Extractor Estimating: 365it [03:47,  1.68it/s]Extractor Estimating: 366it [03:47,  1.67it/s]Extractor Estimating: 367it [03:48,  1.58it/s]Extractor Estimating: 368it [03:49,  1.59it/s]Extractor Estimating: 369it [03:49,  1.57it/s]Extractor Estimating: 370it [03:50,  1.58it/s]Extractor Estimating: 371it [03:51,  1.58it/s]Extractor Estimating: 372it [03:51,  1.58it/s]Extractor Estimating: 373it [03:52,  1.59it/s]Extractor Estimating: 374it [03:53,  1.61it/s]Extractor Estimating: 375it [03:53,  1.72it/s]Extractor Estimating: 375it [03:53,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:39,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:39,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:39,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:39,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:39,808 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:28:40,098 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:28:40,099 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:28:40,390 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:28:41,448 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:28:41,448 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:42,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:42,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:42,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:42,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:28:42,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:28:43,108 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:28:43,116 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:28:43,374 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:28:43,527 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:28:43,527 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:36:05,777 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:36:05,801 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5996 mean pseudo reward: 0.9598701971275941
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 18291
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18391, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18391, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.098, loss:514.2909
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.100, loss:503.2403
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.100, loss:500.4511
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 1.084, loss:478.5164
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 1.113, loss:476.7850
>> valid entity prec:0.4956, rec:0.4863, f1:0.4909
>> valid relation prec:0.0365, rec:0.0210, f1:0.0266
>> valid relation with NER prec:0.0365, rec:0.0210, f1:0.0266
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.113, loss:425.7823
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.100, loss:459.5891
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.081, loss:437.8709
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.110, loss:439.6185
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.108, loss:462.2091
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4717, rec:0.5587, f1:0.5115
>> valid relation prec:0.0313, rec:0.0174, f1:0.0224
>> valid relation with NER prec:0.0313, rec:0.0174, f1:0.0224
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 1.093, loss:405.1458
g_step 1200, step 200, avg_time 1.110, loss:452.4685
g_step 1300, step 50, avg_time 1.111, loss:413.1644
g_step 1400, step 150, avg_time 1.097, loss:404.1242
g_step 1500, step 250, avg_time 1.096, loss:409.9917
>> valid entity prec:0.5218, rec:0.4676, f1:0.4932
>> valid relation prec:0.0357, rec:0.0186, f1:0.0244
>> valid relation with NER prec:0.0357, rec:0.0186, f1:0.0244
g_step 1600, step 100, avg_time 1.093, loss:378.5082
g_step 1700, step 200, avg_time 1.091, loss:398.4810
g_step 1800, step 50, avg_time 1.092, loss:361.8865
g_step 1900, step 150, avg_time 1.094, loss:364.4926
g_step 2000, step 250, avg_time 1.108, loss:383.8241
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5170, rec:0.4783, f1:0.4969
>> valid relation prec:0.0411, rec:0.0197, f1:0.0266
>> valid relation with NER prec:0.0411, rec:0.0197, f1:0.0266
g_step 2100, step 100, avg_time 1.084, loss:353.2953
g_step 2200, step 200, avg_time 1.095, loss:352.9781
g_step 2300, step 50, avg_time 1.114, loss:323.2582
g_step 2400, step 150, avg_time 1.091, loss:332.2280
g_step 2500, step 250, avg_time 1.099, loss:348.1308
>> valid entity prec:0.4939, rec:0.4472, f1:0.4694
>> valid relation prec:0.0256, rec:0.0138, f1:0.0179
>> valid relation with NER prec:0.0256, rec:0.0138, f1:0.0179
g_step 2600, step 100, avg_time 1.108, loss:298.8069
g_step 2700, step 200, avg_time 1.097, loss:329.4684
g_step 2800, step 50, avg_time 1.079, loss:311.1653
g_step 2900, step 150, avg_time 1.098, loss:317.1607
g_step 3000, step 250, avg_time 1.090, loss:309.9184
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4862, rec:0.5054, f1:0.4956
>> valid relation prec:0.0319, rec:0.0192, f1:0.0240
>> valid relation with NER prec:0.0319, rec:0.0192, f1:0.0240
g_step 3100, step 100, avg_time 1.095, loss:284.9006
g_step 3200, step 200, avg_time 1.093, loss:300.8408
g_step 3300, step 50, avg_time 1.093, loss:278.7560
g_step 3400, step 150, avg_time 1.099, loss:283.0019
g_step 3500, step 250, avg_time 1.108, loss:286.1757
>> valid entity prec:0.5053, rec:0.4699, f1:0.4869
>> valid relation prec:0.0349, rec:0.0186, f1:0.0242
>> valid relation with NER prec:0.0349, rec:0.0186, f1:0.0242
g_step 3600, step 100, avg_time 1.102, loss:261.0102
g_step 3700, step 200, avg_time 1.090, loss:292.5216
g_step 3800, step 50, avg_time 1.110, loss:282.3139
g_step 3900, step 150, avg_time 1.085, loss:265.8204
g_step 4000, step 250, avg_time 1.097, loss:275.6122
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5143, rec:0.4558, f1:0.4833
>> valid relation prec:0.0165, rec:0.0088, f1:0.0115
>> valid relation with NER prec:0.0165, rec:0.0088, f1:0.0115
g_step 4100, step 100, avg_time 1.078, loss:248.0739
g_step 4200, step 200, avg_time 1.111, loss:267.6225
g_step 4300, step 50, avg_time 1.101, loss:247.8848
g_step 4400, step 150, avg_time 1.089, loss:243.1512
g_step 4500, step 250, avg_time 1.098, loss:259.2717
>> valid entity prec:0.5002, rec:0.4771, f1:0.4884
>> valid relation prec:0.0343, rec:0.0194, f1:0.0248
>> valid relation with NER prec:0.0343, rec:0.0194, f1:0.0248
g_step 4600, step 100, avg_time 1.081, loss:229.5444
g_step 4700, step 200, avg_time 1.100, loss:249.6365
g_step 4800, step 50, avg_time 1.094, loss:235.4394
g_step 4900, step 150, avg_time 1.088, loss:224.2577
g_step 5000, step 250, avg_time 1.101, loss:235.6731
learning rate was adjusted to 0.0008
>> valid entity prec:0.5166, rec:0.4582, f1:0.4857
>> valid relation prec:0.0234, rec:0.0123, f1:0.0162
>> valid relation with NER prec:0.0234, rec:0.0123, f1:0.0162
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:36:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:36:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-36-05_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:36:06 - WARNING - datasets.builder -   Using custom data configuration default-ffc344079e7e05b1
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ffc344079e7e05b1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:36:07,176 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:36:07,178 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:36:07,178 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:36:07,179 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:36:07,187 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:36:07,192 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:36:07,192 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:36:07,192 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:36:07,192 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:36:07,192 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:36:07,192 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:36:07,378 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:36:10,485 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:36:10,492 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ffc344079e7e05b1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.09ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.90ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.32ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.53ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.64ba/s]100%|██████████| 6/6 [00:01<00:00,  3.88ba/s]100%|██████████| 6/6 [00:01<00:00,  4.04ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.03ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.29ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.38ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.43ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.44ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.46ba/s]100%|██████████| 7/7 [00:01<00:00,  4.92ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.33ba/s] 50%|█████     | 3/6 [00:00<00:00,  9.99ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.29ba/s]100%|██████████| 6/6 [00:00<00:00, 10.27ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.53ba/s] 43%|████▎     | 3/7 [00:00<00:00, 10.10ba/s] 71%|███████▏  | 5/7 [00:00<00:00, 10.38ba/s]100%|██████████| 7/7 [00:00<00:00, 12.29ba/s]100%|██████████| 7/7 [00:00<00:00, 11.43ba/s]
[INFO|trainer.py:414] 2023-08-29 01:36:15,045 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:36:15,065 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:36:15,065 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-29 01:36:15,065 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:36:15,065 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:36:15,065 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:36:15,065 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:36:15,065 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:22,  3.29it/s]  0%|          | 2/470 [00:00<02:17,  3.41it/s]  1%|          | 3/470 [00:00<02:15,  3.45it/s]  1%|          | 4/470 [00:01<02:14,  3.47it/s]  1%|          | 5/470 [00:01<02:13,  3.48it/s]  1%|▏         | 6/470 [00:01<02:13,  3.49it/s]  1%|▏         | 7/470 [00:02<02:12,  3.49it/s]  2%|▏         | 8/470 [00:02<02:12,  3.49it/s]  2%|▏         | 9/470 [00:02<02:11,  3.50it/s]  2%|▏         | 10/470 [00:02<02:12,  3.48it/s]  2%|▏         | 11/470 [00:03<02:11,  3.49it/s]  3%|▎         | 12/470 [00:03<02:11,  3.49it/s]  3%|▎         | 13/470 [00:03<02:10,  3.50it/s]  3%|▎         | 14/470 [00:04<02:10,  3.50it/s]  3%|▎         | 15/470 [00:04<02:10,  3.50it/s]  3%|▎         | 16/470 [00:04<02:09,  3.50it/s]  4%|▎         | 17/470 [00:04<02:09,  3.50it/s]  4%|▍         | 18/470 [00:05<02:09,  3.50it/s]  4%|▍         | 19/470 [00:05<02:08,  3.50it/s]  4%|▍         | 20/470 [00:05<02:08,  3.50it/s]  4%|▍         | 21/470 [00:06<02:08,  3.50it/s]  5%|▍         | 22/470 [00:06<02:08,  3.50it/s]  5%|▍         | 23/470 [00:06<02:07,  3.50it/s]  5%|▌         | 24/470 [00:06<02:07,  3.50it/s]  5%|▌         | 25/470 [00:07<02:07,  3.50it/s]  6%|▌         | 26/470 [00:07<02:06,  3.50it/s]  6%|▌         | 27/470 [00:07<02:06,  3.50it/s]  6%|▌         | 28/470 [00:08<02:06,  3.49it/s]  6%|▌         | 29/470 [00:08<02:06,  3.49it/s]  6%|▋         | 30/470 [00:08<02:06,  3.49it/s]  7%|▋         | 31/470 [00:08<02:05,  3.49it/s]  7%|▋         | 32/470 [00:09<02:05,  3.49it/s]  7%|▋         | 33/470 [00:09<02:05,  3.49it/s]  7%|▋         | 34/470 [00:09<02:04,  3.49it/s]  7%|▋         | 35/470 [00:10<02:04,  3.49it/s]  8%|▊         | 36/470 [00:10<02:04,  3.49it/s]  8%|▊         | 37/470 [00:10<02:04,  3.49it/s]  8%|▊         | 38/470 [00:10<02:03,  3.49it/s]  8%|▊         | 39/470 [00:11<02:03,  3.49it/s]  9%|▊         | 40/470 [00:11<02:03,  3.49it/s]  9%|▊         | 41/470 [00:11<02:02,  3.49it/s]  9%|▉         | 42/470 [00:12<02:02,  3.49it/s]  9%|▉         | 43/470 [00:12<02:02,  3.49it/s]  9%|▉         | 44/470 [00:12<02:02,  3.49it/s] 10%|▉         | 45/470 [00:12<02:01,  3.49it/s] 10%|▉         | 46/470 [00:13<02:01,  3.48it/s] 10%|█         | 47/470 [00:13<02:01,  3.48it/s] 10%|█         | 48/470 [00:13<02:01,  3.48it/s] 10%|█         | 49/470 [00:14<02:00,  3.48it/s] 11%|█         | 50/470 [00:14<02:00,  3.48it/s] 11%|█         | 51/470 [00:14<02:00,  3.48it/s] 11%|█         | 52/470 [00:14<01:59,  3.49it/s] 11%|█▏        | 53/470 [00:15<01:59,  3.49it/s] 11%|█▏        | 54/470 [00:15<01:59,  3.49it/s] 12%|█▏        | 55/470 [00:15<01:59,  3.48it/s] 12%|█▏        | 56/470 [00:16<01:58,  3.49it/s] 12%|█▏        | 57/470 [00:16<01:58,  3.49it/s] 12%|█▏        | 58/470 [00:16<01:58,  3.49it/s] 13%|█▎        | 59/470 [00:16<01:57,  3.49it/s] 13%|█▎        | 60/470 [00:17<01:57,  3.49it/s] 13%|█▎        | 61/470 [00:17<01:57,  3.49it/s] 13%|█▎        | 62/470 [00:17<01:57,  3.48it/s] 13%|█▎        | 63/470 [00:18<01:56,  3.48it/s] 14%|█▎        | 64/470 [00:18<01:56,  3.47it/s] 14%|█▍        | 65/470 [00:18<01:56,  3.47it/s] 14%|█▍        | 66/470 [00:18<01:56,  3.48it/s] 14%|█▍        | 67/470 [00:19<01:55,  3.48it/s] 14%|█▍        | 68/470 [00:19<01:55,  3.48it/s] 15%|█▍        | 69/470 [00:19<01:55,  3.48it/s] 15%|█▍        | 70/470 [00:20<01:54,  3.48it/s] 15%|█▌        | 71/470 [00:20<01:54,  3.48it/s] 15%|█▌        | 72/470 [00:20<01:54,  3.48it/s] 16%|█▌        | 73/470 [00:20<01:54,  3.48it/s] 16%|█▌        | 74/470 [00:21<01:53,  3.48it/s] 16%|█▌        | 75/470 [00:21<01:53,  3.48it/s] 16%|█▌        | 76/470 [00:21<01:53,  3.48it/s] 16%|█▋        | 77/470 [00:22<01:52,  3.48it/s] 17%|█▋        | 78/470 [00:22<01:52,  3.48it/s] 17%|█▋        | 79/470 [00:22<01:52,  3.48it/s] 17%|█▋        | 80/470 [00:22<01:51,  3.48it/s] 17%|█▋        | 81/470 [00:23<01:52,  3.47it/s] 17%|█▋        | 82/470 [00:23<01:51,  3.48it/s] 18%|█▊        | 83/470 [00:23<01:51,  3.48it/s] 18%|█▊        | 84/470 [00:24<01:50,  3.48it/s] 18%|█▊        | 85/470 [00:24<01:50,  3.48it/s] 18%|█▊        | 86/470 [00:24<01:50,  3.48it/s] 19%|█▊        | 87/470 [00:24<01:49,  3.48it/s] 19%|█▊        | 88/470 [00:25<01:49,  3.49it/s] 19%|█▉        | 89/470 [00:25<01:49,  3.49it/s] 19%|█▉        | 90/470 [00:25<01:49,  3.48it/s] 19%|█▉        | 91/470 [00:26<01:48,  3.48it/s] 20%|█▉        | 92/470 [00:26<01:48,  3.48it/s] 20%|█▉        | 93/470 [00:26<01:48,  3.48it/s] 20%|██        | 94/470 [00:26<01:41,  3.72it/s][INFO|trainer.py:2140] 2023-08-29 01:36:41,980 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:36:41,980 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 01:36:41,980 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.68it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.18it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.42it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.77it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.32it/s][A
  4%|▍         | 33/782 [00:00<00:15, 48.09it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.68it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.26it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.19it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.19it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.23it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.29it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.33it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.35it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.29it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.34it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.22it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.08it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.20it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.12it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.09it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.19it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.28it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.21it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.25it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.09it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.02it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.10it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.09it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.10it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.06it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.18it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.32it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.27it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.20it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.21it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.12it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.13it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.17it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.15it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.22it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.22it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.18it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.20it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.17it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.26it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.30it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.16it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.13it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.16it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.20it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.27it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.32it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.24it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.14it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.20it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.19it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.21it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.25it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.28it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.07it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.19it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.25it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.21it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.20it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.25it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.20it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.12it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.24it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.19it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.19it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.14it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.20it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.14it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.22it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.24it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.21it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.17it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.28it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.23it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.19it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.08it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.19it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.17it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.22it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.26it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.12it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.19it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.22it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.31it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.16it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.05it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.10it/s][A
 60%|██████    | 473/782 [00:09<00:06, 47.14it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.13it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.29it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.28it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.12it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.18it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.17it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.10it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.21it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.22it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.11it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.12it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.20it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.15it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.10it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.15it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.05it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.10it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.18it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.17it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.12it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.20it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.10it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.05it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.03it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.10it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.13it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.12it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.14it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.07it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.15it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.13it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.15it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.18it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.13it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.06it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.01it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.00it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.03it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.05it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.07it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.13it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.14it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.01it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.09it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.11it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.03it/s][A
 91%|█████████ | 708/782 [00:14<00:01, 47.23it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.09it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.05it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.03it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.07it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.11it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.13it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.12it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.15it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.05it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.07it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.04it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.04it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.11it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.06it/s][A
                                                 [A                                                
100%|██████████| 782/782 [00:16<00:00, 47.06it/s][A 20%|██        | 94/470 [00:43<01:41,  3.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:36:58,574 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 01:36:58,607 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:37:00,873 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:37:00,913 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:37:00,927 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:50<46:16,  7.41s/it] 20%|██        | 96/470 [00:51<32:51,  5.27s/it] 21%|██        | 97/470 [00:51<23:28,  3.78s/it] 21%|██        | 98/470 [00:51<16:55,  2.73s/it] 21%|██        | 99/470 [00:52<12:20,  2.00s/it] 21%|██▏       | 100/470 [00:52<09:09,  1.48s/it] 21%|██▏       | 101/470 [00:52<06:55,  1.12s/it] 22%|██▏       | 102/470 [00:52<05:21,  1.14it/s] 22%|██▏       | 103/470 [00:53<04:16,  1.43it/s] 22%|██▏       | 104/470 [00:53<03:30,  1.74it/s] 22%|██▏       | 105/470 [00:53<02:58,  2.05it/s] 23%|██▎       | 106/470 [00:54<02:35,  2.34it/s] 23%|██▎       | 107/470 [00:54<02:20,  2.59it/s] 23%|██▎       | 108/470 [00:54<02:09,  2.80it/s] 23%|██▎       | 109/470 [00:54<02:01,  2.98it/s] 23%|██▎       | 110/470 [00:55<01:55,  3.11it/s] 24%|██▎       | 111/470 [00:55<01:51,  3.21it/s] 24%|██▍       | 112/470 [00:55<01:48,  3.29it/s] 24%|██▍       | 113/470 [00:56<01:46,  3.34it/s] 24%|██▍       | 114/470 [00:56<01:45,  3.38it/s] 24%|██▍       | 115/470 [00:56<01:44,  3.41it/s] 25%|██▍       | 116/470 [00:57<01:43,  3.43it/s] 25%|██▍       | 117/470 [00:57<01:42,  3.44it/s] 25%|██▌       | 118/470 [00:57<01:42,  3.44it/s] 25%|██▌       | 119/470 [00:57<01:41,  3.46it/s] 26%|██▌       | 120/470 [00:58<01:41,  3.46it/s] 26%|██▌       | 121/470 [00:58<01:40,  3.46it/s] 26%|██▌       | 122/470 [00:58<01:40,  3.47it/s] 26%|██▌       | 123/470 [00:59<01:39,  3.47it/s] 26%|██▋       | 124/470 [00:59<01:39,  3.48it/s] 27%|██▋       | 125/470 [00:59<01:39,  3.48it/s] 27%|██▋       | 126/470 [00:59<01:38,  3.48it/s] 27%|██▋       | 127/470 [01:00<01:38,  3.48it/s] 27%|██▋       | 128/470 [01:00<01:38,  3.48it/s] 27%|██▋       | 129/470 [01:00<01:38,  3.47it/s] 28%|██▊       | 130/470 [01:01<01:37,  3.47it/s] 28%|██▊       | 131/470 [01:01<01:37,  3.47it/s] 28%|██▊       | 132/470 [01:01<01:37,  3.47it/s] 28%|██▊       | 133/470 [01:01<01:36,  3.48it/s] 29%|██▊       | 134/470 [01:02<01:36,  3.47it/s] 29%|██▊       | 135/470 [01:02<01:36,  3.48it/s] 29%|██▉       | 136/470 [01:02<01:36,  3.47it/s] 29%|██▉       | 137/470 [01:03<01:35,  3.48it/s] 29%|██▉       | 138/470 [01:03<01:35,  3.47it/s] 30%|██▉       | 139/470 [01:03<01:35,  3.47it/s] 30%|██▉       | 140/470 [01:03<01:35,  3.47it/s] 30%|███       | 141/470 [01:04<01:34,  3.47it/s] 30%|███       | 142/470 [01:04<01:34,  3.47it/s] 30%|███       | 143/470 [01:04<01:34,  3.47it/s] 31%|███       | 144/470 [01:05<01:33,  3.47it/s] 31%|███       | 145/470 [01:05<01:33,  3.47it/s] 31%|███       | 146/470 [01:05<01:33,  3.47it/s] 31%|███▏      | 147/470 [01:05<01:33,  3.47it/s] 31%|███▏      | 148/470 [01:06<01:32,  3.47it/s] 32%|███▏      | 149/470 [01:06<01:32,  3.47it/s] 32%|███▏      | 150/470 [01:06<01:32,  3.47it/s] 32%|███▏      | 151/470 [01:07<01:32,  3.47it/s] 32%|███▏      | 152/470 [01:07<01:31,  3.47it/s] 33%|███▎      | 153/470 [01:07<01:31,  3.47it/s] 33%|███▎      | 154/470 [01:07<01:31,  3.47it/s] 33%|███▎      | 155/470 [01:08<01:30,  3.47it/s] 33%|███▎      | 156/470 [01:08<01:30,  3.47it/s] 33%|███▎      | 157/470 [01:08<01:30,  3.47it/s] 34%|███▎      | 158/470 [01:09<01:29,  3.47it/s] 34%|███▍      | 159/470 [01:09<01:29,  3.47it/s] 34%|███▍      | 160/470 [01:09<01:29,  3.47it/s] 34%|███▍      | 161/470 [01:09<01:29,  3.47it/s] 34%|███▍      | 162/470 [01:10<01:29,  3.45it/s] 35%|███▍      | 163/470 [01:10<01:28,  3.46it/s] 35%|███▍      | 164/470 [01:10<01:28,  3.46it/s] 35%|███▌      | 165/470 [01:11<01:28,  3.47it/s] 35%|███▌      | 166/470 [01:11<01:28,  3.45it/s] 36%|███▌      | 167/470 [01:11<01:27,  3.46it/s] 36%|███▌      | 168/470 [01:11<01:27,  3.47it/s] 36%|███▌      | 169/470 [01:12<01:26,  3.47it/s] 36%|███▌      | 170/470 [01:12<01:26,  3.47it/s] 36%|███▋      | 171/470 [01:12<01:26,  3.47it/s] 37%|███▋      | 172/470 [01:13<01:25,  3.47it/s] 37%|███▋      | 173/470 [01:13<01:25,  3.47it/s] 37%|███▋      | 174/470 [01:13<01:25,  3.47it/s] 37%|███▋      | 175/470 [01:14<01:24,  3.47it/s] 37%|███▋      | 176/470 [01:14<01:24,  3.47it/s] 38%|███▊      | 177/470 [01:14<01:25,  3.44it/s] 38%|███▊      | 178/470 [01:14<01:24,  3.45it/s] 38%|███▊      | 179/470 [01:15<01:24,  3.46it/s] 38%|███▊      | 180/470 [01:15<01:23,  3.46it/s] 39%|███▊      | 181/470 [01:15<01:23,  3.47it/s] 39%|███▊      | 182/470 [01:16<01:23,  3.47it/s] 39%|███▉      | 183/470 [01:16<01:22,  3.47it/s] 39%|███▉      | 184/470 [01:16<01:22,  3.47it/s] 39%|███▉      | 185/470 [01:16<01:22,  3.47it/s] 40%|███▉      | 186/470 [01:17<01:21,  3.47it/s] 40%|███▉      | 187/470 [01:17<01:21,  3.47it/s] 40%|████      | 188/470 [01:17<01:16,  3.70it/s][INFO|trainer.py:2140] 2023-08-29 01:37:32,772 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:37:32,773 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 01:37:32,773 >>   Batch size = 8
{'eval_loss': 1.015544056892395, 'eval_runtime': 16.5755, 'eval_samples_per_second': 377.243, 'eval_steps_per_second': 47.178, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.95it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.11it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.35it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.65it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.20it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.86it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.63it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.23it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.05it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.19it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.18it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.18it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.24it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.25it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.30it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.26it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.12it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.99it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.89it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.00it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.12it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.19it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.21it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.22it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.20it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.12it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.02it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.99it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.98it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.08it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.03it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.10it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.20it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.23it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.19it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.09it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.02it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.05it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.03it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.11it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.19it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.21it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.16it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.23it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.09it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.14it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.10it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.05it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.04it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.10it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.17it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.09it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.18it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.10it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.03it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.13it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.13it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.97it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.00it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.15it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.06it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.17it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.15it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.08it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.07it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.16it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.11it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.09it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.10it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.08it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 47.09it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.08it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.02it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.02it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.11it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.18it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.18it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.06it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.09it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.06it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.10it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.19it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.07it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.06it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.10it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.13it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.13it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.11it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.06it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.10it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.07it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.08it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.09it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.08it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.15it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.16it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.04it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.06it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.06it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.05it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.06it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.09it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.01it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.08it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.15it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.11it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.10it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.05it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.03it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.05it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.03it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.05it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.02it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.09it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.04it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.08it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.11it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.07it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.00it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.99it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.95it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 46.96it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.99it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.13it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.16it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.15it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.19it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.97it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.95it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.98it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.05it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.02it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.06it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.02it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.06it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.14it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.10it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.00it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.94it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.00it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.02it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.09it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.13it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.96it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.13it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.09it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.01it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.03it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.07it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.10it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.00it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.08it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.08it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.03it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.09it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.09it/s][A 40%|████      | 188/470 [01:34<01:16,  3.70it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:37:49,390 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 01:37:49,409 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:37:51,885 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:37:51,911 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:37:51,947 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:41<34:29,  7.37s/it] 40%|████      | 190/470 [01:41<24:28,  5.24s/it] 41%|████      | 191/470 [01:42<17:28,  3.76s/it] 41%|████      | 192/470 [01:42<12:34,  2.72s/it] 41%|████      | 193/470 [01:42<09:10,  1.99s/it] 41%|████▏     | 194/470 [01:43<06:47,  1.48s/it] 41%|████▏     | 195/470 [01:43<05:08,  1.12s/it] 42%|████▏     | 196/470 [01:43<03:58,  1.15it/s] 42%|████▏     | 197/470 [01:43<03:09,  1.44it/s] 42%|████▏     | 198/470 [01:44<02:35,  1.74it/s] 42%|████▏     | 199/470 [01:44<02:12,  2.05it/s] 43%|████▎     | 200/470 [01:44<01:55,  2.34it/s] 43%|████▎     | 201/470 [01:45<01:43,  2.59it/s] 43%|████▎     | 202/470 [01:45<01:35,  2.80it/s] 43%|████▎     | 203/470 [01:45<01:29,  2.98it/s] 43%|████▎     | 204/470 [01:45<01:25,  3.11it/s] 44%|████▎     | 205/470 [01:46<01:22,  3.21it/s] 44%|████▍     | 206/470 [01:46<01:20,  3.29it/s] 44%|████▍     | 207/470 [01:46<01:18,  3.34it/s] 44%|████▍     | 208/470 [01:47<01:17,  3.38it/s] 44%|████▍     | 209/470 [01:47<01:16,  3.41it/s] 45%|████▍     | 210/470 [01:47<01:15,  3.42it/s] 45%|████▍     | 211/470 [01:47<01:15,  3.44it/s] 45%|████▌     | 212/470 [01:48<01:15,  3.43it/s] 45%|████▌     | 213/470 [01:48<01:14,  3.45it/s] 46%|████▌     | 214/470 [01:48<01:14,  3.45it/s] 46%|████▌     | 215/470 [01:49<01:13,  3.46it/s] 46%|████▌     | 216/470 [01:49<01:13,  3.47it/s] 46%|████▌     | 217/470 [01:49<01:13,  3.47it/s] 46%|████▋     | 218/470 [01:49<01:12,  3.47it/s] 47%|████▋     | 219/470 [01:50<01:12,  3.47it/s] 47%|████▋     | 220/470 [01:50<01:12,  3.47it/s] 47%|████▋     | 221/470 [01:50<01:11,  3.46it/s] 47%|████▋     | 222/470 [01:51<01:13,  3.37it/s] 47%|████▋     | 223/470 [01:51<01:13,  3.38it/s] 48%|████▊     | 224/470 [01:51<01:12,  3.41it/s] 48%|████▊     | 225/470 [01:52<01:11,  3.42it/s] 48%|████▊     | 226/470 [01:52<01:10,  3.44it/s] 48%|████▊     | 227/470 [01:52<01:10,  3.45it/s] 49%|████▊     | 228/470 [01:52<01:10,  3.46it/s] 49%|████▊     | 229/470 [01:53<01:09,  3.46it/s] 49%|████▉     | 230/470 [01:53<01:09,  3.46it/s] 49%|████▉     | 231/470 [01:53<01:08,  3.47it/s] 49%|████▉     | 232/470 [01:54<01:08,  3.47it/s] 50%|████▉     | 233/470 [01:54<01:08,  3.47it/s] 50%|████▉     | 234/470 [01:54<01:08,  3.43it/s] 50%|█████     | 235/470 [01:54<01:08,  3.44it/s] 50%|█████     | 236/470 [01:55<01:07,  3.45it/s] 50%|█████     | 237/470 [01:55<01:07,  3.46it/s] 51%|█████     | 238/470 [01:55<01:07,  3.46it/s] 51%|█████     | 239/470 [01:56<01:06,  3.47it/s] 51%|█████     | 240/470 [01:56<01:06,  3.47it/s] 51%|█████▏    | 241/470 [01:56<01:06,  3.47it/s] 51%|█████▏    | 242/470 [01:56<01:05,  3.47it/s] 52%|█████▏    | 243/470 [01:57<01:05,  3.47it/s] 52%|█████▏    | 244/470 [01:57<01:05,  3.47it/s] 52%|█████▏    | 245/470 [01:57<01:05,  3.45it/s] 52%|█████▏    | 246/470 [01:58<01:04,  3.46it/s] 53%|█████▎    | 247/470 [01:58<01:04,  3.46it/s] 53%|█████▎    | 248/470 [01:58<01:04,  3.47it/s] 53%|█████▎    | 249/470 [01:58<01:03,  3.47it/s] 53%|█████▎    | 250/470 [01:59<01:03,  3.47it/s] 53%|█████▎    | 251/470 [01:59<01:03,  3.47it/s] 54%|█████▎    | 252/470 [01:59<01:02,  3.47it/s] 54%|█████▍    | 253/470 [02:00<01:02,  3.47it/s] 54%|█████▍    | 254/470 [02:00<01:02,  3.47it/s] 54%|█████▍    | 255/470 [02:00<01:01,  3.47it/s] 54%|█████▍    | 256/470 [02:00<01:01,  3.45it/s] 55%|█████▍    | 257/470 [02:01<01:01,  3.46it/s] 55%|█████▍    | 258/470 [02:01<01:01,  3.46it/s] 55%|█████▌    | 259/470 [02:01<01:00,  3.47it/s] 55%|█████▌    | 260/470 [02:02<01:00,  3.47it/s] 56%|█████▌    | 261/470 [02:02<01:00,  3.47it/s] 56%|█████▌    | 262/470 [02:02<00:59,  3.47it/s] 56%|█████▌    | 263/470 [02:02<00:59,  3.47it/s] 56%|█████▌    | 264/470 [02:03<00:59,  3.47it/s] 56%|█████▋    | 265/470 [02:03<00:59,  3.47it/s] 57%|█████▋    | 266/470 [02:03<00:58,  3.47it/s] 57%|█████▋    | 267/470 [02:04<00:58,  3.47it/s] 57%|█████▋    | 268/470 [02:04<00:58,  3.47it/s] 57%|█████▋    | 269/470 [02:04<00:57,  3.47it/s] 57%|█████▋    | 270/470 [02:05<00:57,  3.47it/s] 58%|█████▊    | 271/470 [02:05<00:57,  3.47it/s] 58%|█████▊    | 272/470 [02:05<00:57,  3.47it/s] 58%|█████▊    | 273/470 [02:05<00:57,  3.45it/s] 58%|█████▊    | 274/470 [02:06<00:56,  3.46it/s] 59%|█████▊    | 275/470 [02:06<00:56,  3.47it/s] 59%|█████▊    | 276/470 [02:06<00:55,  3.47it/s] 59%|█████▉    | 277/470 [02:07<00:55,  3.47it/s] 59%|█████▉    | 278/470 [02:07<00:55,  3.47it/s] 59%|█████▉    | 279/470 [02:07<00:55,  3.47it/s] 60%|█████▉    | 280/470 [02:07<00:54,  3.47it/s] 60%|█████▉    | 281/470 [02:08<00:54,  3.47it/s] 60%|██████    | 282/470 [02:08<00:50,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 01:38:23,483 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:38:23,483 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 01:38:23,483 >>   Batch size = 8
{'eval_loss': 1.0233596563339233, 'eval_runtime': 16.6016, 'eval_samples_per_second': 376.65, 'eval_steps_per_second': 47.104, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.58it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.00it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.23it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.45it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.01it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.79it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.56it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.19it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.13it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.18it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.26it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.23it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.16it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.26it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.14it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.19it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.09it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.94it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.07it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.09it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.07it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.16it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.10it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.18it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.00it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.99it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.88it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.01it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.13it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.13it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.15it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.07it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.13it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.10it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.10it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.09it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.99it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.04it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.12it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.24it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.19it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.08it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.08it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.12it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.08it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.08it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.07it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.08it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.15it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.11it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.18it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.11it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.06it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.07it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.08it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.09it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.05it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.15it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.18it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.08it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.20it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.16it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.05it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.14it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.10it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.02it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.07it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.14it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.10it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.15it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.15it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.08it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.01it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.07it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.92it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.01it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.09it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.00it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.03it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.16it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.10it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.07it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.04it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.02it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.01it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.11it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.14it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.10it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.08it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.12it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.08it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.11it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.11it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.12it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.13it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.08it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.06it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.10it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.01it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.20it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.13it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.06it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.15it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.12it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.99it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.09it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.99it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.94it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.12it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.14it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.96it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.09it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.11it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.06it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.07it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.09it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.98it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.02it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.11it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.06it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.11it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.09it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.07it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.99it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.95it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.02it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.98it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.07it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.10it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.02it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.07it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.10it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.10it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.05it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.10it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.07it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.98it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.07it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.06it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.03it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.07it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.18it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.02it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.03it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.99it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.98it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.03it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.01it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.02it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 47.07it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.09it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.15it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.04it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.00it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.02it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.02it/s][A 60%|██████    | 282/470 [02:25<00:50,  3.71it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:38:40,108 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 01:38:40,130 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:38:42,567 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:38:42,586 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:38:42,596 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:32<23:21,  7.50s/it] 60%|██████    | 284/470 [02:33<16:32,  5.33s/it] 61%|██████    | 285/470 [02:33<11:46,  3.82s/it] 61%|██████    | 286/470 [02:33<08:27,  2.76s/it] 61%|██████    | 287/470 [02:33<06:09,  2.02s/it] 61%|██████▏   | 288/470 [02:34<04:32,  1.50s/it] 61%|██████▏   | 289/470 [02:34<03:25,  1.14s/it] 62%|██████▏   | 290/470 [02:34<02:38,  1.13it/s] 62%|██████▏   | 291/470 [02:35<02:05,  1.42it/s] 62%|██████▏   | 292/470 [02:35<01:42,  1.73it/s] 62%|██████▏   | 293/470 [02:35<01:26,  2.03it/s] 63%|██████▎   | 294/470 [02:35<01:15,  2.32it/s] 63%|██████▎   | 295/470 [02:36<01:07,  2.57it/s] 63%|██████▎   | 296/470 [02:36<01:02,  2.79it/s] 63%|██████▎   | 297/470 [02:36<00:58,  2.97it/s] 63%|██████▎   | 298/470 [02:37<00:55,  3.11it/s] 64%|██████▎   | 299/470 [02:37<00:53,  3.21it/s] 64%|██████▍   | 300/470 [02:37<00:51,  3.29it/s] 64%|██████▍   | 301/470 [02:37<00:50,  3.34it/s] 64%|██████▍   | 302/470 [02:38<00:49,  3.38it/s] 64%|██████▍   | 303/470 [02:38<00:49,  3.41it/s] 65%|██████▍   | 304/470 [02:38<00:48,  3.43it/s] 65%|██████▍   | 305/470 [02:39<00:47,  3.44it/s] 65%|██████▌   | 306/470 [02:39<00:47,  3.44it/s] 65%|██████▌   | 307/470 [02:39<00:47,  3.45it/s] 66%|██████▌   | 308/470 [02:39<00:46,  3.46it/s] 66%|██████▌   | 309/470 [02:40<00:46,  3.46it/s] 66%|██████▌   | 310/470 [02:40<00:46,  3.47it/s] 66%|██████▌   | 311/470 [02:40<00:45,  3.47it/s] 66%|██████▋   | 312/470 [02:41<00:45,  3.47it/s] 67%|██████▋   | 313/470 [02:41<00:45,  3.47it/s] 67%|██████▋   | 314/470 [02:41<00:44,  3.47it/s] 67%|██████▋   | 315/470 [02:41<00:44,  3.48it/s] 67%|██████▋   | 316/470 [02:42<00:44,  3.47it/s] 67%|██████▋   | 317/470 [02:42<00:44,  3.45it/s] 68%|██████▊   | 318/470 [02:42<00:43,  3.46it/s] 68%|██████▊   | 319/470 [02:43<00:43,  3.46it/s] 68%|██████▊   | 320/470 [02:43<00:43,  3.47it/s] 68%|██████▊   | 321/470 [02:43<00:42,  3.47it/s] 69%|██████▊   | 322/470 [02:44<00:42,  3.47it/s] 69%|██████▊   | 323/470 [02:44<00:42,  3.47it/s] 69%|██████▉   | 324/470 [02:44<00:42,  3.47it/s] 69%|██████▉   | 325/470 [02:44<00:41,  3.47it/s] 69%|██████▉   | 326/470 [02:45<00:41,  3.47it/s] 70%|██████▉   | 327/470 [02:45<00:41,  3.47it/s] 70%|██████▉   | 328/470 [02:45<00:41,  3.45it/s] 70%|███████   | 329/470 [02:46<00:40,  3.46it/s] 70%|███████   | 330/470 [02:46<00:40,  3.46it/s] 70%|███████   | 331/470 [02:46<00:40,  3.47it/s] 71%|███████   | 332/470 [02:46<00:39,  3.47it/s] 71%|███████   | 333/470 [02:47<00:39,  3.47it/s] 71%|███████   | 334/470 [02:47<00:39,  3.47it/s] 71%|███████▏  | 335/470 [02:47<00:38,  3.47it/s] 71%|███████▏  | 336/470 [02:48<00:38,  3.47it/s] 72%|███████▏  | 337/470 [02:48<00:38,  3.47it/s] 72%|███████▏  | 338/470 [02:48<00:37,  3.47it/s] 72%|███████▏  | 339/470 [02:48<00:37,  3.45it/s] 72%|███████▏  | 340/470 [02:49<00:37,  3.46it/s] 73%|███████▎  | 341/470 [02:49<00:37,  3.46it/s] 73%|███████▎  | 342/470 [02:49<00:36,  3.47it/s] 73%|███████▎  | 343/470 [02:50<00:36,  3.47it/s] 73%|███████▎  | 344/470 [02:50<00:36,  3.47it/s] 73%|███████▎  | 345/470 [02:50<00:36,  3.47it/s] 74%|███████▎  | 346/470 [02:50<00:36,  3.44it/s] 74%|███████▍  | 347/470 [02:51<00:36,  3.38it/s] 74%|███████▍  | 348/470 [02:51<00:35,  3.40it/s] 74%|███████▍  | 349/470 [02:51<00:35,  3.42it/s] 74%|███████▍  | 350/470 [02:52<00:35,  3.43it/s] 75%|███████▍  | 351/470 [02:52<00:34,  3.44it/s] 75%|███████▍  | 352/470 [02:52<00:34,  3.45it/s] 75%|███████▌  | 353/470 [02:52<00:33,  3.46it/s] 75%|███████▌  | 354/470 [02:53<00:33,  3.46it/s] 76%|███████▌  | 355/470 [02:53<00:33,  3.46it/s] 76%|███████▌  | 356/470 [02:53<00:32,  3.47it/s] 76%|███████▌  | 357/470 [02:54<00:32,  3.47it/s] 76%|███████▌  | 358/470 [02:54<00:32,  3.47it/s] 76%|███████▋  | 359/470 [02:54<00:31,  3.47it/s] 77%|███████▋  | 360/470 [02:54<00:31,  3.47it/s] 77%|███████▋  | 361/470 [02:55<00:31,  3.47it/s] 77%|███████▋  | 362/470 [02:55<00:31,  3.47it/s] 77%|███████▋  | 363/470 [02:55<00:30,  3.47it/s] 77%|███████▋  | 364/470 [02:56<00:30,  3.47it/s] 78%|███████▊  | 365/470 [02:56<00:30,  3.47it/s] 78%|███████▊  | 366/470 [02:56<00:29,  3.47it/s] 78%|███████▊  | 367/470 [02:57<00:30,  3.42it/s] 78%|███████▊  | 368/470 [02:57<00:29,  3.44it/s] 79%|███████▊  | 369/470 [02:57<00:29,  3.44it/s] 79%|███████▊  | 370/470 [02:57<00:28,  3.45it/s] 79%|███████▉  | 371/470 [02:58<00:28,  3.46it/s] 79%|███████▉  | 372/470 [02:58<00:28,  3.46it/s] 79%|███████▉  | 373/470 [02:58<00:28,  3.46it/s] 80%|███████▉  | 374/470 [02:59<00:27,  3.47it/s] 80%|███████▉  | 375/470 [02:59<00:27,  3.47it/s] 80%|████████  | 376/470 [02:59<00:25,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 01:39:14,626 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:39:14,626 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 01:39:14,626 >>   Batch size = 8
{'eval_loss': 1.0366859436035156, 'eval_runtime': 16.6056, 'eval_samples_per_second': 376.56, 'eval_steps_per_second': 47.093, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.67it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.93it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.24it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.54it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.01it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.82it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.47it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.08it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.07it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.10it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.19it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.24it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.14it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.22it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.17it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.08it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.93it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.93it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.92it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.93it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.99it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.13it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.19it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.20it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.12it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.10it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.01it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.88it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.80it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.98it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.03it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.09it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.22it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.06it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.03it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.01it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.93it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.91it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.98it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.03it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.02it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.13it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.11it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.10it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.03it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.96it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.87it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.97it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.93it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.01it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.10it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.13it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.13it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.96it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.90it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.87it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.91it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.02it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.96it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.10it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.14it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.13it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.17it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.98it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.94it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.01it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.95it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.94it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.04it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.09it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.11it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.10it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.11it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.90it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.89it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.01it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.02it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.02it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.14it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.06it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.03it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.11it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.03it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.95it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.90it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.88it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.94it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.09it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.12it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.07it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.13it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.96it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.95it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.96it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.95it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.96it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.00it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.06it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.14it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.08it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.02it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.02it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 46.99it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.94it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.93it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.99it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.07it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.15it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.13it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.05it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.96it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 46.94it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.00it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.03it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.99it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.98it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.06it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.09it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.04it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.05it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.89it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.91it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.97it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.04it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.99it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.03it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.11it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.98it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.01it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.98it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 46.89it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.97it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.05it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.02it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.07it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.99it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.00it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.02it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.97it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.88it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.93it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.97it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.02it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.02it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.05it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.95it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.92it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.02it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.95it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 46.94it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.01it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.02it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.98it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.02it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.99it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.99it/s][A 80%|████████  | 376/470 [03:16<00:25,  3.71it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:39:31,270 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 01:39:31,287 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:39:33,887 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:39:33,911 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:39:33,922 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:24<11:44,  7.57s/it] 80%|████████  | 378/470 [03:24<08:15,  5.39s/it] 81%|████████  | 379/470 [03:24<05:51,  3.86s/it] 81%|████████  | 380/470 [03:25<04:10,  2.79s/it] 81%|████████  | 381/470 [03:25<03:01,  2.04s/it] 81%|████████▏ | 382/470 [03:25<02:13,  1.51s/it] 81%|████████▏ | 383/470 [03:25<01:39,  1.14s/it] 82%|████████▏ | 384/470 [03:26<01:16,  1.13it/s] 82%|████████▏ | 385/470 [03:26<01:00,  1.41it/s] 82%|████████▏ | 386/470 [03:26<00:48,  1.72it/s] 82%|████████▏ | 387/470 [03:27<00:40,  2.03it/s] 83%|████████▎ | 388/470 [03:27<00:35,  2.32it/s] 83%|████████▎ | 389/470 [03:27<00:31,  2.57it/s] 83%|████████▎ | 390/470 [03:27<00:28,  2.79it/s] 83%|████████▎ | 391/470 [03:28<00:26,  2.96it/s] 83%|████████▎ | 392/470 [03:28<00:25,  3.10it/s] 84%|████████▎ | 393/470 [03:28<00:24,  3.20it/s] 84%|████████▍ | 394/470 [03:29<00:23,  3.28it/s] 84%|████████▍ | 395/470 [03:29<00:22,  3.33it/s] 84%|████████▍ | 396/470 [03:29<00:21,  3.38it/s] 84%|████████▍ | 397/470 [03:29<00:21,  3.41it/s] 85%|████████▍ | 398/470 [03:30<00:21,  3.43it/s] 85%|████████▍ | 399/470 [03:30<00:20,  3.44it/s] 85%|████████▌ | 400/470 [03:30<00:20,  3.45it/s] 85%|████████▌ | 401/470 [03:31<00:19,  3.45it/s] 86%|████████▌ | 402/470 [03:31<00:19,  3.46it/s] 86%|████████▌ | 403/470 [03:31<00:19,  3.47it/s] 86%|████████▌ | 404/470 [03:31<00:19,  3.47it/s] 86%|████████▌ | 405/470 [03:32<00:18,  3.47it/s] 86%|████████▋ | 406/470 [03:32<00:18,  3.47it/s] 87%|████████▋ | 407/470 [03:32<00:18,  3.47it/s] 87%|████████▋ | 408/470 [03:33<00:17,  3.47it/s] 87%|████████▋ | 409/470 [03:33<00:17,  3.47it/s] 87%|████████▋ | 410/470 [03:33<00:17,  3.47it/s] 87%|████████▋ | 411/470 [03:33<00:17,  3.46it/s] 88%|████████▊ | 412/470 [03:34<00:16,  3.46it/s] 88%|████████▊ | 413/470 [03:34<00:16,  3.47it/s] 88%|████████▊ | 414/470 [03:34<00:16,  3.47it/s] 88%|████████▊ | 415/470 [03:35<00:15,  3.47it/s] 89%|████████▊ | 416/470 [03:35<00:15,  3.47it/s] 89%|████████▊ | 417/470 [03:35<00:15,  3.47it/s] 89%|████████▉ | 418/470 [03:35<00:14,  3.47it/s] 89%|████████▉ | 419/470 [03:36<00:14,  3.47it/s] 89%|████████▉ | 420/470 [03:36<00:14,  3.47it/s] 90%|████████▉ | 421/470 [03:36<00:14,  3.47it/s] 90%|████████▉ | 422/470 [03:37<00:13,  3.46it/s] 90%|█████████ | 423/470 [03:37<00:13,  3.46it/s] 90%|█████████ | 424/470 [03:37<00:13,  3.47it/s] 90%|█████████ | 425/470 [03:37<00:12,  3.47it/s] 91%|█████████ | 426/470 [03:38<00:12,  3.47it/s] 91%|█████████ | 427/470 [03:38<00:12,  3.47it/s] 91%|█████████ | 428/470 [03:38<00:12,  3.47it/s] 91%|█████████▏| 429/470 [03:39<00:11,  3.47it/s] 91%|█████████▏| 430/470 [03:39<00:11,  3.47it/s] 92%|█████████▏| 431/470 [03:39<00:11,  3.47it/s] 92%|█████████▏| 432/470 [03:40<00:10,  3.47it/s] 92%|█████████▏| 433/470 [03:40<00:10,  3.47it/s] 92%|█████████▏| 434/470 [03:40<00:10,  3.47it/s] 93%|█████████▎| 435/470 [03:40<00:10,  3.47it/s] 93%|█████████▎| 436/470 [03:41<00:09,  3.47it/s] 93%|█████████▎| 437/470 [03:41<00:09,  3.47it/s] 93%|█████████▎| 438/470 [03:41<00:09,  3.48it/s] 93%|█████████▎| 439/470 [03:42<00:08,  3.48it/s] 94%|█████████▎| 440/470 [03:42<00:08,  3.47it/s] 94%|█████████▍| 441/470 [03:42<00:08,  3.47it/s] 94%|█████████▍| 442/470 [03:42<00:08,  3.46it/s] 94%|█████████▍| 443/470 [03:43<00:07,  3.47it/s] 94%|█████████▍| 444/470 [03:43<00:07,  3.47it/s] 95%|█████████▍| 445/470 [03:43<00:07,  3.47it/s] 95%|█████████▍| 446/470 [03:44<00:06,  3.47it/s] 95%|█████████▌| 447/470 [03:44<00:06,  3.47it/s] 95%|█████████▌| 448/470 [03:44<00:06,  3.47it/s] 96%|█████████▌| 449/470 [03:44<00:06,  3.47it/s] 96%|█████████▌| 450/470 [03:45<00:05,  3.47it/s] 96%|█████████▌| 451/470 [03:45<00:05,  3.47it/s] 96%|█████████▌| 452/470 [03:45<00:05,  3.47it/s] 96%|█████████▋| 453/470 [03:46<00:04,  3.46it/s] 97%|█████████▋| 454/470 [03:46<00:04,  3.46it/s] 97%|█████████▋| 455/470 [03:46<00:04,  3.47it/s] 97%|█████████▋| 456/470 [03:46<00:04,  3.47it/s] 97%|█████████▋| 457/470 [03:47<00:03,  3.47it/s] 97%|█████████▋| 458/470 [03:47<00:03,  3.47it/s] 98%|█████████▊| 459/470 [03:47<00:03,  3.47it/s] 98%|█████████▊| 460/470 [03:48<00:02,  3.47it/s] 98%|█████████▊| 461/470 [03:48<00:02,  3.47it/s] 98%|█████████▊| 462/470 [03:48<00:02,  3.47it/s] 99%|█████████▊| 463/470 [03:48<00:02,  3.47it/s] 99%|█████████▊| 464/470 [03:49<00:01,  3.42it/s] 99%|█████████▉| 465/470 [03:49<00:01,  3.43it/s] 99%|█████████▉| 466/470 [03:49<00:01,  3.45it/s] 99%|█████████▉| 467/470 [03:50<00:00,  3.45it/s]100%|█████████▉| 468/470 [03:50<00:00,  3.46it/s]100%|█████████▉| 469/470 [03:50<00:00,  3.46it/s]100%|██████████| 470/470 [03:50<00:00,  3.70it/s][INFO|trainer.py:2140] 2023-08-29 01:40:05,992 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:40:05,993 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 01:40:05,993 >>   Batch size = 8
{'eval_loss': 1.0519787073135376, 'eval_runtime': 16.6264, 'eval_samples_per_second': 376.089, 'eval_steps_per_second': 47.034, 'epoch': 4.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.98it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.12it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.62it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.07it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.84it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.64it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.54it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.30it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.07it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.12it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.11it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.12it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.25it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.23it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.26it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.18it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.08it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.00it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.01it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.00it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.13it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.12it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.11it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.14it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.25it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.12it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.99it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.03it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.04it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.00it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.10it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.17it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.20it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.14it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.18it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.04it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.08it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.96it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.96it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.02it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.10it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.14it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.10it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.11it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.09it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.04it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.05it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.98it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.95it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.07it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.17it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.20it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.07it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.10it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.09it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.92it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.01it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.05it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.99it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.02it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.14it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.08it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.16it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.10it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.00it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.94it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.01it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.01it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.10it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.21it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 47.09it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.03it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.16it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.02it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.03it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.02it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.02it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.93it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.17it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.23it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.11it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.08it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.11it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.93it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.05it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.00it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.99it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.02it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.12it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.13it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.09it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.12it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.08it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.08it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.12it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.99it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.02it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.11it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.10it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.10it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.12it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.05it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.02it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.02it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.97it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.06it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.00it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.11it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.11it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.11it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.07it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.09it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.07it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.02it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.57it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.71it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.89it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.96it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.04it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.97it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.97it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.02it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.01it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.05it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.01it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.00it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.10it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.09it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.04it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.04it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.07it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.95it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.99it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.01it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.01it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.09it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.13it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.01it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.06it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.03it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.97it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.98it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.06it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.01it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.04it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.13it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.13it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.09it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.04it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 46.97it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.93it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.99it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.07it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.04it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.10it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.10it/s][A100%|██████████| 470/470 [04:07<00:00,  3.70it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:40:22,628 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 01:40:22,652 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:40:25,079 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:40:25,100 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:40:25,115 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:40:30,140 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:40:30,144 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94 (score: 1.015544056892395).
                                                 100%|██████████| 470/470 [04:16<00:00,  3.70it/s]100%|██████████| 470/470 [04:16<00:00,  1.83it/s]
[INFO|trainer.py:1894] 2023-08-29 01:40:31,876 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 01:40:31,895 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:40:34,204 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:40:34,275 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:40:34,284 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:40:34,497 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,497 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,497 >>   train_loss               =      0.383
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,497 >>   train_runtime            = 0:04:16.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,497 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,497 >>   train_samples_per_second =     116.82
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,497 >>   train_steps_per_second   =       1.83
{'eval_loss': 1.0550624132156372, 'eval_runtime': 16.6149, 'eval_samples_per_second': 376.349, 'eval_steps_per_second': 47.066, 'epoch': 5.0}
{'train_runtime': 256.8059, 'train_samples_per_second': 116.82, 'train_steps_per_second': 1.83, 'train_loss': 0.3830241751163564, 'epoch': 5.0}
08/29/2023 01:40:34 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:40:34,537 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:40:34,538 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 01:40:34,538 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.10it/s]  2%|▏         | 12/782 [00:00<00:15, 51.31it/s]  2%|▏         | 18/782 [00:00<00:15, 49.45it/s]  3%|▎         | 23/782 [00:00<00:15, 48.65it/s]  4%|▎         | 28/782 [00:00<00:15, 48.23it/s]  4%|▍         | 33/782 [00:00<00:15, 47.91it/s]  5%|▍         | 38/782 [00:00<00:15, 47.78it/s]  5%|▌         | 43/782 [00:00<00:15, 47.61it/s]  6%|▌         | 48/782 [00:00<00:15, 47.26it/s]  7%|▋         | 53/782 [00:01<00:15, 47.22it/s]  7%|▋         | 58/782 [00:01<00:15, 47.18it/s]  8%|▊         | 63/782 [00:01<00:15, 47.14it/s]  9%|▊         | 68/782 [00:01<00:15, 47.25it/s]  9%|▉         | 73/782 [00:01<00:14, 47.31it/s] 10%|▉         | 78/782 [00:01<00:14, 47.21it/s] 11%|█         | 83/782 [00:01<00:14, 47.23it/s] 11%|█▏        | 88/782 [00:01<00:14, 47.21it/s] 12%|█▏        | 93/782 [00:01<00:14, 47.08it/s] 13%|█▎        | 98/782 [00:02<00:14, 47.10it/s] 13%|█▎        | 103/782 [00:02<00:14, 47.08it/s] 14%|█▍        | 108/782 [00:02<00:14, 47.05it/s] 14%|█▍        | 113/782 [00:02<00:14, 47.05it/s] 15%|█▌        | 118/782 [00:02<00:14, 47.14it/s] 16%|█▌        | 123/782 [00:02<00:13, 47.22it/s] 16%|█▋        | 128/782 [00:02<00:13, 47.19it/s] 17%|█▋        | 133/782 [00:02<00:13, 47.24it/s] 18%|█▊        | 138/782 [00:02<00:13, 47.12it/s] 18%|█▊        | 143/782 [00:03<00:13, 47.10it/s] 19%|█▉        | 148/782 [00:03<00:13, 47.17it/s] 20%|█▉        | 153/782 [00:03<00:13, 47.05it/s] 20%|██        | 158/782 [00:03<00:13, 47.03it/s] 21%|██        | 163/782 [00:03<00:13, 47.08it/s] 21%|██▏       | 168/782 [00:03<00:13, 47.15it/s] 22%|██▏       | 173/782 [00:03<00:12, 47.18it/s] 23%|██▎       | 178/782 [00:03<00:12, 47.19it/s] 23%|██▎       | 183/782 [00:03<00:12, 47.25it/s] 24%|██▍       | 188/782 [00:03<00:12, 47.17it/s] 25%|██▍       | 193/782 [00:04<00:12, 47.05it/s] 25%|██▌       | 198/782 [00:04<00:12, 46.99it/s] 26%|██▌       | 203/782 [00:04<00:12, 47.01it/s] 27%|██▋       | 208/782 [00:04<00:12, 47.05it/s] 27%|██▋       | 213/782 [00:04<00:12, 47.07it/s] 28%|██▊       | 218/782 [00:04<00:11, 47.21it/s] 29%|██▊       | 223/782 [00:04<00:11, 47.17it/s] 29%|██▉       | 228/782 [00:04<00:11, 47.15it/s] 30%|██▉       | 233/782 [00:04<00:11, 47.18it/s] 30%|███       | 238/782 [00:05<00:11, 47.16it/s] 31%|███       | 243/782 [00:05<00:11, 47.08it/s] 32%|███▏      | 248/782 [00:05<00:11, 47.12it/s] 32%|███▏      | 253/782 [00:05<00:11, 47.08it/s] 33%|███▎      | 258/782 [00:05<00:11, 47.06it/s] 34%|███▎      | 263/782 [00:05<00:11, 47.15it/s] 34%|███▍      | 268/782 [00:05<00:10, 47.08it/s] 35%|███▍      | 273/782 [00:05<00:10, 47.09it/s] 36%|███▌      | 278/782 [00:05<00:10, 47.21it/s] 36%|███▌      | 283/782 [00:05<00:10, 47.15it/s] 37%|███▋      | 288/782 [00:06<00:10, 47.01it/s] 37%|███▋      | 293/782 [00:06<00:10, 47.12it/s] 38%|███▊      | 298/782 [00:06<00:10, 47.02it/s] 39%|███▊      | 303/782 [00:06<00:10, 47.01it/s] 39%|███▉      | 308/782 [00:06<00:10, 47.07it/s] 40%|████      | 313/782 [00:06<00:09, 47.10it/s] 41%|████      | 318/782 [00:06<00:09, 47.12it/s] 41%|████▏     | 323/782 [00:06<00:09, 47.12it/s] 42%|████▏     | 328/782 [00:06<00:09, 47.07it/s] 43%|████▎     | 333/782 [00:07<00:09, 47.09it/s] 43%|████▎     | 338/782 [00:07<00:09, 46.98it/s] 44%|████▍     | 343/782 [00:07<00:09, 47.07it/s] 45%|████▍     | 348/782 [00:07<00:09, 47.04it/s] 45%|████▌     | 353/782 [00:07<00:09, 47.00it/s] 46%|████▌     | 358/782 [00:07<00:08, 47.15it/s] 46%|████▋     | 363/782 [00:07<00:08, 47.15it/s] 47%|████▋     | 368/782 [00:07<00:08, 47.18it/s] 48%|████▊     | 373/782 [00:07<00:08, 47.18it/s] 48%|████▊     | 378/782 [00:07<00:08, 47.13it/s] 49%|████▉     | 383/782 [00:08<00:08, 47.16it/s] 50%|████▉     | 388/782 [00:08<00:08, 47.11it/s] 50%|█████     | 393/782 [00:08<00:08, 46.98it/s] 51%|█████     | 398/782 [00:08<00:08, 47.00it/s] 52%|█████▏    | 403/782 [00:08<00:08, 47.02it/s] 52%|█████▏    | 408/782 [00:08<00:07, 47.10it/s] 53%|█████▎    | 413/782 [00:08<00:07, 47.12it/s] 53%|█████▎    | 418/782 [00:08<00:07, 47.09it/s] 54%|█████▍    | 423/782 [00:08<00:07, 47.18it/s] 55%|█████▍    | 428/782 [00:09<00:07, 47.17it/s] 55%|█████▌    | 433/782 [00:09<00:07, 47.12it/s] 56%|█████▌    | 438/782 [00:09<00:07, 47.14it/s] 57%|█████▋    | 443/782 [00:09<00:07, 46.99it/s] 57%|█████▋    | 448/782 [00:09<00:07, 47.06it/s] 58%|█████▊    | 453/782 [00:09<00:06, 47.02it/s] 59%|█████▊    | 458/782 [00:09<00:06, 47.04it/s] 59%|█████▉    | 463/782 [00:09<00:06, 47.16it/s] 60%|█████▉    | 468/782 [00:09<00:06, 47.19it/s] 60%|██████    | 473/782 [00:10<00:06, 47.24it/s] 61%|██████    | 478/782 [00:10<00:06, 47.17it/s] 62%|██████▏   | 483/782 [00:10<00:06, 47.12it/s] 62%|██████▏   | 488/782 [00:10<00:06, 47.06it/s] 63%|██████▎   | 493/782 [00:10<00:06, 47.02it/s] 64%|██████▎   | 498/782 [00:10<00:06, 47.04it/s] 64%|██████▍   | 503/782 [00:10<00:05, 47.08it/s] 65%|██████▍   | 508/782 [00:10<00:05, 47.17it/s] 66%|██████▌   | 513/782 [00:10<00:05, 47.13it/s] 66%|██████▌   | 518/782 [00:10<00:05, 47.25it/s] 67%|██████▋   | 523/782 [00:11<00:05, 47.22it/s] 68%|██████▊   | 528/782 [00:11<00:05, 47.08it/s] 68%|██████▊   | 533/782 [00:11<00:05, 47.11it/s] 69%|██████▉   | 538/782 [00:11<00:05, 47.07it/s] 69%|██████▉   | 543/782 [00:11<00:05, 47.06it/s] 70%|███████   | 548/782 [00:11<00:04, 46.97it/s] 71%|███████   | 553/782 [00:11<00:04, 47.13it/s] 71%|███████▏  | 558/782 [00:11<00:04, 47.17it/s] 72%|███████▏  | 563/782 [00:11<00:04, 47.13it/s] 73%|███████▎  | 568/782 [00:12<00:04, 47.15it/s] 73%|███████▎  | 573/782 [00:12<00:04, 47.16it/s] 74%|███████▍  | 578/782 [00:12<00:04, 47.12it/s] 75%|███████▍  | 583/782 [00:12<00:04, 47.14it/s] 75%|███████▌  | 588/782 [00:12<00:04, 47.14it/s] 76%|███████▌  | 593/782 [00:12<00:04, 46.98it/s] 76%|███████▋  | 598/782 [00:12<00:03, 46.98it/s] 77%|███████▋  | 603/782 [00:12<00:03, 47.03it/s] 78%|███████▊  | 608/782 [00:12<00:03, 47.12it/s] 78%|███████▊  | 613/782 [00:12<00:03, 47.11it/s] 79%|███████▉  | 618/782 [00:13<00:03, 47.12it/s] 80%|███████▉  | 623/782 [00:13<00:03, 47.12it/s] 80%|████████  | 628/782 [00:13<00:03, 47.03it/s] 81%|████████  | 633/782 [00:13<00:03, 47.06it/s] 82%|████████▏ | 638/782 [00:13<00:03, 47.06it/s] 82%|████████▏ | 643/782 [00:13<00:02, 47.04it/s] 83%|████████▎ | 648/782 [00:13<00:02, 46.94it/s] 84%|████████▎ | 653/782 [00:13<00:02, 46.94it/s] 84%|████████▍ | 658/782 [00:13<00:02, 46.89it/s] 85%|████████▍ | 663/782 [00:14<00:02, 46.95it/s] 85%|████████▌ | 668/782 [00:14<00:02, 46.98it/s] 86%|████████▌ | 673/782 [00:14<00:02, 47.06it/s] 87%|████████▋ | 678/782 [00:14<00:02, 47.08it/s] 87%|████████▋ | 683/782 [00:14<00:02, 47.19it/s] 88%|████████▊ | 688/782 [00:14<00:01, 47.19it/s] 89%|████████▊ | 693/782 [00:14<00:01, 47.11it/s] 89%|████████▉ | 698/782 [00:14<00:01, 47.21it/s] 90%|████████▉ | 703/782 [00:14<00:01, 47.07it/s] 91%|█████████ | 708/782 [00:15<00:01, 47.11it/s] 91%|█████████ | 713/782 [00:15<00:01, 47.16it/s] 92%|█████████▏| 718/782 [00:15<00:01, 47.24it/s] 92%|█████████▏| 723/782 [00:15<00:01, 47.25it/s] 93%|█████████▎| 728/782 [00:15<00:01, 47.16it/s] 94%|█████████▎| 733/782 [00:15<00:01, 47.17it/s] 94%|█████████▍| 738/782 [00:15<00:00, 47.22it/s] 95%|█████████▌| 743/782 [00:15<00:00, 47.14it/s] 96%|█████████▌| 748/782 [00:15<00:00, 47.23it/s] 96%|█████████▋| 753/782 [00:15<00:00, 47.20it/s] 97%|█████████▋| 758/782 [00:16<00:00, 47.11it/s] 98%|█████████▊| 763/782 [00:16<00:00, 47.11it/s] 98%|█████████▊| 768/782 [00:16<00:00, 47.17it/s] 99%|█████████▉| 773/782 [00:16<00:00, 47.18it/s] 99%|█████████▉| 778/782 [00:16<00:00, 47.21it/s]100%|██████████| 782/782 [00:16<00:00, 47.18it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:40:51,134 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   eval_loss               =     1.0155
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   eval_runtime            = 0:00:16.59
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   eval_samples_per_second =    376.769
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   eval_steps_per_second   =     47.119
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:51,134 >>   perplexity              =     2.7609
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:58,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:58,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:58,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:58,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:58,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:40:59,036 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:40:59,037 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:40:59,602 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:41:00,644 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:41:00,644 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:41:03,616 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:41:03,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:41:03,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:41:03,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:41:03,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:41:04,256 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:41:04,257 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:41:04,818 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:41:04,970 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:41:04,970 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:14,  1.54it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.42it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.46it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:25,  1.50it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:26,  1.49it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.48it/s]Extractor Predicting: 45it [00:30,  1.49it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.52it/s]Extractor Predicting: 49it [00:33,  1.41it/s]Extractor Predicting: 50it [00:33,  1.44it/s]Extractor Predicting: 51it [00:34,  1.45it/s]Extractor Predicting: 52it [00:35,  1.46it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.48it/s]Extractor Predicting: 55it [00:37,  1.52it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.50it/s]Extractor Predicting: 61it [00:41,  1.47it/s]Extractor Predicting: 62it [00:41,  1.47it/s]Extractor Predicting: 63it [00:42,  1.47it/s]Extractor Predicting: 64it [00:43,  1.48it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.49it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:46,  1.48it/s]Extractor Predicting: 70it [00:47,  1.47it/s]Extractor Predicting: 71it [00:47,  1.47it/s]Extractor Predicting: 72it [00:48,  1.48it/s]Extractor Predicting: 73it [00:49,  1.46it/s]Extractor Predicting: 74it [00:49,  1.47it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:51,  1.45it/s]Extractor Predicting: 78it [00:52,  1.47it/s]Extractor Predicting: 79it [00:53,  1.48it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:55,  1.50it/s]Extractor Predicting: 83it [00:55,  1.48it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:57,  1.48it/s]Extractor Predicting: 86it [00:58,  1.48it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [01:00,  1.49it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:01,  1.48it/s]Extractor Predicting: 92it [01:02,  1.47it/s]Extractor Predicting: 93it [01:02,  1.49it/s]Extractor Predicting: 94it [01:03,  1.50it/s]Extractor Predicting: 95it [01:04,  1.51it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:05,  1.46it/s]Extractor Predicting: 98it [01:06,  1.46it/s]Extractor Predicting: 99it [01:06,  1.45it/s]Extractor Predicting: 100it [01:07,  1.47it/s]Extractor Predicting: 101it [01:08,  1.48it/s]Extractor Predicting: 102it [01:08,  1.49it/s]Extractor Predicting: 103it [01:09,  1.49it/s]Extractor Predicting: 104it [01:10,  1.50it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:11,  1.48it/s]Extractor Predicting: 107it [01:12,  1.45it/s]Extractor Predicting: 108it [01:12,  1.44it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.47it/s]Extractor Predicting: 111it [01:14,  1.48it/s]Extractor Predicting: 112it [01:15,  1.47it/s]Extractor Predicting: 113it [01:16,  1.51it/s]Extractor Predicting: 114it [01:17,  1.37it/s]Extractor Predicting: 115it [01:17,  1.39it/s]Extractor Predicting: 116it [01:18,  1.41it/s]Extractor Predicting: 117it [01:19,  1.42it/s]Extractor Predicting: 118it [01:19,  1.43it/s]Extractor Predicting: 119it [01:20,  1.43it/s]Extractor Predicting: 120it [01:21,  1.44it/s]Extractor Predicting: 121it [01:21,  1.46it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.50it/s]Extractor Predicting: 124it [01:23,  1.46it/s]Extractor Predicting: 125it [01:24,  1.44it/s]Extractor Predicting: 126it [01:25,  1.45it/s]Extractor Predicting: 127it [01:26,  1.44it/s]Extractor Predicting: 128it [01:26,  1.44it/s]Extractor Predicting: 129it [01:27,  1.44it/s]Extractor Predicting: 130it [01:28,  1.43it/s]Extractor Predicting: 131it [01:28,  1.42it/s]Extractor Predicting: 132it [01:29,  1.41it/s]Extractor Predicting: 133it [01:30,  1.42it/s]Extractor Predicting: 134it [01:31,  1.42it/s]Extractor Predicting: 135it [01:31,  1.41it/s]Extractor Predicting: 136it [01:32,  1.43it/s]Extractor Predicting: 137it [01:33,  1.43it/s]Extractor Predicting: 138it [01:33,  1.45it/s]Extractor Predicting: 139it [01:34,  1.45it/s]Extractor Predicting: 140it [01:35,  1.46it/s]Extractor Predicting: 141it [01:35,  1.48it/s]Extractor Predicting: 142it [01:36,  1.48it/s]Extractor Predicting: 143it [01:37,  1.45it/s]Extractor Predicting: 144it [01:37,  1.48it/s]Extractor Predicting: 145it [01:38,  1.48it/s]Extractor Predicting: 146it [01:39,  1.48it/s]Extractor Predicting: 147it [01:39,  1.47it/s]Extractor Predicting: 148it [01:40,  1.52it/s]Extractor Predicting: 149it [01:41,  1.51it/s]Extractor Predicting: 150it [01:41,  1.50it/s]Extractor Predicting: 151it [01:42,  1.47it/s]Extractor Predicting: 152it [01:43,  1.52it/s]Extractor Predicting: 153it [01:43,  1.49it/s]Extractor Predicting: 154it [01:44,  1.49it/s]Extractor Predicting: 155it [01:45,  1.50it/s]Extractor Predicting: 156it [01:45,  1.49it/s]Extractor Predicting: 157it [01:46,  1.53it/s]Extractor Predicting: 158it [01:47,  1.53it/s]Extractor Predicting: 159it [01:47,  1.54it/s]Extractor Predicting: 160it [01:48,  1.49it/s]Extractor Predicting: 161it [01:49,  1.48it/s]Extractor Predicting: 162it [01:49,  1.50it/s]Extractor Predicting: 163it [01:50,  1.53it/s]Extractor Predicting: 164it [01:51,  1.54it/s]Extractor Predicting: 165it [01:51,  1.58it/s]Extractor Predicting: 166it [01:52,  1.58it/s]Extractor Predicting: 167it [01:52,  1.55it/s]Extractor Predicting: 168it [01:53,  1.53it/s]Extractor Predicting: 169it [01:54,  1.55it/s]Extractor Predicting: 170it [01:54,  1.56it/s]Extractor Predicting: 171it [01:55,  1.54it/s]Extractor Predicting: 172it [01:56,  1.52it/s]Extractor Predicting: 173it [01:56,  1.55it/s]Extractor Predicting: 174it [01:57,  1.51it/s]Extractor Predicting: 175it [01:58,  1.52it/s]Extractor Predicting: 176it [01:58,  1.52it/s]Extractor Predicting: 177it [01:59,  1.53it/s]Extractor Predicting: 178it [02:00,  1.52it/s]Extractor Predicting: 179it [02:00,  1.54it/s]Extractor Predicting: 180it [02:01,  1.52it/s]Extractor Predicting: 181it [02:02,  1.54it/s]Extractor Predicting: 182it [02:02,  1.52it/s]Extractor Predicting: 183it [02:03,  1.54it/s]Extractor Predicting: 184it [02:04,  1.55it/s]Extractor Predicting: 185it [02:04,  1.54it/s]Extractor Predicting: 186it [02:05,  1.50it/s]Extractor Predicting: 187it [02:06,  1.55it/s]Extractor Predicting: 188it [02:06,  1.37it/s]Extractor Predicting: 189it [02:07,  1.42it/s]Extractor Predicting: 190it [02:08,  1.47it/s]Extractor Predicting: 191it [02:08,  1.51it/s]Extractor Predicting: 192it [02:09,  1.53it/s]Extractor Predicting: 193it [02:10,  1.52it/s]Extractor Predicting: 194it [02:10,  1.54it/s]Extractor Predicting: 195it [02:11,  1.54it/s]Extractor Predicting: 196it [02:12,  1.51it/s]Extractor Predicting: 197it [02:12,  1.54it/s]Extractor Predicting: 198it [02:13,  1.52it/s]Extractor Predicting: 199it [02:14,  1.51it/s]Extractor Predicting: 200it [02:14,  1.57it/s]Extractor Predicting: 201it [02:15,  1.58it/s]Extractor Predicting: 202it [02:15,  1.59it/s]Extractor Predicting: 203it [02:16,  1.59it/s]Extractor Predicting: 204it [02:17,  1.58it/s]Extractor Predicting: 205it [02:17,  1.57it/s]Extractor Predicting: 206it [02:18,  1.60it/s]Extractor Predicting: 207it [02:19,  1.61it/s]Extractor Predicting: 208it [02:19,  1.60it/s]Extractor Predicting: 209it [02:20,  1.57it/s]Extractor Predicting: 210it [02:20,  1.58it/s]Extractor Predicting: 211it [02:21,  1.58it/s]Extractor Predicting: 212it [02:22,  1.58it/s]Extractor Predicting: 213it [02:22,  1.55it/s]Extractor Predicting: 214it [02:23,  1.58it/s]Extractor Predicting: 215it [02:24,  1.56it/s]Extractor Predicting: 216it [02:24,  1.55it/s]Extractor Predicting: 217it [02:25,  1.56it/s]Extractor Predicting: 218it [02:26,  1.57it/s]Extractor Predicting: 219it [02:26,  1.55it/s]Extractor Predicting: 220it [02:27,  1.54it/s]Extractor Predicting: 221it [02:28,  1.56it/s]Extractor Predicting: 222it [02:28,  1.54it/s]Extractor Predicting: 223it [02:29,  1.56it/s]Extractor Predicting: 224it [02:29,  1.57it/s]Extractor Predicting: 225it [02:30,  1.56it/s]Extractor Predicting: 226it [02:31,  1.56it/s]Extractor Predicting: 227it [02:31,  1.57it/s]Extractor Predicting: 228it [02:32,  1.72it/s]Extractor Predicting: 228it [02:32,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:45,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:45,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:45,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:45,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:45,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:43:46,483 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:43:46,484 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:43:46,758 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:43:47,796 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:43:47,796 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:50,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:50,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:50,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:50,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:43:50,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:43:50,654 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:43:50,655 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:43:50,921 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:43:51,083 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:43:51,083 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.07062600321027288,
  "recall": 0.028146489684951224,
  "score": 0.04025157232704402,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:09,  1.45it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:11,  1.49it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.46it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.44it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.44it/s]Extractor Predicting: 37it [00:25,  1.43it/s]Extractor Predicting: 38it [00:25,  1.41it/s]Extractor Predicting: 39it [00:26,  1.37it/s]Extractor Predicting: 40it [00:27,  1.40it/s]Extractor Predicting: 41it [00:27,  1.42it/s]Extractor Predicting: 42it [00:28,  1.43it/s]Extractor Predicting: 43it [00:29,  1.43it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.42it/s]Extractor Predicting: 46it [00:31,  1.46it/s]Extractor Predicting: 47it [00:31,  1.58it/s]Extractor Predicting: 48it [00:32,  1.59it/s]Extractor Predicting: 49it [00:33,  1.61it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:34,  1.57it/s]Extractor Predicting: 52it [00:35,  1.52it/s]Extractor Predicting: 53it [00:35,  1.44it/s]Extractor Predicting: 54it [00:36,  1.49it/s]Extractor Predicting: 55it [00:37,  1.54it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:38,  1.40it/s]Extractor Predicting: 58it [00:39,  1.42it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:41,  1.52it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:42,  1.50it/s]Extractor Predicting: 64it [00:43,  1.52it/s]Extractor Predicting: 65it [00:43,  1.54it/s]Extractor Predicting: 66it [00:44,  1.53it/s]Extractor Predicting: 67it [00:45,  1.53it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:46,  1.53it/s]Extractor Predicting: 70it [00:47,  1.54it/s]Extractor Predicting: 71it [00:47,  1.56it/s]Extractor Predicting: 72it [00:48,  1.56it/s]Extractor Predicting: 73it [00:49,  1.54it/s]Extractor Predicting: 74it [00:49,  1.53it/s]Extractor Predicting: 75it [00:50,  1.50it/s]Extractor Predicting: 76it [00:50,  1.54it/s]Extractor Predicting: 77it [00:51,  1.53it/s]Extractor Predicting: 78it [00:52,  1.51it/s]Extractor Predicting: 79it [00:52,  1.52it/s]Extractor Predicting: 80it [00:53,  1.51it/s]Extractor Predicting: 81it [00:54,  1.50it/s]Extractor Predicting: 82it [00:54,  1.51it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:56,  1.53it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:57,  1.53it/s]Extractor Predicting: 87it [00:58,  1.53it/s]Extractor Predicting: 88it [00:58,  1.54it/s]Extractor Predicting: 89it [00:59,  1.55it/s]Extractor Predicting: 90it [01:00,  1.55it/s]Extractor Predicting: 91it [01:00,  1.51it/s]Extractor Predicting: 92it [01:01,  1.52it/s]Extractor Predicting: 93it [01:02,  1.53it/s]Extractor Predicting: 94it [01:02,  1.53it/s]Extractor Predicting: 95it [01:03,  1.55it/s]Extractor Predicting: 96it [01:04,  1.54it/s]Extractor Predicting: 97it [01:04,  1.53it/s]Extractor Predicting: 98it [01:05,  1.53it/s]Extractor Predicting: 99it [01:06,  1.51it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:07,  1.54it/s]Extractor Predicting: 102it [01:08,  1.54it/s]Extractor Predicting: 103it [01:08,  1.55it/s]Extractor Predicting: 104it [01:09,  1.51it/s]Extractor Predicting: 105it [01:10,  1.51it/s]Extractor Predicting: 106it [01:10,  1.51it/s]Extractor Predicting: 107it [01:11,  1.54it/s]Extractor Predicting: 108it [01:11,  1.53it/s]Extractor Predicting: 109it [01:12,  1.53it/s]Extractor Predicting: 110it [01:13,  1.53it/s]Extractor Predicting: 111it [01:13,  1.54it/s]Extractor Predicting: 112it [01:14,  1.56it/s]Extractor Predicting: 113it [01:15,  1.57it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.54it/s]Extractor Predicting: 116it [01:17,  1.55it/s]Extractor Predicting: 117it [01:17,  1.58it/s]Extractor Predicting: 118it [01:18,  1.58it/s]Extractor Predicting: 119it [01:18,  1.62it/s]Extractor Predicting: 120it [01:19,  1.64it/s]Extractor Predicting: 121it [01:20,  1.63it/s]Extractor Predicting: 122it [01:20,  1.64it/s]Extractor Predicting: 123it [01:21,  1.65it/s]Extractor Predicting: 124it [01:21,  1.65it/s]Extractor Predicting: 125it [01:22,  1.59it/s]Extractor Predicting: 126it [01:23,  1.62it/s]Extractor Predicting: 127it [01:23,  1.57it/s]Extractor Predicting: 128it [01:24,  1.54it/s]Extractor Predicting: 129it [01:25,  1.54it/s]Extractor Predicting: 130it [01:25,  1.58it/s]Extractor Predicting: 131it [01:26,  1.56it/s]Extractor Predicting: 132it [01:27,  1.60it/s]Extractor Predicting: 133it [01:27,  1.59it/s]Extractor Predicting: 134it [01:28,  1.58it/s]Extractor Predicting: 135it [01:28,  1.60it/s]Extractor Predicting: 136it [01:29,  1.62it/s]Extractor Predicting: 137it [01:30,  1.60it/s]Extractor Predicting: 138it [01:30,  1.57it/s]Extractor Predicting: 139it [01:31,  1.57it/s]Extractor Predicting: 140it [01:32,  1.60it/s]Extractor Predicting: 141it [01:32,  1.61it/s]Extractor Predicting: 142it [01:33,  1.65it/s]Extractor Predicting: 143it [01:33,  1.63it/s]Extractor Predicting: 144it [01:34,  1.64it/s]Extractor Predicting: 145it [01:35,  1.61it/s]Extractor Predicting: 146it [01:35,  1.60it/s]Extractor Predicting: 147it [01:36,  1.61it/s]Extractor Predicting: 148it [01:37,  1.62it/s]Extractor Predicting: 149it [01:37,  1.56it/s]Extractor Predicting: 150it [01:38,  1.51it/s]Extractor Predicting: 151it [01:39,  1.54it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:40,  1.58it/s]Extractor Predicting: 154it [01:40,  1.55it/s]Extractor Predicting: 155it [01:41,  1.50it/s]Extractor Predicting: 156it [01:42,  1.51it/s]Extractor Predicting: 157it [01:43,  1.50it/s]Extractor Predicting: 158it [01:43,  1.54it/s]Extractor Predicting: 159it [01:44,  1.54it/s]Extractor Predicting: 160it [01:44,  1.56it/s]Extractor Predicting: 161it [01:45,  1.43it/s]Extractor Predicting: 162it [01:46,  1.48it/s]Extractor Predicting: 163it [01:47,  1.48it/s]Extractor Predicting: 164it [01:47,  1.52it/s]Extractor Predicting: 165it [01:48,  1.52it/s]Extractor Predicting: 166it [01:48,  1.55it/s]Extractor Predicting: 167it [01:49,  1.53it/s]Extractor Predicting: 168it [01:50,  1.54it/s]Extractor Predicting: 169it [01:50,  1.55it/s]Extractor Predicting: 170it [01:51,  1.54it/s]Extractor Predicting: 171it [01:52,  1.52it/s]Extractor Predicting: 172it [01:52,  1.52it/s]Extractor Predicting: 173it [01:53,  1.50it/s]Extractor Predicting: 174it [01:54,  1.51it/s]Extractor Predicting: 175it [01:54,  1.51it/s]Extractor Predicting: 176it [01:55,  1.52it/s]Extractor Predicting: 177it [01:56,  1.51it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:57,  1.51it/s]Extractor Predicting: 180it [01:58,  1.53it/s]Extractor Predicting: 181it [01:58,  1.56it/s]Extractor Predicting: 182it [01:59,  1.58it/s]Extractor Predicting: 183it [02:00,  1.57it/s]Extractor Predicting: 184it [02:00,  1.59it/s]Extractor Predicting: 185it [02:01,  1.59it/s]Extractor Predicting: 186it [02:01,  1.57it/s]Extractor Predicting: 187it [02:02,  1.60it/s]Extractor Predicting: 188it [02:03,  1.61it/s]Extractor Predicting: 189it [02:03,  1.56it/s]Extractor Predicting: 190it [02:04,  1.50it/s]Extractor Predicting: 191it [02:05,  1.52it/s]Extractor Predicting: 192it [02:05,  1.50it/s]Extractor Predicting: 193it [02:06,  1.49it/s]Extractor Predicting: 194it [02:07,  1.50it/s]Extractor Predicting: 195it [02:07,  1.47it/s]Extractor Predicting: 196it [02:08,  1.46it/s]Extractor Predicting: 197it [02:09,  1.48it/s]Extractor Predicting: 198it [02:09,  1.49it/s]Extractor Predicting: 199it [02:10,  1.48it/s]Extractor Predicting: 200it [02:11,  1.48it/s]Extractor Predicting: 201it [02:11,  1.47it/s]Extractor Predicting: 202it [02:12,  1.47it/s]Extractor Predicting: 203it [02:13,  1.48it/s]Extractor Predicting: 204it [02:14,  1.47it/s]Extractor Predicting: 205it [02:14,  1.46it/s]Extractor Predicting: 206it [02:15,  1.46it/s]Extractor Predicting: 207it [02:16,  1.42it/s]Extractor Predicting: 208it [02:16,  1.42it/s]Extractor Predicting: 209it [02:17,  1.48it/s]Extractor Predicting: 210it [02:18,  1.51it/s]Extractor Predicting: 211it [02:18,  1.51it/s]Extractor Predicting: 212it [02:19,  1.53it/s]Extractor Predicting: 213it [02:20,  1.51it/s]Extractor Predicting: 214it [02:20,  1.48it/s]Extractor Predicting: 215it [02:21,  1.48it/s]Extractor Predicting: 216it [02:22,  1.47it/s]Extractor Predicting: 217it [02:22,  1.44it/s]Extractor Predicting: 218it [02:23,  1.48it/s]Extractor Predicting: 219it [02:24,  1.49it/s]Extractor Predicting: 220it [02:24,  1.49it/s]Extractor Predicting: 221it [02:25,  1.51it/s]Extractor Predicting: 222it [02:26,  1.47it/s]Extractor Predicting: 223it [02:26,  1.44it/s]Extractor Predicting: 224it [02:27,  1.42it/s]Extractor Predicting: 225it [02:28,  1.41it/s]Extractor Predicting: 226it [02:29,  1.41it/s]Extractor Predicting: 227it [02:29,  1.39it/s]Extractor Predicting: 228it [02:30,  1.39it/s]Extractor Predicting: 229it [02:31,  1.38it/s]Extractor Predicting: 230it [02:31,  1.40it/s]Extractor Predicting: 231it [02:32,  1.40it/s]Extractor Predicting: 232it [02:33,  1.40it/s]Extractor Predicting: 233it [02:34,  1.40it/s]Extractor Predicting: 234it [02:34,  1.42it/s]Extractor Predicting: 235it [02:35,  1.42it/s]Extractor Predicting: 236it [02:36,  1.41it/s]Extractor Predicting: 237it [02:36,  1.38it/s]Extractor Predicting: 238it [02:37,  1.38it/s]Extractor Predicting: 239it [02:38,  1.41it/s]Extractor Predicting: 240it [02:39,  1.44it/s]Extractor Predicting: 241it [02:39,  1.45it/s]Extractor Predicting: 242it [02:40,  1.45it/s]Extractor Predicting: 243it [02:41,  1.46it/s]Extractor Predicting: 244it [02:42,  1.30it/s]Extractor Predicting: 245it [02:42,  1.38it/s]Extractor Predicting: 246it [02:43,  1.43it/s]Extractor Predicting: 246it [02:43,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:42,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:42,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:42,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:42,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:42,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:46:42,826 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:46:42,827 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:46:43,389 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:46:44,427 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:46:44,427 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:47,360 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:47,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:47,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:47,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:46:47,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:46:48,009 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:46:48,010 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:46:48,588 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:46:48,744 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:46:48,744 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3337229690239626,
  "recall": 0.1935593220338983,
  "score": 0.24501180004290923,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.44it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:09,  1.69it/s]Extractor Predicting: 15it [00:09,  1.50it/s]
[INFO|configuration_utils.py:515] 2023-08-29 01:46:59,471 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:46:59,471 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:46:59,475 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:46:59,475 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 01:46:59,477 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:47:02,377 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 01:47:02,378 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 01:47:02,389 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:47:02,390 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:47:02,394 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:02,398 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:02,398 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:02,398 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:02,398 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:02,398 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:02,398 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.49777777777777776,
  "recall": 0.15730337078651685,
  "score": 0.2390608324439701,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 01:47:02,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:03,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:04,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:04,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:05,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:06,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:06,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:07,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:08,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:08,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:09,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:10,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:10,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:11,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:12,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:12,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:13,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:14,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:14,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:15,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:10, 13.64s/it][WARNING|generation_utils.py:914] 2023-08-29 01:47:16,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:17,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:17,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:18,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:19,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:19,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:20,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:21,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:21,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:22,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:23,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:23,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:24,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:25,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:25,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:26,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:27,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:27,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:28,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:29,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:57, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-29 01:47:30,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:30,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:31,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:32,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:32,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:33,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:34,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:34,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:35,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:36,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:37,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:37,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:38,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:39,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:39,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:40,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:41,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:41,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:42,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:43,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:44,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:49, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-29 01:47:44,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:45,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:46,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:46,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:47,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:47,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:48,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:49,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:50,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:50,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:51,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:52,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:52,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:53,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:54,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:54,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:55,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:56,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:57,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:33, 13.92s/it][WARNING|generation_utils.py:914] 2023-08-29 01:47:58,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:58,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:59,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:59,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:00,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:00,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:01,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:02,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:02,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:03,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:03,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:04,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:04,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:05,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:06,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:06,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:07,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:07,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:08,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:09,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:09, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:09,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:10,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:10,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:11,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:12,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:12,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:14,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:15,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:15,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:16,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:17,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:17,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:18,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:19,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:19,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:20,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:21,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:21,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:22,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:23,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:21<02:01, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:24,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:24,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:25,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:25,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:26,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:27,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:27,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:28,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:28,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:29,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:30,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:30,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:31,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:32,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:32,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:33,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:33,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:34,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:35,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:36,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:34<01:45, 13.24s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:36,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:37,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:38,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:38,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:39,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:40,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:41,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:41,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:42,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:43,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:43,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:44,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:45,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:45,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:46,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:47,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:48,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:48,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:49,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:50,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:48<01:35, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:51,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:51,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:52,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:53,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:54,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:54,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:55,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:56,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:56,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:57,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:58,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:58,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:59,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:00,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:01,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:01,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:02,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:03,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:03,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:04,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:05,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:03<01:23, 13.98s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:05,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:06,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:07,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:08,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:08,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:09,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:10,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:11,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:11,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:12,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:13,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:14,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:14,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:15,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:16,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:17,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:18,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:18,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:19,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:20,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:21,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:19<01:12, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:21,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:22,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:23,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:23,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:24,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:25,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:25,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:26,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:27,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:27,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:28,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:29,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:29,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:30,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:31,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:32,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:32,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:33,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:34,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:34,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:32<00:57, 14.31s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:35,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:36,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:36,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:37,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:38,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:38,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:39,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:40,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:40,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:41,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:42,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:42,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:43,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:44,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:44,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:45,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:46,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:46,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:47,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:48,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:46<00:42, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:49,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:49,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:50,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:51,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:51,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:52,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:53,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:53,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:54,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:55,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:55,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:56,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:57,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:58,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:58,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:59,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:00,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:00,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:01,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:02,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:00<00:28, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:03,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:03,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:04,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:04,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:05,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:06,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:06,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:07,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:08,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:09,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:09,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:10,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:10,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:11,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:11,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:12,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:13,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:13,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:14,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:14,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:13<00:13, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:15,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:16,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:16,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:17,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:18,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:18,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:19,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:20,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:20,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:21,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:22,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:23,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:23,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:24,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:25,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:25,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:26,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:27,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:27,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:28,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:29,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:29,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:30,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:28<00:00, 14.22s/it]Generating: 100%|██████████| 15/15 [03:28<00:00, 13.91s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:37,118 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:37,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:37,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:37,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:37,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:50:37,868 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:50:37,869 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:50:38,143 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:50:39,205 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:50:39,205 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:40,917 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:40,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:40,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:40,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:50:40,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:50:41,658 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:50:41,659 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:50:41,932 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:50:42,100 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:50:42,100 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : member of .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.975, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.9703125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : main subject .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9330357142857143, 'errors': {'', "('second daughter of the second king', 'mother', '', 'She married the second wife of the second daughter of the second king , the second king ( d.')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : use .', 'success_rate': 0.946875, 'errors': {'', "('Windows Foundation', 'use', '', 'The Windows Foundation is a community based open source software and web development framework for the Windows operating system .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : voice type .', 'success_rate': 0.8206521739130435, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 7529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.27it/s]Extractor Estimating: 2it [00:01,  1.32it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.61it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:05,  1.65it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.65it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.65it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.68it/s]Extractor Estimating: 16it [00:09,  1.67it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:11,  1.69it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:14,  1.62it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.56it/s]Extractor Estimating: 28it [00:17,  1.57it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:18,  1.57it/s]Extractor Estimating: 31it [00:19,  1.48it/s]Extractor Estimating: 32it [00:20,  1.52it/s]Extractor Estimating: 33it [00:20,  1.53it/s]Extractor Estimating: 34it [00:21,  1.54it/s]Extractor Estimating: 35it [00:22,  1.55it/s]Extractor Estimating: 36it [00:22,  1.55it/s]Extractor Estimating: 37it [00:23,  1.54it/s]Extractor Estimating: 38it [00:23,  1.55it/s]Extractor Estimating: 39it [00:24,  1.54it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:25,  1.54it/s]Extractor Estimating: 42it [00:26,  1.56it/s]Extractor Estimating: 43it [00:27,  1.56it/s]Extractor Estimating: 44it [00:27,  1.53it/s]Extractor Estimating: 45it [00:28,  1.56it/s]Extractor Estimating: 46it [00:29,  1.56it/s]Extractor Estimating: 47it [00:29,  1.57it/s]Extractor Estimating: 48it [00:30,  1.55it/s]Extractor Estimating: 49it [00:31,  1.54it/s]Extractor Estimating: 50it [00:31,  1.54it/s]Extractor Estimating: 51it [00:32,  1.57it/s]Extractor Estimating: 52it [00:32,  1.57it/s]Extractor Estimating: 53it [00:33,  1.59it/s]Extractor Estimating: 54it [00:34,  1.61it/s]Extractor Estimating: 55it [00:34,  1.69it/s]Extractor Estimating: 56it [00:35,  1.69it/s]Extractor Estimating: 57it [00:35,  1.69it/s]Extractor Estimating: 58it [00:36,  1.69it/s]Extractor Estimating: 59it [00:37,  1.73it/s]Extractor Estimating: 60it [00:37,  1.67it/s]Extractor Estimating: 61it [00:38,  1.67it/s]Extractor Estimating: 62it [00:38,  1.69it/s]Extractor Estimating: 63it [00:39,  1.66it/s]Extractor Estimating: 64it [00:40,  1.62it/s]Extractor Estimating: 65it [00:40,  1.63it/s]Extractor Estimating: 66it [00:41,  1.64it/s]Extractor Estimating: 67it [00:41,  1.63it/s]Extractor Estimating: 68it [00:42,  1.68it/s]Extractor Estimating: 69it [00:43,  1.69it/s]Extractor Estimating: 70it [00:43,  1.70it/s]Extractor Estimating: 71it [00:44,  1.72it/s]Extractor Estimating: 72it [00:44,  1.67it/s]Extractor Estimating: 73it [00:45,  1.64it/s]Extractor Estimating: 74it [00:46,  1.65it/s]Extractor Estimating: 75it [00:46,  1.63it/s]Extractor Estimating: 76it [00:47,  1.66it/s]Extractor Estimating: 77it [00:47,  1.70it/s]Extractor Estimating: 78it [00:48,  1.65it/s]Extractor Estimating: 79it [00:49,  1.70it/s]Extractor Estimating: 80it [00:49,  1.70it/s]Extractor Estimating: 81it [00:50,  1.70it/s]Extractor Estimating: 82it [00:50,  1.67it/s]Extractor Estimating: 83it [00:51,  1.67it/s]Extractor Estimating: 84it [00:52,  1.68it/s]Extractor Estimating: 85it [00:52,  1.72it/s]Extractor Estimating: 86it [00:53,  1.71it/s]Extractor Estimating: 87it [00:53,  1.70it/s]Extractor Estimating: 88it [00:54,  1.71it/s]Extractor Estimating: 89it [00:54,  1.73it/s]Extractor Estimating: 90it [00:55,  1.69it/s]Extractor Estimating: 91it [00:56,  1.66it/s]Extractor Estimating: 92it [00:56,  1.72it/s]Extractor Estimating: 93it [00:57,  1.72it/s]Extractor Estimating: 94it [00:57,  1.68it/s]Extractor Estimating: 95it [00:58,  1.71it/s]Extractor Estimating: 96it [00:59,  1.72it/s]Extractor Estimating: 97it [00:59,  1.70it/s]Extractor Estimating: 98it [01:00,  1.69it/s]Extractor Estimating: 99it [01:00,  1.70it/s]Extractor Estimating: 100it [01:01,  1.71it/s]Extractor Estimating: 101it [01:01,  1.78it/s]Extractor Estimating: 102it [01:02,  1.81it/s]Extractor Estimating: 103it [01:02,  1.84it/s]Extractor Estimating: 104it [01:03,  1.89it/s]Extractor Estimating: 105it [01:03,  1.88it/s]Extractor Estimating: 106it [01:04,  1.90it/s]Extractor Estimating: 107it [01:04,  1.95it/s]Extractor Estimating: 108it [01:05,  1.92it/s]Extractor Estimating: 109it [01:06,  1.91it/s]Extractor Estimating: 110it [01:06,  1.97it/s]Extractor Estimating: 111it [01:07,  1.95it/s]Extractor Estimating: 112it [01:07,  1.97it/s]Extractor Estimating: 113it [01:08,  2.03it/s]Extractor Estimating: 114it [01:08,  2.02it/s]Extractor Estimating: 115it [01:09,  2.02it/s]Extractor Estimating: 116it [01:09,  1.98it/s]Extractor Estimating: 117it [01:10,  1.95it/s]Extractor Estimating: 118it [01:10,  1.96it/s]Extractor Estimating: 119it [01:11,  1.94it/s]Extractor Estimating: 120it [01:11,  1.90it/s]Extractor Estimating: 121it [01:12,  1.70it/s]Extractor Estimating: 122it [01:12,  1.78it/s]Extractor Estimating: 123it [01:13,  1.71it/s]Extractor Estimating: 124it [01:14,  1.76it/s]Extractor Estimating: 125it [01:14,  1.83it/s]Extractor Estimating: 126it [01:15,  1.74it/s]Extractor Estimating: 127it [01:15,  1.67it/s]Extractor Estimating: 128it [01:16,  1.65it/s]Extractor Estimating: 129it [01:17,  1.65it/s]Extractor Estimating: 130it [01:17,  1.66it/s]Extractor Estimating: 131it [01:18,  1.68it/s]Extractor Estimating: 132it [01:18,  1.65it/s]Extractor Estimating: 133it [01:19,  1.61it/s]Extractor Estimating: 134it [01:20,  1.63it/s]Extractor Estimating: 135it [01:20,  1.65it/s]Extractor Estimating: 136it [01:21,  1.66it/s]Extractor Estimating: 137it [01:21,  1.68it/s]Extractor Estimating: 138it [01:22,  1.69it/s]Extractor Estimating: 139it [01:23,  1.64it/s]Extractor Estimating: 140it [01:23,  1.64it/s]Extractor Estimating: 141it [01:24,  1.65it/s]Extractor Estimating: 142it [01:24,  1.62it/s]Extractor Estimating: 143it [01:25,  1.61it/s]Extractor Estimating: 144it [01:26,  1.64it/s]Extractor Estimating: 145it [01:26,  1.63it/s]Extractor Estimating: 146it [01:27,  1.64it/s]Extractor Estimating: 147it [01:28,  1.60it/s]Extractor Estimating: 148it [01:28,  1.63it/s]Extractor Estimating: 149it [01:29,  1.62it/s]Extractor Estimating: 150it [01:29,  1.66it/s]Extractor Estimating: 151it [01:30,  1.71it/s]Extractor Estimating: 152it [01:30,  1.69it/s]Extractor Estimating: 153it [01:31,  1.66it/s]Extractor Estimating: 154it [01:32,  1.66it/s]Extractor Estimating: 155it [01:32,  1.72it/s]Extractor Estimating: 156it [01:33,  1.74it/s]Extractor Estimating: 157it [01:33,  1.68it/s]Extractor Estimating: 158it [01:34,  1.67it/s]Extractor Estimating: 159it [01:35,  1.68it/s]Extractor Estimating: 160it [01:35,  1.69it/s]Extractor Estimating: 161it [01:36,  1.67it/s]Extractor Estimating: 162it [01:36,  1.67it/s]Extractor Estimating: 163it [01:37,  1.59it/s]Extractor Estimating: 164it [01:38,  1.62it/s]Extractor Estimating: 165it [01:38,  1.64it/s]Extractor Estimating: 166it [01:39,  1.60it/s]Extractor Estimating: 167it [01:40,  1.60it/s]Extractor Estimating: 168it [01:40,  1.63it/s]Extractor Estimating: 169it [01:41,  1.65it/s]Extractor Estimating: 170it [01:41,  1.65it/s]Extractor Estimating: 171it [01:42,  1.65it/s]Extractor Estimating: 172it [01:43,  1.63it/s]Extractor Estimating: 173it [01:43,  1.62it/s]Extractor Estimating: 174it [01:44,  1.56it/s]Extractor Estimating: 175it [01:45,  1.53it/s]Extractor Estimating: 176it [01:45,  1.51it/s]Extractor Estimating: 177it [01:46,  1.52it/s]Extractor Estimating: 178it [01:47,  1.52it/s]Extractor Estimating: 179it [01:47,  1.50it/s]Extractor Estimating: 180it [01:48,  1.54it/s]Extractor Estimating: 181it [01:49,  1.52it/s]Extractor Estimating: 182it [01:49,  1.47it/s]Extractor Estimating: 183it [01:50,  1.47it/s]Extractor Estimating: 184it [01:51,  1.48it/s]Extractor Estimating: 185it [01:51,  1.53it/s]Extractor Estimating: 186it [01:52,  1.53it/s]Extractor Estimating: 187it [01:53,  1.55it/s]Extractor Estimating: 188it [01:53,  1.55it/s]Extractor Estimating: 189it [01:54,  1.55it/s]Extractor Estimating: 190it [01:54,  1.56it/s]Extractor Estimating: 191it [01:55,  1.50it/s]Extractor Estimating: 192it [01:56,  1.51it/s]Extractor Estimating: 193it [01:57,  1.51it/s]Extractor Estimating: 194it [01:57,  1.50it/s]Extractor Estimating: 195it [01:58,  1.45it/s]Extractor Estimating: 196it [01:59,  1.32it/s]Extractor Estimating: 197it [02:00,  1.38it/s]Extractor Estimating: 198it [02:00,  1.42it/s]Extractor Estimating: 199it [02:01,  1.46it/s]Extractor Estimating: 200it [02:02,  1.43it/s]Extractor Estimating: 201it [02:02,  1.44it/s]Extractor Estimating: 202it [02:03,  1.49it/s]Extractor Estimating: 203it [02:03,  1.50it/s]Extractor Estimating: 204it [02:04,  1.57it/s]Extractor Estimating: 205it [02:05,  1.62it/s]Extractor Estimating: 206it [02:05,  1.68it/s]Extractor Estimating: 207it [02:06,  1.68it/s]Extractor Estimating: 208it [02:06,  1.63it/s]Extractor Estimating: 209it [02:07,  1.68it/s]Extractor Estimating: 210it [02:08,  1.64it/s]Extractor Estimating: 211it [02:08,  1.68it/s]Extractor Estimating: 212it [02:09,  1.73it/s]Extractor Estimating: 213it [02:09,  1.67it/s]Extractor Estimating: 214it [02:10,  1.69it/s]Extractor Estimating: 215it [02:11,  1.69it/s]Extractor Estimating: 216it [02:11,  1.70it/s]Extractor Estimating: 217it [02:12,  1.64it/s]Extractor Estimating: 218it [02:12,  1.65it/s]Extractor Estimating: 219it [02:13,  1.68it/s]Extractor Estimating: 220it [02:13,  1.71it/s]Extractor Estimating: 221it [02:14,  1.71it/s]Extractor Estimating: 222it [02:15,  1.71it/s]Extractor Estimating: 223it [02:15,  1.71it/s]Extractor Estimating: 224it [02:16,  1.76it/s]Extractor Estimating: 225it [02:16,  1.77it/s]Extractor Estimating: 226it [02:17,  1.74it/s]Extractor Estimating: 227it [02:18,  1.66it/s]Extractor Estimating: 228it [02:18,  1.63it/s]Extractor Estimating: 229it [02:19,  1.63it/s]Extractor Estimating: 230it [02:19,  1.61it/s]Extractor Estimating: 231it [02:20,  1.59it/s]Extractor Estimating: 232it [02:21,  1.55it/s]Extractor Estimating: 233it [02:21,  1.52it/s]Extractor Estimating: 234it [02:22,  1.56it/s]Extractor Estimating: 235it [02:23,  1.51it/s]Extractor Estimating: 236it [02:24,  1.46it/s]Extractor Estimating: 237it [02:24,  1.48it/s]Extractor Estimating: 238it [02:25,  1.46it/s]Extractor Estimating: 239it [02:26,  1.46it/s]Extractor Estimating: 240it [02:26,  1.49it/s]Extractor Estimating: 241it [02:27,  1.53it/s]Extractor Estimating: 242it [02:27,  1.55it/s]Extractor Estimating: 243it [02:28,  1.53it/s]Extractor Estimating: 244it [02:29,  1.46it/s]Extractor Estimating: 245it [02:30,  1.47it/s]Extractor Estimating: 246it [02:30,  1.49it/s]Extractor Estimating: 247it [02:31,  1.51it/s]Extractor Estimating: 248it [02:32,  1.51it/s]Extractor Estimating: 249it [02:32,  1.57it/s]Extractor Estimating: 250it [02:33,  1.64it/s]Extractor Estimating: 251it [02:33,  1.64it/s]Extractor Estimating: 252it [02:34,  1.62it/s]Extractor Estimating: 253it [02:35,  1.60it/s]Extractor Estimating: 254it [02:35,  1.60it/s]Extractor Estimating: 255it [02:36,  1.61it/s]Extractor Estimating: 256it [02:36,  1.62it/s]Extractor Estimating: 257it [02:37,  1.65it/s]Extractor Estimating: 258it [02:38,  1.62it/s]Extractor Estimating: 259it [02:38,  1.64it/s]Extractor Estimating: 260it [02:39,  1.54it/s]Extractor Estimating: 261it [02:40,  1.59it/s]Extractor Estimating: 262it [02:40,  1.62it/s]Extractor Estimating: 263it [02:41,  1.61it/s]Extractor Estimating: 264it [02:41,  1.57it/s]Extractor Estimating: 265it [02:42,  1.57it/s]Extractor Estimating: 266it [02:43,  1.58it/s]Extractor Estimating: 267it [02:43,  1.62it/s]Extractor Estimating: 268it [02:44,  1.63it/s]Extractor Estimating: 269it [02:44,  1.64it/s]Extractor Estimating: 270it [02:45,  1.65it/s]Extractor Estimating: 271it [02:46,  1.61it/s]Extractor Estimating: 272it [02:46,  1.61it/s]Extractor Estimating: 273it [02:47,  1.46it/s]Extractor Estimating: 274it [02:48,  1.52it/s]Extractor Estimating: 275it [02:48,  1.52it/s]Extractor Estimating: 276it [02:49,  1.59it/s]Extractor Estimating: 277it [02:50,  1.60it/s]Extractor Estimating: 278it [02:50,  1.63it/s]Extractor Estimating: 279it [02:51,  1.63it/s]Extractor Estimating: 280it [02:51,  1.64it/s]Extractor Estimating: 281it [02:52,  1.66it/s]Extractor Estimating: 282it [02:53,  1.68it/s]Extractor Estimating: 283it [02:53,  1.67it/s]Extractor Estimating: 284it [02:54,  1.65it/s]Extractor Estimating: 285it [02:54,  1.61it/s]Extractor Estimating: 286it [02:55,  1.67it/s]Extractor Estimating: 287it [02:56,  1.66it/s]Extractor Estimating: 288it [02:56,  1.67it/s]Extractor Estimating: 289it [02:57,  1.67it/s]Extractor Estimating: 290it [02:57,  1.69it/s]Extractor Estimating: 291it [02:58,  1.69it/s]Extractor Estimating: 292it [02:59,  1.70it/s]Extractor Estimating: 293it [02:59,  1.68it/s]Extractor Estimating: 294it [03:00,  1.68it/s]Extractor Estimating: 295it [03:00,  1.67it/s]Extractor Estimating: 296it [03:01,  1.69it/s]Extractor Estimating: 297it [03:02,  1.65it/s]Extractor Estimating: 298it [03:02,  1.69it/s]Extractor Estimating: 299it [03:03,  1.73it/s]Extractor Estimating: 300it [03:03,  1.66it/s]Extractor Estimating: 301it [03:04,  1.65it/s]Extractor Estimating: 302it [03:05,  1.63it/s]Extractor Estimating: 303it [03:05,  1.66it/s]Extractor Estimating: 304it [03:06,  1.68it/s]Extractor Estimating: 305it [03:06,  1.67it/s]Extractor Estimating: 306it [03:07,  1.67it/s]Extractor Estimating: 307it [03:08,  1.69it/s]Extractor Estimating: 308it [03:08,  1.73it/s]Extractor Estimating: 309it [03:09,  1.71it/s]Extractor Estimating: 310it [03:09,  1.73it/s]Extractor Estimating: 311it [03:10,  1.70it/s]Extractor Estimating: 312it [03:10,  1.70it/s]Extractor Estimating: 313it [03:11,  1.64it/s]Extractor Estimating: 314it [03:12,  1.65it/s]Extractor Estimating: 315it [03:12,  1.63it/s]Extractor Estimating: 316it [03:13,  1.58it/s]Extractor Estimating: 317it [03:14,  1.64it/s]Extractor Estimating: 318it [03:14,  1.70it/s]Extractor Estimating: 319it [03:15,  1.65it/s]Extractor Estimating: 320it [03:15,  1.61it/s]Extractor Estimating: 321it [03:16,  1.63it/s]Extractor Estimating: 322it [03:17,  1.64it/s]Extractor Estimating: 323it [03:17,  1.59it/s]Extractor Estimating: 324it [03:18,  1.65it/s]Extractor Estimating: 325it [03:18,  1.72it/s]Extractor Estimating: 326it [03:19,  1.75it/s]Extractor Estimating: 327it [03:19,  1.79it/s]Extractor Estimating: 328it [03:20,  1.75it/s]Extractor Estimating: 329it [03:21,  1.76it/s]Extractor Estimating: 330it [03:21,  1.78it/s]Extractor Estimating: 331it [03:22,  1.84it/s]Extractor Estimating: 332it [03:22,  1.87it/s]Extractor Estimating: 333it [03:23,  1.85it/s]Extractor Estimating: 334it [03:23,  1.80it/s]Extractor Estimating: 335it [03:24,  1.87it/s]Extractor Estimating: 336it [03:24,  1.78it/s]Extractor Estimating: 337it [03:25,  1.82it/s]Extractor Estimating: 338it [03:26,  1.78it/s]Extractor Estimating: 339it [03:26,  1.77it/s]Extractor Estimating: 340it [03:27,  1.78it/s]Extractor Estimating: 341it [03:27,  1.79it/s]Extractor Estimating: 342it [03:28,  1.79it/s]Extractor Estimating: 343it [03:28,  1.65it/s]Extractor Estimating: 344it [03:29,  1.69it/s]Extractor Estimating: 345it [03:30,  1.74it/s]Extractor Estimating: 346it [03:30,  1.74it/s]Extractor Estimating: 347it [03:31,  1.69it/s]Extractor Estimating: 348it [03:31,  1.74it/s]Extractor Estimating: 349it [03:32,  1.78it/s]Extractor Estimating: 350it [03:33,  1.68it/s]Extractor Estimating: 351it [03:33,  1.68it/s]Extractor Estimating: 352it [03:34,  1.69it/s]Extractor Estimating: 353it [03:34,  1.70it/s]Extractor Estimating: 354it [03:35,  1.71it/s]Extractor Estimating: 355it [03:35,  1.69it/s]Extractor Estimating: 356it [03:36,  1.69it/s]Extractor Estimating: 357it [03:37,  1.67it/s]Extractor Estimating: 358it [03:37,  1.63it/s]Extractor Estimating: 359it [03:38,  1.62it/s]Extractor Estimating: 360it [03:39,  1.62it/s]Extractor Estimating: 361it [03:39,  1.66it/s]Extractor Estimating: 362it [03:40,  1.65it/s]Extractor Estimating: 363it [03:40,  1.64it/s]Extractor Estimating: 364it [03:41,  1.65it/s]Extractor Estimating: 365it [03:42,  1.61it/s]Extractor Estimating: 366it [03:42,  1.62it/s]Extractor Estimating: 367it [03:43,  1.60it/s]Extractor Estimating: 368it [03:44,  1.57it/s]Extractor Estimating: 369it [03:44,  1.61it/s]Extractor Estimating: 370it [03:45,  1.61it/s]Extractor Estimating: 371it [03:45,  1.63it/s]Extractor Estimating: 372it [03:46,  1.63it/s]Extractor Estimating: 373it [03:47,  1.60it/s]Extractor Estimating: 374it [03:47,  1.59it/s]Extractor Estimating: 375it [03:48,  1.84it/s]Extractor Estimating: 375it [03:48,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:48,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:48,094 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:48,094 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:48,094 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:48,094 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:54:48,398 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:54:48,399 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:54:49,080 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:54:50,130 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:54:50,131 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:51,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:51,898 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:51,898 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:51,898 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:54:51,898 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:54:52,227 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:54:52,229 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:54:52,898 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:54:53,056 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:54:53,057 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 04:31:22,336 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 04:31:22,365 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7487 mean pseudo reward: 0.9583622974173933
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 18539
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18639, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18639, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.092, loss:573.4024
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.073, loss:567.0883
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.093, loss:557.3201
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.096, loss:521.7025
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.075, loss:530.5284
>> valid entity prec:0.5213, rec:0.4565, f1:0.4868
>> valid relation prec:0.0222, rec:0.0109, f1:0.0146
>> valid relation with NER prec:0.0222, rec:0.0109, f1:0.0146
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.246, loss:525.6257
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.086, loss:513.1041
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.108, loss:516.5552
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.104, loss:529.7734
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.088, loss:494.1164
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5065, rec:0.4883, f1:0.4972
>> valid relation prec:0.0304, rec:0.0152, f1:0.0203
>> valid relation with NER prec:0.0304, rec:0.0152, f1:0.0203
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 3.251, loss:511.7088
g_step 1200, step 264, avg_time 1.107, loss:505.4645
g_step 1300, step 52, avg_time 1.095, loss:464.3757
g_step 1400, step 152, avg_time 1.089, loss:471.2130
g_step 1500, step 252, avg_time 1.089, loss:500.8226
>> valid entity prec:0.4590, rec:0.4652, f1:0.4621
>> valid relation prec:0.0307, rec:0.0136, f1:0.0189
>> valid relation with NER prec:0.0307, rec:0.0136, f1:0.0189
g_step 1600, step 40, avg_time 3.233, loss:483.5173
g_step 1700, step 140, avg_time 1.086, loss:466.6565
g_step 1800, step 240, avg_time 1.091, loss:451.6010
g_step 1900, step 28, avg_time 1.079, loss:446.6930
g_step 2000, step 128, avg_time 1.110, loss:430.5390
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4898, rec:0.4634, f1:0.4762
>> valid relation prec:0.0297, rec:0.0142, f1:0.0193
>> valid relation with NER prec:0.0297, rec:0.0142, f1:0.0193
g_step 2100, step 228, avg_time 3.233, loss:445.7391
g_step 2200, step 16, avg_time 1.070, loss:437.4922
g_step 2300, step 116, avg_time 1.086, loss:392.8354
g_step 2400, step 216, avg_time 1.098, loss:440.1684
g_step 2500, step 4, avg_time 1.082, loss:434.5868
>> valid entity prec:0.5224, rec:0.4703, f1:0.4950
>> valid relation prec:0.0273, rec:0.0133, f1:0.0179
>> valid relation with NER prec:0.0273, rec:0.0133, f1:0.0179
g_step 2600, step 104, avg_time 3.232, loss:389.4065
g_step 2700, step 204, avg_time 1.081, loss:404.8768
g_step 2800, step 304, avg_time 1.096, loss:429.8422
g_step 2900, step 92, avg_time 1.085, loss:378.3554
g_step 3000, step 192, avg_time 1.099, loss:404.4675
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5093, rec:0.4703, f1:0.4890
>> valid relation prec:0.0329, rec:0.0174, f1:0.0228
>> valid relation with NER prec:0.0329, rec:0.0174, f1:0.0228
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 292, avg_time 3.245, loss:408.6570
g_step 3200, step 80, avg_time 1.099, loss:390.2753
g_step 3300, step 180, avg_time 1.094, loss:364.4539
g_step 3400, step 280, avg_time 1.091, loss:393.6080
g_step 3500, step 68, avg_time 1.094, loss:362.2452
>> valid entity prec:0.5007, rec:0.4600, f1:0.4794
>> valid relation prec:0.0294, rec:0.0139, f1:0.0189
>> valid relation with NER prec:0.0294, rec:0.0139, f1:0.0189
g_step 3600, step 168, avg_time 3.242, loss:363.4132
g_step 3700, step 268, avg_time 1.095, loss:374.1993
g_step 3800, step 56, avg_time 1.090, loss:378.9559
g_step 3900, step 156, avg_time 1.092, loss:350.6602
g_step 4000, step 256, avg_time 1.098, loss:359.0429
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4983, rec:0.4731, f1:0.4854
>> valid relation prec:0.0248, rec:0.0125, f1:0.0166
>> valid relation with NER prec:0.0248, rec:0.0125, f1:0.0166
g_step 4100, step 44, avg_time 3.239, loss:342.4600
g_step 4200, step 144, avg_time 1.088, loss:348.8590
g_step 4300, step 244, avg_time 1.084, loss:341.8978
g_step 4400, step 32, avg_time 1.087, loss:356.1487
g_step 4500, step 132, avg_time 1.101, loss:317.8840
>> valid entity prec:0.5092, rec:0.4704, f1:0.4890
>> valid relation prec:0.0384, rec:0.0208, f1:0.0270
>> valid relation with NER prec:0.0384, rec:0.0208, f1:0.0270
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 232, avg_time 3.238, loss:333.0204
g_step 4700, step 20, avg_time 1.097, loss:336.7210
g_step 4800, step 120, avg_time 1.091, loss:311.8852
g_step 4900, step 220, avg_time 1.087, loss:331.0977
g_step 5000, step 8, avg_time 1.093, loss:335.3309
learning rate was adjusted to 0.0008
>> valid entity prec:0.4911, rec:0.4806, f1:0.4858
>> valid relation prec:0.0419, rec:0.0234, f1:0.0300
>> valid relation with NER prec:0.0419, rec:0.0234, f1:0.0300
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 108, avg_time 3.225, loss:313.1954
g_step 5200, step 208, avg_time 1.105, loss:304.7158
g_step 5300, step 308, avg_time 1.089, loss:334.3937
g_step 5400, step 96, avg_time 1.099, loss:281.7773
g_step 5500, step 196, avg_time 1.092, loss:301.1733
>> valid entity prec:0.5014, rec:0.4401, f1:0.4688
>> valid relation prec:0.0344, rec:0.0162, f1:0.0220
>> valid relation with NER prec:0.0344, rec:0.0162, f1:0.0220
g_step 5600, step 296, avg_time 3.242, loss:314.2938
g_step 5700, step 84, avg_time 1.086, loss:287.1273
g_step 5800, step 184, avg_time 1.090, loss:297.0388
g_step 5900, step 284, avg_time 1.081, loss:296.7668
g_step 6000, step 72, avg_time 1.076, loss:282.4022
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5104, rec:0.4497, f1:0.4781
>> valid relation prec:0.0383, rec:0.0194, f1:0.0257
>> valid relation with NER prec:0.0383, rec:0.0194, f1:0.0257
g_step 6100, step 172, avg_time 3.255, loss:285.5662
g_step 6200, step 272, avg_time 1.092, loss:285.8258
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:31:22 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:31:22 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-31-22_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:31:23 - WARNING - datasets.builder -   Using custom data configuration default-78697464bea69ad5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-78697464bea69ad5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:31:23,649 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:31:23,650 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:31:23,651 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:31:23,652 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:31:23,660 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:31:23,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:31:23,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:31:23,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:31:23,666 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:31:23,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:31:23,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:31:23,804 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:31:26,865 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:31:26,866 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-78697464bea69ad5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.12ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.99ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.38ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.56ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.67ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.74ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.79ba/s]100%|██████████| 8/8 [00:01<00:00,  5.70ba/s]100%|██████████| 8/8 [00:01<00:00,  4.85ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.11ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.34ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.40ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.45ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.46ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.46ba/s]100%|██████████| 7/7 [00:01<00:00,  4.94ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.02ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.56ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.22ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.15ba/s]100%|██████████| 8/8 [00:00<00:00,  9.22ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.82ba/s] 43%|████▎     | 3/7 [00:00<00:00, 10.18ba/s] 71%|███████▏  | 5/7 [00:00<00:00, 10.45ba/s]100%|██████████| 7/7 [00:00<00:00, 12.40ba/s]100%|██████████| 7/7 [00:00<00:00, 11.55ba/s]
[INFO|trainer.py:414] 2023-08-29 04:31:31,816 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:31:31,828 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:31:31,829 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 04:31:31,829 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:31:31,829 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:31:31,829 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:31:31,829 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:31:31,829 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:49,  3.43it/s]  1%|          | 3/585 [00:00<02:48,  3.46it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:45,  3.48it/s]  1%|▏         | 8/585 [00:02<02:45,  3.48it/s]  2%|▏         | 9/585 [00:02<02:45,  3.48it/s]  2%|▏         | 10/585 [00:02<02:44,  3.49it/s]  2%|▏         | 11/585 [00:03<02:44,  3.49it/s]  2%|▏         | 12/585 [00:03<02:44,  3.49it/s]  2%|▏         | 13/585 [00:03<02:44,  3.48it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.49it/s]  3%|▎         | 17/585 [00:04<02:42,  3.49it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:42,  3.49it/s]  3%|▎         | 20/585 [00:05<02:41,  3.49it/s]  4%|▎         | 21/585 [00:06<02:41,  3.49it/s]  4%|▍         | 22/585 [00:06<02:41,  3.49it/s]  4%|▍         | 23/585 [00:06<02:41,  3.49it/s]  4%|▍         | 24/585 [00:06<02:40,  3.49it/s]  4%|▍         | 25/585 [00:07<02:40,  3.49it/s]  4%|▍         | 26/585 [00:07<02:40,  3.49it/s]  5%|▍         | 27/585 [00:07<02:40,  3.49it/s]  5%|▍         | 28/585 [00:08<02:39,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:38,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:38,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.48it/s]  7%|▋         | 39/585 [00:11<02:36,  3.48it/s]  7%|▋         | 40/585 [00:11<02:36,  3.48it/s]  7%|▋         | 41/585 [00:11<02:36,  3.48it/s]  7%|▋         | 42/585 [00:12<02:36,  3.48it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:12<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:34,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:33,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.48it/s]  9%|▉         | 52/585 [00:14<02:33,  3.48it/s]  9%|▉         | 53/585 [00:15<02:32,  3.48it/s]  9%|▉         | 54/585 [00:15<02:32,  3.48it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.48it/s] 10%|█         | 59/585 [00:16<02:31,  3.48it/s] 10%|█         | 60/585 [00:17<02:30,  3.48it/s] 10%|█         | 61/585 [00:17<02:30,  3.48it/s] 11%|█         | 62/585 [00:17<02:30,  3.48it/s] 11%|█         | 63/585 [00:18<02:30,  3.48it/s] 11%|█         | 64/585 [00:18<02:29,  3.48it/s] 11%|█         | 65/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 66/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 71/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:20<02:27,  3.48it/s] 13%|█▎        | 74/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.48it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 80/585 [00:22<02:25,  3.48it/s] 14%|█▍        | 81/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.48it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.45it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:22,  3.47it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:19,  3.48it/s] 17%|█▋        | 100/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.48it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 104/585 [00:29<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 106/585 [00:30<02:17,  3.47it/s] 18%|█▊        | 107/585 [00:30<02:17,  3.47it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.47it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.47it/s] 19%|█▉        | 110/585 [00:31<02:16,  3.47it/s] 19%|█▉        | 111/585 [00:31<02:16,  3.48it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 113/585 [00:32<02:15,  3.48it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.48it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.48it/s] 20%|█▉        | 116/585 [00:33<02:14,  3.48it/s] 20%|██        | 117/585 [00:33<02:14,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 04:32:05,530 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:32:05,530 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 04:32:05,530 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 58.20it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.04it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.43it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.61it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.33it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.96it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.62it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.11it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.14it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.21it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.31it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.24it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.41it/s][A
  9%|▉         | 73/782 [00:01<00:14, 47.28it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.34it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.37it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.99it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.97it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.10it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.17it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.24it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.23it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.35it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.27it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.25it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.17it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.01it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.96it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.14it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.22it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.22it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.21it/s][A
 21%|██▏       | 168/782 [00:03<00:12, 47.32it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.29it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.07it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.10it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.04it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.03it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.20it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.30it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.17it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.24it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.31it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.19it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.94it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.00it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.99it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.08it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.17it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.26it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.33it/s][A
 34%|███▎      | 263/782 [00:05<00:10, 47.31it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.23it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 47.10it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 47.09it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 47.01it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.06it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.13it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.21it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.27it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.30it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.25it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.13it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.03it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 47.06it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.98it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.99it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.10it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.24it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.29it/s][A
 46%|████▌     | 358/782 [00:07<00:08, 47.22it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 47.19it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.11it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 47.14it/s][A
 48%|████▊     | 378/782 [00:07<00:08, 47.06it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 47.08it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.03it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.17it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.24it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.25it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.26it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.13it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.09it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 47.11it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 47.09it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.09it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.05it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.12it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.17it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.24it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.20it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 47.17it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.15it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.05it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.04it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.11it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.02it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.05it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.14it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.14it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.23it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.08it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 47.00it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.05it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.02it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.08it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.10it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.06it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.01it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.11it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.09it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.01it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.02it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.98it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.03it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.07it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.05it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.07it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.03it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.92it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.07it/s][A
 78%|███████▊  | 613/782 [00:12<00:03, 47.08it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 47.00it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.05it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.06it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.99it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.05it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.09it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 47.03it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.01it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 47.08it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.98it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.05it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.01it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 47.07it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.09it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.09it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.05it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 47.05it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 47.01it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.06it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.08it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 47.06it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.99it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.90it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.14it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.03it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.04it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 47.04it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 46.95it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 47.01it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.02it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.01it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.99it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.10it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.10it/s][A 20%|██        | 117/585 [00:50<02:14,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:32:22,164 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 04:32:22,188 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:32:24,532 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:32:24,557 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:32:24,573 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:57<57:57,  7.45s/it] 20%|██        | 119/585 [00:58<41:09,  5.30s/it] 21%|██        | 120/585 [00:58<29:25,  3.80s/it] 21%|██        | 121/585 [00:58<21:13,  2.74s/it] 21%|██        | 122/585 [00:58<15:29,  2.01s/it] 21%|██        | 123/585 [00:59<11:28,  1.49s/it] 21%|██        | 124/585 [00:59<08:40,  1.13s/it] 21%|██▏       | 125/585 [00:59<06:43,  1.14it/s] 22%|██▏       | 126/585 [01:00<05:21,  1.43it/s] 22%|██▏       | 127/585 [01:00<04:23,  1.74it/s] 22%|██▏       | 128/585 [01:00<03:43,  2.04it/s] 22%|██▏       | 129/585 [01:00<03:15,  2.33it/s] 22%|██▏       | 130/585 [01:01<02:56,  2.58it/s] 22%|██▏       | 131/585 [01:01<02:42,  2.80it/s] 23%|██▎       | 132/585 [01:01<02:32,  2.97it/s] 23%|██▎       | 133/585 [01:02<02:25,  3.11it/s] 23%|██▎       | 134/585 [01:02<02:20,  3.21it/s] 23%|██▎       | 135/585 [01:02<02:16,  3.29it/s] 23%|██▎       | 136/585 [01:02<02:14,  3.34it/s] 23%|██▎       | 137/585 [01:03<02:12,  3.38it/s] 24%|██▎       | 138/585 [01:03<02:11,  3.41it/s] 24%|██▍       | 139/585 [01:03<02:10,  3.43it/s] 24%|██▍       | 140/585 [01:04<02:09,  3.44it/s] 24%|██▍       | 141/585 [01:04<02:09,  3.43it/s] 24%|██▍       | 142/585 [01:04<02:08,  3.44it/s] 24%|██▍       | 143/585 [01:05<02:07,  3.46it/s] 25%|██▍       | 144/585 [01:05<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:05<02:06,  3.47it/s] 25%|██▍       | 146/585 [01:05<02:06,  3.47it/s] 25%|██▌       | 147/585 [01:06<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:06<02:05,  3.47it/s] 25%|██▌       | 149/585 [01:06<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:07<02:05,  3.48it/s] 26%|██▌       | 151/585 [01:07<02:04,  3.48it/s] 26%|██▌       | 152/585 [01:07<02:04,  3.47it/s] 26%|██▌       | 153/585 [01:07<02:04,  3.47it/s] 26%|██▋       | 154/585 [01:08<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:08<02:03,  3.47it/s] 27%|██▋       | 156/585 [01:08<02:03,  3.48it/s] 27%|██▋       | 157/585 [01:09<02:03,  3.48it/s] 27%|██▋       | 158/585 [01:09<02:02,  3.48it/s] 27%|██▋       | 159/585 [01:09<02:02,  3.48it/s] 27%|██▋       | 160/585 [01:09<02:02,  3.48it/s] 28%|██▊       | 161/585 [01:10<02:01,  3.48it/s] 28%|██▊       | 162/585 [01:10<02:01,  3.48it/s] 28%|██▊       | 163/585 [01:10<02:01,  3.47it/s] 28%|██▊       | 164/585 [01:11<02:01,  3.47it/s] 28%|██▊       | 165/585 [01:11<02:00,  3.47it/s] 28%|██▊       | 166/585 [01:11<02:00,  3.47it/s] 29%|██▊       | 167/585 [01:11<02:00,  3.48it/s] 29%|██▊       | 168/585 [01:12<01:59,  3.48it/s] 29%|██▉       | 169/585 [01:12<01:59,  3.47it/s] 29%|██▉       | 170/585 [01:12<01:59,  3.47it/s] 29%|██▉       | 171/585 [01:13<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:13<01:58,  3.47it/s] 30%|██▉       | 173/585 [01:13<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:13<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:14<01:58,  3.47it/s] 30%|███       | 176/585 [01:14<01:57,  3.47it/s] 30%|███       | 177/585 [01:14<01:57,  3.47it/s] 30%|███       | 178/585 [01:15<01:57,  3.47it/s] 31%|███       | 179/585 [01:15<01:56,  3.47it/s] 31%|███       | 180/585 [01:15<01:56,  3.47it/s] 31%|███       | 181/585 [01:15<01:56,  3.47it/s] 31%|███       | 182/585 [01:16<01:55,  3.48it/s] 31%|███▏      | 183/585 [01:16<01:55,  3.48it/s] 31%|███▏      | 184/585 [01:16<01:55,  3.48it/s] 32%|███▏      | 185/585 [01:17<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:17<01:55,  3.47it/s] 32%|███▏      | 187/585 [01:17<01:54,  3.46it/s] 32%|███▏      | 188/585 [01:17<01:54,  3.47it/s] 32%|███▏      | 189/585 [01:18<01:54,  3.47it/s] 32%|███▏      | 190/585 [01:18<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:18<01:53,  3.47it/s] 33%|███▎      | 192/585 [01:19<01:53,  3.48it/s] 33%|███▎      | 193/585 [01:19<01:52,  3.48it/s] 33%|███▎      | 194/585 [01:19<01:52,  3.48it/s] 33%|███▎      | 195/585 [01:19<01:52,  3.47it/s] 34%|███▎      | 196/585 [01:20<01:51,  3.47it/s] 34%|███▎      | 197/585 [01:20<01:53,  3.43it/s] 34%|███▍      | 198/585 [01:20<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:21<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:21<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:21<01:50,  3.46it/s] 35%|███▍      | 202/585 [01:22<01:50,  3.47it/s] 35%|███▍      | 203/585 [01:22<01:50,  3.47it/s] 35%|███▍      | 204/585 [01:22<01:49,  3.47it/s] 35%|███▌      | 205/585 [01:22<01:49,  3.47it/s] 35%|███▌      | 206/585 [01:23<01:49,  3.47it/s] 35%|███▌      | 207/585 [01:23<01:48,  3.47it/s] 36%|███▌      | 208/585 [01:23<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:24<01:48,  3.47it/s] 36%|███▌      | 210/585 [01:24<01:48,  3.47it/s] 36%|███▌      | 211/585 [01:24<01:47,  3.47it/s] 36%|███▌      | 212/585 [01:24<01:47,  3.47it/s] 36%|███▋      | 213/585 [01:25<01:47,  3.47it/s] 37%|███▋      | 214/585 [01:25<01:46,  3.47it/s] 37%|███▋      | 215/585 [01:25<01:46,  3.47it/s] 37%|███▋      | 216/585 [01:26<01:46,  3.47it/s] 37%|███▋      | 217/585 [01:26<01:45,  3.47it/s] 37%|███▋      | 218/585 [01:26<01:45,  3.48it/s] 37%|███▋      | 219/585 [01:26<01:45,  3.47it/s] 38%|███▊      | 220/585 [01:27<01:45,  3.47it/s] 38%|███▊      | 221/585 [01:27<01:44,  3.47it/s] 38%|███▊      | 222/585 [01:27<01:44,  3.47it/s] 38%|███▊      | 223/585 [01:28<01:44,  3.47it/s] 38%|███▊      | 224/585 [01:28<01:43,  3.47it/s] 38%|███▊      | 225/585 [01:28<01:43,  3.47it/s] 39%|███▊      | 226/585 [01:28<01:43,  3.47it/s] 39%|███▉      | 227/585 [01:29<01:43,  3.48it/s] 39%|███▉      | 228/585 [01:29<01:42,  3.48it/s] 39%|███▉      | 229/585 [01:29<01:42,  3.47it/s] 39%|███▉      | 230/585 [01:30<01:42,  3.47it/s] 39%|███▉      | 231/585 [01:30<01:42,  3.47it/s] 40%|███▉      | 232/585 [01:30<01:41,  3.47it/s] 40%|███▉      | 233/585 [01:30<01:41,  3.47it/s] 40%|████      | 234/585 [01:31<01:41,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 04:33:03,098 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:33:03,098 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 04:33:03,098 >>   Batch size = 8
{'eval_loss': 1.0428075790405273, 'eval_runtime': 16.6035, 'eval_samples_per_second': 376.608, 'eval_steps_per_second': 47.099, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.56it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.06it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.24it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.43it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.17it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.91it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.55it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.21it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.08it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.12it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.09it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.17it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.20it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.17it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.17it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.22it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.98it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.06it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.98it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.99it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.10it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.09it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.11it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.25it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.15it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.08it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.05it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 47.02it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.00it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.03it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.10it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.19it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.14it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.08it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.13it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 47.02it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.95it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.96it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.93it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.03it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.09it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.05it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.98it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.06it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.98it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 47.00it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.99it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.91it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.01it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.12it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.07it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.99it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.93it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.97it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.99it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.97it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.05it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.95it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.08it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.12it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.08it/s][A
 40%|████      | 313/782 [00:06<00:09, 46.95it/s][A
 41%|████      | 318/782 [00:06<00:09, 47.03it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.93it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.95it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 47.09it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.97it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 47.04it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.06it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 47.03it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 47.09it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.98it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.99it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.96it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.03it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.91it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 47.03it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.09it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.03it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 47.03it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.09it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.00it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.97it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.99it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.97it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.05it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.98it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 47.03it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 47.00it/s][A
 58%|█████▊    | 453/782 [00:09<00:06, 47.06it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 47.06it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.93it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.97it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.00it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.00it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.08it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 47.08it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.03it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 47.01it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.09it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.02it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.94it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 46.94it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.95it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.99it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.05it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.02it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.00it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.02it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.03it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.02it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 46.99it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.95it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.94it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 47.03it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.03it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.98it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 47.07it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 47.03it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 47.02it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 47.02it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.95it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.95it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.02it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.02it/s][A
 81%|████████  | 633/782 [00:13<00:03, 47.00it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 47.05it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 47.01it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.97it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.92it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 46.64it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.67it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.87it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.86it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.91it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 47.00it/s][A
 88%|████████▊ | 688/782 [00:14<00:01, 47.01it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 47.08it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.97it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.99it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.93it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.96it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.99it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.94it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.02it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 47.03it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 47.03it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.05it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.96it/s][A
 96%|█████████▋| 753/782 [00:15<00:00, 46.99it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.95it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 47.05it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.04it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.00it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.03it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.03it/s][A 40%|████      | 234/585 [01:47<01:41,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:33:19,758 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 04:33:19,812 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:33:22,218 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:33:22,240 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:33:22,253 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:55<43:46,  7.51s/it] 40%|████      | 236/585 [01:55<31:03,  5.34s/it] 41%|████      | 237/585 [01:56<22:10,  3.82s/it] 41%|████      | 238/585 [01:56<15:58,  2.76s/it] 41%|████      | 239/585 [01:56<11:39,  2.02s/it] 41%|████      | 240/585 [01:57<08:37,  1.50s/it] 41%|████      | 241/585 [01:57<06:31,  1.14s/it] 41%|████▏     | 242/585 [01:57<05:02,  1.13it/s] 42%|████▏     | 243/585 [01:57<04:00,  1.42it/s] 42%|████▏     | 244/585 [01:58<03:17,  1.73it/s] 42%|████▏     | 245/585 [01:58<02:47,  2.03it/s] 42%|████▏     | 246/585 [01:58<02:25,  2.32it/s] 42%|████▏     | 247/585 [01:59<02:11,  2.57it/s] 42%|████▏     | 248/585 [01:59<02:00,  2.79it/s] 43%|████▎     | 249/585 [01:59<01:53,  2.97it/s] 43%|████▎     | 250/585 [01:59<01:48,  3.10it/s] 43%|████▎     | 251/585 [02:00<01:44,  3.20it/s] 43%|████▎     | 252/585 [02:00<01:41,  3.28it/s] 43%|████▎     | 253/585 [02:00<01:39,  3.34it/s] 43%|████▎     | 254/585 [02:01<01:38,  3.38it/s] 44%|████▎     | 255/585 [02:01<01:36,  3.40it/s] 44%|████▍     | 256/585 [02:01<01:36,  3.42it/s] 44%|████▍     | 257/585 [02:01<01:35,  3.44it/s] 44%|████▍     | 258/585 [02:02<01:35,  3.44it/s] 44%|████▍     | 259/585 [02:02<01:34,  3.45it/s] 44%|████▍     | 260/585 [02:02<01:34,  3.46it/s] 45%|████▍     | 261/585 [02:03<01:33,  3.46it/s] 45%|████▍     | 262/585 [02:03<01:33,  3.47it/s] 45%|████▍     | 263/585 [02:03<01:32,  3.47it/s] 45%|████▌     | 264/585 [02:03<01:32,  3.47it/s] 45%|████▌     | 265/585 [02:04<01:32,  3.47it/s] 45%|████▌     | 266/585 [02:04<01:31,  3.47it/s] 46%|████▌     | 267/585 [02:04<01:31,  3.47it/s] 46%|████▌     | 268/585 [02:05<01:31,  3.47it/s] 46%|████▌     | 269/585 [02:05<01:31,  3.46it/s] 46%|████▌     | 270/585 [02:05<01:30,  3.47it/s] 46%|████▋     | 271/585 [02:05<01:30,  3.47it/s] 46%|████▋     | 272/585 [02:06<01:30,  3.47it/s] 47%|████▋     | 273/585 [02:06<01:29,  3.47it/s] 47%|████▋     | 274/585 [02:06<01:29,  3.47it/s] 47%|████▋     | 275/585 [02:07<01:29,  3.47it/s] 47%|████▋     | 276/585 [02:07<01:28,  3.48it/s] 47%|████▋     | 277/585 [02:07<01:28,  3.48it/s] 48%|████▊     | 278/585 [02:07<01:28,  3.48it/s] 48%|████▊     | 279/585 [02:08<01:28,  3.47it/s] 48%|████▊     | 280/585 [02:08<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:08<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:09<01:27,  3.47it/s] 48%|████▊     | 283/585 [02:09<01:27,  3.47it/s] 49%|████▊     | 284/585 [02:09<01:26,  3.47it/s] 49%|████▊     | 285/585 [02:09<01:26,  3.47it/s] 49%|████▉     | 286/585 [02:10<01:26,  3.47it/s] 49%|████▉     | 287/585 [02:10<01:25,  3.47it/s] 49%|████▉     | 288/585 [02:10<01:25,  3.47it/s] 49%|████▉     | 289/585 [02:11<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:11<01:24,  3.47it/s] 50%|████▉     | 291/585 [02:11<01:24,  3.47it/s] 50%|████▉     | 292/585 [02:11<01:24,  3.47it/s] 50%|█████     | 293/585 [02:12<01:24,  3.47it/s] 50%|█████     | 294/585 [02:12<01:23,  3.47it/s] 50%|█████     | 295/585 [02:12<01:23,  3.47it/s] 51%|█████     | 296/585 [02:13<01:23,  3.47it/s] 51%|█████     | 297/585 [02:13<01:22,  3.47it/s] 51%|█████     | 298/585 [02:13<01:22,  3.47it/s] 51%|█████     | 299/585 [02:14<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:14<01:22,  3.47it/s] 51%|█████▏    | 301/585 [02:14<01:21,  3.47it/s] 52%|█████▏    | 302/585 [02:14<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:15<01:21,  3.47it/s] 52%|█████▏    | 304/585 [02:15<01:21,  3.47it/s] 52%|█████▏    | 305/585 [02:15<01:20,  3.47it/s] 52%|█████▏    | 306/585 [02:16<01:20,  3.47it/s] 52%|█████▏    | 307/585 [02:16<01:20,  3.47it/s] 53%|█████▎    | 308/585 [02:16<01:19,  3.47it/s] 53%|█████▎    | 309/585 [02:16<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:17<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:17<01:18,  3.47it/s] 53%|█████▎    | 312/585 [02:17<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:18<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:18<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:18<01:17,  3.47it/s] 54%|█████▍    | 316/585 [02:18<01:17,  3.47it/s] 54%|█████▍    | 317/585 [02:19<01:17,  3.47it/s] 54%|█████▍    | 318/585 [02:19<01:16,  3.47it/s] 55%|█████▍    | 319/585 [02:19<01:16,  3.47it/s] 55%|█████▍    | 320/585 [02:20<01:16,  3.47it/s] 55%|█████▍    | 321/585 [02:20<01:16,  3.47it/s] 55%|█████▌    | 322/585 [02:20<01:15,  3.47it/s] 55%|█████▌    | 323/585 [02:20<01:15,  3.47it/s] 55%|█████▌    | 324/585 [02:21<01:15,  3.47it/s] 56%|█████▌    | 325/585 [02:21<01:14,  3.47it/s] 56%|█████▌    | 326/585 [02:21<01:14,  3.47it/s] 56%|█████▌    | 327/585 [02:22<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:22<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:22<01:13,  3.47it/s] 56%|█████▋    | 330/585 [02:22<01:13,  3.47it/s] 57%|█████▋    | 331/585 [02:23<01:13,  3.47it/s] 57%|█████▋    | 332/585 [02:23<01:12,  3.47it/s] 57%|█████▋    | 333/585 [02:23<01:12,  3.47it/s] 57%|█████▋    | 334/585 [02:24<01:12,  3.47it/s] 57%|█████▋    | 335/585 [02:24<01:12,  3.47it/s] 57%|█████▋    | 336/585 [02:24<01:11,  3.47it/s] 58%|█████▊    | 337/585 [02:24<01:11,  3.47it/s] 58%|█████▊    | 338/585 [02:25<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:25<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:25<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:26<01:10,  3.46it/s] 58%|█████▊    | 342/585 [02:26<01:10,  3.47it/s] 59%|█████▊    | 343/585 [02:26<01:09,  3.47it/s] 59%|█████▉    | 344/585 [02:26<01:09,  3.47it/s] 59%|█████▉    | 345/585 [02:27<01:09,  3.47it/s] 59%|█████▉    | 346/585 [02:27<01:08,  3.47it/s] 59%|█████▉    | 347/585 [02:27<01:08,  3.47it/s] 59%|█████▉    | 348/585 [02:28<01:08,  3.47it/s] 60%|█████▉    | 349/585 [02:28<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:28<01:07,  3.46it/s] 60%|██████    | 351/585 [02:29<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 04:34:00,883 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:34:00,883 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 04:34:00,883 >>   Batch size = 8
{'eval_loss': 1.0569777488708496, 'eval_runtime': 16.6372, 'eval_samples_per_second': 375.845, 'eval_steps_per_second': 47.003, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 58.08it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.08it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.31it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.47it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.08it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.81it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.51it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.05it/s][A
  6%|▌         | 48/782 [00:00<00:15, 46.99it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.09it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.20it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.26it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.15it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.19it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.22it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.17it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.92it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.88it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.88it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.94it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.15it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.06it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.13it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.14it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.14it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.00it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.96it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.96it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.89it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.07it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.04it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.12it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.16it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 47.04it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.97it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.95it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.97it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.96it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.00it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.02it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.94it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.76it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.98it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 47.01it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.89it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.96it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.95it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.95it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.95it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.97it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.00it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.03it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.00it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.89it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.94it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.99it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.93it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.02it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.93it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.00it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.04it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.04it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.98it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 47.04it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.91it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.91it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.99it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.98it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.98it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.94it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.98it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.94it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.01it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.99it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 47.02it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.96it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.93it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.97it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.00it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.94it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.99it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 47.00it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 47.01it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.97it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.99it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 47.03it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 47.03it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.97it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.89it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.91it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.96it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.96it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 47.02it/s][A
 60%|██████    | 473/782 [00:10<00:06, 47.01it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.05it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 47.04it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.93it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.93it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.97it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 47.01it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.93it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.91it/s][A
 66%|██████▌   | 518/782 [00:10<00:05, 46.99it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 47.01it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.03it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 47.01it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.06it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.98it/s][A
 70%|███████   | 548/782 [00:11<00:04, 46.93it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.99it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.01it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.00it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.07it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 47.02it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.95it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 47.06it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.04it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.97it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.93it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.95it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.93it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.92it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.93it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.94it/s][A
 80%|████████  | 628/782 [00:13<00:03, 47.02it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.98it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.96it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.94it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.94it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 47.02it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 46.94it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.91it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.97it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.92it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.91it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.91it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.68it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.82it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.84it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.92it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.88it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.98it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.90it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.96it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 47.01it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.93it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.98it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 47.06it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.99it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.98it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.97it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.92it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.96it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.01it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.99it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.99it/s][A 60%|██████    | 351/585 [02:45<01:07,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:34:17,567 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 04:34:17,607 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:34:20,235 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:34:20,260 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:34:20,271 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:53<29:05,  7.49s/it] 60%|██████    | 353/585 [02:53<20:36,  5.33s/it] 61%|██████    | 354/585 [02:53<14:41,  3.82s/it] 61%|██████    | 355/585 [02:54<10:34,  2.76s/it] 61%|██████    | 356/585 [02:54<07:41,  2.02s/it] 61%|██████    | 357/585 [02:54<05:41,  1.50s/it] 61%|██████    | 358/585 [02:55<04:17,  1.14s/it] 61%|██████▏   | 359/585 [02:55<03:20,  1.13it/s] 62%|██████▏   | 360/585 [02:55<02:39,  1.42it/s] 62%|██████▏   | 361/585 [02:55<02:10,  1.72it/s] 62%|██████▏   | 362/585 [02:56<01:50,  2.03it/s] 62%|██████▏   | 363/585 [02:56<01:35,  2.32it/s] 62%|██████▏   | 364/585 [02:56<01:26,  2.57it/s] 62%|██████▏   | 365/585 [02:57<01:18,  2.79it/s] 63%|██████▎   | 366/585 [02:57<01:13,  2.96it/s] 63%|██████▎   | 367/585 [02:57<01:10,  3.10it/s] 63%|██████▎   | 368/585 [02:57<01:07,  3.21it/s] 63%|██████▎   | 369/585 [02:58<01:05,  3.28it/s] 63%|██████▎   | 370/585 [02:58<01:04,  3.34it/s] 63%|██████▎   | 371/585 [02:58<01:03,  3.38it/s] 64%|██████▎   | 372/585 [02:59<01:02,  3.40it/s] 64%|██████▍   | 373/585 [02:59<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:59<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:59<01:01,  3.44it/s] 64%|██████▍   | 376/585 [03:00<01:00,  3.45it/s] 64%|██████▍   | 377/585 [03:00<01:00,  3.46it/s] 65%|██████▍   | 378/585 [03:00<00:59,  3.47it/s] 65%|██████▍   | 379/585 [03:01<00:59,  3.47it/s] 65%|██████▍   | 380/585 [03:01<00:59,  3.47it/s] 65%|██████▌   | 381/585 [03:01<00:58,  3.47it/s] 65%|██████▌   | 382/585 [03:01<00:58,  3.47it/s] 65%|██████▌   | 383/585 [03:02<00:58,  3.47it/s] 66%|██████▌   | 384/585 [03:02<00:57,  3.48it/s] 66%|██████▌   | 385/585 [03:02<00:57,  3.48it/s] 66%|██████▌   | 386/585 [03:03<00:57,  3.45it/s] 66%|██████▌   | 387/585 [03:03<00:57,  3.46it/s] 66%|██████▋   | 388/585 [03:03<00:56,  3.46it/s] 66%|██████▋   | 389/585 [03:03<00:56,  3.47it/s] 67%|██████▋   | 390/585 [03:04<00:56,  3.47it/s] 67%|██████▋   | 391/585 [03:04<00:55,  3.47it/s] 67%|██████▋   | 392/585 [03:04<00:55,  3.47it/s] 67%|██████▋   | 393/585 [03:05<00:55,  3.47it/s] 67%|██████▋   | 394/585 [03:05<00:54,  3.47it/s] 68%|██████▊   | 395/585 [03:05<00:54,  3.48it/s] 68%|██████▊   | 396/585 [03:05<00:54,  3.48it/s] 68%|██████▊   | 397/585 [03:06<00:54,  3.46it/s] 68%|██████▊   | 398/585 [03:06<00:53,  3.47it/s] 68%|██████▊   | 399/585 [03:06<00:53,  3.47it/s] 68%|██████▊   | 400/585 [03:07<00:53,  3.47it/s] 69%|██████▊   | 401/585 [03:07<00:53,  3.47it/s] 69%|██████▊   | 402/585 [03:07<00:52,  3.47it/s] 69%|██████▉   | 403/585 [03:08<00:52,  3.47it/s] 69%|██████▉   | 404/585 [03:08<00:52,  3.47it/s] 69%|██████▉   | 405/585 [03:08<00:51,  3.47it/s] 69%|██████▉   | 406/585 [03:08<00:51,  3.47it/s] 70%|██████▉   | 407/585 [03:09<00:51,  3.47it/s] 70%|██████▉   | 408/585 [03:09<00:51,  3.47it/s] 70%|██████▉   | 409/585 [03:09<00:50,  3.47it/s] 70%|███████   | 410/585 [03:10<00:50,  3.47it/s] 70%|███████   | 411/585 [03:10<00:50,  3.47it/s] 70%|███████   | 412/585 [03:10<00:49,  3.47it/s] 71%|███████   | 413/585 [03:10<00:49,  3.47it/s] 71%|███████   | 414/585 [03:11<00:49,  3.48it/s] 71%|███████   | 415/585 [03:11<00:48,  3.48it/s] 71%|███████   | 416/585 [03:11<00:48,  3.48it/s] 71%|███████▏  | 417/585 [03:12<00:48,  3.48it/s] 71%|███████▏  | 418/585 [03:12<00:48,  3.48it/s] 72%|███████▏  | 419/585 [03:12<00:47,  3.47it/s] 72%|███████▏  | 420/585 [03:12<00:47,  3.47it/s] 72%|███████▏  | 421/585 [03:13<00:47,  3.47it/s] 72%|███████▏  | 422/585 [03:13<00:46,  3.47it/s] 72%|███████▏  | 423/585 [03:13<00:46,  3.47it/s] 72%|███████▏  | 424/585 [03:14<00:46,  3.47it/s] 73%|███████▎  | 425/585 [03:14<00:46,  3.47it/s] 73%|███████▎  | 426/585 [03:14<00:45,  3.47it/s] 73%|███████▎  | 427/585 [03:14<00:45,  3.48it/s] 73%|███████▎  | 428/585 [03:15<00:45,  3.48it/s] 73%|███████▎  | 429/585 [03:15<00:44,  3.48it/s] 74%|███████▎  | 430/585 [03:15<00:44,  3.46it/s] 74%|███████▎  | 431/585 [03:16<00:44,  3.47it/s] 74%|███████▍  | 432/585 [03:16<00:44,  3.47it/s] 74%|███████▍  | 433/585 [03:16<00:43,  3.47it/s] 74%|███████▍  | 434/585 [03:16<00:43,  3.47it/s] 74%|███████▍  | 435/585 [03:17<00:43,  3.47it/s] 75%|███████▍  | 436/585 [03:17<00:42,  3.47it/s] 75%|███████▍  | 437/585 [03:17<00:42,  3.48it/s] 75%|███████▍  | 438/585 [03:18<00:42,  3.47it/s] 75%|███████▌  | 439/585 [03:18<00:42,  3.47it/s] 75%|███████▌  | 440/585 [03:18<00:41,  3.47it/s] 75%|███████▌  | 441/585 [03:18<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:19<00:41,  3.46it/s] 76%|███████▌  | 443/585 [03:19<00:41,  3.46it/s] 76%|███████▌  | 444/585 [03:19<00:40,  3.47it/s] 76%|███████▌  | 445/585 [03:20<00:40,  3.47it/s] 76%|███████▌  | 446/585 [03:20<00:40,  3.47it/s] 76%|███████▋  | 447/585 [03:20<00:39,  3.47it/s] 77%|███████▋  | 448/585 [03:20<00:39,  3.47it/s] 77%|███████▋  | 449/585 [03:21<00:39,  3.47it/s] 77%|███████▋  | 450/585 [03:21<00:38,  3.47it/s] 77%|███████▋  | 451/585 [03:21<00:38,  3.48it/s] 77%|███████▋  | 452/585 [03:22<00:38,  3.48it/s] 77%|███████▋  | 453/585 [03:22<00:37,  3.48it/s] 78%|███████▊  | 454/585 [03:22<00:37,  3.48it/s] 78%|███████▊  | 455/585 [03:22<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:23<00:37,  3.47it/s] 78%|███████▊  | 457/585 [03:23<00:37,  3.46it/s] 78%|███████▊  | 458/585 [03:23<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:24<00:36,  3.47it/s] 79%|███████▊  | 460/585 [03:24<00:36,  3.47it/s] 79%|███████▉  | 461/585 [03:24<00:35,  3.47it/s] 79%|███████▉  | 462/585 [03:25<00:35,  3.47it/s] 79%|███████▉  | 463/585 [03:25<00:35,  3.47it/s] 79%|███████▉  | 464/585 [03:25<00:34,  3.47it/s] 79%|███████▉  | 465/585 [03:25<00:34,  3.47it/s] 80%|███████▉  | 466/585 [03:26<00:34,  3.47it/s] 80%|███████▉  | 467/585 [03:26<00:34,  3.47it/s] 80%|████████  | 468/585 [03:26<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 04:34:58,613 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:34:58,613 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 04:34:58,613 >>   Batch size = 8
{'eval_loss': 1.0733699798583984, 'eval_runtime': 16.6479, 'eval_samples_per_second': 375.603, 'eval_steps_per_second': 46.973, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.67it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.01it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.29it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.64it/s][A
  4%|▎         | 28/782 [00:00<00:15, 48.25it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.84it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.52it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.00it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.00it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.22it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.13it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.14it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.19it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.19it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.18it/s][A
 11%|█         | 83/782 [00:01<00:14, 47.09it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.79it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.80it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.98it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.97it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.02it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.07it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.17it/s][A
 16%|█▌        | 123/782 [00:02<00:13, 47.12it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.04it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.85it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.79it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.84it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.01it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.05it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.09it/s][A
 21%|██        | 163/782 [00:03<00:13, 47.19it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 47.17it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 46.95it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.96it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.84it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.96it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.92it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.10it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 47.03it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.12it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.12it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.95it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.92it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.79it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.78it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.98it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.04it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.07it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 47.09it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.05it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.04it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 47.03it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.82it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.80it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.91it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.00it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.03it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 47.10it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 47.11it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.07it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.01it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.96it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.91it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.85it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.96it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.91it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.86it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 47.03it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.96it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.94it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.90it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.87it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.84it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.91it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.85it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.87it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.94it/s][A
 51%|█████     | 398/782 [00:08<00:08, 47.05it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.93it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.96it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.97it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.79it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.52it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.68it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.77it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.90it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.99it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.86it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.91it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.79it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.87it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.87it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.88it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.87it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.92it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.96it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.04it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.99it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.91it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.91it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.01it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.99it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.86it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.95it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.93it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.98it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.93it/s][A
 70%|███████   | 548/782 [00:11<00:04, 46.93it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.91it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.02it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.01it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.88it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.92it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.92it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.91it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 47.00it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.93it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.83it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.90it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.95it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.90it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.96it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.91it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.88it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.92it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.98it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.95it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.83it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.93it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 46.92it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.94it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.97it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.97it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.97it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.97it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.99it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.97it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.94it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.92it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.93it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.00it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.94it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.91it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.94it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.90it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.90it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.99it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.96it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.79it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.89it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.95it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.98it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.04it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.94it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.94it/s][A 80%|████████  | 468/585 [03:43<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:35:15,282 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 04:35:15,296 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:35:17,494 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:35:17,516 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:35:17,527 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:51<14:39,  7.59s/it] 80%|████████  | 470/585 [03:51<10:20,  5.40s/it] 81%|████████  | 471/585 [03:51<07:20,  3.86s/it] 81%|████████  | 472/585 [03:52<05:15,  2.79s/it] 81%|████████  | 473/585 [03:52<03:48,  2.04s/it] 81%|████████  | 474/585 [03:52<02:48,  1.51s/it] 81%|████████  | 475/585 [03:53<02:06,  1.15s/it] 81%|████████▏ | 476/585 [03:53<01:36,  1.12it/s] 82%|████████▏ | 477/585 [03:53<01:16,  1.41it/s] 82%|████████▏ | 478/585 [03:53<01:02,  1.72it/s] 82%|████████▏ | 479/585 [03:54<00:52,  2.02it/s] 82%|████████▏ | 480/585 [03:54<00:45,  2.31it/s] 82%|████████▏ | 481/585 [03:54<00:40,  2.57it/s] 82%|████████▏ | 482/585 [03:55<00:36,  2.78it/s] 83%|████████▎ | 483/585 [03:55<00:35,  2.90it/s] 83%|████████▎ | 484/585 [03:55<00:33,  3.05it/s] 83%|████████▎ | 485/585 [03:55<00:31,  3.16it/s] 83%|████████▎ | 486/585 [03:56<00:30,  3.25it/s] 83%|████████▎ | 487/585 [03:56<00:29,  3.31it/s] 83%|████████▎ | 488/585 [03:56<00:28,  3.36it/s] 84%|████████▎ | 489/585 [03:57<00:28,  3.39it/s] 84%|████████▍ | 490/585 [03:57<00:27,  3.40it/s] 84%|████████▍ | 491/585 [03:57<00:27,  3.42it/s] 84%|████████▍ | 492/585 [03:58<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:58<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:58<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:58<00:26,  3.46it/s] 85%|████████▍ | 496/585 [03:59<00:25,  3.46it/s] 85%|████████▍ | 497/585 [03:59<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:59<00:25,  3.46it/s] 85%|████████▌ | 499/585 [04:00<00:24,  3.47it/s] 85%|████████▌ | 500/585 [04:00<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [04:00<00:24,  3.47it/s] 86%|████████▌ | 501/585 [04:00<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:00<00:24,  3.46it/s] 86%|████████▌ | 503/585 [04:01<00:23,  3.46it/s] 86%|████████▌ | 504/585 [04:01<00:23,  3.46it/s] 86%|████████▋ | 505/585 [04:01<00:23,  3.46it/s] 86%|████████▋ | 506/585 [04:02<00:22,  3.47it/s] 87%|████████▋ | 507/585 [04:02<00:22,  3.47it/s] 87%|████████▋ | 508/585 [04:02<00:22,  3.47it/s] 87%|████████▋ | 509/585 [04:02<00:21,  3.47it/s] 87%|████████▋ | 510/585 [04:03<00:21,  3.47it/s] 87%|████████▋ | 511/585 [04:03<00:21,  3.47it/s] 88%|████████▊ | 512/585 [04:03<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:04<00:20,  3.46it/s] 88%|████████▊ | 514/585 [04:04<00:20,  3.46it/s] 88%|████████▊ | 515/585 [04:04<00:20,  3.46it/s] 88%|████████▊ | 516/585 [04:04<00:19,  3.46it/s] 88%|████████▊ | 517/585 [04:05<00:19,  3.46it/s] 89%|████████▊ | 518/585 [04:05<00:19,  3.47it/s] 89%|████████▊ | 519/585 [04:05<00:19,  3.47it/s] 89%|████████▉ | 520/585 [04:06<00:18,  3.47it/s] 89%|████████▉ | 521/585 [04:06<00:18,  3.47it/s] 89%|████████▉ | 522/585 [04:06<00:18,  3.47it/s] 89%|████████▉ | 523/585 [04:06<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:07<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:07<00:17,  3.46it/s] 90%|████████▉ | 526/585 [04:07<00:17,  3.46it/s] 90%|█████████ | 527/585 [04:08<00:16,  3.46it/s] 90%|█████████ | 528/585 [04:08<00:16,  3.46it/s] 90%|█████████ | 529/585 [04:08<00:16,  3.47it/s] 91%|█████████ | 530/585 [04:08<00:15,  3.47it/s] 91%|█████████ | 531/585 [04:09<00:15,  3.47it/s] 91%|█████████ | 532/585 [04:09<00:15,  3.47it/s] 91%|█████████ | 533/585 [04:09<00:14,  3.47it/s] 91%|█████████▏| 534/585 [04:10<00:14,  3.46it/s] 91%|█████████▏| 535/585 [04:10<00:14,  3.46it/s] 92%|█████████▏| 536/585 [04:10<00:14,  3.47it/s] 92%|█████████▏| 537/585 [04:10<00:13,  3.47it/s] 92%|█████████▏| 538/585 [04:11<00:13,  3.47it/s] 92%|█████████▏| 539/585 [04:11<00:13,  3.47it/s] 92%|█████████▏| 540/585 [04:11<00:12,  3.47it/s] 92%|█████████▏| 541/585 [04:12<00:12,  3.47it/s] 93%|█████████▎| 542/585 [04:12<00:12,  3.47it/s] 93%|█████████▎| 543/585 [04:12<00:12,  3.47it/s] 93%|█████████▎| 544/585 [04:13<00:11,  3.47it/s] 93%|█████████▎| 545/585 [04:13<00:11,  3.46it/s] 93%|█████████▎| 546/585 [04:13<00:11,  3.46it/s] 94%|█████████▎| 547/585 [04:13<00:10,  3.46it/s] 94%|█████████▎| 548/585 [04:14<00:10,  3.46it/s] 94%|█████████▍| 549/585 [04:14<00:10,  3.47it/s] 94%|█████████▍| 550/585 [04:14<00:10,  3.47it/s] 94%|█████████▍| 551/585 [04:15<00:09,  3.47it/s] 94%|█████████▍| 552/585 [04:15<00:09,  3.47it/s] 95%|█████████▍| 553/585 [04:15<00:09,  3.47it/s] 95%|█████████▍| 554/585 [04:15<00:08,  3.47it/s] 95%|█████████▍| 555/585 [04:16<00:08,  3.47it/s] 95%|█████████▌| 556/585 [04:16<00:08,  3.46it/s] 95%|█████████▌| 557/585 [04:16<00:08,  3.47it/s] 95%|█████████▌| 558/585 [04:17<00:07,  3.47it/s] 96%|█████████▌| 559/585 [04:17<00:07,  3.47it/s] 96%|█████████▌| 560/585 [04:17<00:07,  3.47it/s] 96%|█████████▌| 561/585 [04:17<00:06,  3.47it/s] 96%|█████████▌| 562/585 [04:18<00:06,  3.46it/s] 96%|█████████▌| 563/585 [04:18<00:06,  3.46it/s] 96%|█████████▋| 564/585 [04:18<00:06,  3.47it/s] 97%|█████████▋| 565/585 [04:19<00:05,  3.47it/s] 97%|█████████▋| 566/585 [04:19<00:05,  3.47it/s] 97%|█████████▋| 567/585 [04:19<00:05,  3.45it/s] 97%|█████████▋| 568/585 [04:19<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:20<00:04,  3.46it/s] 97%|█████████▋| 570/585 [04:20<00:04,  3.46it/s] 98%|█████████▊| 571/585 [04:20<00:04,  3.46it/s] 98%|█████████▊| 572/585 [04:21<00:03,  3.46it/s] 98%|█████████▊| 573/585 [04:21<00:03,  3.46it/s] 98%|█████████▊| 574/585 [04:21<00:03,  3.46it/s] 98%|█████████▊| 575/585 [04:21<00:02,  3.47it/s] 98%|█████████▊| 576/585 [04:22<00:02,  3.47it/s] 99%|█████████▊| 577/585 [04:22<00:02,  3.47it/s] 99%|█████████▉| 578/585 [04:22<00:02,  3.47it/s] 99%|█████████▉| 579/585 [04:23<00:01,  3.47it/s] 99%|█████████▉| 580/585 [04:23<00:01,  3.47it/s] 99%|█████████▉| 581/585 [04:23<00:01,  3.47it/s] 99%|█████████▉| 582/585 [04:23<00:00,  3.47it/s]100%|█████████▉| 583/585 [04:24<00:00,  3.47it/s]100%|█████████▉| 584/585 [04:24<00:00,  3.47it/s]100%|██████████| 585/585 [04:24<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 04:35:56,681 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:35:56,681 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 04:35:56,681 >>   Batch size = 8
{'eval_loss': 1.0847561359405518, 'eval_runtime': 16.6575, 'eval_samples_per_second': 375.386, 'eval_steps_per_second': 46.946, 'epoch': 4.0}
{'loss': 0.3748, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.46it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.69it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.89it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.32it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.94it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.54it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.39it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.33it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.24it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.17it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.99it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.99it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.07it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.11it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.09it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.98it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 47.13it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 47.04it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 47.08it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 47.14it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 47.00it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.93it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.96it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.97it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 47.09it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 47.00it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 47.05it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.99it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 47.01it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.95it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.97it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.98it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.94it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 46.90it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 47.01it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.96it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 47.01it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 47.02it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 47.03it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.93it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.02it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 47.01it/s][A
 28%|██▊       | 218/782 [00:04<00:11, 47.01it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.99it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 47.01it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.95it/s][A
 30%|███       | 238/782 [00:05<00:11, 47.11it/s][A
 31%|███       | 243/782 [00:05<00:11, 47.02it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 47.01it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.95it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.03it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 47.03it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.92it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.92it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.93it/s][A
 36%|███▌      | 283/782 [00:05<00:10, 46.91it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 47.03it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.01it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.99it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.98it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 47.05it/s][A
 40%|████      | 313/782 [00:06<00:09, 47.03it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.96it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.98it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.91it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.98it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 47.03it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.99it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.97it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.93it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.99it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.92it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 47.01it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.98it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.94it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.97it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.91it/s][A
 50%|█████     | 393/782 [00:08<00:08, 47.02it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.97it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.93it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 47.01it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.95it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.97it/s][A
 54%|█████▍    | 423/782 [00:08<00:07, 46.95it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.96it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.96it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.92it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.99it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.98it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.91it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.99it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.98it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.98it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.98it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.95it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.94it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.93it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 47.00it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.96it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.94it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 47.00it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 47.03it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 47.04it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.98it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 47.03it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.99it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 47.05it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 47.04it/s][A
 70%|███████   | 548/782 [00:11<00:04, 47.06it/s][A
 71%|███████   | 553/782 [00:11<00:04, 47.05it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 47.01it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 47.04it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 47.04it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.89it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.99it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.93it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.91it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.88it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.91it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.97it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.96it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 47.10it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.92it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 47.01it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.85it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.88it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.98it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.81it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.75it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.88it/s][A
 84%|████████▍ | 658/782 [00:13<00:02, 46.99it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 47.12it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 47.11it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 47.13it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.95it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.98it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.97it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.91it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.87it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.84it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 47.03it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 47.01it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.95it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 47.02it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.85it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.98it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.93it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.94it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.93it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.81it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.97it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.96it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 47.07it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 47.09it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 47.03it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 47.03it/s][A100%|██████████| 585/585 [04:41<00:00,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:36:13,339 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 04:36:13,355 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:36:15,537 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:36:15,552 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:36:15,564 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:36:20,085 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:36:20,088 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117 (score: 1.0428075790405273).
                                                 100%|██████████| 585/585 [04:50<00:00,  3.47it/s]100%|██████████| 585/585 [04:50<00:00,  2.02it/s]
[INFO|trainer.py:1894] 2023-08-29 04:36:22,159 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 04:36:22,174 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:36:24,544 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:36:24,567 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:36:24,578 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:36:24,786 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:24,786 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:24,786 >>   train_loss               =     0.3722
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:24,786 >>   train_runtime            = 0:04:50.32
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:24,786 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:24,786 >>   train_samples_per_second =    129.167
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:24,786 >>   train_steps_per_second   =      2.015
{'eval_loss': 1.0899556875228882, 'eval_runtime': 16.6394, 'eval_samples_per_second': 375.795, 'eval_steps_per_second': 46.997, 'epoch': 5.0}
{'train_runtime': 290.3229, 'train_samples_per_second': 129.167, 'train_steps_per_second': 2.015, 'train_loss': 0.3722326164571648, 'epoch': 5.0}
08/29/2023 04:36:24 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:36:24,838 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:36:24,839 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 04:36:24,839 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.01it/s]  2%|▏         | 12/782 [00:00<00:14, 51.36it/s]  2%|▏         | 18/782 [00:00<00:15, 49.55it/s]  3%|▎         | 23/782 [00:00<00:15, 48.80it/s]  4%|▎         | 28/782 [00:00<00:15, 48.34it/s]  4%|▍         | 33/782 [00:00<00:15, 47.99it/s]  5%|▍         | 38/782 [00:00<00:15, 47.86it/s]  5%|▌         | 43/782 [00:00<00:15, 47.69it/s]  6%|▌         | 48/782 [00:00<00:15, 47.44it/s]  7%|▋         | 53/782 [00:01<00:15, 47.31it/s]  7%|▋         | 58/782 [00:01<00:15, 47.30it/s]  8%|▊         | 63/782 [00:01<00:15, 47.35it/s]  9%|▊         | 68/782 [00:01<00:15, 47.37it/s]  9%|▉         | 73/782 [00:01<00:14, 47.47it/s] 10%|▉         | 78/782 [00:01<00:14, 47.48it/s] 11%|█         | 83/782 [00:01<00:14, 47.44it/s] 11%|█▏        | 88/782 [00:01<00:14, 47.35it/s] 12%|█▏        | 93/782 [00:01<00:14, 47.21it/s] 13%|█▎        | 98/782 [00:02<00:14, 47.19it/s] 13%|█▎        | 103/782 [00:02<00:14, 47.12it/s] 14%|█▍        | 108/782 [00:02<00:14, 47.17it/s] 14%|█▍        | 113/782 [00:02<00:14, 45.24it/s] 15%|█▌        | 118/782 [00:02<00:14, 46.18it/s] 16%|█▌        | 123/782 [00:02<00:14, 46.52it/s] 16%|█▋        | 128/782 [00:02<00:13, 46.86it/s] 17%|█▋        | 133/782 [00:02<00:13, 46.91it/s] 18%|█▊        | 138/782 [00:02<00:13, 47.08it/s] 18%|█▊        | 143/782 [00:03<00:13, 47.16it/s] 19%|█▉        | 148/782 [00:03<00:13, 47.12it/s] 20%|█▉        | 153/782 [00:03<00:13, 47.12it/s] 20%|██        | 158/782 [00:03<00:13, 46.96it/s] 21%|██        | 163/782 [00:03<00:13, 47.04it/s] 21%|██▏       | 168/782 [00:03<00:13, 47.16it/s] 22%|██▏       | 173/782 [00:03<00:12, 47.24it/s] 23%|██▎       | 178/782 [00:03<00:12, 47.42it/s] 23%|██▎       | 183/782 [00:03<00:12, 47.34it/s] 24%|██▍       | 188/782 [00:03<00:12, 47.30it/s] 25%|██▍       | 193/782 [00:04<00:12, 47.31it/s] 25%|██▌       | 198/782 [00:04<00:12, 47.23it/s] 26%|██▌       | 203/782 [00:04<00:12, 47.19it/s] 27%|██▋       | 208/782 [00:04<00:12, 47.09it/s] 27%|██▋       | 213/782 [00:04<00:12, 47.14it/s] 28%|██▊       | 218/782 [00:04<00:11, 47.14it/s] 29%|██▊       | 223/782 [00:04<00:11, 47.28it/s] 29%|██▉       | 228/782 [00:04<00:11, 47.38it/s] 30%|██▉       | 233/782 [00:04<00:11, 47.34it/s] 30%|███       | 238/782 [00:05<00:11, 47.29it/s] 31%|███       | 243/782 [00:05<00:11, 47.22it/s] 32%|███▏      | 248/782 [00:05<00:11, 47.15it/s] 32%|███▏      | 253/782 [00:05<00:11, 47.18it/s] 33%|███▎      | 258/782 [00:05<00:11, 47.14it/s] 34%|███▎      | 263/782 [00:05<00:11, 47.09it/s] 34%|███▍      | 268/782 [00:05<00:10, 47.09it/s] 35%|███▍      | 273/782 [00:05<00:10, 47.24it/s] 36%|███▌      | 278/782 [00:05<00:10, 47.38it/s] 36%|███▌      | 283/782 [00:05<00:10, 47.28it/s] 37%|███▋      | 288/782 [00:06<00:10, 47.26it/s] 37%|███▋      | 293/782 [00:06<00:10, 47.24it/s] 38%|███▊      | 298/782 [00:06<00:10, 47.21it/s] 39%|███▊      | 303/782 [00:06<00:10, 47.13it/s] 39%|███▉      | 308/782 [00:06<00:10, 47.19it/s] 40%|████      | 313/782 [00:06<00:09, 47.13it/s] 41%|████      | 318/782 [00:06<00:09, 47.11it/s] 41%|████▏     | 323/782 [00:06<00:09, 47.30it/s] 42%|████▏     | 328/782 [00:06<00:09, 47.33it/s] 43%|████▎     | 333/782 [00:07<00:09, 47.13it/s] 43%|████▎     | 338/782 [00:07<00:09, 47.26it/s] 44%|████▍     | 343/782 [00:07<00:09, 47.23it/s] 45%|████▍     | 348/782 [00:07<00:09, 47.22it/s] 45%|████▌     | 353/782 [00:07<00:09, 47.10it/s] 46%|████▌     | 358/782 [00:07<00:09, 46.70it/s] 46%|████▋     | 363/782 [00:07<00:08, 46.87it/s] 47%|████▋     | 368/782 [00:07<00:08, 47.02it/s] 48%|████▊     | 373/782 [00:07<00:08, 47.21it/s] 48%|████▊     | 378/782 [00:07<00:08, 47.20it/s] 49%|████▉     | 383/782 [00:08<00:08, 47.17it/s] 50%|████▉     | 388/782 [00:08<00:08, 47.14it/s] 50%|█████     | 393/782 [00:08<00:08, 47.24it/s] 51%|█████     | 398/782 [00:08<00:08, 47.19it/s] 52%|█████▏    | 403/782 [00:08<00:08, 47.02it/s] 52%|█████▏    | 408/782 [00:08<00:07, 47.15it/s] 53%|█████▎    | 413/782 [00:08<00:07, 47.07it/s] 53%|█████▎    | 418/782 [00:08<00:07, 47.16it/s] 54%|█████▍    | 423/782 [00:08<00:07, 47.24it/s] 55%|█████▍    | 428/782 [00:09<00:07, 47.23it/s] 55%|█████▌    | 433/782 [00:09<00:07, 47.14it/s] 56%|█████▌    | 438/782 [00:09<00:07, 47.16it/s] 57%|█████▋    | 443/782 [00:09<00:07, 47.16it/s] 57%|█████▋    | 448/782 [00:09<00:07, 47.06it/s] 58%|█████▊    | 453/782 [00:09<00:06, 47.12it/s] 59%|█████▊    | 458/782 [00:09<00:06, 47.14it/s] 59%|█████▉    | 463/782 [00:09<00:06, 47.08it/s] 60%|█████▉    | 468/782 [00:09<00:06, 47.22it/s] 60%|██████    | 473/782 [00:10<00:06, 47.26it/s] 61%|██████    | 478/782 [00:10<00:06, 47.16it/s] 62%|██████▏   | 483/782 [00:10<00:06, 47.14it/s] 62%|██████▏   | 488/782 [00:10<00:06, 47.14it/s] 63%|██████▎   | 493/782 [00:10<00:06, 47.11it/s] 64%|██████▎   | 498/782 [00:10<00:06, 47.12it/s] 64%|██████▍   | 503/782 [00:10<00:05, 47.17it/s] 65%|██████▍   | 508/782 [00:10<00:05, 47.09it/s] 66%|██████▌   | 513/782 [00:10<00:05, 47.17it/s] 66%|██████▌   | 518/782 [00:10<00:05, 47.20it/s] 67%|██████▋   | 523/782 [00:11<00:05, 47.19it/s] 68%|██████▊   | 528/782 [00:11<00:05, 47.28it/s] 68%|██████▊   | 533/782 [00:11<00:05, 47.10it/s] 69%|██████▉   | 538/782 [00:11<00:05, 47.18it/s] 69%|██████▉   | 543/782 [00:11<00:05, 47.12it/s] 70%|███████   | 548/782 [00:11<00:04, 47.04it/s] 71%|███████   | 553/782 [00:11<00:04, 47.17it/s] 71%|███████▏  | 558/782 [00:11<00:04, 47.11it/s] 72%|███████▏  | 563/782 [00:11<00:04, 47.17it/s] 73%|███████▎  | 568/782 [00:12<00:04, 47.21it/s] 73%|███████▎  | 573/782 [00:12<00:04, 47.20it/s] 74%|███████▍  | 578/782 [00:12<00:04, 47.21it/s] 75%|███████▍  | 583/782 [00:12<00:04, 47.11it/s] 75%|███████▌  | 588/782 [00:12<00:04, 47.15it/s] 76%|███████▌  | 593/782 [00:12<00:04, 47.15it/s] 76%|███████▋  | 598/782 [00:12<00:03, 47.06it/s] 77%|███████▋  | 603/782 [00:12<00:03, 47.17it/s] 78%|███████▊  | 608/782 [00:12<00:03, 47.02it/s] 78%|███████▊  | 613/782 [00:12<00:03, 47.10it/s] 79%|███████▉  | 618/782 [00:13<00:03, 47.19it/s] 80%|███████▉  | 623/782 [00:13<00:03, 47.19it/s] 80%|████████  | 628/782 [00:13<00:03, 47.19it/s] 81%|████████  | 633/782 [00:13<00:03, 47.13it/s] 82%|████████▏ | 638/782 [00:13<00:03, 47.15it/s] 82%|████████▏ | 643/782 [00:13<00:02, 47.04it/s] 83%|████████▎ | 648/782 [00:13<00:02, 47.15it/s] 84%|████████▎ | 653/782 [00:13<00:02, 47.14it/s] 84%|████████▍ | 658/782 [00:13<00:02, 47.14it/s] 85%|████████▍ | 663/782 [00:14<00:02, 47.12it/s] 85%|████████▌ | 668/782 [00:14<00:02, 47.19it/s] 86%|████████▌ | 673/782 [00:14<00:02, 47.21it/s] 87%|████████▋ | 678/782 [00:14<00:02, 47.13it/s] 87%|████████▋ | 683/782 [00:14<00:02, 47.16it/s] 88%|████████▊ | 688/782 [00:14<00:01, 47.07it/s] 89%|████████▊ | 693/782 [00:14<00:01, 47.07it/s] 89%|████████▉ | 698/782 [00:14<00:01, 47.11it/s] 90%|████████▉ | 703/782 [00:14<00:01, 47.09it/s] 91%|█████████ | 708/782 [00:14<00:01, 47.14it/s] 91%|█████████ | 713/782 [00:15<00:01, 47.21it/s] 92%|█████████▏| 718/782 [00:15<00:01, 47.17it/s] 92%|█████████▏| 723/782 [00:15<00:01, 47.23it/s] 93%|█████████▎| 728/782 [00:15<00:01, 47.15it/s] 94%|█████████▎| 733/782 [00:15<00:01, 47.13it/s] 94%|█████████▍| 738/782 [00:15<00:00, 47.09it/s] 95%|█████████▌| 743/782 [00:15<00:00, 47.05it/s] 96%|█████████▌| 748/782 [00:15<00:00, 47.15it/s] 96%|█████████▋| 753/782 [00:15<00:00, 47.16it/s] 97%|█████████▋| 758/782 [00:16<00:00, 47.22it/s] 98%|█████████▊| 763/782 [00:16<00:00, 47.18it/s] 98%|█████████▊| 768/782 [00:16<00:00, 47.16it/s] 99%|█████████▉| 773/782 [00:16<00:00, 47.19it/s] 99%|█████████▉| 778/782 [00:16<00:00, 47.11it/s]100%|██████████| 782/782 [00:16<00:00, 47.21it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:36:41,426 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   eval_loss               =     1.0428
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   eval_runtime            = 0:00:16.58
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   eval_samples_per_second =    376.977
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   eval_steps_per_second   =     47.145
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:36:41,426 >>   perplexity              =     2.8372
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:47,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:47,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:47,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:47,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:47,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:36:47,351 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:36:47,352 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:36:48,036 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:36:49,079 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:36:49,079 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:52,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:52,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:52,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:52,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:36:52,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:36:52,346 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:36:52,347 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:36:52,611 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:36:52,773 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:36:52,773 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:08,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:10,  1.54it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:12,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.53it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.44it/s]Extractor Predicting: 50it [00:33,  1.46it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:38,  1.54it/s]Extractor Predicting: 60it [00:39,  1.51it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:43,  1.46it/s]Extractor Predicting: 67it [00:44,  1.49it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.47it/s]Extractor Predicting: 72it [00:47,  1.48it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:53,  1.49it/s]Extractor Predicting: 82it [00:54,  1.52it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:55,  1.49it/s]Extractor Predicting: 85it [00:56,  1.49it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [01:00,  1.36it/s]Extractor Predicting: 92it [01:01,  1.39it/s]Extractor Predicting: 93it [01:02,  1.42it/s]Extractor Predicting: 94it [01:02,  1.45it/s]Extractor Predicting: 95it [01:03,  1.47it/s]Extractor Predicting: 96it [01:04,  1.45it/s]Extractor Predicting: 97it [01:04,  1.44it/s]Extractor Predicting: 98it [01:05,  1.45it/s]Extractor Predicting: 99it [01:06,  1.44it/s]Extractor Predicting: 100it [01:06,  1.47it/s]Extractor Predicting: 101it [01:07,  1.47it/s]Extractor Predicting: 102it [01:08,  1.48it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.45it/s]Extractor Predicting: 108it [01:12,  1.44it/s]Extractor Predicting: 109it [01:13,  1.45it/s]Extractor Predicting: 110it [01:13,  1.47it/s]Extractor Predicting: 111it [01:14,  1.48it/s]Extractor Predicting: 112it [01:15,  1.47it/s]Extractor Predicting: 113it [01:15,  1.51it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:18,  1.47it/s]Extractor Predicting: 118it [01:19,  1.48it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:21,  1.49it/s]Extractor Predicting: 122it [01:21,  1.50it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:23,  1.49it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:25,  1.47it/s]Extractor Predicting: 128it [01:25,  1.46it/s]Extractor Predicting: 129it [01:26,  1.46it/s]Extractor Predicting: 130it [01:27,  1.45it/s]Extractor Predicting: 131it [01:27,  1.45it/s]Extractor Predicting: 132it [01:28,  1.44it/s]Extractor Predicting: 133it [01:29,  1.46it/s]Extractor Predicting: 134it [01:29,  1.45it/s]Extractor Predicting: 135it [01:30,  1.44it/s]Extractor Predicting: 136it [01:31,  1.45it/s]Extractor Predicting: 137it [01:32,  1.46it/s]Extractor Predicting: 138it [01:32,  1.48it/s]Extractor Predicting: 139it [01:33,  1.47it/s]Extractor Predicting: 140it [01:34,  1.47it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:35,  1.49it/s]Extractor Predicting: 143it [01:36,  1.47it/s]Extractor Predicting: 144it [01:36,  1.49it/s]Extractor Predicting: 145it [01:37,  1.49it/s]Extractor Predicting: 146it [01:38,  1.50it/s]Extractor Predicting: 147it [01:38,  1.49it/s]Extractor Predicting: 148it [01:39,  1.53it/s]Extractor Predicting: 149it [01:40,  1.52it/s]Extractor Predicting: 150it [01:40,  1.50it/s]Extractor Predicting: 151it [01:41,  1.48it/s]Extractor Predicting: 152it [01:41,  1.53it/s]Extractor Predicting: 153it [01:42,  1.51it/s]Extractor Predicting: 154it [01:43,  1.51it/s]Extractor Predicting: 155it [01:43,  1.52it/s]Extractor Predicting: 156it [01:44,  1.51it/s]Extractor Predicting: 157it [01:45,  1.55it/s]Extractor Predicting: 158it [01:45,  1.55it/s]Extractor Predicting: 159it [01:46,  1.56it/s]Extractor Predicting: 160it [01:47,  1.51it/s]Extractor Predicting: 161it [01:47,  1.49it/s]Extractor Predicting: 162it [01:48,  1.51it/s]Extractor Predicting: 163it [01:49,  1.54it/s]Extractor Predicting: 164it [01:49,  1.56it/s]Extractor Predicting: 165it [01:50,  1.59it/s]Extractor Predicting: 166it [01:51,  1.59it/s]Extractor Predicting: 167it [01:51,  1.58it/s]Extractor Predicting: 168it [01:52,  1.55it/s]Extractor Predicting: 169it [01:53,  1.56it/s]Extractor Predicting: 170it [01:53,  1.58it/s]Extractor Predicting: 171it [01:54,  1.56it/s]Extractor Predicting: 172it [01:54,  1.54it/s]Extractor Predicting: 173it [01:55,  1.57it/s]Extractor Predicting: 174it [01:56,  1.53it/s]Extractor Predicting: 175it [01:56,  1.54it/s]Extractor Predicting: 176it [01:57,  1.54it/s]Extractor Predicting: 177it [01:58,  1.55it/s]Extractor Predicting: 178it [01:58,  1.54it/s]Extractor Predicting: 179it [01:59,  1.55it/s]Extractor Predicting: 180it [02:00,  1.54it/s]Extractor Predicting: 181it [02:00,  1.55it/s]Extractor Predicting: 182it [02:01,  1.53it/s]Extractor Predicting: 183it [02:02,  1.40it/s]Extractor Predicting: 184it [02:02,  1.45it/s]Extractor Predicting: 185it [02:03,  1.46it/s]Extractor Predicting: 186it [02:04,  1.45it/s]Extractor Predicting: 187it [02:04,  1.50it/s]Extractor Predicting: 188it [02:05,  1.49it/s]Extractor Predicting: 189it [02:06,  1.50it/s]Extractor Predicting: 190it [02:06,  1.53it/s]Extractor Predicting: 191it [02:07,  1.55it/s]Extractor Predicting: 192it [02:08,  1.56it/s]Extractor Predicting: 193it [02:08,  1.54it/s]Extractor Predicting: 194it [02:09,  1.55it/s]Extractor Predicting: 195it [02:10,  1.55it/s]Extractor Predicting: 196it [02:10,  1.52it/s]Extractor Predicting: 197it [02:11,  1.55it/s]Extractor Predicting: 198it [02:12,  1.53it/s]Extractor Predicting: 199it [02:12,  1.51it/s]Extractor Predicting: 200it [02:13,  1.57it/s]Extractor Predicting: 201it [02:13,  1.58it/s]Extractor Predicting: 202it [02:14,  1.59it/s]Extractor Predicting: 203it [02:15,  1.58it/s]Extractor Predicting: 204it [02:15,  1.58it/s]Extractor Predicting: 205it [02:16,  1.56it/s]Extractor Predicting: 206it [02:17,  1.59it/s]Extractor Predicting: 207it [02:17,  1.60it/s]Extractor Predicting: 208it [02:18,  1.58it/s]Extractor Predicting: 209it [02:19,  1.56it/s]Extractor Predicting: 210it [02:19,  1.57it/s]Extractor Predicting: 211it [02:20,  1.58it/s]Extractor Predicting: 212it [02:20,  1.58it/s]Extractor Predicting: 213it [02:21,  1.56it/s]Extractor Predicting: 214it [02:22,  1.58it/s]Extractor Predicting: 215it [02:22,  1.56it/s]Extractor Predicting: 216it [02:23,  1.56it/s]Extractor Predicting: 217it [02:24,  1.56it/s]Extractor Predicting: 218it [02:24,  1.57it/s]Extractor Predicting: 219it [02:25,  1.55it/s]Extractor Predicting: 220it [02:26,  1.55it/s]Extractor Predicting: 221it [02:26,  1.57it/s]Extractor Predicting: 222it [02:27,  1.55it/s]Extractor Predicting: 223it [02:27,  1.56it/s]Extractor Predicting: 224it [02:28,  1.58it/s]Extractor Predicting: 225it [02:29,  1.56it/s]Extractor Predicting: 226it [02:29,  1.56it/s]Extractor Predicting: 227it [02:30,  1.57it/s]Extractor Predicting: 228it [02:30,  1.71it/s]Extractor Predicting: 228it [02:30,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:32,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:32,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:32,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:32,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:32,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:39:33,138 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:39:33,140 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:39:33,716 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:39:34,745 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:39:34,745 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:37,620 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:37,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:37,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:37,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:39:37,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:39:38,283 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:39:38,284 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:39:38,871 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:39:39,019 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:39:39,019 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.07407407407407407,
  "recall": 0.02718695026387334,
  "score": 0.0397753860552176,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.48it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.51it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.46it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.44it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.45it/s]Extractor Predicting: 37it [00:25,  1.44it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.39it/s]Extractor Predicting: 40it [00:27,  1.42it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:28,  1.44it/s]Extractor Predicting: 43it [00:29,  1.44it/s]Extractor Predicting: 44it [00:29,  1.45it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.60it/s]Extractor Predicting: 48it [00:32,  1.61it/s]Extractor Predicting: 49it [00:32,  1.63it/s]Extractor Predicting: 50it [00:33,  1.58it/s]Extractor Predicting: 51it [00:34,  1.59it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.55it/s]Extractor Predicting: 61it [00:40,  1.58it/s]Extractor Predicting: 62it [00:41,  1.55it/s]Extractor Predicting: 63it [00:42,  1.55it/s]Extractor Predicting: 64it [00:42,  1.57it/s]Extractor Predicting: 65it [00:43,  1.58it/s]Extractor Predicting: 66it [00:44,  1.57it/s]Extractor Predicting: 67it [00:44,  1.57it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.58it/s]Extractor Predicting: 71it [00:47,  1.59it/s]Extractor Predicting: 72it [00:47,  1.59it/s]Extractor Predicting: 73it [00:48,  1.57it/s]Extractor Predicting: 74it [00:49,  1.57it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.57it/s]Extractor Predicting: 77it [00:51,  1.56it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.56it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:56,  1.56it/s]Extractor Predicting: 87it [00:57,  1.56it/s]Extractor Predicting: 88it [00:58,  1.57it/s]Extractor Predicting: 89it [00:58,  1.58it/s]Extractor Predicting: 90it [00:59,  1.59it/s]Extractor Predicting: 91it [01:00,  1.55it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:01,  1.55it/s]Extractor Predicting: 95it [01:02,  1.57it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:03,  1.56it/s]Extractor Predicting: 98it [01:04,  1.41it/s]Extractor Predicting: 99it [01:05,  1.43it/s]Extractor Predicting: 100it [01:06,  1.47it/s]Extractor Predicting: 101it [01:06,  1.50it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.50it/s]Extractor Predicting: 107it [01:10,  1.53it/s]Extractor Predicting: 108it [01:11,  1.53it/s]Extractor Predicting: 109it [01:11,  1.54it/s]Extractor Predicting: 110it [01:12,  1.53it/s]Extractor Predicting: 111it [01:13,  1.55it/s]Extractor Predicting: 112it [01:13,  1.56it/s]Extractor Predicting: 113it [01:14,  1.57it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:15,  1.53it/s]Extractor Predicting: 116it [01:16,  1.54it/s]Extractor Predicting: 117it [01:17,  1.57it/s]Extractor Predicting: 118it [01:17,  1.58it/s]Extractor Predicting: 119it [01:18,  1.61it/s]Extractor Predicting: 120it [01:18,  1.63it/s]Extractor Predicting: 121it [01:19,  1.62it/s]Extractor Predicting: 122it [01:20,  1.63it/s]Extractor Predicting: 123it [01:20,  1.63it/s]Extractor Predicting: 124it [01:21,  1.64it/s]Extractor Predicting: 125it [01:22,  1.58it/s]Extractor Predicting: 126it [01:22,  1.61it/s]Extractor Predicting: 127it [01:23,  1.57it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:24,  1.55it/s]Extractor Predicting: 130it [01:25,  1.59it/s]Extractor Predicting: 131it [01:25,  1.56it/s]Extractor Predicting: 132it [01:26,  1.60it/s]Extractor Predicting: 133it [01:27,  1.59it/s]Extractor Predicting: 134it [01:27,  1.58it/s]Extractor Predicting: 135it [01:28,  1.60it/s]Extractor Predicting: 136it [01:28,  1.63it/s]Extractor Predicting: 137it [01:29,  1.60it/s]Extractor Predicting: 138it [01:30,  1.57it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.60it/s]Extractor Predicting: 141it [01:32,  1.62it/s]Extractor Predicting: 142it [01:32,  1.65it/s]Extractor Predicting: 143it [01:33,  1.64it/s]Extractor Predicting: 144it [01:33,  1.65it/s]Extractor Predicting: 145it [01:34,  1.62it/s]Extractor Predicting: 146it [01:35,  1.61it/s]Extractor Predicting: 147it [01:35,  1.62it/s]Extractor Predicting: 148it [01:36,  1.62it/s]Extractor Predicting: 149it [01:37,  1.57it/s]Extractor Predicting: 150it [01:37,  1.51it/s]Extractor Predicting: 151it [01:38,  1.54it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:39,  1.58it/s]Extractor Predicting: 154it [01:40,  1.55it/s]Extractor Predicting: 155it [01:41,  1.50it/s]Extractor Predicting: 156it [01:41,  1.51it/s]Extractor Predicting: 157it [01:42,  1.50it/s]Extractor Predicting: 158it [01:42,  1.54it/s]Extractor Predicting: 159it [01:43,  1.55it/s]Extractor Predicting: 160it [01:44,  1.57it/s]Extractor Predicting: 161it [01:44,  1.59it/s]Extractor Predicting: 162it [01:45,  1.60it/s]Extractor Predicting: 163it [01:46,  1.58it/s]Extractor Predicting: 164it [01:46,  1.60it/s]Extractor Predicting: 165it [01:47,  1.58it/s]Extractor Predicting: 166it [01:47,  1.60it/s]Extractor Predicting: 167it [01:48,  1.57it/s]Extractor Predicting: 168it [01:49,  1.58it/s]Extractor Predicting: 169it [01:49,  1.59it/s]Extractor Predicting: 170it [01:50,  1.57it/s]Extractor Predicting: 171it [01:51,  1.55it/s]Extractor Predicting: 172it [01:51,  1.55it/s]Extractor Predicting: 173it [01:52,  1.53it/s]Extractor Predicting: 174it [01:53,  1.54it/s]Extractor Predicting: 175it [01:53,  1.54it/s]Extractor Predicting: 176it [01:54,  1.55it/s]Extractor Predicting: 177it [01:55,  1.53it/s]Extractor Predicting: 178it [01:55,  1.50it/s]Extractor Predicting: 179it [01:56,  1.53it/s]Extractor Predicting: 180it [01:57,  1.55it/s]Extractor Predicting: 181it [01:57,  1.58it/s]Extractor Predicting: 182it [01:58,  1.61it/s]Extractor Predicting: 183it [01:58,  1.59it/s]Extractor Predicting: 184it [01:59,  1.61it/s]Extractor Predicting: 185it [02:00,  1.61it/s]Extractor Predicting: 186it [02:00,  1.59it/s]Extractor Predicting: 187it [02:01,  1.61it/s]Extractor Predicting: 188it [02:01,  1.62it/s]Extractor Predicting: 189it [02:02,  1.57it/s]Extractor Predicting: 190it [02:03,  1.51it/s]Extractor Predicting: 191it [02:04,  1.52it/s]Extractor Predicting: 192it [02:04,  1.50it/s]Extractor Predicting: 193it [02:05,  1.38it/s]Extractor Predicting: 194it [02:06,  1.41it/s]Extractor Predicting: 195it [02:06,  1.41it/s]Extractor Predicting: 196it [02:07,  1.41it/s]Extractor Predicting: 197it [02:08,  1.43it/s]Extractor Predicting: 198it [02:09,  1.46it/s]Extractor Predicting: 199it [02:09,  1.46it/s]Extractor Predicting: 200it [02:10,  1.46it/s]Extractor Predicting: 201it [02:11,  1.46it/s]Extractor Predicting: 202it [02:11,  1.46it/s]Extractor Predicting: 203it [02:12,  1.48it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:13,  1.44it/s]Extractor Predicting: 206it [02:14,  1.45it/s]Extractor Predicting: 207it [02:15,  1.41it/s]Extractor Predicting: 208it [02:15,  1.42it/s]Extractor Predicting: 209it [02:16,  1.47it/s]Extractor Predicting: 210it [02:17,  1.50it/s]Extractor Predicting: 211it [02:17,  1.50it/s]Extractor Predicting: 212it [02:18,  1.52it/s]Extractor Predicting: 213it [02:19,  1.50it/s]Extractor Predicting: 214it [02:19,  1.48it/s]Extractor Predicting: 215it [02:20,  1.48it/s]Extractor Predicting: 216it [02:21,  1.47it/s]Extractor Predicting: 217it [02:21,  1.44it/s]Extractor Predicting: 218it [02:22,  1.48it/s]Extractor Predicting: 219it [02:23,  1.50it/s]Extractor Predicting: 220it [02:23,  1.49it/s]Extractor Predicting: 221it [02:24,  1.51it/s]Extractor Predicting: 222it [02:25,  1.47it/s]Extractor Predicting: 223it [02:26,  1.44it/s]Extractor Predicting: 224it [02:26,  1.42it/s]Extractor Predicting: 225it [02:27,  1.41it/s]Extractor Predicting: 226it [02:28,  1.42it/s]Extractor Predicting: 227it [02:28,  1.40it/s]Extractor Predicting: 228it [02:29,  1.40it/s]Extractor Predicting: 229it [02:30,  1.39it/s]Extractor Predicting: 230it [02:31,  1.41it/s]Extractor Predicting: 231it [02:31,  1.41it/s]Extractor Predicting: 232it [02:32,  1.41it/s]Extractor Predicting: 233it [02:33,  1.41it/s]Extractor Predicting: 234it [02:33,  1.43it/s]Extractor Predicting: 235it [02:34,  1.43it/s]Extractor Predicting: 236it [02:35,  1.43it/s]Extractor Predicting: 237it [02:36,  1.40it/s]Extractor Predicting: 238it [02:36,  1.40it/s]Extractor Predicting: 239it [02:37,  1.42it/s]Extractor Predicting: 240it [02:38,  1.45it/s]Extractor Predicting: 241it [02:38,  1.45it/s]Extractor Predicting: 242it [02:39,  1.46it/s]Extractor Predicting: 243it [02:40,  1.47it/s]Extractor Predicting: 244it [02:40,  1.45it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.52it/s]Extractor Predicting: 246it [02:42,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:28,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:28,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:28,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:28,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:28,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:42:29,526 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:42:29,527 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:42:30,102 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:42:31,108 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:42:31,108 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:33,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:33,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:33,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:33,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:42:33,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:42:34,596 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:42:34,597 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:42:35,179 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:42:35,334 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:42:35,334 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3558588548601864,
  "recall": 0.1811864406779661,
  "score": 0.24011680143755618,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.66it/s]Extractor Predicting: 15it [00:10,  1.49it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4840182648401826,
  "recall": 0.14887640449438203,
  "score": 0.22771213748657357,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
